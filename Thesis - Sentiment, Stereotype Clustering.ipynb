{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import time\n",
    "import dill\n",
    "from typing import List\n",
    "from functools import reduce\n",
    "from calendar import monthrange"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import csv\n",
    "import requests\n",
    "import xml.etree.ElementTree as ET\n",
    "from bs4 import BeautifulSoup\n",
    "import urllib.request as urllib2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from scipy import stats\n",
    "from random import sample, randint, shuffle\n",
    "from sklearn.decomposition import PCA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "import text_summarizer\n",
    "from gensim.utils import simple_preprocess\n",
    "from gensim.summarization.summarizer import summarize\n",
    "from gensim.models import Word2Vec, LsiModel\n",
    "from gensim.test.utils import common_dictionary, common_corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from time import process_time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('debiaswe-master/data/equalize_pairs.json') as f:\n",
    "    pairs = json.loads(f.read())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 1/1:\n",
      "import numpy as np\n",
      "import matplotlib.pyplot as plot\n",
      "import math\n",
      "import random\n",
      "import pickle\n",
      "import scipy.linalg as sp\n",
      " 1/2:\n",
      "def unpickle(file):\n",
      "    with open(file, 'rb') as fo:\n",
      "        dict = pickle.load(fo)\n",
      "    return dict\n",
      " 1/3:\n",
      "# load data\n",
      "data = unpickle('cifar-10-batches-py/data_batch_1')['data']\n",
      "data = np.asarray(data, dtype=float)/255\n",
      " 1/4:\n",
      "# standardize\n",
      "mean = np.mean(data, axis=0)\n",
      "sd = np.std(data, axis=0)\n",
      "centered = (data - mean)\n",
      " 1/5:\n",
      "# covariance matrix\n",
      "cov = np.cov(centered.T)\n",
      " 1/6:\n",
      "# eigendecomposition\n",
      "v, w = np.linalg.eigh(cov)\n",
      " 1/7: v\n",
      " 1/8:\n",
      "# sort eigenvalues in ascending order\n",
      "v = v[::-1]\n",
      " 1/9:\n",
      "# sort eigenvalues in ascending order\n",
      "v = v[::-1]\n",
      "# cumulative sum\n",
      "cumsum = np.cumsum(v)\n",
      "1/10: plot.show()\n",
      "1/11:\n",
      "# scree plot\n",
      "xs = np.array(range(len(cumsum)))+1\n",
      "plot.plot(xs, cumsum)\n",
      "plot.show()\n",
      "1/12:\n",
      "# sort eigenvalues in ascending order\n",
      "v = v[::-1]\n",
      "# cumulative sum\n",
      "cumsum = np.cumsum(v)\n",
      "1/13:\n",
      "# scree plot\n",
      "xs = np.array(range(len(cumsum)))+1\n",
      "plot.plot(xs, cumsum)\n",
      "plot.show()\n",
      "1/14:\n",
      "def std_eig(v):\n",
      "    v = (v - np.min(v))/(np.max(v) - np.min(v))\n",
      "    return v\n",
      "1/15:\n",
      "def plot_v(v, i):\n",
      "    img = std_eig(v)\n",
      "    plot.subplot(2, 8, i)\n",
      "    plot.xticks([])\n",
      "    plot.yticks([])\n",
      "    plot.imshow(np.transpose(img.reshape(3,32,32), (1, 2, 0)), interpolation='nearest')\n",
      "    plot.title(i)\n",
      "1/16:\n",
      "def plot_vs():\n",
      "    for i in range(3071, 3071-16, -1):\n",
      "        u = w[:,i]\n",
      "        plot_v(u, 3072-i)\n",
      "    plot.show()\n",
      "1/17: plot_vs()\n",
      "1/18:\n",
      "n = 3072\n",
      "u1 = w[:,n-1]\n",
      "u2 = w[:,n-2]\n",
      "u3 = w[:,n-3]\n",
      "u4 = w[:,n-4]\n",
      "u5 = w[:,n-5]\n",
      "u6 = w[:,n-6]\n",
      "1/19:\n",
      "def proj_v(pc1, pc2):\n",
      "    xs = []\n",
      "    ys = []\n",
      "    fig = plot.figure()\n",
      "    # plot.axis([-20, 20, -20, 20])\n",
      "    # ax = fig.add_subplot(1, 1, 1)\n",
      "    for i in range(100):\n",
      "        v = data[i]\n",
      "        xs.append(np.dot(v, pc1)*10)\n",
      "        ys.append(np.dot(v, pc2)*10)\n",
      "        plot.figimage(np.transpose(v.reshape(3,32,32), (1, 2, 0)), xo=xs[i], yo=ys[i])\n",
      "    plot.show()\n",
      "1/20: proj_v(u1, u2)\n",
      "1/21:\n",
      "def proj_v(pc1, pc2):\n",
      "    xs = []\n",
      "    ys = []\n",
      "    fig = plot.figure()\n",
      "    # plot.axis([-20, 20, -20, 20])\n",
      "    # ax = fig.add_subplot(1, 1, 1)\n",
      "    for i in range(100):\n",
      "        v = data[i]\n",
      "        xs.append(np.dot(v, pc1)*10)\n",
      "        ys.append(np.dot(v, pc2)*10)\n",
      "        plot.figimage(np.transpose(v.reshape(3,32,32), (1, 2, 0)), xo=xs[i], yo=ys[i])\n",
      "    plot.show()\n",
      "1/22: proj_v(u1, u2)\n",
      "1/23:\n",
      "def proj_v(pc1, pc2):\n",
      "    xs = []\n",
      "    ys = []\n",
      "    fig = plot.figure()\n",
      "    # plot.axis([-20, 20, -20, 20])\n",
      "    # ax = fig.add_subplot(1, 1, 1)\n",
      "    for i in range(100):\n",
      "        v = data[i]\n",
      "        xs.append(np.dot(v, pc1)*10)\n",
      "        ys.append(np.dot(v, pc2)*10)\n",
      "        fig.figimage(np.transpose(v.reshape(3,32,32), (1, 2, 0)), xo=xs[i], yo=ys[i])\n",
      "    plot.show()\n",
      "1/24:\n",
      "def proj_v(pc1, pc2):\n",
      "    xs = []\n",
      "    ys = []\n",
      "    fig = plot.figure()\n",
      "    # plot.axis([-20, 20, -20, 20])\n",
      "    # ax = fig.add_subplot(1, 1, 1)\n",
      "    for i in range(100):\n",
      "        v = data[i]\n",
      "        xs.append(np.dot(v, pc1)*10)\n",
      "        ys.append(np.dot(v, pc2)*10)\n",
      "        fig.figimage(np.transpose(v.reshape(3,32,32), (1, 2, 0)), xo=xs[i], yo=ys[i])\n",
      "    plot.show()\n",
      "1/25: proj_v(u1, u2)\n",
      "1/26:\n",
      "def proj_v(pc1, pc2):\n",
      "    xs = []\n",
      "    ys = []\n",
      "    fig = plot.figure()\n",
      "    # plot.axis([-20, 20, -20, 20])\n",
      "    # ax = fig.add_subplot(1, 1, 1)\n",
      "    for i in range(100):\n",
      "        v = data[i]\n",
      "        xs.append(np.dot(v, pc1)*10)\n",
      "        ys.append(np.dot(v, pc2)*10)\n",
      "        plot.figimage(np.transpose(v.reshape(3,32,32), (1, 2, 0)), xo=xs[i], yo=ys[i])\n",
      "    plot.show()\n",
      "1/27: proj_v(u1, u2)\n",
      "1/28:\n",
      "def proj_v(pc1, pc2):\n",
      "    xs = []\n",
      "    ys = []\n",
      "    fig = plot.figure()\n",
      "    plot.axis([-20, 20, -20, 20])\n",
      "    # ax = fig.add_subplot(1, 1, 1)\n",
      "    for i in range(100):\n",
      "        v = data[i]\n",
      "        xs.append(np.dot(v, pc1)*10)\n",
      "        ys.append(np.dot(v, pc2)*10)\n",
      "        plot.figimage(np.transpose(v.reshape(3,32,32), (1, 2, 0)), xo=xs[i], yo=ys[i])\n",
      "    plot.show()\n",
      "1/29: proj_v(u1, u2)\n",
      "1/30:\n",
      "def proj_v(pc1, pc2):\n",
      "    xs = []\n",
      "    ys = []\n",
      "    fig = plot.figure()\n",
      "    plot.axis([-20, 20, -20, 20])\n",
      "    # ax = fig.add_subplot(1, 1, 1)\n",
      "    for i in range(100):\n",
      "        v = data[i]\n",
      "        xs.append(np.dot(v, pc1)*10)\n",
      "        ys.append(np.dot(v, pc2)*10)\n",
      "        plot.figimage(np.transpose(v.reshape(3,32,32), (1, 2, 0)), xo=xs[i], yo=ys[i])\n",
      "    plot.show()\n",
      "    xs\n",
      "    ys\n",
      "1/31:\n",
      "def proj_v(pc1, pc2):\n",
      "    xs = []\n",
      "    ys = []\n",
      "    fig = plot.figure()\n",
      "    plot.axis([-20, 20, -20, 20])\n",
      "    # ax = fig.add_subplot(1, 1, 1)\n",
      "    for i in range(100):\n",
      "        v = data[i]\n",
      "        xs.append(np.dot(v, pc1)*10)\n",
      "        ys.append(np.dot(v, pc2)*10)\n",
      "        plot.figimage(np.transpose(v.reshape(3,32,32), (1, 2, 0)), xo=xs[i], yo=ys[i])\n",
      "    plot.show()\n",
      "    plot.plot(xs, ys, marker=\"o\")\n",
      "    plot.show()\n",
      "1/32: proj_v(u1, u2)\n",
      "1/33:\n",
      "def proj_v(pc1, pc2):\n",
      "    xs = []\n",
      "    ys = []\n",
      "    fig = plot.figure()\n",
      "    plot.axis([-20, 20, -20, 20])\n",
      "    # ax = fig.add_subplot(1, 1, 1)\n",
      "    for i in range(100):\n",
      "        v = data[i]\n",
      "        xs.append(np.dot(v, pc1))\n",
      "        ys.append(np.dot(v, pc2))\n",
      "        plot.figimage(np.transpose(v.reshape(3,32,32), (1, 2, 0)), xo=xs[i], yo=ys[i])\n",
      "    plot.show()\n",
      "    plot.plot(xs, ys, marker=\"o\")\n",
      "    plot.show()\n",
      "1/34: proj_v(u1, u2)\n",
      "1/35:\n",
      "def proj_v(pc1, pc2):\n",
      "    xs = []\n",
      "    ys = []\n",
      "    fig = plot.figure()\n",
      "    plot.axis([-20, 20, -20, 20])\n",
      "    # ax = fig.add_subplot(1, 1, 1)\n",
      "    for i in range(100):\n",
      "        v = data[i]\n",
      "        xs.append(np.dot(v, pc1)*10)\n",
      "        ys.append(np.dot(v, pc2)*10)\n",
      "        plot.figimage(np.transpose(v.reshape(3,32,32), (1, 2, 0)), xo=xs[i], yo=ys[i])\n",
      "    plot.show()\n",
      "    plot.plot(xs, ys, marker=\"o\")\n",
      "    plot.show()\n",
      "1/36: proj_v(u1, u2)\n",
      "1/37:\n",
      "def proj_v(pc1, pc2):\n",
      "    xs = []\n",
      "    ys = []\n",
      "    fig = plot.figure()\n",
      "    plot.axis([0, 450, -50, 200])\n",
      "    # ax = fig.add_subplot(1, 1, 1)\n",
      "    for i in range(100):\n",
      "        v = data[i]\n",
      "        xs.append(np.dot(v, pc1)*10)\n",
      "        ys.append(np.dot(v, pc2)*10)\n",
      "        plot.figimage(np.transpose(v.reshape(3,32,32), (1, 2, 0)), xo=xs[i], yo=ys[i])\n",
      "    plot.show()\n",
      "    plot.plot(xs, ys, marker=\"o\")\n",
      "    plot.show()\n",
      "1/38: proj_v(u1, u2)\n",
      "1/39:\n",
      "def proj_v(pc1, pc2):\n",
      "    xs = []\n",
      "    ys = []\n",
      "    fig = plot.figure()\n",
      "    plot.axis([0, 450, -50, 200])\n",
      "    # ax = fig.add_subplot(1, 1, 1)\n",
      "    for i in range(100):\n",
      "        v = data[i]\n",
      "        xs.append(np.dot(v, pc1)*10)\n",
      "        ys.append(np.dot(v, pc2)*10)\n",
      "        plot.figimage(np.transpose(v.reshape(3,32,32), (1, 2, 0)), xo=xs[i], yo=ys[i])\n",
      "    plot.show()\n",
      "    plot.plot(xs, ys, marker=\"o\", linestyle=\"\")\n",
      "    plot.show()\n",
      "1/40: proj_v(u1, u2)\n",
      "1/41:\n",
      "def proj_v(pc1, pc2):\n",
      "    xs = []\n",
      "    ys = []\n",
      "    fig = plot.figure()\n",
      "    plot.axis([0, 450, -50, 200])\n",
      "    # ax = fig.add_subplot(1, 1, 1)\n",
      "    for i in range(100):\n",
      "        v = data[i]\n",
      "        xs.append(np.dot(v, pc1)*10)\n",
      "        ys.append(np.dot(v, pc2)*10)\n",
      "        fig.figimage(np.transpose(v.reshape(3,32,32), (1, 2, 0)), xo=xs[i], yo=ys[i])\n",
      "    plot.show()\n",
      "    plot.plot(xs, ys, marker=\"o\", linestyle=\"\")\n",
      "    plot.show()\n",
      "1/42: proj_v(u1, u2)\n",
      "1/43:\n",
      "def proj_v(pc1, pc2):\n",
      "    xs = []\n",
      "    ys = []\n",
      "    fig = plot.figure()\n",
      "    plot.axis([0, 450, -50, 200])\n",
      "    # ax = fig.add_subplot(1, 1, 1)\n",
      "    for i in range(100):\n",
      "        v = data[i]\n",
      "        xs.append(np.dot(v, pc1)*10)\n",
      "        ys.append(np.dot(v, pc2)*10)\n",
      "        fig.figimage(np.transpose(v.reshape(3,32,32), (1, 2, 0)), xo=xs[i], yo=ys[i], zorder=10)\n",
      "    plot.show()\n",
      "    plot.plot(xs, ys, marker=\"o\", linestyle=\"\")\n",
      "    plot.show()\n",
      "1/44: proj_v(u1, u2)\n",
      "1/45:\n",
      "def proj_v(pc1, pc2):\n",
      "    xs = []\n",
      "    ys = []\n",
      "    fig = plot.figure()\n",
      "    plot.axis([0, 450, -50, 200])\n",
      "    # ax = fig.add_subplot(1, 1, 1)\n",
      "    for i in range(100):\n",
      "        v = data[i]\n",
      "        xs.append(np.dot(v, pc1)*9)\n",
      "        ys.append(np.dot(v, pc2)*9)\n",
      "        fig.figimage(np.transpose(v.reshape(3,32,32), (1, 2, 0)), xo=xs[i], yo=ys[i], zorder=10)\n",
      "    plot.show()\n",
      "    plot.plot(xs, ys, marker=\"o\", linestyle=\"\")\n",
      "    plot.show()\n",
      "1/46: proj_v(u1, u2)\n",
      "1/47:\n",
      "def proj_v(pc1, pc2):\n",
      "    xs = []\n",
      "    ys = []\n",
      "    fig = plot.figure()\n",
      "    plot.axis([0, 450, -50, 200])\n",
      "    # ax = fig.add_subplot(1, 1, 1)\n",
      "    for i in range(100):\n",
      "        v = data[i]\n",
      "        xs.append(np.dot(v, pc1))\n",
      "        ys.append(np.dot(v, pc2))\n",
      "    for i in range(100):\n",
      "        x = xs[i]*10+np.min(xs)\n",
      "        y = ys[i]*10+np.min(ys)\n",
      "        fig.figimage(np.transpose(data[i].reshape(3,32,32), (1, 2, 0)), xo=x, yo=y, zorder=10)\n",
      "    plot.show()\n",
      "    plot.plot(xs, ys, marker=\"o\", linestyle=\"\")\n",
      "    plot.show()\n",
      "1/48: proj_v(u1, u2)\n",
      "1/49:\n",
      "def proj_v(pc1, pc2):\n",
      "    xs = []\n",
      "    ys = []\n",
      "    fig = plot.figure()\n",
      "    plot.axis([0, 450, -50, 200])\n",
      "    # ax = fig.add_subplot(1, 1, 1)\n",
      "    for i in range(100):\n",
      "        v = data[i]\n",
      "        xs.append(np.dot(v, pc1))\n",
      "        ys.append(np.dot(v, pc2))\n",
      "    for i in range(100):\n",
      "        x = xs[i]*5+np.min(xs)\n",
      "        y = ys[i]*5+np.min(ys)\n",
      "        fig.figimage(np.transpose(data[i].reshape(3,32,32), (1, 2, 0)), xo=x, yo=y, zorder=10)\n",
      "    plot.show()\n",
      "    plot.plot(xs, ys, marker=\"o\", linestyle=\"\")\n",
      "    plot.show()\n",
      "1/50: proj_v(u1, u2)\n",
      "1/51:\n",
      "def proj_v(pc1, pc2):\n",
      "    xs = []\n",
      "    ys = []\n",
      "    fig = plot.figure()\n",
      "    plot.axis([0, 450, -50, 200])\n",
      "    # ax = fig.add_subplot(1, 1, 1)\n",
      "    for i in range(100):\n",
      "        v = data[i]\n",
      "        xs.append(np.dot(v, pc1))\n",
      "        ys.append(np.dot(v, pc2))\n",
      "    for i in range(100):\n",
      "        x = xs[i]*5-np.min(xs)\n",
      "        y = ys[i]*5-np.min(ys)\n",
      "        fig.figimage(np.transpose(data[i].reshape(3,32,32), (1, 2, 0)), xo=x, yo=y, zorder=10)\n",
      "    plot.show()\n",
      "    plot.plot(xs, ys, marker=\"o\", linestyle=\"\")\n",
      "    plot.show()\n",
      "1/52: proj_v(u1, u2)\n",
      "1/53:\n",
      "def proj_v(pc1, pc2):\n",
      "    xs = []\n",
      "    ys = []\n",
      "    fig = plot.figure()\n",
      "    plot.axis([0, 450, -50, 200])\n",
      "    # ax = fig.add_subplot(1, 1, 1)\n",
      "    for i in range(100):\n",
      "        v = data[i]\n",
      "        xs.append(np.dot(v, pc1))\n",
      "        ys.append(np.dot(v, pc2))\n",
      "    for i in range(100):\n",
      "        x = xs[i]*5-np.min(xs)*5\n",
      "        y = ys[i]*5-np.min(ys)*5\n",
      "        fig.figimage(np.transpose(data[i].reshape(3,32,32), (1, 2, 0)), xo=x, yo=y, zorder=10)\n",
      "    plot.show()\n",
      "    plot.plot(xs, ys, marker=\"o\", linestyle=\"\")\n",
      "    plot.show()\n",
      "1/54: proj_v(u1, u2)\n",
      "1/55:\n",
      "def proj_v(pc1, pc2):\n",
      "    xs = []\n",
      "    ys = []\n",
      "    fig = plot.figure()\n",
      "    plot.axis([0, 450, -50, 200])\n",
      "    # ax = fig.add_subplot(1, 1, 1)\n",
      "    for i in range(100):\n",
      "        v = data[i]\n",
      "        xs.append(np.dot(v, pc1))\n",
      "        ys.append(np.dot(v, pc2))\n",
      "    for i in range(100):\n",
      "        x = xs[i]*7-np.min(xs)*7\n",
      "        y = ys[i]*7-np.min(ys)*7\n",
      "        fig.figimage(np.transpose(data[i].reshape(3,32,32), (1, 2, 0)), xo=x, yo=y, zorder=10)\n",
      "    plot.show()\n",
      "    plot.plot(xs, ys, marker=\"o\", linestyle=\"\")\n",
      "    plot.show()\n",
      "1/56: proj_v(u1, u2)\n",
      "1/57:\n",
      "def proj_v(pc1, pc2):\n",
      "    xs = []\n",
      "    ys = []\n",
      "    fig = plot.figure()\n",
      "    plot.axis([0, 450, -50, 200])\n",
      "    # ax = fig.add_subplot(1, 1, 1)\n",
      "    for i in range(100):\n",
      "        v = data[i]\n",
      "        xs.append(np.dot(v, pc1))\n",
      "        ys.append(np.dot(v, pc2))\n",
      "    for i in range(100):\n",
      "        x = xs[i]*10-np.min(xs)*10\n",
      "        y = ys[i]*10-np.min(ys)*10\n",
      "        fig.figimage(np.transpose(data[i].reshape(3,32,32), (1, 2, 0)), xo=x, yo=y, zorder=10)\n",
      "    plot.show()\n",
      "    plot.plot(xs, ys, marker=\"o\", linestyle=\"\")\n",
      "    plot.show()\n",
      "1/58: proj_v(u1, u2)\n",
      "1/59: proj_v(u1, u2,9)\n",
      "1/60:\n",
      "def proj_v(pc1, pc2, scale):\n",
      "    xs = []\n",
      "    ys = []\n",
      "    fig = plot.figure()\n",
      "    plot.axis([0, 450, -50, 200])\n",
      "    # ax = fig.add_subplot(1, 1, 1)\n",
      "    for i in range(100):\n",
      "        v = data[i]\n",
      "        xs.append(np.dot(v, pc1))\n",
      "        ys.append(np.dot(v, pc2))\n",
      "    for i in range(100):\n",
      "        x = xs[i]*scale-np.min(xs)*scale\n",
      "        y = ys[i]*scale-np.min(ys)*scale\n",
      "        fig.figimage(np.transpose(data[i].reshape(3,32,32), (1, 2, 0)), xo=x, yo=y, zorder=10)\n",
      "    plot.show()\n",
      "#     plot.plot(xs, ys, marker=\"o\", linestyle=\"\")\n",
      "#     plot.show()\n",
      "1/61: proj_v(u1, u2, 9)\n",
      "1/62:\n",
      "def proj_v(pc1, pc2, scale):\n",
      "    xs = []\n",
      "    ys = []\n",
      "    fig = plot.figure()\n",
      "    plot.axis([0, 450, -50, 200])\n",
      "    # ax = fig.add_subplot(1, 1, 1)\n",
      "    for i in range(100):\n",
      "        v = data[i]\n",
      "        xs.append(np.dot(v, pc1))\n",
      "        ys.append(np.dot(v, pc2))\n",
      "    for i in range(100):\n",
      "        x = xs[i]*scale-np.min(xs)*scale+scale\n",
      "        y = ys[i]*scale-np.min(ys)*scale+scale\n",
      "        fig.figimage(np.transpose(data[i].reshape(3,32,32), (1, 2, 0)), xo=x, yo=y, zorder=10)\n",
      "    plot.show()\n",
      "#     plot.plot(xs, ys, marker=\"o\", linestyle=\"\")\n",
      "#     plot.show()\n",
      "1/63: proj_v(u1, u2, 9)\n",
      "1/64: proj_v(u1, u2, 8)\n",
      "1/65:\n",
      "def proj_v(pc1, pc2, scale):\n",
      "    xs = []\n",
      "    ys = []\n",
      "    fig = plot.figure()\n",
      "    plot.axis([0, 450, -50, 200])\n",
      "    # ax = fig.add_subplot(1, 1, 1)\n",
      "    for i in range(100):\n",
      "        v = data[i]\n",
      "        xs.append(np.dot(v, pc1))\n",
      "        ys.append(np.dot(v, pc2))\n",
      "    for i in range(100):\n",
      "        x = xs[i]*scale-np.min(xs)*scale+scale*10\n",
      "        y = ys[i]*scale-np.min(ys)*scale+scale*10\n",
      "        fig.figimage(np.transpose(data[i].reshape(3,32,32), (1, 2, 0)), xo=x, yo=y, zorder=10)\n",
      "    plot.show()\n",
      "#     plot.plot(xs, ys, marker=\"o\", linestyle=\"\")\n",
      "#     plot.show()\n",
      "1/66: proj_v(u1, u2, 8)\n",
      "1/67:\n",
      "def proj_v(pc1, pc2, scale):\n",
      "    xs = []\n",
      "    ys = []\n",
      "    fig = plot.figure()\n",
      "    plot.axis([0, 450, -50, 200])\n",
      "    # ax = fig.add_subplot(1, 1, 1)\n",
      "    for i in range(100):\n",
      "        v = data[i]\n",
      "        xs.append(np.dot(v, pc1))\n",
      "        ys.append(np.dot(v, pc2))\n",
      "    for i in range(100):\n",
      "        x = xs[i]*scale-np.min(xs)*scale+scale*5\n",
      "        y = ys[i]*scale-np.min(ys)*scale+scale*5\n",
      "        fig.figimage(np.transpose(data[i].reshape(3,32,32), (1, 2, 0)), xo=x, yo=y, zorder=10)\n",
      "    plot.show()\n",
      "#     plot.plot(xs, ys, marker=\"o\", linestyle=\"\")\n",
      "#     plot.show()\n",
      "1/68: proj_v(u1, u2, 8)\n",
      "1/69:\n",
      "def proj_v(pc1, pc2, scale):\n",
      "    xs = []\n",
      "    ys = []\n",
      "    fig = plot.figure()\n",
      "    plot.axis([0, 450, -50, 200])\n",
      "    # ax = fig.add_subplot(1, 1, 1)\n",
      "    for i in range(100):\n",
      "        v = data[i]\n",
      "        xs.append(np.dot(v, pc1))\n",
      "        ys.append(np.dot(v, pc2))\n",
      "    for i in range(100):\n",
      "        x = xs[i]*scale-np.min(xs)*scale+scale*3\n",
      "        y = ys[i]*scale-np.min(ys)*scale+scale*3\n",
      "        fig.figimage(np.transpose(data[i].reshape(3,32,32), (1, 2, 0)), xo=x, yo=y, zorder=10)\n",
      "    plot.show()\n",
      "#     plot.plot(xs, ys, marker=\"o\", linestyle=\"\")\n",
      "#     plot.show()\n",
      "1/70: proj_v(u1, u2, 8)\n",
      "1/71:\n",
      "def proj_v(pc1, pc2, scale):\n",
      "    xs = []\n",
      "    ys = []\n",
      "    fig = plot.figure()\n",
      "    plot.axis([0, 450, -50, 200])\n",
      "    # ax = fig.add_subplot(1, 1, 1)\n",
      "    for i in range(100):\n",
      "        v = data[i]\n",
      "        xs.append(np.dot(v, pc1))\n",
      "        ys.append(np.dot(v, pc2))\n",
      "    for i in range(100):\n",
      "        x = xs[i]*scale-np.min(xs)*scale+scale*4\n",
      "        y = ys[i]*scale-np.min(ys)*scale+scale*4\n",
      "        \n",
      "        fig.figimage(np.transpose(data[i].reshape(3,32,32), (1, 2, 0)), xo=x, yo=y, zorder=10)\n",
      "    plot.show()\n",
      "#     plot.plot(xs, ys, marker=\"o\", linestyle=\"\")\n",
      "#     plot.show()\n",
      "1/72: proj_v(u1, u2, 8)\n",
      "1/73: proj_v(u1, u2, 9)\n",
      "1/74: proj_v(u3, u4, 9)\n",
      "1/75:\n",
      "def proj_v(pc1, pc2, scale):\n",
      "    xs = []\n",
      "    ys = []\n",
      "    fig = plot.figure()\n",
      "    plot.axis([0, 450, -50, 200])\n",
      "    # ax = fig.add_subplot(1, 1, 1)\n",
      "    for i in range(100):\n",
      "        v = data[i]\n",
      "        xs.append(np.dot(v, pc1))\n",
      "        ys.append(np.dot(v, pc2))\n",
      "    for i in range(100):\n",
      "        x = xs[i]*scale-np.min(xs)*scale+scale*4\n",
      "        y = ys[i]*scale-np.min(ys)*scale+scale*4\n",
      "        \n",
      "        fig.figimage(np.transpose(data[i].reshape(3,32,32), (1, 2, 0)), xo=x, yo=y, zorder=10)\n",
      "    plot.show()\n",
      "    plot.plot(xs, ys, marker=\"o\", linestyle=\"\")\n",
      "    plot.show()\n",
      "1/76: proj_v(u1, u2, 9)\n",
      "1/77: proj_v(u3, u4, 9)\n",
      "1/78: proj_v(u5, u6, 9)\n",
      "1/79: proj_v(u1, u2, 9, [0, 45, -5, 20])\n",
      "1/80:\n",
      "def proj_v(pc1, pc2, scale, axes):\n",
      "    xs = []\n",
      "    ys = []\n",
      "    fig = plot.figure()\n",
      "    plot.axis(axes)\n",
      "    # ax = fig.add_subplot(1, 1, 1)\n",
      "    for i in range(100):\n",
      "        v = data[i]\n",
      "        xs.append(np.dot(v, pc1))\n",
      "        ys.append(np.dot(v, pc2))\n",
      "    for i in range(100):\n",
      "        x = xs[i]*scale-np.min(xs)*scale+scale*4\n",
      "        y = ys[i]*scale-np.min(ys)*scale+scale*4\n",
      "        \n",
      "        fig.figimage(np.transpose(data[i].reshape(3,32,32), (1, 2, 0)), xo=x, yo=y, zorder=10)\n",
      "    plot.show()\n",
      "    plot.plot(xs, ys, marker=\"o\", linestyle=\"\")\n",
      "    plot.show()\n",
      "1/81: proj_v(u1, u2, 9, [0, 45, -5, 20])\n",
      "1/82:\n",
      "def proj_v(pc1, pc2, scale, axes):\n",
      "    xs = []\n",
      "    ys = []\n",
      "    fig = plot.figure()\n",
      "    plot.axis(axes)\n",
      "    # ax = fig.add_subplot(1, 1, 1)\n",
      "    for i in range(100):\n",
      "        v = data[i]\n",
      "        xs.append(np.dot(v, pc1))\n",
      "        ys.append(np.dot(v, pc2))\n",
      "    for i in range(100):\n",
      "        x = xs[i]*scale-np.min(xs)*scale+scale*3\n",
      "        y = ys[i]*scale-np.min(ys)*scale+scale*3\n",
      "        \n",
      "        fig.figimage(np.transpose(data[i].reshape(3,32,32), (1, 2, 0)), xo=x, yo=y, zorder=10)\n",
      "    plot.show()\n",
      "    plot.plot(xs, ys, marker=\"o\", linestyle=\"\")\n",
      "    plot.show()\n",
      "1/83: proj_v(u1, u2, 9, [0, 45, -5, 20])\n",
      "1/84:\n",
      "def proj_v(pc1, pc2, scale, axes):\n",
      "    xs = []\n",
      "    ys = []\n",
      "    fig = plot.figure()\n",
      "    plot.axis(axes)\n",
      "    for i in range(100):\n",
      "        v = data[i]\n",
      "        xs.append(np.dot(v, pc1))\n",
      "        ys.append(np.dot(v, pc2))\n",
      "    for i in range(100):\n",
      "        x = xs[i]*scale-np.min(xs)*scale+scale*3\n",
      "        y = ys[i]*scale-np.min(ys)*scale+scale*3\n",
      "        \n",
      "        fig.figimage(np.transpose(data[i].reshape(3,32,32), (1, 2, 0)), xo=x, yo=y, zorder=10)\n",
      "    plot.show()\n",
      "    plot.plot(xs, ys, marker=\"o\", linestyle=\"\")\n",
      "    plot.show()\n",
      "1/85: proj_v(u1, u2, 9, [0, 45, -5, 20])\n",
      "1/86: proj_v(u3, u4, 9, [-15, 2.5, -6, 6])\n",
      "1/87: proj_v(u3, u4, 10, [-15, 2.5, -6, 6])\n",
      "1/88: proj_v(u3, u4, 15, [-15, 2.5, -6, 6])\n",
      "1/89: proj_v(u5, u6, 12, [-15, 2.5, -8, 6])\n",
      "1/90: proj_v(u5, u6, 15, [-15, 2.5, -8, 6])\n",
      "1/91: proj_v(u5, u6, 14, [-15, 2.5, -8, 6])\n",
      "1/92:\n",
      "def proj_v(pc1, pc2, scale, axes):\n",
      "    xs = []\n",
      "    ys = []\n",
      "    fig = plot.figure()\n",
      "    fig.tight_layout()\n",
      "    plot.axis(axes)\n",
      "    for i in range(100):\n",
      "        v = data[i]\n",
      "        xs.append(np.dot(v, pc1))\n",
      "        ys.append(np.dot(v, pc2))\n",
      "    for i in range(100):\n",
      "        x = xs[i]*scale-np.min(xs)*scale+scale*3\n",
      "        y = ys[i]*scale-np.min(ys)*scale+scale*3\n",
      "        fig.figimage(np.transpose(data[i].reshape(3,32,32), (1, 2, 0)), xo=x, yo=y, zorder=10)\n",
      "    plot.show()\n",
      "    plot.plot(xs, ys, marker=\"o\", linestyle=\"\")\n",
      "    plot.show()\n",
      "1/93: proj_v(u1, u2, 9, [0, 45, -5, 20])\n",
      "1/94: proj_v(u3, u4, 15, [-15, 2.5, -6, 6])\n",
      "1/95: proj_v(u3, u4, 16, [-15, 2.5, -6, 6])\n",
      "1/96:\n",
      "def proj_v(pc1, pc2, scale, axes, labx, laby):\n",
      "    xs = []\n",
      "    ys = []\n",
      "    fig = plot.figure()\n",
      "    fig.tight_layout()\n",
      "    plot.axis(axes)\n",
      "    for i in range(100):\n",
      "        v = data[i]\n",
      "        xs.append(np.dot(v, pc1))\n",
      "        ys.append(np.dot(v, pc2))\n",
      "    for i in range(100):\n",
      "        x = xs[i]*scale-np.min(xs)*scale+scale*3\n",
      "        y = ys[i]*scale-np.min(ys)*scale+scale*3\n",
      "        fig.figimage(np.transpose(data[i].reshape(3,32,32), (1, 2, 0)), xo=x, yo=y, zorder=10)\n",
      "    plot.ylabel(laby)\n",
      "    plot.xlabel(labx)\n",
      "    plot.show()\n",
      "1/97: proj_v(u1, u2, 9, [0, 45, -5, 20], \"PC 1\", \"PC 2\")\n",
      "1/98: proj_v(u1, u2, 8, [0, 45, -5, 20], \"PC 1\", \"PC 2\")\n",
      "1/99:\n",
      "def proj_v(pc1, pc2, scale, axes, labx, laby):\n",
      "    xs = []\n",
      "    ys = []\n",
      "    fig = plot.figure()\n",
      "    fig.tight_layout()\n",
      "    plot.axis(axes)\n",
      "    for i in range(100):\n",
      "        v = data[i]\n",
      "        xs.append(np.dot(v, pc1))\n",
      "        ys.append(np.dot(v, pc2))\n",
      "    for i in range(100):\n",
      "        x = xs[i]*scale-np.min(xs)*scale+scale*4\n",
      "        y = ys[i]*scale-np.min(ys)*scale+scale*4\n",
      "        fig.figimage(np.transpose(data[i].reshape(3,32,32), (1, 2, 0)), xo=x, yo=y, zorder=10)\n",
      "    plot.ylabel(laby)\n",
      "    plot.xlabel(labx)\n",
      "    plot.show()\n",
      "1/100:\n",
      "def proj_v(pc1, pc2, scale, axes, labx, laby):\n",
      "    xs = []\n",
      "    ys = []\n",
      "    fig = plot.figure()\n",
      "    fig.tight_layout()\n",
      "    plot.axis(axes)\n",
      "    for i in range(100):\n",
      "        v = data[i]\n",
      "        xs.append(np.dot(v, pc1))\n",
      "        ys.append(np.dot(v, pc2))\n",
      "    for i in range(100):\n",
      "        x = xs[i]*scale-np.min(xs)*scale+scale*5\n",
      "        y = ys[i]*scale-np.min(ys)*scale+scale*5\n",
      "        fig.figimage(np.transpose(data[i].reshape(3,32,32), (1, 2, 0)), xo=x, yo=y, zorder=10)\n",
      "    plot.ylabel(laby)\n",
      "    plot.xlabel(labx)\n",
      "    plot.show()\n",
      "1/101: proj_v(u1, u2, 8, [0, 45, -5, 20], \"PC 1\", \"PC 2\")\n",
      "1/102:\n",
      "def proj_v(pc1, pc2, scale, axes, labx, laby):\n",
      "    xs = []\n",
      "    ys = []\n",
      "    fig = plot.figure()\n",
      "    fig.tight_layout()\n",
      "    plot.axis(axes)\n",
      "    for i in range(100):\n",
      "        v = data[i]\n",
      "        xs.append(np.dot(v, pc1))\n",
      "        ys.append(np.dot(v, pc2))\n",
      "    for i in range(100):\n",
      "        x = xs[i]*scale-np.min(xs)*scale+scale*6\n",
      "        y = ys[i]*scale-np.min(ys)*scale+scale*6\n",
      "        fig.figimage(np.transpose(data[i].reshape(3,32,32), (1, 2, 0)), xo=x, yo=y, zorder=10)\n",
      "    plot.ylabel(laby)\n",
      "    plot.xlabel(labx)\n",
      "    plot.show()\n",
      "1/103: proj_v(u1, u2, 8, [0, 45, -5, 20], \"PC 1\", \"PC 2\")\n",
      "1/104: proj_v(u3, u4, 16, [-15, 2.5, -6, 6], \"PC 3\", \"PC 4\")\n",
      "1/105: proj_v(u3, u4, 10, [-15, 2.5, -6, 6], \"PC 3\", \"PC 4\")\n",
      "1/106: proj_v(u3, u4, 14, [-15, 2.5, -6, 6], \"PC 3\", \"PC 4\")\n",
      "1/107: proj_v(u3, u4, 13, [-15, 2.5, -6, 6], \"PC 3\", \"PC 4\")\n",
      "1/108: proj_v(u5, u6, 14, [-15, 2.5, -8, 6])\n",
      "1/109: proj_v(u5, u6, 14, [-15, 2.5, -8, 6], \"PC 5\", \"PC 6\")\n",
      "1/110: proj_v(u5, u6, 12, [-15, 2.5, -8, 6], \"PC 5\", \"PC 6\")\n",
      "1/111:\n",
      "def proj_v(pc1, pc2, scale, axes, labx, laby):\n",
      "    xs = []\n",
      "    ys = []\n",
      "    fig = plot.figure()\n",
      "    fig.tight_layout()\n",
      "    plot.axis(axes)\n",
      "    for i in range(100):\n",
      "        v = data[i]\n",
      "        xs.append(np.dot(v, pc1))\n",
      "        ys.append(np.dot(v, pc2))\n",
      "    for i in range(100):\n",
      "        x = xs[i]*scale-np.min(xs)*scale+50\n",
      "        y = ys[i]*scale-np.min(ys)*scale+50\n",
      "        fig.figimage(np.transpose(data[i].reshape(3,32,32), (1, 2, 0)), xo=x, yo=y, zorder=10)\n",
      "    plot.ylabel(laby)\n",
      "    plot.xlabel(labx)\n",
      "    plot.show()\n",
      "1/112: proj_v(u1, u2, 8, [0, 45, -5, 20], \"PC 1\", \"PC 2\")\n",
      "1/113: proj_v(u3, u4, 13, [-15, 2.5, -6, 6], \"PC 3\", \"PC 4\")\n",
      "1/114: proj_v(u3, u4, 14, [-15, 2.5, -6, 6], \"PC 3\", \"PC 4\")\n",
      "1/115: proj_v(u3, u4, 15, [-15, 2.5, -6, 6], \"PC 3\", \"PC 4\")\n",
      "1/116: proj_v(u5, u6, 12, [-15, 2.5, -8, 6], \"PC 5\", \"PC 6\")\n",
      "1/117: proj_v(u5, u6, 12, [-15, 2.5, -8, 6], \"PC 5\", \"PC 6\")\n",
      "1/118: proj_v(u5, u6, 14, [-15, 2.5, -8, 6], \"PC 5\", \"PC 6\")\n",
      "1/119:\n",
      "def proj_v(pc1, pc2, scale, axes, labx, laby):\n",
      "    xs = []\n",
      "    ys = []\n",
      "    fig = plot.figure()\n",
      "    fig.tight_layout()\n",
      "    plot.axis(axes)\n",
      "    for i in range(1000):\n",
      "        v = data[i]\n",
      "        xs.append(np.dot(v, pc1))\n",
      "        ys.append(np.dot(v, pc2))\n",
      "    for i in range(1000):\n",
      "        x = xs[i]*scale-np.min(xs)*scale+50\n",
      "        y = ys[i]*scale-np.min(ys)*scale+50\n",
      "        fig.figimage(np.transpose(data[i].reshape(3,32,32), (1, 2, 0)), xo=x, yo=y, zorder=10)\n",
      "    plot.ylabel(laby)\n",
      "    plot.xlabel(labx)\n",
      "    plot.show()\n",
      "    plot.plot(xs, ys, marker=\"o\", linestyle=\"\")\n",
      "    plot.show()\n",
      "1/120: proj_v(u1, u2, 8, [0, 45, -5, 20], \"PC 1\", \"PC 2\")\n",
      "1/121: proj_v(u3, u4, 15, [-15, 2.5, -6, 6], \"PC 3\", \"PC 4\")\n",
      "1/122: proj_v(u5, u6, 14, [-15, 2.5, -8, 6], \"PC 5\", \"PC 6\")\n",
      "1/123: proj_v(u3, u4, 10, [-15, 2.5, -6, 6], \"PC 3\", \"PC 4\")\n",
      "1/124: proj_v(u5, u6, 10, [-15, 2.5, -8, 6], \"PC 5\", \"PC 6\")\n",
      "1/125:\n",
      "def proj_v(pc1, pc2, scale, axes, labx, laby):\n",
      "    xs = []\n",
      "    ys = []\n",
      "    fig = plot.figure(figsize=(6,6))\n",
      "    fig.tight_layout()\n",
      "    plot.axis(axes)\n",
      "    for i in range(1000):\n",
      "        v = data[i]\n",
      "        xs.append(np.dot(v, pc1))\n",
      "        ys.append(np.dot(v, pc2))\n",
      "    for i in range(1000):\n",
      "        x = xs[i]*scale-np.min(xs)*scale+50\n",
      "        y = ys[i]*scale-np.min(ys)*scale+50\n",
      "        fig.figimage(np.transpose(data[i].reshape(3,32,32), (1, 2, 0)), xo=x, yo=y, zorder=10)\n",
      "    plot.ylabel(laby)\n",
      "    plot.xlabel(labx)\n",
      "    plot.show()\n",
      "    plot.plot(xs, ys, marker=\"o\", linestyle=\"\")\n",
      "    plot.show()\n",
      "1/126: proj_v(u1, u2, 8, [0, 45, -5, 20], \"PC 1\", \"PC 2\")\n",
      "1/127:\n",
      "def proj_v(pc1, pc2, scale, axes, labx, laby):\n",
      "    xs = []\n",
      "    ys = []\n",
      "    fig = plot.figure(figsize=(5,7))\n",
      "    fig.tight_layout()\n",
      "    plot.axis(axes)\n",
      "    for i in range(1000):\n",
      "        v = data[i]\n",
      "        xs.append(np.dot(v, pc1))\n",
      "        ys.append(np.dot(v, pc2))\n",
      "    for i in range(1000):\n",
      "        x = xs[i]*scale-np.min(xs)*scale+50\n",
      "        y = ys[i]*scale-np.min(ys)*scale+50\n",
      "        fig.figimage(np.transpose(data[i].reshape(3,32,32), (1, 2, 0)), xo=x, yo=y, zorder=10)\n",
      "    plot.ylabel(laby)\n",
      "    plot.xlabel(labx)\n",
      "    plot.show()\n",
      "    plot.plot(xs, ys, marker=\"o\", linestyle=\"\")\n",
      "    plot.show()\n",
      "1/128: proj_v(u1, u2, 8, [0, 45, -5, 20], \"PC 1\", \"PC 2\")\n",
      "1/129:\n",
      "def proj_v(pc1, pc2, scale, axes, labx, laby):\n",
      "    xs = []\n",
      "    ys = []\n",
      "    fig = plot.figure(figsize=(7,5))\n",
      "    fig.tight_layout()\n",
      "    plot.axis(axes)\n",
      "    for i in range(1000):\n",
      "        v = data[i]\n",
      "        xs.append(np.dot(v, pc1))\n",
      "        ys.append(np.dot(v, pc2))\n",
      "    for i in range(1000):\n",
      "        x = xs[i]*scale-np.min(xs)*scale+50\n",
      "        y = ys[i]*scale-np.min(ys)*scale+50\n",
      "        fig.figimage(np.transpose(data[i].reshape(3,32,32), (1, 2, 0)), xo=x, yo=y, zorder=10)\n",
      "    plot.ylabel(laby)\n",
      "    plot.xlabel(labx)\n",
      "    plot.show()\n",
      "    plot.plot(xs, ys, marker=\"o\", linestyle=\"\")\n",
      "    plot.show()\n",
      "1/130: proj_v(u1, u2, 8, [0, 45, -5, 20], \"PC 1\", \"PC 2\")\n",
      "1/131: proj_v(u1, u2, 8, [0, 50, -5, 25], \"PC 1\", \"PC 2\")\n",
      "1/132: proj_v(u3, u4, 10, [-20, 5, -15, 10], \"PC 3\", \"PC 4\")\n",
      "1/133:\n",
      "def proj_v(pc1, pc2, scalex, scaley axes, labx, laby):\n",
      "    xs = []\n",
      "    ys = []\n",
      "    fig = plot.figure(figsize=(7,5))\n",
      "    fig.tight_layout()\n",
      "    plot.axis(axes)\n",
      "    for i in range(1000):\n",
      "        v = data[i]\n",
      "        xs.append(np.dot(v, pc1))\n",
      "        ys.append(np.dot(v, pc2))\n",
      "    for i in range(1000):\n",
      "        x = xs[i]*scalex-np.min(xs)*scalex+50\n",
      "        y = ys[i]*scaley-np.min(ys)*scaley+50\n",
      "        fig.figimage(np.transpose(data[i].reshape(3,32,32), (1, 2, 0)), xo=x, yo=y, zorder=10)\n",
      "    plot.ylabel(laby)\n",
      "    plot.xlabel(labx)\n",
      "    plot.show()\n",
      "    plot.plot(xs, ys, marker=\"o\", linestyle=\"\")\n",
      "    plot.show()\n",
      "1/134: proj_v(u3, u4, 20,15, [-20, 5, -15, 10], \"PC 3\", \"PC 4\")\n",
      "1/135:\n",
      "def proj_v(pc1, pc2, scalex, scaley, axes, labx, laby):\n",
      "    xs = []\n",
      "    ys = []\n",
      "    fig = plot.figure(figsize=(7,5))\n",
      "    fig.tight_layout()\n",
      "    plot.axis(axes)\n",
      "    for i in range(1000):\n",
      "        v = data[i]\n",
      "        xs.append(np.dot(v, pc1))\n",
      "        ys.append(np.dot(v, pc2))\n",
      "    for i in range(1000):\n",
      "        x = xs[i]*scalex-np.min(xs)*scalex+50\n",
      "        y = ys[i]*scaley-np.min(ys)*scaley+50\n",
      "        fig.figimage(np.transpose(data[i].reshape(3,32,32), (1, 2, 0)), xo=x, yo=y, zorder=10)\n",
      "    plot.ylabel(laby)\n",
      "    plot.xlabel(labx)\n",
      "    plot.show()\n",
      "    plot.plot(xs, ys, marker=\"o\", linestyle=\"\")\n",
      "    plot.show()\n",
      "1/136: proj_v(u1, u2, 8, 8, [0, 50, -5, 25], \"PC 1\", \"PC 2\")\n",
      "1/137: proj_v(u3, u4, 20, 15, [-20, 5, -15, 10], \"PC 3\", \"PC 4\")\n",
      "1/138: proj_v(u3, u4, 15, 10, [-20, 5, -15, 10], \"PC 3\", \"PC 4\")\n",
      "1/139: proj_v(u5, u6, 15, 10, [-15, 2.5, -8, 6], \"PC 5\", \"PC 6\")\n",
      "1/140: proj_v(u5, u6, 15, 12, [-15, 2.5, -8, 6], \"PC 5\", \"PC 6\")\n",
      "1/141: proj_v(u5, u6, 15, 12, [-15, 10, -10, 10], \"PC 5\", \"PC 6\")\n",
      "1/142: proj_v(u5, u6, 16, 12, [-15, 10, -10, 10], \"PC 5\", \"PC 6\")\n",
      "1/143:\n",
      "def proj_v(pc1, pc2, scalex, scaley, axes, labx, laby):\n",
      "    xs = []\n",
      "    ys = []\n",
      "    fig = plot.figure(figsize=(10,6))\n",
      "    fig.tight_layout()\n",
      "    plot.axis(axes)\n",
      "    for i in range(1000):\n",
      "        v = data[i]\n",
      "        xs.append(np.dot(v, pc1))\n",
      "        ys.append(np.dot(v, pc2))\n",
      "    for i in range(1000):\n",
      "        x = xs[i]*scalex-np.min(xs)*scalex+50\n",
      "        y = ys[i]*scaley-np.min(ys)*scaley+50\n",
      "        fig.figimage(np.transpose(data[i].reshape(3,32,32), (1, 2, 0)), xo=x, yo=y, zorder=10)\n",
      "    plot.ylabel(laby)\n",
      "    plot.xlabel(labx)\n",
      "    plot.show()\n",
      "    plot.plot(xs, ys, marker=\"o\", linestyle=\"\")\n",
      "    plot.show()\n",
      "1/144: proj_v(u1, u2, 8, 8, [0, 50, -5, 25], \"PC 1\", \"PC 2\")\n",
      "1/145: proj_v(u1, u2, 15, 15, [0, 50, -5, 25], \"PC 1\", \"PC 2\")\n",
      "1/146:\n",
      "def proj_v(pc1, pc2, scalex, scaley, axes, labx, laby):\n",
      "    xs = []\n",
      "    ys = []\n",
      "    fig = plot.figure(figsize=(12,10))\n",
      "    fig.tight_layout()\n",
      "    plot.axis(axes)\n",
      "    for i in range(1000):\n",
      "        v = data[i]\n",
      "        xs.append(np.dot(v, pc1))\n",
      "        ys.append(np.dot(v, pc2))\n",
      "    for i in range(1000):\n",
      "        x = xs[i]*scalex-np.min(xs)*scalex+50\n",
      "        y = ys[i]*scaley-np.min(ys)*scaley+50\n",
      "        fig.figimage(np.transpose(data[i].reshape(3,32,32), (1, 2, 0)), xo=x, yo=y, zorder=10)\n",
      "    plot.ylabel(laby)\n",
      "    plot.xlabel(labx)\n",
      "    plot.show()\n",
      "    plot.plot(xs, ys, marker=\"o\", linestyle=\"\")\n",
      "    plot.show()\n",
      "1/147: proj_v(u1, u2, 15, 15, [0, 50, -5, 25], \"PC 1\", \"PC 2\")\n",
      "1/148:\n",
      "def proj_v(pc1, pc2, scalex, scaley, axes, labx, laby):\n",
      "    xs = []\n",
      "    ys = []\n",
      "    fig = plot.figure(figsize=(12,8))\n",
      "    fig.tight_layout()\n",
      "    plot.axis(axes)\n",
      "    for i in range(1000):\n",
      "        v = data[i]\n",
      "        xs.append(np.dot(v, pc1))\n",
      "        ys.append(np.dot(v, pc2))\n",
      "    for i in range(1000):\n",
      "        x = xs[i]*scalex-np.min(xs)*scalex+50\n",
      "        y = ys[i]*scaley-np.min(ys)*scaley+50\n",
      "        fig.figimage(np.transpose(data[i].reshape(3,32,32), (1, 2, 0)), xo=x, yo=y, zorder=10)\n",
      "    plot.ylabel(laby)\n",
      "    plot.xlabel(labx)\n",
      "    plot.show()\n",
      "    plot.plot(xs, ys, marker=\"o\", linestyle=\"\")\n",
      "    plot.show()\n",
      "1/149: proj_v(u1, u2, 15, 15, [0, 50, -5, 25], \"PC 1\", \"PC 2\")\n",
      "1/150:\n",
      "def proj_v(pc1, pc2, scalex, scaley, axes, labx, laby):\n",
      "    xs = []\n",
      "    ys = []\n",
      "    fig = plot.figure(figsize=(12,9))\n",
      "    fig.tight_layout()\n",
      "    plot.axis(axes)\n",
      "    for i in range(1000):\n",
      "        v = data[i]\n",
      "        xs.append(np.dot(v, pc1))\n",
      "        ys.append(np.dot(v, pc2))\n",
      "    for i in range(1000):\n",
      "        x = xs[i]*scalex-np.min(xs)*scalex+50\n",
      "        y = ys[i]*scaley-np.min(ys)*scaley+50\n",
      "        fig.figimage(np.transpose(data[i].reshape(3,32,32), (1, 2, 0)), xo=x, yo=y, zorder=10)\n",
      "    plot.ylabel(laby)\n",
      "    plot.xlabel(labx)\n",
      "    plot.show()\n",
      "    plot.plot(xs, ys, marker=\"o\", linestyle=\"\")\n",
      "    plot.show()\n",
      "1/151: proj_v(u1, u2, 15, 15, [0, 50, -5, 25], \"PC 1\", \"PC 2\")\n",
      "1/152: proj_v(u3, u4, 15, 10, [-20, 5, -15, 10], \"PC 3\", \"PC 4\")\n",
      "1/153: proj_v(u3, u4, 20, 15, [-20, 5, -15, 10], \"PC 3\", \"PC 4\")\n",
      "1/154: proj_v(u3, u4, 25, 15, [-20, 5, -15, 10], \"PC 3\", \"PC 4\")\n",
      "1/155: proj_v(u3, u4, 25, 20, [-20, 5, -15, 10], \"PC 3\", \"PC 4\")\n",
      "1/156: proj_v(u3, u4, 25, 18, [-20, 5, -15, 10], \"PC 3\", \"PC 4\")\n",
      "1/157: proj_v(u3, u4, 26, 18, [-20, 5, -15, 10], \"PC 3\", \"PC 4\")\n",
      "1/158: proj_v(u3, u4, 27, 18, [-20, 5, -15, 10], \"PC 3\", \"PC 4\")\n",
      "1/159: proj_v(u3, u4, 28, 18, [-20, 5, -15, 10], \"PC 3\", \"PC 4\")\n",
      "1/160: proj_v(u5, u6, 20, 18, [-15, 10, -10, 10], \"PC 5\", \"PC 6\")\n",
      "1/161: proj_v(u5, u6, 28, 18, [-15, 10, -10, 10], \"PC 5\", \"PC 6\")\n",
      "1/162: proj_v(u5, u6, 30, 22, [-15, 10, -10, 10], \"PC 5\", \"PC 6\")\n",
      "1/163: w\n",
      "1/164: cov\n",
      " 2/1: u1\n",
      " 2/2:\n",
      "n = 3072\n",
      "u1 = w[:,n-1]\n",
      "u2 = w[:,n-2]\n",
      "u3 = w[:,n-3]\n",
      "u4 = w[:,n-4]\n",
      "u5 = w[:,n-5]\n",
      "u6 = w[:,n-6]\n",
      " 2/3:\n",
      "import numpy as np\n",
      "import matplotlib.pyplot as plot\n",
      "import math\n",
      "import random\n",
      "import pickle\n",
      "import scipy.linalg as sp\n",
      " 2/4:\n",
      "def unpickle(file):\n",
      "    with open(file, 'rb') as fo:\n",
      "        dict = pickle.load(fo)\n",
      "    return dict\n",
      " 2/5:\n",
      "# load data\n",
      "data = unpickle('cifar-10-batches-py/data_batch_1')['data']\n",
      "data = np.asarray(data, dtype=float)/255\n",
      " 2/6:\n",
      "# standardize\n",
      "mean = np.mean(data, axis=0)\n",
      "sd = np.std(data, axis=0)\n",
      "centered = (data - mean)\n",
      " 2/7:\n",
      "# covariance matrix\n",
      "cov = np.cov(centered.T)\n",
      " 2/8: cov\n",
      " 2/9:\n",
      "# eigendecomposition\n",
      "v, w = np.linalg.eigh(cov)\n",
      "2/10: v\n",
      "2/11: w\n",
      "2/12:\n",
      "# sort eigenvalues in ascending order\n",
      "v = v[::-1]\n",
      "# cumulative sum\n",
      "cumsum = np.cumsum(v)\n",
      "2/13:\n",
      "# scree plot\n",
      "xs = np.array(range(len(cumsum)))+1\n",
      "plot.plot(xs, cumsum)\n",
      "plot.show()\n",
      "2/14:\n",
      "def std_eig(v):\n",
      "    v = (v - np.min(v))/(np.max(v) - np.min(v))\n",
      "    return v\n",
      "2/15:\n",
      "def plot_v(v, i):\n",
      "    img = std_eig(v)\n",
      "    plot.subplot(2, 8, i)\n",
      "    plot.xticks([])\n",
      "    plot.yticks([])\n",
      "    plot.imshow(np.transpose(img.reshape(3,32,32), (1, 2, 0)), interpolation='nearest')\n",
      "    plot.title(i)\n",
      "2/16:\n",
      "def plot_vs():\n",
      "    for i in range(3071, 3071-16, -1):\n",
      "        u = w[:,i]\n",
      "        plot_v(u, 3072-i)\n",
      "    plot.show()\n",
      "2/17: plot_vs()\n",
      "2/18:\n",
      "n = 3072\n",
      "u1 = w[:,n-1]\n",
      "u2 = w[:,n-2]\n",
      "u3 = w[:,n-3]\n",
      "u4 = w[:,n-4]\n",
      "u5 = w[:,n-5]\n",
      "u6 = w[:,n-6]\n",
      "2/19: u1\n",
      "2/20: centered\n",
      "2/21: data\n",
      "2/22: mean\n",
      "2/23: mean.shape\n",
      "2/24: mean\n",
      "2/25:\n",
      "# standardize\n",
      "mean = np.mean(data, axis=0)\n",
      "centered = (data - mean)\n",
      "2/26: mean\n",
      "2/27: centered\n",
      "2/28: u, s, vh = np.linalg.svd(cov)\n",
      "2/29: s\n",
      "2/30: sqrt(s)\n",
      "2/31: math.sqrt(s)\n",
      "2/32: s**(-1/2)\n",
      "2/33: s**(-.5)\n",
      "2/34: s**2\n",
      "2/35:\n",
      "# eigendecomposition\n",
      "v, w = np.linalg.eig(cov)\n",
      "2/36: v\n",
      "2/37: len(s**2)\n",
      "2/38: s**2\n",
      "2/39: s\n",
      "2/40:\n",
      "# eigendecomposition\n",
      "v, w = np.linalg.eigh(cov)\n",
      "2/41: u, s, vh = np.linalg.svd(cov)\n",
      "2/42: s\n",
      "2/43: s**2\n",
      "2/44: v\n",
      "2/45: w\n",
      "2/46: u, s, vh = np.linalg.svd(cov, fullmatrices=false)\n",
      "2/47: u, s, vh = np.linalg.svd(cov, fullmatrices=false)\n",
      "2/48: u, s, vh = np.linalg.svd(cov, fullmatrices=False)\n",
      "2/49: u, s, vh = np.linalg.svd(cov, fullmatrices=False)\n",
      "2/50: w.shape\n",
      "2/51: u, s, vh = np.linalg.svd(cov, full_matrices=False)\n",
      "2/52: s\n",
      "2/53:\n",
      "def plot_v(v, i):\n",
      "    img = std_eig(v)\n",
      "    print img[0], img[1], img[2], img[3], img[4]\n",
      "    plot.subplot(2, 8, i)\n",
      "    plot.xticks([])\n",
      "    plot.yticks([])\n",
      "    plot.imshow(np.transpose(img.reshape(3,32,32), (1, 2, 0)), interpolation='nearest')\n",
      "    plot.title(i)\n",
      "2/54:\n",
      "def plot_vs():\n",
      "    for i in range(3071, 3071-16, -1):\n",
      "        u = w[:,i]\n",
      "        plot_v(u, 3072-i)\n",
      "    plot.show()\n",
      "2/55: plot_vs()\n",
      "2/56: cov\n",
      "2/57:\n",
      "# eigendecomposition\n",
      "v, w = np.linalg.eigh(cov)\n",
      "2/58: v\n",
      "2/59: w\n",
      "2/60: w.shape\n",
      "2/61:\n",
      "# sort eigenvalues in ascending order\n",
      "v = v[::-1]\n",
      "# cumulative sum\n",
      "cumsum = np.cumsum(v)\n",
      "2/62:\n",
      "# scree plot\n",
      "xs = np.array(range(len(cumsum)))+1\n",
      "plot.plot(xs, cumsum)\n",
      "plot.show()\n",
      "2/63:\n",
      "def std_eig(v):\n",
      "    v = (v - np.min(v))/(np.max(v) - np.min(v))\n",
      "    return v\n",
      "2/64:\n",
      "def plot_v(v, i):\n",
      "    img = std_eig(v)\n",
      "    plot.subplot(2, 8, i)\n",
      "    plot.xticks([])\n",
      "    plot.yticks([])\n",
      "    plot.imshow(np.transpose(img.reshape(3,32,32), (1, 2, 0)), interpolation='nearest')\n",
      "    plot.title(i)\n",
      "2/65:\n",
      "def plot_vs():\n",
      "    for i in range(3071, 3071-16, -1):\n",
      "        u = w[:,i]\n",
      "        plot_v(u, 3072-i)\n",
      "    plot.show()\n",
      "2/66: plot_vs()\n",
      "2/67:\n",
      "n = 3072\n",
      "u1 = w[:,n-1]\n",
      "u2 = w[:,n-2]\n",
      "u3 = w[:,n-3]\n",
      "u4 = w[:,n-4]\n",
      "u5 = w[:,n-5]\n",
      "u6 = w[:,n-6]\n",
      "2/68: u1\n",
      "2/69:\n",
      "def proj_v(pc1, pc2, scalex, scaley, axes, labx, laby):\n",
      "    xs = []\n",
      "    ys = []\n",
      "    fig = plot.figure(figsize=(12,9))\n",
      "    fig.tight_layout()\n",
      "    plot.axis(axes)\n",
      "    for i in range(1000):\n",
      "        v = data[i]\n",
      "        xs.append(np.dot(v, pc1))\n",
      "        ys.append(np.dot(v, pc2))\n",
      "    for i in range(1000):\n",
      "        x = xs[i]*scalex-np.min(xs)*scalex+50\n",
      "        y = ys[i]*scaley-np.min(ys)*scaley+50\n",
      "        fig.figimage(np.transpose(data[i].reshape(3,32,32), (1, 2, 0)), xo=x, yo=y, zorder=10)\n",
      "    plot.ylabel(laby)\n",
      "    plot.xlabel(labx)\n",
      "    plot.show()\n",
      "    plot.plot(xs, ys, marker=\"o\", linestyle=\"\")\n",
      "    plot.show()\n",
      "2/70: proj_v(u1, u2, 15, 15, [0, 50, -5, 25], \"PC 1\", \"PC 2\")\n",
      "2/71: proj_v(u3, u4, 28, 18, [-20, 5, -15, 10], \"PC 3\", \"PC 4\")\n",
      " 4/1:\n",
      "# covariance matrix\n",
      "# cov = np.cov(centered.T)\n",
      "cov = np.cov(data.T)\n",
      " 4/2:\n",
      "import numpy as np\n",
      "import matplotlib.pyplot as plot\n",
      "import math\n",
      "import random\n",
      "import pickle\n",
      "import scipy.linalg as sp\n",
      " 4/3:\n",
      "def unpickle(file):\n",
      "    with open(file, 'rb') as fo:\n",
      "        dict = pickle.load(fo)\n",
      "    return dict\n",
      " 4/4:\n",
      "# load data\n",
      "data = unpickle('cifar-10-batches-py/data_batch_1')['data']\n",
      "data = np.asarray(data, dtype=float)/255\n",
      " 4/5: data\n",
      " 4/6:\n",
      "# standardize\n",
      "mean = np.mean(data, axis=0)\n",
      "centered = (data - mean)\n",
      " 4/7: mean\n",
      " 4/8: centered\n",
      " 4/9:\n",
      "# covariance matrix\n",
      "# cov = np.cov(centered.T)\n",
      "cov = np.cov(data.T)\n",
      "4/10: cov\n",
      "4/11:\n",
      "# eigendecomposition\n",
      "v, w = np.linalg.eigh(cov)\n",
      "4/12: v\n",
      "4/13: w\n",
      "4/14:\n",
      "# sort eigenvalues in ascending order\n",
      "v = v[::-1]\n",
      "# cumulative sum\n",
      "cumsum = np.cumsum(v)\n",
      "4/15:\n",
      "# scree plot\n",
      "xs = np.array(range(len(cumsum)))+1\n",
      "plot.plot(xs, cumsum)\n",
      "plot.show()\n",
      "4/16:\n",
      "def std_eig(v):\n",
      "    v = (v - np.min(v))/(np.max(v) - np.min(v))\n",
      "    return v\n",
      "4/17:\n",
      "def plot_v(v, i):\n",
      "    img = std_eig(v)\n",
      "    plot.subplot(2, 8, i)\n",
      "    plot.xticks([])\n",
      "    plot.yticks([])\n",
      "    plot.imshow(np.transpose(img.reshape(3,32,32), (1, 2, 0)), interpolation='nearest')\n",
      "    plot.title(i)\n",
      "4/18:\n",
      "def plot_vs():\n",
      "    for i in range(3071, 3071-16, -1):\n",
      "        u = w[:,i]\n",
      "        plot_v(u, 3072-i)\n",
      "    plot.show()\n",
      "4/19: plot_vs()\n",
      "4/20:\n",
      "n = 3072\n",
      "u1 = w[:,n-1]\n",
      "u2 = w[:,n-2]\n",
      "u3 = w[:,n-3]\n",
      "u4 = w[:,n-4]\n",
      "u5 = w[:,n-5]\n",
      "u6 = w[:,n-6]\n",
      "4/21: u1\n",
      "4/22:\n",
      "def proj_v(pc1, pc2, scalex, scaley, axes, labx, laby):\n",
      "    xs = []\n",
      "    ys = []\n",
      "    fig = plot.figure(figsize=(12,9))\n",
      "    fig.tight_layout()\n",
      "    plot.axis(axes)\n",
      "    for i in range(1000):\n",
      "        v = data[i]\n",
      "        xs.append(np.dot(v, pc1))\n",
      "        ys.append(np.dot(v, pc2))\n",
      "    for i in range(1000):\n",
      "        x = xs[i]*scalex-np.min(xs)*scalex+50\n",
      "        y = ys[i]*scaley-np.min(ys)*scaley+50\n",
      "        fig.figimage(np.transpose(data[i].reshape(3,32,32), (1, 2, 0)), xo=x, yo=y, zorder=10)\n",
      "    plot.ylabel(laby)\n",
      "    plot.xlabel(labx)\n",
      "    plot.show()\n",
      "    plot.plot(xs, ys, marker=\"o\", linestyle=\"\")\n",
      "    plot.show()\n",
      "4/23: proj_v(u1, u2, 15, 15, [0, 50, -5, 25], \"PC 1\", \"PC 2\")\n",
      "4/24:\n",
      "# covariance matrix\n",
      "# cov = np.cov(centered.T)\n",
      "cov = np.dot(data.T, data)/len(data)\n",
      "4/25: cov\n",
      "4/26:\n",
      "# eigendecomposition\n",
      "v, w = np.linalg.eigh(cov)\n",
      "4/27: v\n",
      "4/28: w\n",
      "4/29:\n",
      "# sort eigenvalues in ascending order\n",
      "v = v[::-1]\n",
      "# cumulative sum\n",
      "cumsum = np.cumsum(v)\n",
      "4/30:\n",
      "# scree plot\n",
      "xs = np.array(range(len(cumsum)))+1\n",
      "plot.plot(xs, cumsum)\n",
      "plot.show()\n",
      "4/31:\n",
      "def std_eig(v):\n",
      "    v = (v - np.min(v))/(np.max(v) - np.min(v))\n",
      "    return v\n",
      "4/32:\n",
      "def plot_v(v, i):\n",
      "    img = std_eig(v)\n",
      "    plot.subplot(2, 8, i)\n",
      "    plot.xticks([])\n",
      "    plot.yticks([])\n",
      "    plot.imshow(np.transpose(img.reshape(3,32,32), (1, 2, 0)), interpolation='nearest')\n",
      "    plot.title(i)\n",
      "4/33:\n",
      "def plot_vs():\n",
      "    for i in range(3071, 3071-16, -1):\n",
      "        u = w[:,i]\n",
      "        plot_v(u, 3072-i)\n",
      "    plot.show()\n",
      "4/34: plot_vs()\n",
      "4/35: len(data)\n",
      "4/36: np.cov(centered.T)\n",
      "4/37: np.cov(data.T)\n",
      "4/38:\n",
      "# covariance matrix\n",
      "# cov = np.cov(centered.T)\n",
      "cov = np.matmul(data.T, data)/len(data)\n",
      "4/39: np.cov(centered.T)\n",
      "4/40: cov\n",
      "4/41: len(cov)\n",
      "4/42: cov.shape\n",
      "4/43: cov\n",
      "4/44: np.cov(data.T)\n",
      "4/45:\n",
      "# eigendecomposition\n",
      "v, w = np.linalg.eigh(cov)\n",
      "4/46: v\n",
      "4/47: w\n",
      "4/48:\n",
      "# sort eigenvalues in ascending order\n",
      "v = v[::-1]\n",
      "# cumulative sum\n",
      "cumsum = np.cumsum(v)\n",
      "4/49:\n",
      "# scree plot\n",
      "xs = np.array(range(len(cumsum)))+1\n",
      "plot.plot(xs, cumsum)\n",
      "plot.show()\n",
      "4/50:\n",
      "def std_eig(v):\n",
      "    v = (v - np.min(v))/(np.max(v) - np.min(v))\n",
      "    return v\n",
      "4/51:\n",
      "def plot_v(v, i):\n",
      "    img = std_eig(v)\n",
      "    plot.subplot(2, 8, i)\n",
      "    plot.xticks([])\n",
      "    plot.yticks([])\n",
      "    plot.imshow(np.transpose(img.reshape(3,32,32), (1, 2, 0)), interpolation='nearest')\n",
      "    plot.title(i)\n",
      "4/52:\n",
      "def plot_vs():\n",
      "    for i in range(3071, 3071-16, -1):\n",
      "        u = w[:,i]\n",
      "        plot_v(u, 3072-i)\n",
      "    plot.show()\n",
      "4/53: plot_vs()\n",
      "4/54:\n",
      "# covariance matrix\n",
      "# cov = np.cov(centered.T)\n",
      "cov = np.matmul(data.T, data)\n",
      "4/55: cov.shape\n",
      "4/56: cov\n",
      "4/57:\n",
      "# covariance matrix\n",
      "# cov = np.cov(centered.T)\n",
      "cov = data.T * data\n",
      "4/58:\n",
      "# covariance matrix\n",
      "# cov = np.cov(centered.T)\n",
      "cov = np.matmul(centered.T, centered)\n",
      "4/59: cov.shape\n",
      "4/60: cov\n",
      "4/61:\n",
      "# covariance matrix\n",
      "# cov = np.cov(centered.T)\n",
      "cov = np.matmul(centered.T, centered)/len(centered)\n",
      "4/62: cov.shape\n",
      "4/63: cov\n",
      "4/64: np.cov(data.T)\n",
      "4/65:\n",
      "# eigendecomposition\n",
      "v, w = np.linalg.eigh(cov)\n",
      "4/66: v\n",
      "4/67: w\n",
      "4/68:\n",
      "# sort eigenvalues in ascending order\n",
      "v = v[::-1]\n",
      "# cumulative sum\n",
      "cumsum = np.cumsum(v)\n",
      "4/69:\n",
      "# scree plot\n",
      "xs = np.array(range(len(cumsum)))+1\n",
      "plot.plot(xs, cumsum)\n",
      "plot.show()\n",
      "4/70:\n",
      "def std_eig(v):\n",
      "    v = (v - np.min(v))/(np.max(v) - np.min(v))\n",
      "    return v\n",
      "4/71:\n",
      "def plot_v(v, i):\n",
      "    img = std_eig(v)\n",
      "    plot.subplot(2, 8, i)\n",
      "    plot.xticks([])\n",
      "    plot.yticks([])\n",
      "    plot.imshow(np.transpose(img.reshape(3,32,32), (1, 2, 0)), interpolation='nearest')\n",
      "    plot.title(i)\n",
      "4/72:\n",
      "def plot_vs():\n",
      "    for i in range(3071, 3071-16, -1):\n",
      "        u = w[:,i]\n",
      "        plot_v(u, 3072-i)\n",
      "    plot.show()\n",
      "4/73: plot_vs()\n",
      "4/74: plot_vs()\n",
      "4/75:\n",
      "n = 3072\n",
      "u1 = w[:,n-1]\n",
      "u2 = w[:,n-2]\n",
      "u3 = w[:,n-3]\n",
      "u4 = w[:,n-4]\n",
      "u5 = w[:,n-5]\n",
      "u6 = w[:,n-6]\n",
      "4/76: u1\n",
      "4/77:\n",
      "def proj_v(pc1, pc2, scalex, scaley, axes, labx, laby):\n",
      "    xs = []\n",
      "    ys = []\n",
      "    fig = plot.figure(figsize=(12,9))\n",
      "    fig.tight_layout()\n",
      "    plot.axis(axes)\n",
      "    for i in range(1000):\n",
      "        v = data[i]\n",
      "        xs.append(np.dot(v, pc1))\n",
      "        ys.append(np.dot(v, pc2))\n",
      "    for i in range(1000):\n",
      "        x = xs[i]*scalex-np.min(xs)*scalex+50\n",
      "        y = ys[i]*scaley-np.min(ys)*scaley+50\n",
      "        fig.figimage(np.transpose(data[i].reshape(3,32,32), (1, 2, 0)), xo=x, yo=y, zorder=10)\n",
      "    plot.ylabel(laby)\n",
      "    plot.xlabel(labx)\n",
      "    plot.show()\n",
      "    plot.plot(xs, ys, marker=\"o\", linestyle=\"\")\n",
      "    plot.show()\n",
      "4/78: proj_v(u1, u2, 15, 15, [0, 50, -5, 25], \"PC 1\", \"PC 2\")\n",
      "4/79: proj_v(u3, u4, 28, 18, [-20, 5, -15, 10], \"PC 3\", \"PC 4\")\n",
      "4/80: proj_v(u5, u6, 30, 22, [-15, 10, -10, 10], \"PC 5\", \"PC 6\")\n",
      "4/81: cov.shape\n",
      "4/82:\n",
      "import numpy as np\n",
      "import matplotlib.pyplot as plot\n",
      "import math\n",
      "import random\n",
      "import pickle\n",
      "import scipy.linalg as sp\n",
      "4/83:\n",
      "def unpickle(file):\n",
      "    with open(file, 'rb') as fo:\n",
      "        dict = pickle.load(fo)\n",
      "    return dict\n",
      "4/84:\n",
      "# load data\n",
      "data = unpickle('cifar-10-batches-py/data_batch_1')['data']\n",
      "data = np.asarray(data, dtype=float)/255\n",
      "4/85: data\n",
      "4/86:\n",
      "# standardize\n",
      "mean = np.mean(data, axis=0)\n",
      "centered = (data - mean)\n",
      "4/87: mean\n",
      "4/88: centered\n",
      "4/89:\n",
      "# covariance matrix\n",
      "cov = np.cov(centered.T)\n",
      "# equivalently, np.cov(data.T) or\n",
      "# cov = np.matmul(centered.T, centered)/len(centered)\n",
      "4/90: cov.shape\n",
      "4/91: cov\n",
      "4/92: np.cov(data.T)\n",
      "4/93:\n",
      "# eigendecomposition\n",
      "v, w = np.linalg.eigh(cov)\n",
      "4/94: v\n",
      "4/95: w\n",
      "4/96:\n",
      "# sort eigenvalues in ascending order\n",
      "v = v[::-1]\n",
      "# cumulative sum\n",
      "cumsum = np.cumsum(v)\n",
      "4/97:\n",
      "# scree plot\n",
      "xs = np.array(range(len(cumsum)))+1\n",
      "plot.plot(xs, cumsum)\n",
      "plot.show()\n",
      "4/98:\n",
      "def std_eig(v):\n",
      "    v = (v - np.min(v))/(np.max(v) - np.min(v))\n",
      "    return v\n",
      "4/99:\n",
      "def plot_v(v, i):\n",
      "    img = std_eig(v)\n",
      "    plot.subplot(2, 8, i)\n",
      "    plot.xticks([])\n",
      "    plot.yticks([])\n",
      "    plot.imshow(np.transpose(img.reshape(3,32,32), (1, 2, 0)), interpolation='nearest')\n",
      "    plot.title(i)\n",
      "4/100:\n",
      "def plot_vs():\n",
      "    for i in range(3071, 3071-16, -1):\n",
      "        u = w[:,i]\n",
      "        plot_v(u, 3072-i)\n",
      "    plot.show()\n",
      "4/101: plot_vs()\n",
      "4/102:\n",
      "n = 3072\n",
      "u1 = w[:,n-1]\n",
      "u2 = w[:,n-2]\n",
      "u3 = w[:,n-3]\n",
      "u4 = w[:,n-4]\n",
      "u5 = w[:,n-5]\n",
      "u6 = w[:,n-6]\n",
      "4/103: u1\n",
      "4/104:\n",
      "def proj_v(pc1, pc2, scalex, scaley, axes, labx, laby):\n",
      "    xs = []\n",
      "    ys = []\n",
      "    fig = plot.figure(figsize=(12,9))\n",
      "    fig.tight_layout()\n",
      "    plot.axis(axes)\n",
      "    for i in range(1000):\n",
      "        v = data[i]\n",
      "        xs.append(np.dot(v, pc1))\n",
      "        ys.append(np.dot(v, pc2))\n",
      "    for i in range(1000):\n",
      "        x = xs[i]*scalex-np.min(xs)*scalex+50\n",
      "        y = ys[i]*scaley-np.min(ys)*scaley+50\n",
      "        fig.figimage(np.transpose(data[i].reshape(3,32,32), (1, 2, 0)), xo=x, yo=y, zorder=10)\n",
      "    plot.ylabel(laby)\n",
      "    plot.xlabel(labx)\n",
      "    plot.show()\n",
      "    plot.plot(xs, ys, marker=\"o\", linestyle=\"\")\n",
      "    plot.show()\n",
      "4/105: proj_v(u1, u2, 15, 15, [0, 50, -5, 25], \"PC 1\", \"PC 2\")\n",
      "4/106: proj_v(u3, u4, 28, 18, [-20, 5, -15, 10], \"PC 3\", \"PC 4\")\n",
      "4/107: proj_v(u5, u6, 30, 22, [-15, 10, -10, 10], \"PC 5\", \"PC 6\")\n",
      "4/108: np.matmul(centered.T, centered)/len(centered)\n",
      " 5/1:\n",
      "import numpy as np\n",
      "import matplotlib.pyplot as plot\n",
      "import math\n",
      "import random\n",
      "import pickle\n",
      "import scipy.linalg as sp\n",
      " 5/2:\n",
      "def unpickle(file):\n",
      "    with open(file, 'rb') as fo:\n",
      "        dict = pickle.load(fo)\n",
      "    return dict\n",
      " 5/3:\n",
      "# load data\n",
      "data = unpickle('cifar-10-batches-py/data_batch_1')['data']\n",
      "data = np.asarray(data, dtype=float)/255\n",
      " 5/4: data\n",
      " 5/5:\n",
      "# standardize\n",
      "mean = np.mean(data, axis=0)\n",
      "centered = (data - mean)\n",
      " 5/6: mean\n",
      " 5/7: centered\n",
      " 5/8:\n",
      "# covariance matrix\n",
      "cov = np.cov(centered.T)\n",
      "# equivalently, np.cov(data.T) or\n",
      "# cov = np.matmul(centered.T, centered)/len(centered)\n",
      " 5/9: cov.shape\n",
      "5/10: cov\n",
      "5/11: np.matmul(centered.T, centered)/len(centered)\n",
      "5/12:\n",
      "# eigendecomposition\n",
      "v, w = np.linalg.eig(cov)\n",
      "5/13: v\n",
      "5/14: w\n",
      "5/15:\n",
      "def plot_vs():\n",
      "    for i in range(16):\n",
      "        u = w[:,i]\n",
      "        plot_v(u, i)\n",
      "    plot.show()\n",
      "5/16: plot_vs()\n",
      "5/17:\n",
      "def plot_v(v, i):\n",
      "    img = std_eig(v)\n",
      "    plot.subplot(2, 8, i)\n",
      "    plot.xticks([])\n",
      "    plot.yticks([])\n",
      "    plot.imshow(np.transpose(img.reshape(3,32,32), (1, 2, 0)), interpolation='nearest')\n",
      "    plot.title(i)\n",
      "5/18:\n",
      "def plot_vs():\n",
      "    for i in range(16):\n",
      "        u = w[:,i]\n",
      "        plot_v(u, i)\n",
      "    plot.show()\n",
      "5/19: plot_vs()\n",
      "5/20:\n",
      "def std_eig(v):\n",
      "    v = (v - np.min(v))/(np.max(v) - np.min(v))\n",
      "    return v\n",
      "5/21:\n",
      "def plot_v(v, i):\n",
      "    img = std_eig(v)\n",
      "    plot.subplot(2, 8, i)\n",
      "    plot.xticks([])\n",
      "    plot.yticks([])\n",
      "    plot.imshow(np.transpose(img.reshape(3,32,32), (1, 2, 0)), interpolation='nearest')\n",
      "    plot.title(i)\n",
      "5/22:\n",
      "def plot_vs():\n",
      "    for i in range(16):\n",
      "        u = w[:,i]\n",
      "        plot_v(u, i)\n",
      "    plot.show()\n",
      "5/23: plot_vs()\n",
      "5/24:\n",
      "def plot_vs():\n",
      "    for i in range(16):\n",
      "        u = w[:,i]\n",
      "        plot_v(u, i+1)\n",
      "    plot.show()\n",
      "5/25: plot_vs()\n",
      "5/26:\n",
      "n = 3072\n",
      "u1 = w[:,n-1]\n",
      "u2 = w[:,n-2]\n",
      "u3 = w[:,n-3]\n",
      "u4 = w[:,n-4]\n",
      "u5 = w[:,n-5]\n",
      "u6 = w[:,n-6]\n",
      "5/27:\n",
      "n = 3072\n",
      "u1 = w[:,1]\n",
      "u2 = w[:,2]\n",
      "u3 = w[:,3]\n",
      "u4 = w[:,4]\n",
      "u5 = w[:,5]\n",
      "u6 = w[:,6]\n",
      "5/28: u1\n",
      "5/29:\n",
      "def proj_v(pc1, pc2, scalex, scaley, axes, labx, laby):\n",
      "    xs = []\n",
      "    ys = []\n",
      "    fig = plot.figure(figsize=(12,9))\n",
      "    fig.tight_layout()\n",
      "    plot.axis(axes)\n",
      "    for i in range(1000):\n",
      "        v = data[i]\n",
      "        xs.append(np.dot(v, pc1))\n",
      "        ys.append(np.dot(v, pc2))\n",
      "    for i in range(1000):\n",
      "        x = xs[i]*scalex-np.min(xs)*scalex+50\n",
      "        y = ys[i]*scaley-np.min(ys)*scaley+50\n",
      "        fig.figimage(np.transpose(data[i].reshape(3,32,32), (1, 2, 0)), xo=x, yo=y, zorder=10)\n",
      "    plot.ylabel(laby)\n",
      "    plot.xlabel(labx)\n",
      "    plot.show()\n",
      "    plot.plot(xs, ys, marker=\"o\", linestyle=\"\")\n",
      "    plot.show()\n",
      "5/30: proj_v(u1, u2, 15, 15, [0, 50, -5, 25], \"PC 1\", \"PC 2\")\n",
      "5/31:\n",
      "n = 3072\n",
      "u1 = w[:,0]\n",
      "u2 = w[:,1]\n",
      "u3 = w[:,2]\n",
      "u4 = w[:,3]\n",
      "u5 = w[:,4]\n",
      "u6 = w[:,5]\n",
      "5/32: u1\n",
      "5/33:\n",
      "def proj_v(pc1, pc2, scalex, scaley, axes, labx, laby):\n",
      "    xs = []\n",
      "    ys = []\n",
      "    fig = plot.figure(figsize=(12,9))\n",
      "    fig.tight_layout()\n",
      "    plot.axis(axes)\n",
      "    for i in range(1000):\n",
      "        v = data[i]\n",
      "        xs.append(np.dot(v, pc1))\n",
      "        ys.append(np.dot(v, pc2))\n",
      "    for i in range(1000):\n",
      "        x = xs[i]*scalex-np.min(xs)*scalex+50\n",
      "        y = ys[i]*scaley-np.min(ys)*scaley+50\n",
      "        fig.figimage(np.transpose(data[i].reshape(3,32,32), (1, 2, 0)), xo=x, yo=y, zorder=10)\n",
      "    plot.ylabel(laby)\n",
      "    plot.xlabel(labx)\n",
      "    plot.show()\n",
      "    plot.plot(xs, ys, marker=\"o\", linestyle=\"\")\n",
      "    plot.show()\n",
      "5/34: proj_v(u1, u2, 15, 15, [0, 50, -5, 25], \"PC 1\", \"PC 2\")\n",
      "5/35: proj_v(u3, u4, 28, 18, [-20, 5, -15, 10], \"PC 3\", \"PC 4\")\n",
      "5/36: proj_v(u3, u4, 28, 18, [-20, 5, -15, 10], \"PC 3\", \"PC 4\")\n",
      "5/37: proj_v(u5, u6, 30, 22, [-15, 10, -10, 10], \"PC 5\", \"PC 6\")\n",
      "5/38: v_, w_ = np.linalg.eigh(cov)\n",
      "5/39: v_\n",
      "5/40: v[0:15]\n",
      "5/41: v_[3071:3072-16]\n",
      "5/42: v[::-1][0:15]\n",
      "5/43: v_[::-1][0:15]\n",
      "5/44: w_\n",
      "5/45: w[0:15]\n",
      "5/46: w_[::-1][0:15]\n",
      "5/47: w\n",
      "5/48: w[0:15]\n",
      "5/49: w\n",
      "5/50: w_\n",
      "5/51: w_.shape\n",
      "5/52: w.shape\n",
      "5/53:\n",
      "# cumulative sum\n",
      "cumsum = np.cumsum(v)\n",
      "5/54:\n",
      "# scree plot\n",
      "xs = np.array(range(len(cumsum)))+1\n",
      "plot.plot(xs, cumsum)\n",
      "plot.show()\n",
      "5/55:\n",
      "# eigendecomposition\n",
      "v, w = np.linalg.eig(cov)\n",
      "5/56: v[0:15]\n",
      "5/57: w[0:15]\n",
      "5/58:\n",
      "# cumulative sum\n",
      "cumsum = np.cumsum(v)\n",
      "5/59:\n",
      "# scree plot\n",
      "xs = np.array(range(len(cumsum)))+1\n",
      "plot.plot(xs, cumsum)\n",
      "plot.show()\n",
      "5/60:\n",
      "def std_eig(v):\n",
      "    v = (v - np.min(v))/(np.max(v) - np.min(v))\n",
      "    return v\n",
      "5/61:\n",
      "def plot_v(v, i):\n",
      "    img = std_eig(v)\n",
      "    plot.subplot(2, 8, i)\n",
      "    plot.xticks([])\n",
      "    plot.yticks([])\n",
      "    plot.imshow(np.transpose(img.reshape(3,32,32), (1, 2, 0)), interpolation='nearest')\n",
      "    plot.title(i)\n",
      "5/62:\n",
      "def plot_vs():\n",
      "    for i in range(16):\n",
      "        u = w[:,i]\n",
      "        plot_v(u, i+1)\n",
      "    plot.show()\n",
      "5/63: plot_vs()\n",
      "5/64:\n",
      "n = 3072\n",
      "u1 = w[:,0]\n",
      "u2 = w[:,1]\n",
      "u3 = w[:,2]\n",
      "u4 = w[:,3]\n",
      "u5 = w[:,4]\n",
      "u6 = w[:,5]\n",
      "5/65: u1\n",
      "5/66:\n",
      "def proj_v(pc1, pc2, scalex, scaley, axes, labx, laby):\n",
      "    xs = []\n",
      "    ys = []\n",
      "    fig = plot.figure(figsize=(12,9))\n",
      "    fig.tight_layout()\n",
      "    plot.axis(axes)\n",
      "    for i in range(1000):\n",
      "        v = data[i]\n",
      "        xs.append(np.dot(v, pc1))\n",
      "        ys.append(np.dot(v, pc2))\n",
      "    for i in range(1000):\n",
      "        x = xs[i]*scalex-np.min(xs)*scalex+50\n",
      "        y = ys[i]*scaley-np.min(ys)*scaley+50\n",
      "        fig.figimage(np.transpose(data[i].reshape(3,32,32), (1, 2, 0)), xo=x, yo=y, zorder=10)\n",
      "    plot.ylabel(laby)\n",
      "    plot.xlabel(labx)\n",
      "    plot.show()\n",
      "    plot.plot(xs, ys, marker=\"o\", linestyle=\"\")\n",
      "    plot.show()\n",
      "5/67: proj_v(u1, u2, 15, 15, [0, 50, -5, 25], \"PC 1\", \"PC 2\")\n",
      "5/68: proj_v(u3, u4, 28, 18, [-20, 5, -15, 10], \"PC 3\", \"PC 4\")\n",
      "5/69: proj_v(u5, u6, 30, 22, [-15, 10, -10, 10], \"PC 5\", \"PC 6\")\n",
      " 6/1:\n",
      "import math\n",
      "import numpy as np\n",
      " 6/2:\n",
      "S = [i for i in range(101)]\n",
      "A = [i for i in range(16)]\n",
      "grid = [[7,12,1,14],[2,13,8,11],[16,3,10,5],[9,6,15,4]]\n",
      " 6/3:\n",
      "def grid(dir, a):\n",
      "    i = a/4\n",
      "    j = a - 4*i\n",
      "    if dir == \"\":\n",
      "        return grid[i][j]\n",
      "    elif dir == \"up\":\n",
      "        return 0 if i == 0 else grid[i-1][j]\n",
      "    elif dir == \"down\":\n",
      "        return 0 if i == 3 else grid[i+1][j]\n",
      "    elif dir == \"left\":\n",
      "        return 0 if j == 0 else grid[i][j-1]\n",
      "    else:\n",
      "        return 0 if j == 3 else grid[i][j+1]\n",
      " 6/4:\n",
      "def reward(s):\n",
      "    if s < 101:\n",
      "        return 0\n",
      "    elif s == 101:\n",
      "        return 1\n",
      "    else:\n",
      "        return -1\n",
      " 6/5:\n",
      "def pdist(discount, s, a, V):\n",
      "    sum = 0\n",
      "    sum += 0.6*grid(\"\", a)\n",
      "    sum += 0.1*grid(\"up\", a) + 0.1*grid(\"down\", a)\n",
      "    sum += 0.1*grid(\"left\", a) + 0.1*grid(\"right\", a)\n",
      "    sum *= discount\n",
      "    return sum\n",
      " 6/6:\n",
      "def value_iteration(discount):\n",
      "    V = np.array([0 for i in range(101)])\n",
      "    policy = [0 for i in range]\n",
      "    while True:\n",
      "        V_old = V.copy()\n",
      "        for s in S:\n",
      "            Q = [0 for i in range(16)]\n",
      "            for i in range(16):\n",
      "                Q[i] = reward(i) + discount*pdist(dicount, s, i, V_old)\n",
      "            policy[s] = np.argmax(Q)\n",
      "            V[s] = Q[policy[s]]\n",
      "        if np.allclose(V, V_old, atol=0.001):\n",
      "            break\n",
      "    return V, policy\n",
      " 6/7:\n",
      "def value_iteration(discount):\n",
      "    V = np.array([0 for i in range(101)])\n",
      "    policy = [0 for i in range(101)]\n",
      "    while True:\n",
      "        V_old = V.copy()\n",
      "        for s in S:\n",
      "            Q = [0 for i in range(16)]\n",
      "            for i in range(16):\n",
      "                Q[i] = reward(i) + discount*pdist(dicount, s, i, V_old)\n",
      "            policy[s] = np.argmax(Q)\n",
      "            V[s] = Q[policy[s]]\n",
      "        if np.allclose(V, V_old, atol=0.001):\n",
      "            break\n",
      "    return V, policy\n",
      " 6/8:\n",
      "V_, policy_ = value_iteration(.001)\n",
      "print V_, policy_\n",
      " 6/9:\n",
      "def value_iteration(discount):\n",
      "    V = np.array([0 for i in range(101)])\n",
      "    policy = [0 for i in range(101)]\n",
      "    while True:\n",
      "        V_old = V.copy()\n",
      "        for s in S:\n",
      "            Q = [0 for i in range(16)]\n",
      "            for i in range(16):\n",
      "                Q[i] = reward(i) + discount*pdist(discount, s, i, V_old)\n",
      "            policy[s] = np.argmax(Q)\n",
      "            V[s] = Q[policy[s]]\n",
      "        if np.allclose(V, V_old, atol=0.001):\n",
      "            break\n",
      "    return V, policy\n",
      "6/10:\n",
      "V_, policy_ = value_iteration(.001)\n",
      "print V_, policy_\n",
      "6/11:\n",
      "def grid(dir, a):\n",
      "    i = a/4\n",
      "    j = a - 4*i\n",
      "    if dir == \"\":\n",
      "        return g[i][j]\n",
      "    elif dir == \"up\":\n",
      "        return 0 if i == 0 else g[i-1][j]\n",
      "    elif dir == \"down\":\n",
      "        return 0 if i == 3 else g[i+1][j]\n",
      "    elif dir == \"left\":\n",
      "        return 0 if j == 0 else g[i][j-1]\n",
      "    else:\n",
      "        return 0 if j == 3 else g[i][j+1]\n",
      "6/12:\n",
      "def reward(s):\n",
      "    if s < 101:\n",
      "        return 0\n",
      "    elif s == 101:\n",
      "        return 1\n",
      "    else:\n",
      "        return -1\n",
      "6/13:\n",
      "def pdist(discount, s, a, V):\n",
      "    sum = 0\n",
      "    sum += 0.6*grid(\"\", a)\n",
      "    sum += 0.1*grid(\"up\", a) + 0.1*grid(\"down\", a)\n",
      "    sum += 0.1*grid(\"left\", a) + 0.1*grid(\"right\", a)\n",
      "    sum *= discount\n",
      "    return sum\n",
      "6/14:\n",
      "def value_iteration(discount):\n",
      "    V = np.array([0 for i in range(101)])\n",
      "    policy = [0 for i in range(101)]\n",
      "    while True:\n",
      "        V_old = V.copy()\n",
      "        for s in S:\n",
      "            Q = [0 for i in range(16)]\n",
      "            for i in range(16):\n",
      "                Q[i] = reward(i) + discount*pdist(discount, s, i, V_old)\n",
      "            policy[s] = np.argmax(Q)\n",
      "            V[s] = Q[policy[s]]\n",
      "        if np.allclose(V, V_old, atol=0.001):\n",
      "            break\n",
      "    return V, policy\n",
      "6/15:\n",
      "V_, policy_ = value_iteration(.001)\n",
      "print V_, policy_\n",
      "6/16:\n",
      "S = [i for i in range(101)]\n",
      "A = [i for i in range(16)]\n",
      "6/17:\n",
      "def grid(dir, a):\n",
      "    g = [[7,12,1,14],[2,13,8,11],[16,3,10,5],[9,6,15,4]]\n",
      "    i = a/4\n",
      "    j = a - 4*i\n",
      "    if dir == \"\":\n",
      "        return g[i][j]\n",
      "    elif dir == \"up\":\n",
      "        return 0 if i == 0 else g[i-1][j]\n",
      "    elif dir == \"down\":\n",
      "        return 0 if i == 3 else g[i+1][j]\n",
      "    elif dir == \"left\":\n",
      "        return 0 if j == 0 else g[i][j-1]\n",
      "    else:\n",
      "        return 0 if j == 3 else g[i][j+1]\n",
      "6/18:\n",
      "def reward(s):\n",
      "    if s < 101:\n",
      "        return 0\n",
      "    elif s == 101:\n",
      "        return 1\n",
      "    else:\n",
      "        return -1\n",
      "6/19:\n",
      "def pdist(discount, s, a, V):\n",
      "    sum = 0\n",
      "    sum += 0.6*grid(\"\", a)\n",
      "    sum += 0.1*grid(\"up\", a) + 0.1*grid(\"down\", a)\n",
      "    sum += 0.1*grid(\"left\", a) + 0.1*grid(\"right\", a)\n",
      "    sum *= discount\n",
      "    return sum\n",
      "6/20:\n",
      "def value_iteration(discount):\n",
      "    V = np.array([0 for i in range(101)])\n",
      "    policy = [0 for i in range(101)]\n",
      "    while True:\n",
      "        V_old = V.copy()\n",
      "        for s in S:\n",
      "            Q = [0 for i in range(16)]\n",
      "            for i in range(16):\n",
      "                Q[i] = reward(i) + discount*pdist(discount, s, i, V_old)\n",
      "            policy[s] = np.argmax(Q)\n",
      "            V[s] = Q[policy[s]]\n",
      "        if np.allclose(V, V_old, atol=0.001):\n",
      "            break\n",
      "    return V, policy\n",
      "6/21:\n",
      "V_, policy_ = value_iteration(.001)\n",
      "print V_, policy_\n",
      "6/22:\n",
      "S = [i for i in range(101)]\n",
      "A = [i for i in range(16)]\n",
      "g = [[7,12,1,14],[2,13,8,11],[16,3,10,5],[9,6,15,4]]\n",
      "6/23:\n",
      "def grid(dir, a):\n",
      "    i = a/4\n",
      "    j = a - 4*i\n",
      "    if dir == \"\":\n",
      "        return g[i][j]\n",
      "    elif dir == \"up\":\n",
      "        return 0 if i == 0 else g[i-1][j]\n",
      "    elif dir == \"down\":\n",
      "        return 0 if i == 3 else g[i+1][j]\n",
      "    elif dir == \"left\":\n",
      "        return 0 if j == 0 else g[i][j-1]\n",
      "    else:\n",
      "        return 0 if j == 3 else g[i][j+1]\n",
      "6/24:\n",
      "def reward(s):\n",
      "    if s < 101:\n",
      "        return 0\n",
      "    elif s == 101:\n",
      "        return 1\n",
      "    else:\n",
      "        return -1\n",
      "6/25:\n",
      "def pdist(discount, s, a, V):\n",
      "    sum = 0\n",
      "    sum += 0.6*grid(\"\", a)\n",
      "    sum += 0.1*grid(\"up\", a) + 0.1*grid(\"down\", a)\n",
      "    sum += 0.1*grid(\"left\", a) + 0.1*grid(\"right\", a)\n",
      "    sum *= discount\n",
      "    return sum\n",
      "6/26:\n",
      "def value_iteration(discount):\n",
      "    V = np.array([0 for i in range(101)])\n",
      "    policy = [0 for i in range(101)]\n",
      "    while True:\n",
      "        V_old = V.copy()\n",
      "        for s in S:\n",
      "            Q = [0 for i in range(16)]\n",
      "            for i in range(16):\n",
      "                Q[i] = reward(i) + discount*pdist(discount, s, i, V_old)\n",
      "            policy[s] = np.argmax(Q)\n",
      "            V[s] = Q[policy[s]]\n",
      "        if np.allclose(V, V_old, atol=0.001):\n",
      "            break\n",
      "    return V, policy\n",
      "6/27:\n",
      "V_, policy_ = value_iteration(.001)\n",
      "print V_, policy_\n",
      "6/28:\n",
      "def psum(s, a, V):\n",
      "    sum = 0\n",
      "    sum += 0.6*grid(\"\", a)\n",
      "    sum += 0.1*grid(\"up\", a) + 0.1*grid(\"down\", a)\n",
      "    sum += 0.1*grid(\"left\", a) + 0.1*grid(\"right\", a)\n",
      "    return sum\n",
      "6/29:\n",
      "def value_iteration(discount):\n",
      "    V = np.array([0 for i in range(101)])\n",
      "    policy = [0 for i in range(101)]\n",
      "    while True:\n",
      "        V_old = V.copy()\n",
      "        for s in S:\n",
      "            Q = [0 for i in range(16)]\n",
      "            for a in A:\n",
      "                Q[a] = reward(s) + discount*psum(s, a, V_old)\n",
      "            policy[s] = np.argmax(Q)\n",
      "            V[s] = Q[policy[s]]\n",
      "        if np.allclose(V, V_old, atol=0.001):\n",
      "            break\n",
      "    return V, policy\n",
      "6/30:\n",
      "V_, policy_ = value_iteration(.001)\n",
      "print V_, policy_\n",
      "6/31:\n",
      "def value_iteration(discount):\n",
      "    V = np.array([0 for i in range(101)])\n",
      "    policy = [0 for i in range(101)]\n",
      "    while True:\n",
      "        V_old = V.copy()\n",
      "        for s in S:\n",
      "            Q = [0 for i in range(16)]\n",
      "            for a in A:\n",
      "                Q[a] = reward(s) + discount*psum(s, a, V_old)\n",
      "            policy[s] = np.argmax(Q)\n",
      "            V[s] = Q[policy[s]]\n",
      "        print V\n",
      "        if np.allclose(V, V_old, atol=0.001):\n",
      "            break\n",
      "    return V, policy\n",
      "6/32:\n",
      "V_, policy_ = value_iteration(.001)\n",
      "print V_, policy_\n",
      "6/33:\n",
      "def value_iteration(discount):\n",
      "    V = np.array([0 for i in range(101)])\n",
      "    policy = [0 for i in range(101)]\n",
      "    while True:\n",
      "        V_old = V.copy()\n",
      "        for s in S:\n",
      "            Q = [0 for i in range(16)]\n",
      "            for a in A:\n",
      "                Q[a] = reward(s) + discount*psum(s, a, V_old)\n",
      "            policy[s] = np.argmax(Q)\n",
      "            print policy[s]\n",
      "            V[s] = Q[policy[s]]\n",
      "        if np.allclose(V, V_old, atol=0.001):\n",
      "            break\n",
      "    return V, policy\n",
      "6/34:\n",
      "V_, policy_ = value_iteration(.001)\n",
      "print V_, policy_\n",
      "6/35:\n",
      "def value_iteration(discount):\n",
      "    V = np.array([0 for i in range(101)])\n",
      "    policy = [0 for i in range(101)]\n",
      "    while True:\n",
      "        V_old = V.copy()\n",
      "        for s in S:\n",
      "            Q = [0 for i in range(16)]\n",
      "            for a in A:\n",
      "                Q[a] = reward(s) + discount*psum(s, a, V_old)\n",
      "            policy[s] = np.argmax(Q)\n",
      "            print Q\n",
      "            V[s] = Q[policy[s]]\n",
      "        if np.allclose(V, V_old, atol=0.001):\n",
      "            break\n",
      "    return V, policy\n",
      "6/36:\n",
      "V_, policy_ = value_iteration(.001)\n",
      "print V_, policy_\n",
      "6/37:\n",
      "def value_iteration(discount):\n",
      "    V = np.array([0 for i in range(101)])\n",
      "    policy = [0 for i in range(101)]\n",
      "    it = 0\n",
      "    while True:\n",
      "        print \"iteration \" + it\n",
      "        it = it + 1\n",
      "        V_old = V.copy()\n",
      "        for s in S:\n",
      "            Q = [0 for i in range(16)]\n",
      "            for a in A:\n",
      "                Q[a] = reward(s) + discount*psum(s, a, V_old)\n",
      "            policy[s] = np.argmax(Q)\n",
      "            print Q\n",
      "            V[s] = Q[policy[s]]\n",
      "        if np.allclose(V, V_old, atol=0.001):\n",
      "            break\n",
      "    return V, policy\n",
      "6/38:\n",
      "V_, policy_ = value_iteration(.001)\n",
      "print V_, policy_\n",
      "6/39:\n",
      "def value_iteration(discount):\n",
      "    V = np.array([0 for i in range(101)])\n",
      "    policy = [0 for i in range(101)]\n",
      "    it = 0\n",
      "    while True:\n",
      "        print \"iteration \" + str(it)\n",
      "        it = it + 1\n",
      "        V_old = V.copy()\n",
      "        for s in S:\n",
      "            Q = [0 for i in range(16)]\n",
      "            for a in A:\n",
      "                Q[a] = reward(s) + discount*psum(s, a, V_old)\n",
      "            policy[s] = np.argmax(Q)\n",
      "            print Q\n",
      "            V[s] = Q[policy[s]]\n",
      "        if np.allclose(V, V_old, atol=0.001):\n",
      "            break\n",
      "    return V, policy\n",
      "6/40:\n",
      "V_, policy_ = value_iteration(.001)\n",
      "print V_, policy_\n",
      "6/41:\n",
      "V_, policy_ = value_iteration(.001)\n",
      "print V_, policy_, it\n",
      "6/42:\n",
      "def value_iteration(discount):\n",
      "    V = np.array([0 for i in range(101)])\n",
      "    policy = [0 for i in range(101)]\n",
      "    it = 0\n",
      "    while True:\n",
      "        it = it + 1\n",
      "        V_old = V.copy()\n",
      "        for s in S:\n",
      "            Q = [0 for i in range(16)]\n",
      "            for a in A:\n",
      "                Q[a] = reward(s) + discount*psum(s, a, V_old)\n",
      "            policy[s] = np.argmax(Q)\n",
      "            print Q\n",
      "            V[s] = Q[policy[s]]\n",
      "        if np.allclose(V, V_old, atol=0.001):\n",
      "            break\n",
      "    return V, policy, it\n",
      "6/43:\n",
      "V_, policy_ = value_iteration(.001)\n",
      "print V_, policy_, it\n",
      "6/44:\n",
      "def value_iteration(discount):\n",
      "    V = np.array([0 for i in range(101)])\n",
      "    policy = [0 for i in range(101)]\n",
      "    it = 0\n",
      "    while True:\n",
      "        it = it + 1\n",
      "        V_old = V.copy()\n",
      "        for s in S:\n",
      "            Q = [0 for i in range(16)]\n",
      "            for a in A:\n",
      "                Q[a] = reward(s) + discount*psum(s, a, V_old)\n",
      "            policy[s] = np.argmax(Q)\n",
      "            print Q\n",
      "            V[s] = Q[policy[s]]\n",
      "        if np.allclose(V, V_old, atol=0.001):\n",
      "            break\n",
      "    return V, policy, it\n",
      "6/45:\n",
      "V_, policy_ = value_iteration(.001)\n",
      "print V_, policy_, it\n",
      "6/46:\n",
      "V_, policy_, it = value_iteration(.001)\n",
      "print V_, policy_, it\n",
      "6/47:\n",
      "V_, policy_, it = value_iteration(.001)\n",
      "print V_, policy_\n",
      "print \"iterations: \" + str(it_\n",
      "6/48:\n",
      "V_, policy_, it = value_iteration(.001)\n",
      "print V_, policy_\n",
      "print \"iterations: \" + str(it_)\n",
      "6/49:\n",
      "V_, policy_, it = value_iteration(.001)\n",
      "print V_, policy_\n",
      "print \"iterations: \" + str(it)\n",
      "6/50:\n",
      "S = [i for i in range(117)]\n",
      "states = 117\n",
      "A = [i for i in range(16)]\n",
      "g = [[7,12,1,14],[2,13,8,11],[16,3,10,5],[9,6,15,4]]\n",
      "6/51:\n",
      "def grid(dir, a):\n",
      "    i = a/4\n",
      "    j = a - 4*i\n",
      "    if dir == \"\":\n",
      "        return g[i][j]\n",
      "    elif dir == \"up\":\n",
      "        return 0 if i == 0 else g[i-1][j]\n",
      "    elif dir == \"down\":\n",
      "        return 0 if i == 3 else g[i+1][j]\n",
      "    elif dir == \"left\":\n",
      "        return 0 if j == 0 else g[i][j-1]\n",
      "    else:\n",
      "        return 0 if j == 3 else g[i][j+1]\n",
      "6/52:\n",
      "def reward(s):\n",
      "    if s < 101:\n",
      "        return 0\n",
      "    elif s == 101:\n",
      "        return 1\n",
      "    else:\n",
      "        return -1\n",
      "6/53:\n",
      "def psum(s, a, V):\n",
      "    sum = 0\n",
      "    sum += 0.6*grid(\"\", a)\n",
      "    sum += 0.1*grid(\"up\", a) + 0.1*grid(\"down\", a)\n",
      "    sum += 0.1*grid(\"left\", a) + 0.1*grid(\"right\", a)\n",
      "    return sum\n",
      "6/54:\n",
      "def value_iteration(discount):\n",
      "    V = np.array([0 for i in range(states)])\n",
      "    policy = [0 for i in range(states)]\n",
      "    it = 0\n",
      "    while True:\n",
      "        it = it + 1\n",
      "        V_old = V.copy()\n",
      "        for s in S:\n",
      "            Q = [0 for i in range(16)]\n",
      "            for a in A:\n",
      "                Q[a] = reward(s) + discount*psum(s, a, V_old)\n",
      "            policy[s] = np.argmax(Q)\n",
      "            print Q\n",
      "            V[s] = Q[policy[s]]\n",
      "        if np.allclose(V, V_old, atol=0.001):\n",
      "            break\n",
      "    return V, policy, it\n",
      "6/55:\n",
      "V_, policy_, it = value_iteration(.001)\n",
      "print V_, policy_\n",
      "print \"iterations: \" + str(it)\n",
      "6/56:\n",
      "V_, policy_, it = value_iteration(.1)\n",
      "print V_, policy_\n",
      "print \"iterations: \" + str(it)\n",
      "6/57:\n",
      "def value_iteration(discount):\n",
      "    V = np.array([0 for i in range(states)])\n",
      "    policy = [0 for i in range(states)]\n",
      "    it = 0\n",
      "    while True:\n",
      "        it = it + 1\n",
      "        V_old = V.copy()\n",
      "        for s in S:\n",
      "            Q = [0 for i in range(16)]\n",
      "            for a in A:\n",
      "                Q[a] = reward(s) + discount*psum(s, a, V_old)\n",
      "            policy[s] = np.argmax(Q)\n",
      "            print Q\n",
      "            V[s] = Q[policy[s]]\n",
      "        if np.allclose(V, V_old, atol=0.001):\n",
      "            break\n",
      "    return V, policy, it\n",
      "6/58:\n",
      "def value_iteration(discount):\n",
      "    V = np.array([0 for i in range(states)])\n",
      "    policy = [0 for i in range(states)]\n",
      "    it = 0\n",
      "    while True:\n",
      "        it = it + 1\n",
      "        V_old = V.copy()\n",
      "        for s in S:\n",
      "            Q = [0 for i in range(16)]\n",
      "            for a in A:\n",
      "                Q[a] = reward(s) + discount*psum(s, a, V_old)\n",
      "            policy[s] = np.argmax(Q)\n",
      "#             print Q\n",
      "            V[s] = Q[policy[s]]\n",
      "        if np.allclose(V, V_old, atol=0.001):\n",
      "            break\n",
      "    return V, policy, it\n",
      "6/59:\n",
      "V_, policy_, it = value_iteration(.1)\n",
      "print V_\n",
      "print policy_\n",
      "print \"iterations: \" + str(it)\n",
      "6/60:\n",
      "def value_iteration(discount):\n",
      "    V = np.array([0 for i in range(states)])\n",
      "    policy = [0 for i in range(states)]\n",
      "    it = 0\n",
      "    while True:\n",
      "        it = it + 1\n",
      "        V_old = V.copy()\n",
      "        for s in S:\n",
      "            Q = [0 for i in range(16)]\n",
      "            for a in A:\n",
      "                Q[a] = reward(s) + discount*psum(s, a, V_old)\n",
      "            policy[s] = np.argmax(Q)\n",
      "            V[s] = Q[policy[s]]\n",
      "        print s, V\n",
      "        if np.allclose(V, V_old, atol=0.001):\n",
      "            break\n",
      "    return V, policy, it\n",
      "6/61:\n",
      "V_, policy_, it = value_iteration(.1)\n",
      "print V_\n",
      "print policy_\n",
      "print \"iterations: \" + str(it)\n",
      "6/62:\n",
      "def psum(s, a, V):\n",
      "    sum = 0\n",
      "    sum += 0.6*V[grid(\"\", a)+s]\n",
      "    sum += 0.1*V[grid(\"up\", a)+s] + 0.1*V[grid(\"down\", a)+s]\n",
      "    sum += 0.1*V[grid(\"left\", a)+s] + 0.1*V[grid(\"right\", a)+s]\n",
      "    return sum\n",
      "6/63:\n",
      "def value_iteration(discount):\n",
      "    V = np.array([0 for i in range(states)])\n",
      "    policy = [0 for i in range(states)]\n",
      "    it = 0\n",
      "    while True:\n",
      "        it = it + 1\n",
      "        V_old = V.copy()\n",
      "        for s in S:\n",
      "            Q = [0 for i in range(16)]\n",
      "            for a in A:\n",
      "                Q[a] = reward(s) + discount*psum(s, a, V_old)\n",
      "            policy[s] = np.argmax(Q)\n",
      "            V[s] = Q[policy[s]]\n",
      "        print s, V\n",
      "        if np.allclose(V, V_old, atol=0.001):\n",
      "            break\n",
      "    return V, policy, it\n",
      "6/64:\n",
      "V_, policy_, it = value_iteration(.1)\n",
      "print V_\n",
      "print policy_\n",
      "print \"iterations: \" + str(it)\n",
      "6/65:\n",
      "def value_iteration(discount):\n",
      "    V = np.array([0 for i in range(states+1)])\n",
      "    policy = [0 for i in range(states)]\n",
      "    it = 0\n",
      "    while True:\n",
      "        it = it + 1\n",
      "        V_old = V.copy()\n",
      "        for s in S:\n",
      "            Q = [0 for i in range(16)]\n",
      "            for a in A:\n",
      "                Q[a] = reward(s) + discount*psum(s, a, V_old)\n",
      "            policy[s] = np.argmax(Q)\n",
      "            V[s] = Q[policy[s]]\n",
      "        print s, V\n",
      "        if np.allclose(V, V_old, atol=0.001):\n",
      "            break\n",
      "    return V, policy, it\n",
      "6/66:\n",
      "V_, policy_, it = value_iteration(.1)\n",
      "print V_\n",
      "print policy_\n",
      "print \"iterations: \" + str(it)\n",
      "6/67:\n",
      "def psum(s, a, V):\n",
      "    if s > 100:\n",
      "        return 0\n",
      "    sum = 0\n",
      "    sum += 0.6*V[grid(\"\", a)+s]\n",
      "    sum += 0.1*V[grid(\"up\", a)+s] + 0.1*V[grid(\"down\", a)+s]\n",
      "    sum += 0.1*V[grid(\"left\", a)+s] + 0.1*V[grid(\"right\", a)+s]\n",
      "    return sum\n",
      "6/68:\n",
      "def value_iteration(discount):\n",
      "    V = np.array([0 for i in range(states)])\n",
      "    policy = [0 for i in range(states)]\n",
      "    it = 0\n",
      "    while True:\n",
      "        it = it + 1\n",
      "        V_old = V.copy()\n",
      "        for s in S:\n",
      "            Q = [0 for i in range(16)]\n",
      "            for a in A:\n",
      "                Q[a] = reward(s) + discount*psum(s, a, V_old)\n",
      "            policy[s] = np.argmax(Q)\n",
      "            V[s] = Q[policy[s]]\n",
      "        print s, V\n",
      "        if np.allclose(V, V_old, atol=0.001):\n",
      "            break\n",
      "    return V, policy, it\n",
      "6/69:\n",
      "V_, policy_, it = value_iteration(.1)\n",
      "print V_\n",
      "print policy_\n",
      "print \"iterations: \" + str(it)\n",
      "6/70:\n",
      "def psum(s, a, V):\n",
      "    if s > 101:\n",
      "        return 0\n",
      "    sum = 0\n",
      "    sum += 0.6*V[grid(\"\", a)+s]\n",
      "    sum += 0.1*V[grid(\"up\", a)+s] + 0.1*V[grid(\"down\", a)+s]\n",
      "    sum += 0.1*V[grid(\"left\", a)+s] + 0.1*V[grid(\"right\", a)+s]\n",
      "    return sum\n",
      "6/71:\n",
      "def value_iteration(discount):\n",
      "    V = np.array([0 for i in range(states)])\n",
      "    policy = [0 for i in range(states)]\n",
      "    it = 0\n",
      "    while True:\n",
      "        it = it + 1\n",
      "        V_old = V.copy()\n",
      "        for s in S:\n",
      "            Q = [0 for i in range(16)]\n",
      "            for a in A:\n",
      "                Q[a] = reward(s) + discount*psum(s, a, V_old)\n",
      "            policy[s] = np.argmax(Q)\n",
      "            V[s] = Q[policy[s]]\n",
      "        print s, V\n",
      "        if np.allclose(V, V_old, atol=0.001):\n",
      "            break\n",
      "    return V, policy, it\n",
      "6/72:\n",
      "V_, policy_, it = value_iteration(.1)\n",
      "print V_\n",
      "print policy_\n",
      "print \"iterations: \" + str(it)\n",
      "6/73:\n",
      "def psum(s, a, V):\n",
      "    if s > 100:\n",
      "        return 0\n",
      "    sum = 0\n",
      "    sum += 0.6*V[grid(\"\", a)+s]\n",
      "    sum += 0.1*V[grid(\"up\", a)+s] + 0.1*V[grid(\"down\", a)+s]\n",
      "    sum += 0.1*V[grid(\"left\", a)+s] + 0.1*V[grid(\"right\", a)+s]\n",
      "    return sum\n",
      "6/74:\n",
      "def value_iteration(discount):\n",
      "    V = np.array([0 for i in range(states)])\n",
      "    policy = [0 for i in range(states)]\n",
      "    it = 0\n",
      "    while True:\n",
      "        it = it + 1\n",
      "        V_old = V.copy()\n",
      "        for s in S:\n",
      "            Q = [0 for i in range(16)]\n",
      "            for a in A:\n",
      "                Q[a] = reward(s) + discount*psum(s, a, V_old)\n",
      "            policy[s] = np.argmax(Q)\n",
      "            V[s] = Q[policy[s]]\n",
      "        print s, V\n",
      "        if np.allclose(V, V_old, atol=0.001):\n",
      "            break\n",
      "    return V, policy, it\n",
      "6/75:\n",
      "V_, policy_, it = value_iteration(.1)\n",
      "print V_\n",
      "print policy_\n",
      "print \"iterations: \" + str(it)\n",
      "6/76:\n",
      "V_, policy_, it = value_iteration(1)\n",
      "print V_\n",
      "print policy_\n",
      "print \"iterations: \" + str(it)\n",
      "6/77:\n",
      "V_, policy_, it = value_iteration(.01)\n",
      "print V_\n",
      "print policy_\n",
      "print \"iterations: \" + str(it)\n",
      "6/78:\n",
      "V_, policy_, it = value_iteration(.9)\n",
      "print V_\n",
      "print policy_\n",
      "print \"iterations: \" + str(it)\n",
      "6/79:\n",
      "def value_iteration(discount):\n",
      "    V = np.array([0 for i in range(states)])\n",
      "    policy = [0 for i in range(states)]\n",
      "    it = 0\n",
      "    while True:\n",
      "        it = it + 1\n",
      "        V_old = V.copy()\n",
      "        for s in S:\n",
      "            Q = [0 for i in range(16)]\n",
      "            for a in A:\n",
      "                Q[a] = reward(s) + discount*psum(s, a, V_old)\n",
      "            policy[s] = np.argmax(Q)\n",
      "            V[s] = Q[policy[s]]\n",
      "        print s, V\n",
      "        if np.allclose(V, V_old, atol=0.00001):\n",
      "            break\n",
      "    return V, policy, it\n",
      "6/80:\n",
      "V_, policy_, it = value_iteration(.9)\n",
      "print V_\n",
      "print policy_\n",
      "print \"iterations: \" + str(it)\n",
      "6/81:\n",
      "def value_iteration(discount):\n",
      "    V = np.array([0 for i in range(states)])\n",
      "    policy = [0 for i in range(states)]\n",
      "    it = 0\n",
      "    while True:\n",
      "        it = it + 1\n",
      "        V_old = V.copy()\n",
      "        for s in S:\n",
      "            Q = [0 for i in range(16)]\n",
      "            for a in A:\n",
      "                Q[a] = reward(s) + discount*psum(s, a, V_old)\n",
      "            policy[s] = np.argmax(Q)\n",
      "            V[s] = Q[policy[s]]\n",
      "        print s, V\n",
      "        if np.allclose(V, V_old, atol=0.0001):\n",
      "            break\n",
      "    return V, policy, it\n",
      "6/82:\n",
      "V_, policy_, it = value_iteration(.9)\n",
      "print V_\n",
      "print policy_\n",
      "print \"iterations: \" + str(it)\n",
      "6/83:\n",
      "def value_iteration(discount):\n",
      "    V = np.array([0 for i in range(states)])\n",
      "    policy = [0 for i in range(states)]\n",
      "    it = 0\n",
      "    while True:\n",
      "        it = it + 1\n",
      "        V_old = V.copy()\n",
      "        for s in S:\n",
      "            Q = [0 for i in range(16)]\n",
      "            for a in A:\n",
      "                Q[a] = reward(s) + discount*psum(s, a, V_old)\n",
      "            policy[s] = np.argmax(Q)\n",
      "            V[s] = Q[policy[s]]\n",
      "        print s, V, policy, Q\n",
      "        if np.allclose(V, V_old, atol=0.0001):\n",
      "            break\n",
      "    return V, policy, it\n",
      "6/84:\n",
      "V_, policy_, it = value_iteration(.9)\n",
      "print V_\n",
      "print policy_\n",
      "print \"iterations: \" + str(it)\n",
      "6/85:\n",
      "def value_iteration(discount):\n",
      "    V = np.array([0 for i in range(states)])\n",
      "    policy = [0 for i in range(states)]\n",
      "    it = 0\n",
      "    while True:\n",
      "        it = it + 1\n",
      "        V_old = V.copy()\n",
      "        for s in S:\n",
      "            Q = [0 for i in range(16)]\n",
      "            for a in A:\n",
      "                Q[a] = reward(s) + discount*psum(s, a, V_old)\n",
      "            print \"state \" + str(s) + \" Q \", Q\n",
      "            policy[s] = np.argmax(Q)\n",
      "            V[s] = Q[policy[s]]\n",
      "        print s, V, policy\n",
      "        if np.allclose(V, V_old, atol=0.0001):\n",
      "            break\n",
      "    return V, policy, it\n",
      "6/86:\n",
      "V_, policy_, it = value_iteration(.9)\n",
      "print V_\n",
      "print policy_\n",
      "print \"iterations: \" + str(it)\n",
      "6/87:\n",
      "def value_iteration(discount):\n",
      "    V = np.array([0 for i in range(states)])\n",
      "    policy = [0 for i in range(states)]\n",
      "    it = 0\n",
      "    while True:\n",
      "        it = it + 1\n",
      "        V_old = V.copy()\n",
      "        for s in S:\n",
      "            Q = [0 for i in range(16)]\n",
      "            for a in A:\n",
      "                Q[a] = reward(s) + discount*psum(s, a, V_old)\n",
      "#             print \"state \" + str(s) + \" Q \", Q\n",
      "            policy[s] = np.argmax(Q)\n",
      "            print \"state \" + str(s) + \" policy \", policy\n",
      "            V[s] = Q[policy[s]]\n",
      "        print s, V, policy\n",
      "        if np.allclose(V, V_old, atol=0.0001):\n",
      "            break\n",
      "    return V, policy, it\n",
      "6/88:\n",
      "V_, policy_, it = value_iteration(.9)\n",
      "print V_\n",
      "print policy_\n",
      "print \"iterations: \" + str(it)\n",
      "6/89:\n",
      "def value_iteration(discount):\n",
      "    V = np.array([0 for i in range(states)])\n",
      "    policy = [0 for i in range(states)]\n",
      "    it = 0\n",
      "    while True:\n",
      "        it = it + 1\n",
      "        V_old = V.copy()\n",
      "        for s in S:\n",
      "            Q = [0 for i in range(16)]\n",
      "            for a in A:\n",
      "                Q[a] = reward(s) + discount*psum(s, a, V_old)\n",
      "#             print \"state \" + str(s) + \" Q \", Q\n",
      "            policy[s] = np.argmax(Q)\n",
      "            V[s] = Q[policy[s]]\n",
      "            print \"V \" + str(V[s])\n",
      "        print s, V, policy\n",
      "        if np.allclose(V, V_old, atol=0.0001):\n",
      "            break\n",
      "    return V, policy, it\n",
      "6/90:\n",
      "V_, policy_, it = value_iteration(.9)\n",
      "print V_\n",
      "print policy_\n",
      "print \"iterations: \" + str(it)\n",
      "6/91:\n",
      "def value_iteration(discount):\n",
      "    V = np.array([0 for i in range(states)])\n",
      "    policy = [0 for i in range(states)]\n",
      "    it = 0\n",
      "    while True:\n",
      "        it = it + 1\n",
      "        V_old = V.copy()\n",
      "        for s in S:\n",
      "            Q = [0 for i in range(16)]\n",
      "            for a in A:\n",
      "                Q[a] = reward(s) + discount*psum(s, a, V_old)\n",
      "#             print \"state \" + str(s) + \" Q \", Q\n",
      "            policy[s] = np.argmax(Q)\n",
      "            V[s] = Q[policy[s]]\n",
      "            print \"V \" + str(V[s]) + Q[100]\n",
      "        print s, V, policy\n",
      "        if np.allclose(V, V_old, atol=0.0001):\n",
      "            break\n",
      "    return V, policy, it\n",
      "6/92:\n",
      "V_, policy_, it = value_iteration(.9)\n",
      "print V_\n",
      "print policy_\n",
      "print \"iterations: \" + str(it)\n",
      "6/93:\n",
      "def value_iteration(discount):\n",
      "    V = np.array([0 for i in range(states)])\n",
      "    policy = [0 for i in range(states)]\n",
      "    it = 0\n",
      "    while True:\n",
      "        it = it + 1\n",
      "        V_old = V.copy()\n",
      "        for s in S:\n",
      "            Q = [0 for i in range(16)]\n",
      "            for a in A:\n",
      "                Q[a] = reward(s) + discount*psum(s, a, V_old)\n",
      "#             print \"state \" + str(s) + \" Q \", Q\n",
      "            policy[s] = np.argmax(Q)\n",
      "            V[s] = Q[policy[s]]\n",
      "            print \"V \" + str(V[s]) + Q[5]\n",
      "        print s, V, policy\n",
      "        if np.allclose(V, V_old, atol=0.0001):\n",
      "            break\n",
      "    return V, policy, it\n",
      "6/94:\n",
      "V_, policy_, it = value_iteration(.9)\n",
      "print V_\n",
      "print policy_\n",
      "print \"iterations: \" + str(it)\n",
      "6/95:\n",
      "def value_iteration(discount):\n",
      "    V = np.array([0.0 for i in range(states)])\n",
      "    policy = [0 for i in range(states)]\n",
      "    it = 0\n",
      "    while True:\n",
      "        it = it + 1\n",
      "        V_old = V.copy()\n",
      "        for s in S:\n",
      "            Q = [0 for i in range(16)]\n",
      "            for a in A:\n",
      "                Q[a] = reward(s) + discount*psum(s, a, V_old)\n",
      "#             print \"state \" + str(s) + \" Q \", Q\n",
      "            policy[s] = np.argmax(Q)\n",
      "            V[s] = Q[policy[s]]\n",
      "        print s, V, policy\n",
      "        if np.allclose(V, V_old, atol=0.0001):\n",
      "            break\n",
      "    return V, policy, it\n",
      "6/96:\n",
      "V_, policy_, it = value_iteration(.9)\n",
      "print V_\n",
      "print policy_\n",
      "print \"iterations: \" + str(it)\n",
      "6/97:\n",
      "def value_iteration(discount):\n",
      "    V = np.array([0.0 for i in range(states)])\n",
      "    policy = [0 for i in range(states)]\n",
      "    it = 0\n",
      "    while True:\n",
      "        it = it + 1\n",
      "        V_old = V.copy()\n",
      "        for s in S:\n",
      "            Q = [0 for i in range(16)]\n",
      "            for a in A:\n",
      "                Q[a] = reward(s) + discount*psum(s, a, V_old)\n",
      "            policy[s] = np.argmax(Q)\n",
      "            V[s] = Q[policy[s]]\n",
      "        if np.allclose(V, V_old, atol=0.0001):\n",
      "            break\n",
      "    return V, policy, it\n",
      "6/98:\n",
      "V_, policy_, it = value_iteration(.9)\n",
      "print V_\n",
      "print policy_\n",
      "print \"iterations: \" + str(it)\n",
      "6/99:\n",
      "def value_iteration(discount):\n",
      "    V = np.array([0.0 for i in range(states)])\n",
      "    policy = [0 for i in range(states)]\n",
      "    it = 0\n",
      "    while True:\n",
      "        it = it + 1\n",
      "        V_old = V.copy()\n",
      "        for s in S:\n",
      "            Q = [0 for i in range(16)]\n",
      "            for a in A:\n",
      "                Q[a] = reward(s) + discount*psum(s, a, V_old)\n",
      "            policy[s] = np.argmax(Q)\n",
      "            V[s] = Q[policy[s]]\n",
      "        if np.allclose(V, V_old, atol=0.00001):\n",
      "            break\n",
      "    return V, policy, it\n",
      "6/100:\n",
      "V_, policy_, it = value_iteration(.9)\n",
      "print V_\n",
      "print policy_\n",
      "print \"iterations: \" + str(it)\n",
      "6/101:\n",
      "def value_iteration(discount):\n",
      "    V = np.array([0.0 for i in range(states)])\n",
      "    policy = [0 for i in range(states)]\n",
      "    it = 0\n",
      "    while True:\n",
      "        it = it + 1\n",
      "        V_old = V.copy()\n",
      "        for s in S:\n",
      "            Q = [0 for i in range(16)]\n",
      "            for a in A:\n",
      "                Q[a] = reward(s) + discount*psum(s, a, V_old)\n",
      "            policy[s] = np.argmax(Q)\n",
      "            V[s] = Q[policy[s]]\n",
      "        if np.allclose(V, V_old, atol=0.0000000001):\n",
      "            break\n",
      "    return V, policy, it\n",
      "6/102:\n",
      "V_, policy_, it = value_iteration(.9)\n",
      "print V_\n",
      "print policy_\n",
      "print \"iterations: \" + str(it)\n",
      "6/103:\n",
      "def value_iteration(discount):\n",
      "    V = np.array([0.0 for i in range(states)])\n",
      "    policy = [0 for i in range(states)]\n",
      "    it = 0\n",
      "    while True:\n",
      "        it = it + 1\n",
      "        V_old = V.copy()\n",
      "        for s in S:\n",
      "            Q = [0 for i in range(16)]\n",
      "            for a in A:\n",
      "                Q[a] = reward(s) + discount*psum(s, a, V_old)\n",
      "            policy[s] = np.argmax(Q)\n",
      "            V[s] = Q[policy[s]]\n",
      "        if np.allclose(V, V_old, atol=1e-20):\n",
      "            break\n",
      "    return V, policy, it\n",
      "6/104:\n",
      "V_, policy_, it = value_iteration(.9)\n",
      "print V_\n",
      "print policy_\n",
      "print \"iterations: \" + str(it)\n",
      "6/105:\n",
      "def value_iteration(discount):\n",
      "    V = np.array([0.0 for i in range(states)])\n",
      "    policy = [0 for i in range(states)]\n",
      "    it = 0\n",
      "    while True:\n",
      "        it = it + 1\n",
      "        V_old = V.copy()\n",
      "        for s in S:\n",
      "            Q = [0 for i in range(16)]\n",
      "            for a in A:\n",
      "                Q[a] = reward(s) + discount*psum(s, a, V_old)\n",
      "            policy[s] = np.argmax(Q)\n",
      "            V[s] = Q[policy[s]]\n",
      "        if np.allclose(V, V_old, atol=1e-50):\n",
      "            break\n",
      "    return V, policy, it\n",
      "6/106:\n",
      "V_, policy_, it = value_iteration(.9)\n",
      "print V_\n",
      "print policy_\n",
      "print \"iterations: \" + str(it)\n",
      "6/107:\n",
      "def value_iteration(discount):\n",
      "    V = np.array([0.0 for i in range(states)])\n",
      "    policy = [0 for i in range(states)]\n",
      "    it = 0\n",
      "    while True:\n",
      "        it = it + 1\n",
      "        V_old = V.copy()\n",
      "        for s in S:\n",
      "            Q = [0 for i in range(16)]\n",
      "            for a in A:\n",
      "                Q[a] = reward(s) + discount*psum(s, a, V_old)\n",
      "            policy[s] = np.argmax(Q)\n",
      "            V[s] = Q[policy[s]]\n",
      "        if np.allclose(V, V_old, atol=1e-100):\n",
      "            break\n",
      "    return V, policy, it\n",
      "6/108:\n",
      "V_, policy_, it = value_iteration(.9)\n",
      "print V_\n",
      "print policy_\n",
      "print \"iterations: \" + str(it)\n",
      "6/109:\n",
      "def value_iteration(gamma, epsilon):\n",
      "    V = np.array([0.0 for i in range(states)])\n",
      "    policy = [0 for i in range(states)]\n",
      "    it = 0\n",
      "    while True:\n",
      "        it = it + 1\n",
      "        V_old = V.copy()\n",
      "        for s in S:\n",
      "            Q = [0 for i in range(16)]\n",
      "            for a in A:\n",
      "                Q[a] = reward(s) + gamma*psum(s, a, V_old)\n",
      "            policy[s] = np.argmax(Q)\n",
      "            V[s] = Q[policy[s]]\n",
      "        if np.allclose(V, V_old, atol=epsilon):\n",
      "            break\n",
      "    return V, policy, it\n",
      "6/110:\n",
      "V_, policy_, it = value_iteration(gamma=.9, epsilon=1e-100)\n",
      "print V_\n",
      "print policy_\n",
      "print \"iterations: \" + str(it)\n",
      "6/111:\n",
      "V_, policy_, it = value_iteration(gamma=.5, epsilon=1e-100)\n",
      "print V_\n",
      "print policy_\n",
      "print \"iterations: \" + str(it)\n",
      "6/112:\n",
      "V_, policy_, it = value_iteration(gamma=1, epsilon=1e-100)\n",
      "print V_\n",
      "print policy_\n",
      "print \"iterations: \" + str(it)\n",
      "6/113:\n",
      "V_, policy_, it = value_iteration(gamma=1, epsilon=1e-1000)\n",
      "print V_\n",
      "print policy_\n",
      "print \"iterations: \" + str(it)\n",
      "6/114:\n",
      "V_, policy_, it = value_iteration(gamma=1, epsilon=0)\n",
      "print V_\n",
      "print policy_\n",
      "print \"iterations: \" + str(it)\n",
      "6/115:\n",
      "V_, policy_, it = value_iteration(gamma=.9, epsilon=0)\n",
      "print V_\n",
      "print policy_\n",
      "print \"iterations: \" + str(it)\n",
      "6/116:\n",
      "V_, policy_, it = value_iteration(gamma=.5, epsilon=0)\n",
      "print V_\n",
      "print policy_\n",
      "print \"iterations: \" + str(it)\n",
      "6/117:\n",
      "V_, policy_, it = value_iteration(gamma=.5, epsilon=0)\n",
      "print V_\n",
      "print policy_\n",
      "print \"iterations: \" + str(it)\n",
      "6/118:\n",
      "V_, policy_, it = value_iteration(gamma=.9, epsilon=0)\n",
      "print V_\n",
      "print policy_\n",
      "print \"iterations: \" + str(it)\n",
      "6/119:\n",
      "V_, policy_, it = value_iteration(gamma=1, epsilon=0)\n",
      "print V_\n",
      "print policy_\n",
      "print \"iterations: \" + str(it)\n",
      "6/120:\n",
      "V_, policy_, it = value_iteration(gamma=.1, epsilon=0)\n",
      "print V_\n",
      "print policy_\n",
      "print \"iterations: \" + str(it)\n",
      "6/121:\n",
      "import math\n",
      "import numpy as np\n",
      "import random\n",
      "6/122:\n",
      "def simulate(policy, trials):\n",
      "    successes = 0\n",
      "    for i in range(trials):\n",
      "        score = 0\n",
      "        while score < 101:\n",
      "            r = random.random()\n",
      "            p = policy[score]\n",
      "            if r < 0.6:\n",
      "                score += grid(\"\", p)\n",
      "            elif r < 0.7:\n",
      "                score += grid(\"up\", p)\n",
      "            elif r < 0.8:\n",
      "                score += grid(\"down\", p)\n",
      "            elif r < 0.9:\n",
      "                score += grid(\"left\", p)\n",
      "            else:\n",
      "                score += grid(\"right\", p)\n",
      "        successes += 1 if score == 101 else 0\n",
      "        print score\n",
      "    print float(successes)/trials\n",
      "    return\n",
      "6/123:\n",
      "V1, policy1, it = value_iteration(gamma=.1, epsilon=0)\n",
      "print V1\n",
      "print policy1\n",
      "print \"iterations: \" + str(it)\n",
      "6/124:\n",
      "V9, policy9, it = value_iteration(gamma=.9, epsilon=0)\n",
      "print V9\n",
      "print policy9\n",
      "print \"iterations: \" + str(it)\n",
      "6/125: simulate(policy1, 100)\n",
      "6/126:\n",
      "def simulate(policy, trials):\n",
      "    successes = 0\n",
      "    for i in range(trials):\n",
      "        score = 0\n",
      "        while score < 101:\n",
      "            r = random.random()\n",
      "            p = policy[score]\n",
      "            if r < 0.6:\n",
      "                score += grid(\"\", p)\n",
      "            elif r < 0.7:\n",
      "                score += grid(\"up\", p)\n",
      "            elif r < 0.8:\n",
      "                score += grid(\"down\", p)\n",
      "            elif r < 0.9:\n",
      "                score += grid(\"left\", p)\n",
      "            else:\n",
      "                score += grid(\"right\", p)\n",
      "        successes += 1 if score == 101 else 0\n",
      "    print float(successes)/trials\n",
      "    return\n",
      "6/127: simulate(policy1, 100)\n",
      "6/128: simulate(policy1, 1000)\n",
      "6/129: simulate(policy1, 10000)\n",
      "6/130: simulate(policy1, 10000)\n",
      "6/131: simulate(policy1, 100000)\n",
      "6/132: simulate(policy1, 10000)\n",
      "6/133: simulate(policy9, 10000)\n",
      "6/134: simulate(policy9, 10000)\n",
      "6/135: simulate(policy9, 10000)\n",
      "6/136: simulate(policy_, 10000)\n",
      "6/137: simulate(policy_, 10000)\n",
      "6/138: simulate(policy_, 10000)\n",
      "6/139:\n",
      "simulate(policy1, 10000)\n",
      "simulate(policy1, 10000)\n",
      "simulate(policy1, 10000)\n",
      "6/140:\n",
      "simulate(policy9, 10000)\n",
      "simulate(policy9, 10000)\n",
      "simulate(policy9, 10000)\n",
      "6/141:\n",
      "simulate(policy_, 10000)\n",
      "simulate(policy_, 10000)\n",
      "simulate(policy_, 10000)\n",
      "6/142: V99, policy99, it = value_iteration(gamma=.99, epsilon=0)\n",
      "6/143:\n",
      "simulate(policy99, 10000)\n",
      "simulate(policy99, 10000)\n",
      "simulate(policy99, 10000)\n",
      "6/144: V99, policy99, it = value_iteration(gamma=.999, epsilon=0)\n",
      "6/145:\n",
      "simulate(policy99, 10000)\n",
      "simulate(policy99, 10000)\n",
      "simulate(policy99, 10000)\n",
      "6/146: V99, policy99, it = value_iteration(gamma=.9999, epsilon=0)\n",
      "6/147:\n",
      "simulate(policy99, 10000)\n",
      "simulate(policy99, 10000)\n",
      "simulate(policy99, 10000)\n",
      "6/148:\n",
      "simulate(policy99, 10000)\n",
      "simulate(policy99, 10000)\n",
      "simulate(policy99, 10000)\n",
      "6/149:\n",
      "simulate(policy99, 10000)\n",
      "simulate(policy99, 10000)\n",
      "simulate(policy99, 10000)\n",
      "6/150: V99, policy99, it = value_iteration(gamma=.999, epsilon=0)\n",
      "6/151:\n",
      "simulate(policy99, 10000)\n",
      "simulate(policy99, 10000)\n",
      "simulate(policy99, 10000)\n",
      "6/152:\n",
      "simulate(policy99, 10000)\n",
      "simulate(policy99, 10000)\n",
      "simulate(policy99, 10000)\n",
      "6/153: V99, policy99, it = value_iteration(gamma=.99, epsilon=0)\n",
      "6/154:\n",
      "simulate(policy99, 10000)\n",
      "simulate(policy99, 10000)\n",
      "simulate(policy99, 10000)\n",
      "6/155: V99, policy99, it = value_iteration(gamma=.95, epsilon=0)\n",
      "6/156:\n",
      "simulate(policy99, 10000)\n",
      "simulate(policy99, 10000)\n",
      "simulate(policy99, 10000)\n",
      "6/157:\n",
      "simulate(policy99, 10000)\n",
      "simulate(policy99, 10000)\n",
      "simulate(policy99, 10000)\n",
      "6/158: V99, policy99, it = value_iteration(gamma=.97, epsilon=0)\n",
      "6/159:\n",
      "simulate(policy99, 10000)\n",
      "simulate(policy99, 10000)\n",
      "simulate(policy99, 10000)\n",
      "6/160:\n",
      "simulate(policy99, 10000)\n",
      "simulate(policy99, 10000)\n",
      "simulate(policy99, 10000)\n",
      "6/161: V99, policy99, it = value_iteration(gamma=.99, epsilon=0)\n",
      "6/162:\n",
      "simulate(policy99, 10000)\n",
      "simulate(policy99, 10000)\n",
      "simulate(policy99, 10000)\n",
      "6/163:\n",
      "simulate(policy99, 10000)\n",
      "simulate(policy99, 10000)\n",
      "simulate(policy99, 10000)\n",
      "6/164:\n",
      "simulate(policy9, 10000)\n",
      "simulate(policy9, 10000)\n",
      "simulate(policy9, 10000)\n",
      "6/165:\n",
      "simulate(policy9, 10000)\n",
      "simulate(policy9, 10000)\n",
      "simulate(policy9, 10000)\n",
      "6/166:\n",
      "simulate(policy9, 10000)\n",
      "simulate(policy9, 10000)\n",
      "simulate(policy9, 10000)\n",
      "6/167:\n",
      "simulate(policy99, 10000)\n",
      "simulate(policy99, 10000)\n",
      "simulate(policy99, 10000)\n",
      "6/168:\n",
      "simulate(policy99, 10000)\n",
      "simulate(policy99, 10000)\n",
      "simulate(policy99, 10000)\n",
      "6/169:\n",
      "V99, policy99, it = value_iteration(gamma=.99, epsilon=0)\n",
      "print V_\n",
      "print policy_\n",
      "print \"iterations: \" + str(it)\n",
      "6/170:\n",
      "V99, policy99, it = value_iteration(gamma=.99, epsilon=0)\n",
      "print V_\n",
      "print policy_\n",
      "print \"iterations: \" + str(it)\n",
      "6/171:\n",
      "V99, policy99, it = value_iteration(gamma=.99, epsilon=0)\n",
      "print V_\n",
      "print policy_\n",
      "print \"iterations: \" + str(it)\n",
      "6/172:\n",
      "simulate(policy9, 10000)\n",
      "simulate(policy9, 10000)\n",
      "simulate(policy9, 10000)\n",
      "6/173:\n",
      "simulate(policy1, 10000)\n",
      "simulate(policy1, 10000)\n",
      "simulate(policy1, 10000)\n",
      "6/174:\n",
      "simulate(policy_, 10000)\n",
      "simulate(policy_, 10000)\n",
      "simulate(policy_, 10000)\n",
      "6/175:\n",
      "simulate(policy99, 10000)\n",
      "simulate(policy99, 10000)\n",
      "simulate(policy99, 10000)\n",
      "6/176:\n",
      "simulate(policy1, 10000)\n",
      "simulate(policy1, 10000)\n",
      "simulate(policy1, 10000)\n",
      "6/177:\n",
      "simulate(policy9, 10000)\n",
      "simulate(policy9, 10000)\n",
      "simulate(policy9, 10000)\n",
      "6/178:\n",
      "simulate(policy_, 10000)\n",
      "simulate(policy_, 10000)\n",
      "simulate(policy_, 10000)\n",
      "6/179:\n",
      "simulate(policy99, 10000)\n",
      "simulate(policy99, 10000)\n",
      "simulate(policy99, 10000)\n",
      "6/180:\n",
      "simulate(policy99, 10000)\n",
      "simulate(policy99, 10000)\n",
      "simulate(policy99, 10000)\n",
      "6/181:\n",
      "simulate(policy1, 10000)\n",
      "simulate(policy1, 10000)\n",
      "simulate(policy1, 10000)\n",
      "6/182:\n",
      "simulate(policy9, 10000)\n",
      "simulate(policy9, 10000)\n",
      "simulate(policy9, 10000)\n",
      "6/183:\n",
      "simulate(policy99, 10000)\n",
      "simulate(policy99, 10000)\n",
      "simulate(policy99, 10000)\n",
      "6/184:\n",
      "def simulate(policy, trials):\n",
      "    successes = 0\n",
      "    for i in range(trials):\n",
      "        score = 0\n",
      "        while score < 101:\n",
      "            r = random.random()\n",
      "            p = policy[score]\n",
      "            if r < 0.6:\n",
      "                score += grid(\"\", p)\n",
      "            elif r < 0.7:\n",
      "                score += grid(\"up\", p)\n",
      "            elif r < 0.8:\n",
      "                score += grid(\"down\", p)\n",
      "            elif r < 0.9:\n",
      "                score += grid(\"left\", p)\n",
      "            else:\n",
      "                score += grid(\"right\", p)\n",
      "        successes += 1 if score == 101 else 0\n",
      "    print float(successes)/trials\n",
      "    return\n",
      "6/185:\n",
      "def simulate(policy, trials):\n",
      "    successes = 0\n",
      "    for i in range(trials):\n",
      "        score = 0\n",
      "        while score < 101:\n",
      "            r = random.random()\n",
      "            p = policy[score]\n",
      "            if r < 0.6:\n",
      "                score += grid(\"\", p)\n",
      "            elif r < 0.7:\n",
      "                score += grid(\"up\", p)\n",
      "            elif r < 0.8:\n",
      "                score += grid(\"down\", p)\n",
      "            elif r < 0.9:\n",
      "                score += grid(\"left\", p)\n",
      "            else:\n",
      "                score += grid(\"right\", p)\n",
      "            print score\n",
      "        successes += 1 if score == 101 else 0\n",
      "    print float(successes)/trials\n",
      "    return\n",
      "6/186:\n",
      "simulate(policy99, 10000)\n",
      "simulate(policy99, 10000)\n",
      "simulate(policy99, 10000)\n",
      "6/187:\n",
      "def simulate(policy, trials):\n",
      "    successes = 0\n",
      "    for i in range(trials):\n",
      "        score = 0\n",
      "        while score < 101:\n",
      "            r = random.random()\n",
      "            p = policy[score]\n",
      "            if r < 0.6:\n",
      "                score += grid(\"\", p)\n",
      "            elif r < 0.7:\n",
      "                score += grid(\"up\", p)\n",
      "            elif r < 0.8:\n",
      "                score += grid(\"down\", p)\n",
      "            elif r < 0.9:\n",
      "                score += grid(\"left\", p)\n",
      "            else:\n",
      "                score += grid(\"right\", p)\n",
      "        successes += 1 if score == 101 else 0\n",
      "    print float(successes)/trials\n",
      "    return\n",
      "6/188:\n",
      "simulate(policy99, 10000)\n",
      "simulate(policy99, 10000)\n",
      "simulate(policy99, 10000)\n",
      "6/189:\n",
      "simulate(policy99, 10000)\n",
      "simulate(policy99, 10000)\n",
      "simulate(policy99, 10000)\n",
      "6/190:\n",
      "simulate(policy99, 10000)\n",
      "simulate(policy99, 10000)\n",
      "simulate(policy99, 10000)\n",
      "6/191:\n",
      "def value_iteration(gamma, epsilon):\n",
      "    V = np.array([0.0 for i in range(states)])\n",
      "    policy = [0 for i in range(states)]\n",
      "    it = 0\n",
      "    vfuncs = [V]\n",
      "    while True:\n",
      "        it = it + 1\n",
      "        V_old = V.copy()\n",
      "        for s in S:\n",
      "            Q = [0 for i in range(16)]\n",
      "            for a in A:\n",
      "                Q[a] = reward(s) + gamma*psum(s, a, V_old)\n",
      "            policy[s] = np.argmax(Q)\n",
      "            V[s] = Q[policy[s]]\n",
      "        vfuncs.append(V)\n",
      "        if np.allclose(V, V_old, atol=epsilon):\n",
      "            break\n",
      "    return V, policy, it\n",
      "6/192:\n",
      "def value_iteration(gamma, epsilon):\n",
      "    V = np.array([0.0 for i in range(states)])\n",
      "    policy = [0 for i in range(states)]\n",
      "    it = 0\n",
      "    vfuncs = [V]\n",
      "    while True:\n",
      "        it = it + 1\n",
      "        V_old = V.copy()\n",
      "        for s in S:\n",
      "            Q = [0 for i in range(16)]\n",
      "            for a in A:\n",
      "                Q[a] = reward(s) + gamma*psum(s, a, V_old)\n",
      "            policy[s] = np.argmax(Q)\n",
      "            V[s] = Q[policy[s]]\n",
      "        vfuncs.append(V)\n",
      "        if np.allclose(V, V_old, atol=epsilon):\n",
      "            break\n",
      "    return V, policy, it, vfuncs\n",
      "6/193:\n",
      "V1, policy1, it, _ = value_iteration(gamma=.1, epsilon=0)\n",
      "print V1\n",
      "print policy1\n",
      "print \"iterations: \" + str(it)\n",
      "6/194:\n",
      "V9, policy9, it, _ = value_iteration(gamma=.9, epsilon=0)\n",
      "print V9\n",
      "print policy9\n",
      "print \"iterations: \" + str(it)\n",
      "6/195:\n",
      "V99, policy99, it, _ = value_iteration(gamma=.99, epsilon=0)\n",
      "print V_\n",
      "print policy_\n",
      "print \"iterations: \" + str(it)\n",
      "6/196:\n",
      "V1, policy1, it, vfuncs1 = value_iteration(gamma=.1, epsilon=0)\n",
      "print V1\n",
      "print policy1\n",
      "print \"iterations: \" + str(it)\n",
      "6/197:\n",
      "V9, policy9, it, vfuncs9 = value_iteration(gamma=.9, epsilon=0)\n",
      "print V9\n",
      "print policy9\n",
      "print \"iterations: \" + str(it)\n",
      "6/198:\n",
      "V99, policy99, it, vfuncs99 = value_iteration(gamma=.99, epsilon=0)\n",
      "print V_\n",
      "print policy_\n",
      "print \"iterations: \" + str(it)\n",
      "6/199:\n",
      "def simulate(policy, trials):\n",
      "    successes = 0\n",
      "    for i in range(trials):\n",
      "        score = 0\n",
      "        while score < 101:\n",
      "            r = random.random()\n",
      "            p = policy[score]\n",
      "            if r < 0.6:\n",
      "                score += grid(\"\", p)\n",
      "            elif r < 0.7:\n",
      "                score += grid(\"up\", p)\n",
      "            elif r < 0.8:\n",
      "                score += grid(\"down\", p)\n",
      "            elif r < 0.9:\n",
      "                score += grid(\"left\", p)\n",
      "            else:\n",
      "                score += grid(\"right\", p)\n",
      "        successes += 1 if score == 101 else 0\n",
      "    print float(successes)/trials\n",
      "    return\n",
      "6/200:\n",
      "simulate(policy1, 10000)\n",
      "simulate(policy1, 10000)\n",
      "simulate(policy1, 10000)\n",
      "6/201:\n",
      "simulate(policy9, 10000)\n",
      "simulate(policy9, 10000)\n",
      "simulate(policy9, 10000)\n",
      "6/202:\n",
      "simulate(policy99, 10000)\n",
      "simulate(policy99, 10000)\n",
      "simulate(policy99, 10000)\n",
      "6/203:\n",
      "simulate(policy99, 10000)\n",
      "simulate(policy99, 10000)\n",
      "simulate(policy99, 10000)\n",
      "6/204: vfuncs99\n",
      "6/205: vfuncs99.shape\n",
      "6/206: len(vfuncs99)\n",
      "6/207:\n",
      "import math\n",
      "import numpy as np\n",
      "import random\n",
      "import matplotlib.pyplot as plot\n",
      "6/208: len(vfuncs99[0])\n",
      "6/209:\n",
      "xs = [i for i in range(len(vfuncs99))]\n",
      "for i in range(states):\n",
      "    plot.plot(xs, vfuncs99[:,i])\n",
      "6/210:\n",
      "xs = [i for i in range(len(vfuncs99))]\n",
      "for i in range(states):\n",
      "    plot.plot(xs, vfuncs99[:][i])\n",
      "6/211:\n",
      "xs = [i for i in range(len(vfuncs99))]\n",
      "for i in range(states):\n",
      "    plot.plot(xs, vfuncs99[i][])\n",
      "6/212:\n",
      "xs = [i for i in range(len(vfuncs99))]\n",
      "for i in range(states):\n",
      "    plot.plot(xs, vfuncs99[i][:])\n",
      "6/213:\n",
      "xs = [i for i in range(len(vfuncs99))]\n",
      "len(vfuncs99)\n",
      "for i in range(states):\n",
      "#     plot.plot(xs, vfuncs99[i][:])\n",
      "6/214:\n",
      "xs = [i for i in range(len(vfuncs99))]\n",
      "len(vfuncs99)\n",
      "for i in range(states):\n",
      "#     plot.plot(xs, vfuncs99[i][:])\n",
      "6/215:\n",
      "xs = [i for i in range(len(vfuncs99))]\n",
      "len(vfuncs99)\n",
      "# for i in range(states):\n",
      "#     plot.plot(xs, vfuncs99[i][:])\n",
      "6/216:\n",
      "xs = [i for i in range(len(vfuncs99))]\n",
      "len(vfuncs99[:][0])\n",
      "# for i in range(states):\n",
      "#     plot.plot(xs, vfuncs99[i][:])\n",
      "6/217:\n",
      "xs = [i for i in range(len(vfuncs99))]\n",
      "len(vfuncs99.T[0])\n",
      "# for i in range(states):\n",
      "#     plot.plot(xs, vfuncs99[i][:])\n",
      "6/218:\n",
      "xs = [i for i in range(len(vfuncs99))]\n",
      "len(np.array(vfuncs99).T[0])\n",
      "# for i in range(states):\n",
      "#     plot.plot(xs, vfuncs99[i][:])\n",
      "6/219:\n",
      "xs = [i for i in range(len(vfuncs99))]\n",
      "np.array(vfuncs99).T[0]\n",
      "# for i in range(states):\n",
      "#     plot.plot(xs, vfuncs99[i][:])\n",
      "6/220:\n",
      "xs = [i for i in range(len(vfuncs99))]\n",
      "np.array(vfuncs99).T[100]\n",
      "# for i in range(states):\n",
      "#     plot.plot(xs, vfuncs99[i][:])\n",
      "6/221:\n",
      "xs = [i for i in range(len(vfuncs99))]\n",
      "np.array(vfuncs99).T[101]\n",
      "# for i in range(states):\n",
      "#     plot.plot(xs, vfuncs99[i][:])\n",
      "6/222:\n",
      "xs = [i for i in range(len(vfuncs99))]\n",
      "np.array(vfuncs99).T[100]\n",
      "# for i in range(states):\n",
      "#     plot.plot(xs, vfuncs99[i][:])\n",
      "6/223:\n",
      "xs = [i for i in range(len(vfuncs99))]\n",
      "np.array(vfuncs99)\n",
      "# for i in range(states):\n",
      "#     plot.plot(xs, vfuncs99[i][:])\n",
      "6/224:\n",
      "def value_iteration(gamma, epsilon):\n",
      "    V = np.array([0.0 for i in range(states)])\n",
      "    policy = [0 for i in range(states)]\n",
      "    it = 0\n",
      "    vfuncs = [V]\n",
      "    while True:\n",
      "        it = it + 1\n",
      "        V_old = V.copy()\n",
      "        for s in S:\n",
      "            Q = [0 for i in range(16)]\n",
      "            for a in A:\n",
      "                Q[a] = reward(s) + gamma*psum(s, a, V_old)\n",
      "            policy[s] = np.argmax(Q)\n",
      "            V[s] = Q[policy[s]]\n",
      "        vfuncs.append(V.copy())\n",
      "        if np.allclose(V, V_old, atol=epsilon):\n",
      "            break\n",
      "    return V, policy, it, vfuncs\n",
      "6/225:\n",
      "V1, policy1, it, vfuncs1 = value_iteration(gamma=.1, epsilon=0)\n",
      "print V1\n",
      "print policy1\n",
      "print \"iterations: \" + str(it)\n",
      "6/226:\n",
      "V9, policy9, it, vfuncs9 = value_iteration(gamma=.9, epsilon=0)\n",
      "print V9\n",
      "print policy9\n",
      "print \"iterations: \" + str(it)\n",
      "6/227:\n",
      "V99, policy99, it, vfuncs99 = value_iteration(gamma=.99, epsilon=0)\n",
      "print V_\n",
      "print policy_\n",
      "print \"iterations: \" + str(it)\n",
      "6/228:\n",
      "def simulate(policy, trials):\n",
      "    successes = 0\n",
      "    for i in range(trials):\n",
      "        score = 0\n",
      "        while score < 101:\n",
      "            r = random.random()\n",
      "            p = policy[score]\n",
      "            if r < 0.6:\n",
      "                score += grid(\"\", p)\n",
      "            elif r < 0.7:\n",
      "                score += grid(\"up\", p)\n",
      "            elif r < 0.8:\n",
      "                score += grid(\"down\", p)\n",
      "            elif r < 0.9:\n",
      "                score += grid(\"left\", p)\n",
      "            else:\n",
      "                score += grid(\"right\", p)\n",
      "        successes += 1 if score == 101 else 0\n",
      "    print float(successes)/trials\n",
      "    return\n",
      "6/229:\n",
      "simulate(policy1, 10000)\n",
      "simulate(policy1, 10000)\n",
      "simulate(policy1, 10000)\n",
      "6/230:\n",
      "xs = [i for i in range(len(vfuncs99))]\n",
      "np.array(vfuncs99).T[100]\n",
      "# for i in range(states):\n",
      "#     plot.plot(xs, vfuncs99[i][:])\n",
      "6/231:\n",
      "def value_iteration(gamma, epsilon):\n",
      "    V = np.array([0.0 for i in range(states)])\n",
      "    policy = [0 for i in range(states)]\n",
      "    it = 0\n",
      "    vfuncs = [V.copy()]\n",
      "    while True:\n",
      "        it = it + 1\n",
      "        V_old = V.copy()\n",
      "        for s in S:\n",
      "            Q = [0 for i in range(16)]\n",
      "            for a in A:\n",
      "                Q[a] = reward(s) + gamma*psum(s, a, V_old)\n",
      "            policy[s] = np.argmax(Q)\n",
      "            V[s] = Q[policy[s]]\n",
      "        vfuncs.append(V.copy())\n",
      "        if np.allclose(V, V_old, atol=epsilon):\n",
      "            break\n",
      "    return V, policy, it, vfuncs\n",
      "6/232:\n",
      "V1, policy1, it, vfuncs1 = value_iteration(gamma=.1, epsilon=0)\n",
      "print V1\n",
      "print policy1\n",
      "print \"iterations: \" + str(it)\n",
      "6/233:\n",
      "V9, policy9, it, vfuncs9 = value_iteration(gamma=.9, epsilon=0)\n",
      "print V9\n",
      "print policy9\n",
      "print \"iterations: \" + str(it)\n",
      "6/234:\n",
      "V99, policy99, it, vfuncs99 = value_iteration(gamma=.99, epsilon=0)\n",
      "print V_\n",
      "print policy_\n",
      "print \"iterations: \" + str(it)\n",
      "6/235:\n",
      "V99, policy99, it, vfuncs99 = value_iteration(gamma=.99, epsilon=0)\n",
      "print V99\n",
      "print policy99\n",
      "print \"iterations: \" + str(it)\n",
      "6/236:\n",
      "xs = [i for i in range(len(vfuncs99))]\n",
      "np.array(vfuncs99).T[100]\n",
      "# for i in range(states):\n",
      "#     plot.plot(xs, vfuncs99[i][:])\n",
      "6/237:\n",
      "xs = [i for i in range(len(vfuncs99))]\n",
      "values = np.array(vfuncs99).T\n",
      "for i in range(states):\n",
      "    plot.plot(xs, values[i])\n",
      "6/238:\n",
      "def plot_vfuncs(vfuncs):\n",
      "    xs = [i for i in range(len(vfuncs))]\n",
      "    values = np.array(vfuncs).T\n",
      "    for i in range(states):\n",
      "        plot.plot(xs, values[i])\n",
      "6/239: plot_vfuncs(vfuncs1)\n",
      "6/240: plot_vfuncs(vfuncs9)\n",
      "6/241: plot_vfuncs(vfuncs99)\n",
      "6/242:\n",
      "def plot_vfuncs(vfuncs):\n",
      "    xs = [i for i in range(len(vfuncs))]\n",
      "    values = np.array(vfuncs).T\n",
      "    for i in range(states):\n",
      "        plot.plot(xs, values[i], label=str(i))\n",
      "6/243: plot_vfuncs(vfuncs1)\n",
      "6/244:\n",
      "def plot_vfuncs(vfuncs):\n",
      "    xs = [i for i in range(len(vfuncs))]\n",
      "    values = np.array(vfuncs).T\n",
      "    for i in range(states):\n",
      "        plot.plot(xs, values[i], label=str(i))\n",
      "    plot.legend()\n",
      "6/245: plot_vfuncs(vfuncs1)\n",
      "6/246: plot_vfuncs(vfuncs9)\n",
      "6/247:\n",
      "def plot_vfuncs(vfuncs):\n",
      "    xs = [i for i in range(len(vfuncs))]\n",
      "    values = np.array(vfuncs).T\n",
      "    for i in range(states):\n",
      "        plot.plot(xs, values[i])\n",
      "    plot.legend()\n",
      "6/248:\n",
      "def plot_vfuncs(vfuncs):\n",
      "    xs = [i for i in range(len(vfuncs))]\n",
      "    values = np.array(vfuncs).T\n",
      "    for i in range(states):\n",
      "        plot.plot(xs, values[i])\n",
      "6/249: plot_vfuncs(vfuncs1)\n",
      "6/250: plot_vfuncs(vfuncs9)\n",
      "6/251: plot_vfuncs(vfuncs99)\n",
      "6/252: indices1 = np.argsort(V1)[::-1]\n",
      "6/253: np.argsort(V1)[::-1]\n",
      "6/254: np.argsort(V9)[::-1]\n",
      "6/255: np.argsort(V99)[::-1]\n",
      "6/256: plot.imshow(vfuncs99)\n",
      "6/257: plot.imshow(vfuncs99.T)\n",
      "6/258:\n",
      "def plot_vfuncs(vfuncs):\n",
      "    xs = [i for i in range(len(vfuncs))]\n",
      "    values = np.array(vfuncs).T\n",
      "    for i in range(states):\n",
      "        plot.plot(xs, values[i])\n",
      "    plot.imshow(values)\n",
      "6/259: plot_vfuncs(vfuncs1)\n",
      "6/260:\n",
      "def plot_vfuncs(vfuncs):\n",
      "    xs = [i for i in range(len(vfuncs))]\n",
      "    values = np.array(vfuncs).T\n",
      "    for i in range(states):\n",
      "        plot.plot(xs, values[i])\n",
      "6/261: plot_vfuncs(vfuncs1)\n",
      "6/262:\n",
      "def plot_vfuncs_color(vfuncs):\n",
      "    values = np.array(vfuncs).T\n",
      "    plot.imshow(values)\n",
      "6/263: plot_vfuncs_color(vfuncs1)\n",
      "6/264: plot_vfuncs_color(vfuncs9)\n",
      "6/265: plot_vfuncs_color(vfuncs99)\n",
      "6/266:\n",
      "def plot_vfuncs_color(vfuncs):\n",
      "    plot.figure(figsize=(10,1))\n",
      "    values = np.array(vfuncs).T\n",
      "    plot.imshow(values)\n",
      "6/267: plot_vfuncs_color(vfuncs1)\n",
      "6/268:\n",
      "def plot_vfuncs_color(vfuncs):\n",
      "    plot.figure(figsize=(10,5))\n",
      "    values = np.array(vfuncs).T\n",
      "    plot.imshow(values)\n",
      "6/269: plot_vfuncs_color(vfuncs1)\n",
      "6/270:\n",
      "def plot_vfuncs_color(vfuncs):\n",
      "    plot.figure(figsize=(20,5))\n",
      "    values = np.array(vfuncs).T\n",
      "    plot.imshow(values)\n",
      "6/271: plot_vfuncs_color(vfuncs1)\n",
      "6/272:\n",
      "def plot_vfuncs_color(vfuncs):\n",
      "    plot.figure(figsize=(20,10))\n",
      "    values = np.array(vfuncs).T\n",
      "    plot.imshow(values)\n",
      "6/273: plot_vfuncs_color(vfuncs1)\n",
      "6/274:\n",
      "def plot_vfuncs_color(vfuncs):\n",
      "    plot.figure(figsize=(2,10))\n",
      "    values = np.array(vfuncs).T\n",
      "    plot.imshow(values)\n",
      "6/275:\n",
      "def plot_vfuncs_color(vfuncs):\n",
      "    plot.figure(figsize=(2,100))\n",
      "    values = np.array(vfuncs).T\n",
      "    plot.imshow(values)\n",
      "6/276: plot_vfuncs_color(vfuncs1)\n",
      "6/277:\n",
      "def plot_vfuncs_color(vfuncs):\n",
      "    plot.figure(figsize=(5,50))\n",
      "    values = np.array(vfuncs).T\n",
      "    plot.imshow(values)\n",
      "6/278: plot_vfuncs_color(vfuncs1)\n",
      "6/279:\n",
      "def plot_vfuncs_color(vfuncs):\n",
      "    plot.figure(figsize=(5,25))\n",
      "    values = np.array(vfuncs).T\n",
      "    plot.imshow(values)\n",
      "6/280: plot_vfuncs_color(vfuncs1)\n",
      "6/281: plot_vfuncs_color(vfuncs9)\n",
      "6/282: plot_vfuncs_color(vfuncs99)\n",
      "6/283:\n",
      "def plot_vfuncs_color(vfuncs):\n",
      "    plot.figure(figsize=(5,15))\n",
      "    values = np.array(vfuncs).T\n",
      "    plot.imshow(values)\n",
      "6/284: plot_vfuncs_color(vfuncs1)\n",
      "6/285:\n",
      "def plot_vfuncs_color(vfuncs):\n",
      "    plot.figure(figsize=(5,20))\n",
      "    values = np.array(vfuncs).T\n",
      "    plot.imshow(values)\n",
      "6/286: plot_vfuncs_color(vfuncs1)\n",
      "6/287: plot_vfuncs_color(vfuncs9)\n",
      "6/288: plot_vfuncs_color(vfuncs99)\n",
      "6/289:\n",
      "plot.subplot(1,3,1)\n",
      "plot_vfuncs_color(vfuncs1)\n",
      "plot.subplot(1,3,2)\n",
      "plot_vfuncs_color(vfuncs9)\n",
      "plot.subplot(1,3,3)\n",
      "plot_vfuncs_color(vfuncs99)\n",
      "6/290:\n",
      "def plot_vfuncs_color(vfuncs):\n",
      "    \n",
      "    values = np.array(vfuncs).T\n",
      "    plot.imshow(values)\n",
      "6/291:\n",
      "plot.figure(figsize=(5,20))\n",
      "plot.subplot(1,3,1)\n",
      "plot_vfuncs_color(vfuncs1)\n",
      "plot.subplot(1,3,2)\n",
      "plot_vfuncs_color(vfuncs9)\n",
      "plot.subplot(1,3,3)\n",
      "plot_vfuncs_color(vfuncs99)\n",
      "6/292:\n",
      "plot.figure(figsize=(10,50))\n",
      "plot.subplot(1,3,1)\n",
      "plot_vfuncs_color(vfuncs1)\n",
      "plot.subplot(1,3,2)\n",
      "plot_vfuncs_color(vfuncs9)\n",
      "plot.subplot(1,3,3)\n",
      "plot_vfuncs_color(vfuncs99)\n",
      "6/293:\n",
      "# plot.figure(figsize=(10,50))\n",
      "plot.subplot(1,3,1)\n",
      "plot_vfuncs_color(vfuncs1)\n",
      "plot.subplot(1,3,2)\n",
      "plot_vfuncs_color(vfuncs9)\n",
      "plot.subplot(1,3,3)\n",
      "plot_vfuncs_color(vfuncs99)\n",
      "6/294:\n",
      "plot.figure(figsize=(10,20))\n",
      "plot.subplot(1,3,1)\n",
      "plot_vfuncs_color(vfuncs1)\n",
      "plot.subplot(1,3,2)\n",
      "plot_vfuncs_color(vfuncs9)\n",
      "plot.subplot(1,3,3)\n",
      "plot_vfuncs_color(vfuncs99)\n",
      "6/295:\n",
      "plot.figure(figsize=(,20))\n",
      "plot.subplot(1,3,1)\n",
      "plot_vfuncs_color(vfuncs1)\n",
      "plot.subplot(1,3,2)\n",
      "plot_vfuncs_color(vfuncs9)\n",
      "plot.subplot(1,3,3)\n",
      "plot_vfuncs_color(vfuncs99)\n",
      "6/296:\n",
      "plot.figure(figsize=(10,20))\n",
      "plot.subplot(1,3,1)\n",
      "plot_vfuncs_color(vfuncs1)\n",
      "plot.subplot(1,3,2)\n",
      "plot_vfuncs_color(vfuncs9)\n",
      "plot.subplot(1,3,3)\n",
      "plot_vfuncs_color(vfuncs99)\n",
      "plot.subplots_adjust()\n",
      "6/297:\n",
      "plot.figure(figsize=(10,20))\n",
      "plot.subplot(1,3,1)\n",
      "plot_vfuncs_color(vfuncs1)\n",
      "plot.subplot(1,3,2)\n",
      "plot_vfuncs_color(vfuncs9)\n",
      "plot.subplot(1,3,3)\n",
      "plot_vfuncs_color(vfuncs99)\n",
      "plot.subplots_adjust(top=0.1)\n",
      "6/298:\n",
      "plot.figure(figsize=(10,20))\n",
      "plot.subplot(1,3,1)\n",
      "plot_vfuncs_color(vfuncs1)\n",
      "plot.subplot(1,3,2)\n",
      "plot_vfuncs_color(vfuncs9)\n",
      "plot.subplot(1,3,3)\n",
      "plot_vfuncs_color(vfuncs99)\n",
      "plot.subplots_adjust(top=0.1, bottom=0.1)\n",
      "6/299:\n",
      "plot.figure(figsize=(10,20))\n",
      "plot.subplot(1,3,1)\n",
      "plot_vfuncs_color(vfuncs1)\n",
      "plot.subplot(1,3,2)\n",
      "plot_vfuncs_color(vfuncs9)\n",
      "plot.subplot(1,3,3)\n",
      "plot_vfuncs_color(vfuncs99)\n",
      "plot.subplots_adjust(bottom=0.1)\n",
      "6/300:\n",
      "def plot_vfuncs_color(vfuncs):\n",
      "    while len(vfuncs) < 24:\n",
      "        vfuncs.append(vfuncs[len(vfuncs)-1])\n",
      "    values = np.array(vfuncs).T\n",
      "    plot.imshow(values)\n",
      "6/301:\n",
      "plot.figure(figsize=(10,20))\n",
      "plot.subplot(1,3,1)\n",
      "plot_vfuncs_color(vfuncs1)\n",
      "plot.subplot(1,3,2)\n",
      "plot_vfuncs_color(vfuncs9)\n",
      "plot.subplot(1,3,3)\n",
      "plot_vfuncs_color(vfuncs99)\n",
      "plot.subplots_adjust(bottom=0.1)\n",
      "6/302:\n",
      "plot.figure(figsize=(20,20))\n",
      "plot.subplot(1,3,1)\n",
      "plot_vfuncs_color(vfuncs1)\n",
      "plot.subplot(1,3,2)\n",
      "plot_vfuncs_color(vfuncs9)\n",
      "plot.subplot(1,3,3)\n",
      "plot_vfuncs_color(vfuncs99)\n",
      "plot.subplots_adjust(bottom=0.1)\n",
      "6/303:\n",
      "plot.figure(figsize=(15,20))\n",
      "plot.subplot(1,3,1)\n",
      "plot_vfuncs_color(vfuncs1)\n",
      "plot.subplot(1,3,2)\n",
      "plot_vfuncs_color(vfuncs9)\n",
      "plot.subplot(1,3,3)\n",
      "plot_vfuncs_color(vfuncs99)\n",
      "plot.subplots_adjust(bottom=0.1)\n",
      "6/304:\n",
      "plot.figure(figsize=(12,15))\n",
      "plot.subplot(1,3,1)\n",
      "plot_vfuncs_color(vfuncs1)\n",
      "plot.subplot(1,3,2)\n",
      "plot_vfuncs_color(vfuncs9)\n",
      "plot.subplot(1,3,3)\n",
      "plot_vfuncs_color(vfuncs99)\n",
      "plot.subplots_adjust(bottom=0.1)\n",
      "6/305:\n",
      "def value_iteration(gamma, epsilon):\n",
      "    V = np.array([0.0 for i in range(states)])\n",
      "    policy = [0 for i in range(states)]\n",
      "    it = 0\n",
      "    vfuncs = [V.copy()]\n",
      "    Q = [[0 for i in range(16)] for j in range(states)]\n",
      "    while True:\n",
      "        it = it + 1\n",
      "        V_old = V.copy()\n",
      "        for s in S:\n",
      "            for a in A:\n",
      "                Q[s][a] = reward(s) + gamma*psum(s, a, V_old)\n",
      "            policy[s] = np.argmax(Q[s])\n",
      "            V[s] = Q[policy[s]]\n",
      "        vfuncs.append(V.copy())\n",
      "        if np.allclose(V, V_old, atol=epsilon):\n",
      "            break\n",
      "    return V, policy, it, vfuncs\n",
      "6/306:\n",
      "V1, policy1, it, vfuncs1 = value_iteration(gamma=.1, epsilon=0)\n",
      "print V1\n",
      "print policy1\n",
      "print \"iterations: \" + str(it)\n",
      "6/307:\n",
      "def value_iteration(gamma, epsilon):\n",
      "    V = np.array([0.0 for i in range(states)])\n",
      "    policy = [0 for i in range(states)]\n",
      "    it = 0\n",
      "    vfuncs = [V.copy()]\n",
      "    Q = [[0 for i in range(16)] for j in range(states)]\n",
      "    while True:\n",
      "        it = it + 1\n",
      "        V_old = V.copy()\n",
      "        for s in S:\n",
      "            for a in A:\n",
      "                Q[s][a] = reward(s) + gamma*psum(s, a, V_old)\n",
      "            policy[s] = np.argmax(Q[s])\n",
      "            V[s] = Q[s][policy[s]]\n",
      "        vfuncs.append(V.copy())\n",
      "        if np.allclose(V, V_old, atol=epsilon):\n",
      "            break\n",
      "    return V, policy, it, vfuncs\n",
      "6/308:\n",
      "V1, policy1, it, vfuncs1 = value_iteration(gamma=.1, epsilon=0)\n",
      "print V1\n",
      "print policy1\n",
      "print \"iterations: \" + str(it)\n",
      "6/309:\n",
      "import math\n",
      "import numpy as np\n",
      "import random\n",
      "import matplotlib.pyplot as plot\n",
      "6/310:\n",
      "states = 117\n",
      "S = [i for i in range(states)]\n",
      "A = [i for i in range(16)]\n",
      "g = [[7,12,1,14],[2,13,8,11],[16,3,10,5],[9,6,15,4]]\n",
      "6/311:\n",
      "def grid(dir, a):\n",
      "    i = a/4\n",
      "    j = a - 4*i\n",
      "    if dir == \"\":\n",
      "        return g[i][j]\n",
      "    elif dir == \"up\":\n",
      "        return 0 if i == 0 else g[i-1][j]\n",
      "    elif dir == \"down\":\n",
      "        return 0 if i == 3 else g[i+1][j]\n",
      "    elif dir == \"left\":\n",
      "        return 0 if j == 0 else g[i][j-1]\n",
      "    else:\n",
      "        return 0 if j == 3 else g[i][j+1]\n",
      "6/312:\n",
      "def reward(s):\n",
      "    if s < 101:\n",
      "        return 0\n",
      "    elif s == 101:\n",
      "        return 1\n",
      "    else:\n",
      "        return -1\n",
      "6/313:\n",
      "def psum(s, a, V):\n",
      "    if s > 100:\n",
      "        return 0\n",
      "    sum = 0\n",
      "    sum += 0.6*V[grid(\"\", a)+s]\n",
      "    sum += 0.1*V[grid(\"up\", a)+s] + 0.1*V[grid(\"down\", a)+s]\n",
      "    sum += 0.1*V[grid(\"left\", a)+s] + 0.1*V[grid(\"right\", a)+s]\n",
      "    return sum\n",
      "6/314:\n",
      "def value_iteration(gamma, epsilon):\n",
      "    V = np.array([0.0 for i in range(states)])\n",
      "    policy = [0 for i in range(states)]\n",
      "    it = 0\n",
      "    vfuncs = [V.copy()]\n",
      "    Q = [[0 for i in range(16)] for j in range(states)]\n",
      "    while True:\n",
      "        it = it + 1\n",
      "        V_old = V.copy()\n",
      "        for s in S:\n",
      "            for a in A:\n",
      "                Q[s][a] = reward(s) + gamma*psum(s, a, V_old)\n",
      "            policy[s] = np.argmax(Q[s])\n",
      "            V[s] = Q[s][policy[s]]\n",
      "        vfuncs.append(V.copy())\n",
      "        if np.allclose(V, V_old, atol=epsilon):\n",
      "            break\n",
      "    return V, policy, it, vfuncs\n",
      "6/315:\n",
      "V1, policy1, it, vfuncs1 = value_iteration(gamma=.1, epsilon=0)\n",
      "print V1\n",
      "print policy1\n",
      "print \"iterations: \" + str(it)\n",
      "6/316:\n",
      "V9, policy9, it, vfuncs9 = value_iteration(gamma=.9, epsilon=0)\n",
      "print V9\n",
      "print policy9\n",
      "print \"iterations: \" + str(it)\n",
      "6/317:\n",
      "V99, policy99, it, vfuncs99 = value_iteration(gamma=.99, epsilon=0)\n",
      "print V99\n",
      "print policy99\n",
      "print \"iterations: \" + str(it)\n",
      "6/318:\n",
      "def simulate(policy, trials):\n",
      "    successes = 0\n",
      "    for i in range(trials):\n",
      "        score = 0\n",
      "        while score < 101:\n",
      "            r = random.random()\n",
      "            p = policy[score]\n",
      "            if r < 0.6:\n",
      "                score += grid(\"\", p)\n",
      "            elif r < 0.7:\n",
      "                score += grid(\"up\", p)\n",
      "            elif r < 0.8:\n",
      "                score += grid(\"down\", p)\n",
      "            elif r < 0.9:\n",
      "                score += grid(\"left\", p)\n",
      "            else:\n",
      "                score += grid(\"right\", p)\n",
      "        successes += 1 if score == 101 else 0\n",
      "    print float(successes)/trials\n",
      "    return\n",
      "6/319:\n",
      "simulate(policy1, 10000)\n",
      "simulate(policy1, 10000)\n",
      "simulate(policy1, 10000)\n",
      "6/320:\n",
      "simulate(policy9, 10000)\n",
      "simulate(policy9, 10000)\n",
      "simulate(policy9, 10000)\n",
      "6/321:\n",
      "simulate(policy99, 10000)\n",
      "simulate(policy99, 10000)\n",
      "simulate(policy99, 10000)\n",
      "6/322:\n",
      "def plot_vfuncs(vfuncs):\n",
      "    xs = [i for i in range(len(vfuncs))]\n",
      "    values = np.array(vfuncs).T\n",
      "    for i in range(states):\n",
      "        plot.plot(xs, values[i])\n",
      "6/323: plot_vfuncs(vfuncs1)\n",
      "6/324: plot_vfuncs(vfuncs9)\n",
      "6/325: plot_vfuncs(vfuncs99)\n",
      "6/326: np.argsort(V1)[::-1]\n",
      "6/327: np.argsort(V9)[::-1]\n",
      "6/328: np.argsort(V99)[::-1]\n",
      "6/329:\n",
      "def plot_vfuncs_color(vfuncs):\n",
      "    while len(vfuncs) < 24:\n",
      "        vfuncs.append(vfuncs[len(vfuncs)-1])\n",
      "    values = np.array(vfuncs).T\n",
      "    plot.imshow(values)\n",
      "6/330:\n",
      "plot.figure(figsize=(12,15))\n",
      "plot.subplot(1,3,1)\n",
      "plot_vfuncs_color(vfuncs1)\n",
      "plot.subplot(1,3,2)\n",
      "plot_vfuncs_color(vfuncs9)\n",
      "plot.subplot(1,3,3)\n",
      "plot_vfuncs_color(vfuncs99)\n",
      "plot.subplots_adjust(bottom=0.1)\n",
      " 7/1:\n",
      "V1, policy1, it, vfuncs1 = value_iteration(gamma=1, epsilon=0)\n",
      "print V1\n",
      "print policy1\n",
      "print \"iterations: \" + str(it)\n",
      " 7/2:\n",
      "import math\n",
      "import numpy as np\n",
      "import random\n",
      "import matplotlib.pyplot as plot\n",
      " 7/3:\n",
      "states = 117\n",
      "S = [i for i in range(states)]\n",
      "A = [i for i in range(16)]\n",
      "g = [[7,12,1,14],[2,13,8,11],[16,3,10,5],[9,6,15,4]]\n",
      " 7/4:\n",
      "def grid(dir, a):\n",
      "    i = a/4\n",
      "    j = a - 4*i\n",
      "    if dir == \"\":\n",
      "        return g[i][j]\n",
      "    elif dir == \"up\":\n",
      "        return 0 if i == 0 else g[i-1][j]\n",
      "    elif dir == \"down\":\n",
      "        return 0 if i == 3 else g[i+1][j]\n",
      "    elif dir == \"left\":\n",
      "        return 0 if j == 0 else g[i][j-1]\n",
      "    else:\n",
      "        return 0 if j == 3 else g[i][j+1]\n",
      " 7/5:\n",
      "def reward(s):\n",
      "    if s < 101:\n",
      "        return 0\n",
      "    elif s == 101:\n",
      "        return 1\n",
      "    else:\n",
      "        return -1\n",
      " 7/6:\n",
      "def psum(s, a, V):\n",
      "    if s > 100:\n",
      "        return 0\n",
      "    sum = 0\n",
      "    sum += 0.6*V[grid(\"\", a)+s]\n",
      "    sum += 0.1*V[grid(\"up\", a)+s] + 0.1*V[grid(\"down\", a)+s]\n",
      "    sum += 0.1*V[grid(\"left\", a)+s] + 0.1*V[grid(\"right\", a)+s]\n",
      "    return sum\n",
      " 7/7:\n",
      "def value_iteration(gamma, epsilon):\n",
      "    V = np.array([0.0 for i in range(states)])\n",
      "    policy = [0 for i in range(states)]\n",
      "    it = 0\n",
      "    vfuncs = [V.copy()]\n",
      "    Q = [[0 for i in range(16)] for j in range(states)]\n",
      "    while True:\n",
      "        it = it + 1\n",
      "        V_old = V.copy()\n",
      "        for s in S:\n",
      "            for a in A:\n",
      "                Q[s][a] = reward(s) + gamma*psum(s, a, V_old)\n",
      "            policy[s] = np.argmax(Q[s])\n",
      "            V[s] = Q[s][policy[s]]\n",
      "        vfuncs.append(V.copy())\n",
      "        if np.allclose(V, V_old, atol=epsilon):\n",
      "            break\n",
      "    return V, policy, it, vfuncs\n",
      " 7/8:\n",
      "V1, policy1, it, vfuncs1 = value_iteration(gamma=1, epsilon=0)\n",
      "print V1\n",
      "print policy1\n",
      "print \"iterations: \" + str(it)\n",
      " 7/9:\n",
      "simulate(policy1, 10000)\n",
      "simulate(policy1, 10000)\n",
      "simulate(policy1, 10000)\n",
      "7/10:\n",
      "def simulate(policy, trials):\n",
      "    successes = 0\n",
      "    for i in range(trials):\n",
      "        score = 0\n",
      "        while score < 101:\n",
      "            r = random.random()\n",
      "            p = policy[score]\n",
      "            if r < 0.6:\n",
      "                score += grid(\"\", p)\n",
      "            elif r < 0.7:\n",
      "                score += grid(\"up\", p)\n",
      "            elif r < 0.8:\n",
      "                score += grid(\"down\", p)\n",
      "            elif r < 0.9:\n",
      "                score += grid(\"left\", p)\n",
      "            else:\n",
      "                score += grid(\"right\", p)\n",
      "        successes += 1 if score == 101 else 0\n",
      "    print float(successes)/trials\n",
      "    return\n",
      "7/11:\n",
      "simulate(policy1, 10000)\n",
      "simulate(policy1, 10000)\n",
      "simulate(policy1, 10000)\n",
      "7/12:\n",
      "def plot_vfuncs(vfuncs):\n",
      "    xs = [i for i in range(len(vfuncs))]\n",
      "    values = np.array(vfuncs).T\n",
      "    for i in range(states):\n",
      "        plot.plot(xs, values[i][0])\n",
      "7/13: plot_vfuncs(vfuncs1)\n",
      "7/14:\n",
      "def plot_vfuncs(vfuncs):\n",
      "    xs = [i for i in range(len(vfuncs))]\n",
      "    values = np.array(vfuncs).T\n",
      "    for i in range(states):\n",
      "        plot.plot(xs, values[0])\n",
      "7/15: plot_vfuncs(vfuncs1)\n",
      "7/16:\n",
      "plot.figure(figsize=(12,15))\n",
      "plot_vfuncs_color(vfuncs1)\n",
      "7/17:\n",
      "def plot_vfuncs_color(vfuncs):\n",
      "    while len(vfuncs) < 24:\n",
      "        vfuncs.append(vfuncs[len(vfuncs)-1])\n",
      "    values = np.array(vfuncs).T\n",
      "    plot.imshow(values)\n",
      "7/18:\n",
      "plot.figure(figsize=(12,15))\n",
      "plot_vfuncs_color(vfuncs1)\n",
      "7/19:\n",
      "def plot_vfuncs(vfuncs, n):\n",
      "    xs = [i for i in range(len(vfuncs))]\n",
      "    values = np.array(vfuncs).T\n",
      "    for i in range(n):\n",
      "        plot.plot(xs, values[i])\n",
      "7/20: plot_vfuncs(vfuncs1, 1)\n",
      "7/21: plot_vfuncs(vfuncs, 10)\n",
      "7/22: plot_vfuncs(vfuncs1, 10)\n",
      "7/23:\n",
      "V, policy, it, vfuncs = value_iteration(gamma=1, epsilon=0)\n",
      "print V\n",
      "print policy\n",
      "print \"iterations: \" + str(it)\n",
      "7/24:\n",
      "def simulate(policy, trials):\n",
      "    successes = 0\n",
      "    for i in range(trials):\n",
      "        score = 0\n",
      "        while score < 101:\n",
      "            r = random.random()\n",
      "            p = policy[score]\n",
      "            if r < 0.6:\n",
      "                score += grid(\"\", p)\n",
      "            elif r < 0.7:\n",
      "                score += grid(\"up\", p)\n",
      "            elif r < 0.8:\n",
      "                score += grid(\"down\", p)\n",
      "            elif r < 0.9:\n",
      "                score += grid(\"left\", p)\n",
      "            else:\n",
      "                score += grid(\"right\", p)\n",
      "        successes += 1 if score == 101 else 0\n",
      "    print float(successes)/trials\n",
      "    return\n",
      "7/25:\n",
      "simulate(policy, 10000)\n",
      "simulate(policy, 10000)\n",
      "simulate(policy, 10000)\n",
      "7/26:\n",
      "def plot_vfuncs(vfuncs, n):\n",
      "    xs = [i for i in range(len(vfuncs))]\n",
      "    values = np.array(vfuncs).T\n",
      "    for i in range(n):\n",
      "        plot.plot(xs, values[i])\n",
      "7/27: plot_vfuncs(vfuncs1, 1)\n",
      "7/28: plot_vfuncs(vfuncs, 1)\n",
      "7/29: plot_vfuncs(vfuncs, 10)\n",
      "7/30: plot_vfuncs(vfuncs, states)\n",
      "7/31: plot_vfuncs(vfuncs, 5)\n",
      "7/32: plot_vfuncs(vfuncs, 20)\n",
      "7/33: plot_vfuncs(vfuncs, 10)\n",
      "7/34: np.argsort(V1)[::-1]\n",
      "7/35:\n",
      "def plot_vfuncs_color(vfuncs):\n",
      "    while len(vfuncs) < 24:\n",
      "        vfuncs.append(vfuncs[len(vfuncs)-1])\n",
      "    values = np.array(vfuncs).T\n",
      "    plot.imshow(values)\n",
      "7/36:\n",
      "plot.figure(figsize=(12,15))\n",
      "plot_vfuncs_color(vfuncs1)\n",
      "7/37:\n",
      "plot.figure(figsize=(12,15))\n",
      "plot_vfuncs_color(vfuncs)\n",
      "7/38:\n",
      "def plot_states(V):\n",
      "    xs = [i for i in range(len(V))]\n",
      "    for i in range(len(V)):\n",
      "        plot.plot(xs, V)\n",
      "7/39: plot_states(V)\n",
      "7/40:\n",
      "plot.figure(figsize=(7,10))\n",
      "plot_states(V)\n",
      "7/41:\n",
      "plot.figure(figsize=(10,6))\n",
      "plot_states(V)\n",
      "7/42:\n",
      "plot.figure(figsize=(12,8))\n",
      "plot_states(V)\n",
      "7/43:\n",
      "def plot_vfuncs(vfuncs, n):\n",
      "    xs = [i for i in range(len(vfuncs))]\n",
      "    values = np.array(vfuncs).T\n",
      "    for i in range(n):\n",
      "        plot.plot(xs, values[i])\n",
      "7/44: plot_vfuncs(vfuncs, 1)\n",
      "7/45:\n",
      "states = 117+1\n",
      "S = [i for i in range(states)]\n",
      "A = [i for i in range(16)]\n",
      "g = [[7,12,1,14],[2,13,8,11],[16,3,10,5],[9,6,15,4]]\n",
      "7/46:\n",
      "def grid(dir, a):\n",
      "    i = a/4\n",
      "    j = a - 4*i\n",
      "    if dir == \"\":\n",
      "        return g[i][j]\n",
      "    elif dir == \"up\":\n",
      "        return 0 if i == 0 else g[i-1][j]\n",
      "    elif dir == \"down\":\n",
      "        return 0 if i == 3 else g[i+1][j]\n",
      "    elif dir == \"left\":\n",
      "        return 0 if j == 0 else g[i][j-1]\n",
      "    else:\n",
      "        return 0 if j == 3 else g[i][j+1]\n",
      "7/47:\n",
      "def reward(s):\n",
      "    if s < 101:\n",
      "        return 0\n",
      "    elif s == 101:\n",
      "        return 1\n",
      "    else:\n",
      "        return -1\n",
      "7/48:\n",
      "def psum(s, a, V):\n",
      "    if s > 100:\n",
      "        return 0\n",
      "    sum = 0\n",
      "    sum += 0.6*V[grid(\"\", a)+s]\n",
      "    sum += 0.1*V[grid(\"up\", a)+s] + 0.1*V[grid(\"down\", a)+s]\n",
      "    sum += 0.1*V[grid(\"left\", a)+s] + 0.1*V[grid(\"right\", a)+s]\n",
      "    return sum\n",
      "7/49:\n",
      "def value_iteration(gamma, epsilon):\n",
      "    V = np.array([0.0 for i in range(states)])\n",
      "    policy = [0 for i in range(states)]\n",
      "    it = 0\n",
      "    vfuncs = [V.copy()]\n",
      "    Q = [[0 for i in range(16)] for j in range(states)]\n",
      "    while True:\n",
      "        it = it + 1\n",
      "        V_old = V.copy()\n",
      "        for s in S:\n",
      "            for a in A:\n",
      "                Q[s][a] = reward(s) + gamma*psum(s, a, V_old)\n",
      "            policy[s] = np.argmax(Q[s])\n",
      "            V[s] = Q[s][policy[s]]\n",
      "        vfuncs.append(V.copy())\n",
      "        if np.allclose(V, V_old, atol=epsilon):\n",
      "            break\n",
      "    return V, policy, it, vfuncs\n",
      "7/50:\n",
      "V, policy, it, vfuncs = value_iteration(gamma=1, epsilon=0)\n",
      "print V\n",
      "print policy\n",
      "print \"iterations: \" + str(it)\n",
      "7/51:\n",
      "def simulate(policy, trials):\n",
      "    successes = 0\n",
      "    for i in range(trials):\n",
      "        score = 0\n",
      "        while score < 101:\n",
      "            r = random.random()\n",
      "            p = policy[score]\n",
      "            if r < 0.6:\n",
      "                score += grid(\"\", p)\n",
      "            elif r < 0.7:\n",
      "                score += grid(\"up\", p)\n",
      "            elif r < 0.8:\n",
      "                score += grid(\"down\", p)\n",
      "            elif r < 0.9:\n",
      "                score += grid(\"left\", p)\n",
      "            else:\n",
      "                score += grid(\"right\", p)\n",
      "        successes += 1 if score == 101 else 0\n",
      "    print float(successes)/trials\n",
      "    return\n",
      "7/52:\n",
      "simulate(policy, 10000)\n",
      "simulate(policy, 10000)\n",
      "simulate(policy, 10000)\n",
      "7/53:\n",
      "def plot_states(V):\n",
      "    xs = [i for i in range(len(V))]\n",
      "    for i in range(len(V)):\n",
      "        plot.plot(xs, V)\n",
      "7/54:\n",
      "plot.figure(figsize=(12,8))\n",
      "plot_states(V)\n",
      "7/55:\n",
      "def plot_vfuncs(vfuncs, n):\n",
      "    xs = [i for i in range(len(vfuncs))]\n",
      "    values = np.array(vfuncs).T\n",
      "    for i in range(n):\n",
      "        plot.plot(xs, values[i])\n",
      "7/56: plot_vfuncs(vfuncs, 1)\n",
      "7/57: np.argsort(V1)[::-1]\n",
      "7/58:\n",
      "def plot_vfuncs_color(vfuncs):\n",
      "    while len(vfuncs) < 24:\n",
      "        vfuncs.append(vfuncs[len(vfuncs)-1])\n",
      "    values = np.array(vfuncs).T\n",
      "    plot.imshow(values)\n",
      "7/59:\n",
      "plot.figure(figsize=(12,15))\n",
      "plot_vfuncs_color(vfuncs)\n",
      "7/60:\n",
      "plot.figure(figsize=(12,8))\n",
      "plot_states(V)\n",
      "7/61:\n",
      "plot.figure(figsize=(12,8))\n",
      "plot_states(V)\n",
      "7/62:\n",
      "def plot_states(V):\n",
      "    xs = [i for i in range(len(V))]\n",
      "    for i in range(len(V)):\n",
      "        plot.plot(xs, V)\n",
      "7/63:\n",
      "plot.figure(figsize=(12,8))\n",
      "plot_states(V)\n",
      "7/64:\n",
      "def plot_states(V):\n",
      "    xs = [i for i in range(len(V))]\n",
      "    for i in range(len(V)):\n",
      "        plot.plot(xs, V)\n",
      "7/65:\n",
      "plot.figure(figsize=(12,8))\n",
      "plot_states(V)\n",
      "7/66:\n",
      "import math\n",
      "import numpy as np\n",
      "import random\n",
      "import matplotlib.pyplot as plot\n",
      "7/67:\n",
      "states = 117+1\n",
      "S = [i for i in range(states)]\n",
      "A = [i for i in range(16)]\n",
      "g = [[7,12,1,14],[2,13,8,11],[16,3,10,5],[9,6,15,4]]\n",
      "7/68:\n",
      "def grid(dir, a):\n",
      "    i = a/4\n",
      "    j = a - 4*i\n",
      "    if dir == \"\":\n",
      "        return g[i][j]\n",
      "    elif dir == \"up\":\n",
      "        return 0 if i == 0 else g[i-1][j]\n",
      "    elif dir == \"down\":\n",
      "        return 0 if i == 3 else g[i+1][j]\n",
      "    elif dir == \"left\":\n",
      "        return 0 if j == 0 else g[i][j-1]\n",
      "    else:\n",
      "        return 0 if j == 3 else g[i][j+1]\n",
      "7/69:\n",
      "def reward(s):\n",
      "    if s < 101:\n",
      "        return 0\n",
      "    elif s == 101:\n",
      "        return 1\n",
      "    else:\n",
      "        return -1\n",
      "7/70:\n",
      "def psum(s, a, V):\n",
      "    if s > 100:\n",
      "        return 0\n",
      "    sum = 0\n",
      "    sum += 0.6*V[grid(\"\", a)+s]\n",
      "    sum += 0.1*V[grid(\"up\", a)+s] + 0.1*V[grid(\"down\", a)+s]\n",
      "    sum += 0.1*V[grid(\"left\", a)+s] + 0.1*V[grid(\"right\", a)+s]\n",
      "    return sum\n",
      "7/71:\n",
      "def value_iteration(gamma, epsilon):\n",
      "    V = np.array([0.0 for i in range(states)])\n",
      "    policy = [0 for i in range(states)]\n",
      "    it = 0\n",
      "    vfuncs = [V.copy()]\n",
      "    Q = [[0 for i in range(16)] for j in range(states)]\n",
      "    while True:\n",
      "        it = it + 1\n",
      "        V_old = V.copy()\n",
      "        for s in S:\n",
      "            for a in A:\n",
      "                Q[s][a] = reward(s) + gamma*psum(s, a, V_old)\n",
      "            policy[s] = np.argmax(Q[s])\n",
      "            V[s] = Q[s][policy[s]]\n",
      "        vfuncs.append(V.copy())\n",
      "        if np.allclose(V, V_old, atol=epsilon):\n",
      "            break\n",
      "    return V, policy, it, vfuncs\n",
      "7/72:\n",
      "V, policy, it, vfuncs = value_iteration(gamma=1, epsilon=0)\n",
      "print V\n",
      "print policy\n",
      "print \"iterations: \" + str(it)\n",
      "7/73:\n",
      "def simulate(policy, trials):\n",
      "    successes = 0\n",
      "    for i in range(trials):\n",
      "        score = 0\n",
      "        while score < 101:\n",
      "            r = random.random()\n",
      "            p = policy[score]\n",
      "            if r < 0.6:\n",
      "                score += grid(\"\", p)\n",
      "            elif r < 0.7:\n",
      "                score += grid(\"up\", p)\n",
      "            elif r < 0.8:\n",
      "                score += grid(\"down\", p)\n",
      "            elif r < 0.9:\n",
      "                score += grid(\"left\", p)\n",
      "            else:\n",
      "                score += grid(\"right\", p)\n",
      "        successes += 1 if score == 101 else 0\n",
      "    print float(successes)/trials\n",
      "    return\n",
      "7/74:\n",
      "simulate(policy, 10000)\n",
      "simulate(policy, 10000)\n",
      "simulate(policy, 10000)\n",
      "7/75:\n",
      "def plot_states(V):\n",
      "    xs = [i for i in range(len(V))]\n",
      "    for i in range(len(V)):\n",
      "        plot.plot(xs, V)\n",
      "7/76:\n",
      "plot.figure(figsize=(12,8))\n",
      "plot_states(V)\n",
      "7/77:\n",
      "def plot_states(V):\n",
      "    xs = [i for i in range(len(V))]\n",
      "    for i in range(len(V)):\n",
      "        plot.plot(xs, V, color=\"b\")\n",
      "7/78:\n",
      "plot.figure(figsize=(12,8))\n",
      "plot_states(V)\n",
      "7/79: np.argsort(V1)[::-1]\n",
      "7/80: np.argsort(V)[::-1]\n",
      "7/81:\n",
      "plot.figure(figsize=(12,8))\n",
      "plot_states(V)\n",
      "plot.xlabel(\"state (score)\")\n",
      "plot.ylabel(\"value function\")\n",
      "7/82:\n",
      "plot_vfuncs(vfuncs, 1)\n",
      "plot.xlabel(\"iterations\")\n",
      "plot.ylabel(\"value function of state 0\")\n",
      "7/83:\n",
      "def plot_states(V):\n",
      "    xs = [i for i in range(len(V))]\n",
      "    for i in range(len(V)):\n",
      "        plot.plot(xs, V, color=\"b\")\n",
      "7/84:\n",
      "plot.figure(figsize=(12,8))\n",
      "plot_states(V)\n",
      "plot.xlabel(\"state (score)\")\n",
      "plot.ylabel(\"value function\")\n",
      "7/85:\n",
      "def plot_vfuncs(vfuncs, n):\n",
      "    xs = [i for i in range(len(vfuncs))]\n",
      "    values = np.array(vfuncs).T\n",
      "    for i in range(n):\n",
      "        plot.plot(xs, values[i])\n",
      "7/86:\n",
      "plot_vfuncs(vfuncs, 1)\n",
      "plot.xlabel(\"iterations\")\n",
      "plot.ylabel(\"value function of state 0\")\n",
      "7/87:\n",
      "plot.figure(figsize=(10,12))\n",
      "plot_vfuncs_color(vfuncs)\n",
      "7/88:\n",
      "plot.figure(figsize=(10,10))\n",
      "plot_vfuncs_color(vfuncs)\n",
      "7/89:\n",
      "import math\n",
      "import numpy as np\n",
      "import random\n",
      "import matplotlib.pyplot as plot\n",
      " 8/1:\n",
      "import math\n",
      "import numpy as np\n",
      "import random\n",
      "import matplotlib.pyplot as plot\n",
      " 8/2:\n",
      "states = 117+1\n",
      "S = [i for i in range(states)]\n",
      "A = [i for i in range(16)]\n",
      "g = [[7,12,1,14],[2,13,8,11],[16,3,10,5],[9,6,15,4]]\n",
      " 8/3:\n",
      "def grid(dir, a):\n",
      "    i = a/4\n",
      "    j = a - 4*i\n",
      "    if dir == \"\":\n",
      "        return g[i][j]\n",
      "    elif dir == \"up\":\n",
      "        return 0 if i == 0 else g[i-1][j]\n",
      "    elif dir == \"down\":\n",
      "        return 0 if i == 3 else g[i+1][j]\n",
      "    elif dir == \"left\":\n",
      "        return 0 if j == 0 else g[i][j-1]\n",
      "    else:\n",
      "        return 0 if j == 3 else g[i][j+1]\n",
      " 8/4:\n",
      "def reward(s):\n",
      "    if s < 101:\n",
      "        return 0\n",
      "    elif s == 101:\n",
      "        return 1\n",
      "    else:\n",
      "        return -1\n",
      " 8/5:\n",
      "def psum(s, a, V):\n",
      "    if s > 100:\n",
      "        return 0\n",
      "    sum = 0\n",
      "    sum += 0.6*V[grid(\"\", a)+s]\n",
      "    sum += 0.1*V[grid(\"up\", a)+s] + 0.1*V[grid(\"down\", a)+s]\n",
      "    sum += 0.1*V[grid(\"left\", a)+s] + 0.1*V[grid(\"right\", a)+s]\n",
      "    return sum\n",
      " 8/6:\n",
      "def value_iteration(gamma, epsilon):\n",
      "    V = np.array([0.0 for i in range(states)])\n",
      "    policy = [0 for i in range(states)]\n",
      "    it = 0\n",
      "    vfuncs = [V.copy()]\n",
      "    Q = [[0 for i in range(16)] for j in range(states)]\n",
      "    while True:\n",
      "        it = it + 1\n",
      "        V_old = V.copy()\n",
      "        for s in S:\n",
      "            for a in A:\n",
      "                Q[s][a] = reward(s) + gamma*psum(s, a, V_old)\n",
      "            policy[s] = np.argmax(Q[s])\n",
      "            V[s] = Q[s][policy[s]]\n",
      "        vfuncs.append(V.copy())\n",
      "        if np.allclose(V, V_old, atol=epsilon):\n",
      "            break\n",
      "    return V, policy, it, vfuncs\n",
      " 8/7:\n",
      "V, policy, it, vfuncs = value_iteration(gamma=1, epsilon=0)\n",
      "print V\n",
      "print policy\n",
      "print \"iterations: \" + str(it)\n",
      " 8/8:\n",
      "def simulate(policy, trials):\n",
      "    successes = 0\n",
      "    for i in range(trials):\n",
      "        score = 0\n",
      "        while score < 101:\n",
      "            r = random.random()\n",
      "            p = policy[score]\n",
      "            if r < 0.6:\n",
      "                score += grid(\"\", p)\n",
      "            elif r < 0.7:\n",
      "                score += grid(\"up\", p)\n",
      "            elif r < 0.8:\n",
      "                score += grid(\"down\", p)\n",
      "            elif r < 0.9:\n",
      "                score += grid(\"left\", p)\n",
      "            else:\n",
      "                score += grid(\"right\", p)\n",
      "        successes += 1 if score == 101 else 0\n",
      "    print float(successes)/trials\n",
      "    return\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 8/9:\n",
      "simulate(policy, 10000)\n",
      "simulate(policy, 10000)\n",
      "simulate(policy, 10000)\n",
      "8/10:\n",
      "def plot_states(V):\n",
      "    xs = [i for i in range(len(V))]\n",
      "    for i in range(len(V)):\n",
      "        plot.plot(xs, V, color=\"b\")\n",
      "8/11:\n",
      "plot.figure(figsize=(12,8))\n",
      "plot_states(V)\n",
      "plot.xlabel(\"state (score)\")\n",
      "plot.ylabel(\"value function\")\n",
      "8/12:\n",
      "def plot_vfuncs(vfuncs, n):\n",
      "    xs = [i for i in range(len(vfuncs))]\n",
      "    values = np.array(vfuncs).T\n",
      "    for i in range(n):\n",
      "        plot.plot(xs, values[i])\n",
      "8/13:\n",
      "plot_vfuncs(vfuncs, 1)\n",
      "plot.xlabel(\"iterations\")\n",
      "plot.ylabel(\"value function of state 0\")\n",
      "8/14: np.argsort(V)[::-1]\n",
      "8/15:\n",
      "def plot_vfuncs_color(vfuncs):\n",
      "    while len(vfuncs) < 24:\n",
      "        vfuncs.append(vfuncs[len(vfuncs)-1])\n",
      "    values = np.array(vfuncs).T\n",
      "    plot.imshow(values)\n",
      "8/16:\n",
      "plot.figure(figsize=(10,10))\n",
      "plot_vfuncs_color(vfuncs)\n",
      "8/17:\n",
      "plot_vfuncs(vfuncs, len(states))\n",
      "plot.xlabel(\"iterations\")\n",
      "plot.ylabel(\"value functions of all states\")\n",
      "8/18:\n",
      "plot_vfuncs(vfuncs, states)\n",
      "plot.xlabel(\"iterations\")\n",
      "plot.ylabel(\"value functions of all states\")\n",
      " 9/1:\n",
      "import math\n",
      "import numpy as np\n",
      "import random\n",
      "import matplotlib.pyplot as plot\n",
      " 9/2:\n",
      "states = 117+1\n",
      "S = [i for i in range(states)]\n",
      "A = [i for i in range(16)]\n",
      "g = [[7,12,1,14],[2,13,8,11],[16,3,10,5],[9,6,15,4]]\n",
      " 9/3:\n",
      "def grid(dir, a):\n",
      "    i = a/4\n",
      "    j = a - 4*i\n",
      "    if dir == \"\":\n",
      "        return g[i][j]\n",
      "    elif dir == \"up\":\n",
      "        return 0 if i == 0 else g[i-1][j]\n",
      "    elif dir == \"down\":\n",
      "        return 0 if i == 3 else g[i+1][j]\n",
      "    elif dir == \"left\":\n",
      "        return 0 if j == 0 else g[i][j-1]\n",
      "    else:\n",
      "        return 0 if j == 3 else g[i][j+1]\n",
      " 9/4:\n",
      "def reward(s):\n",
      "    if s < 101:\n",
      "        return 0\n",
      "    elif s == 101:\n",
      "        return 1\n",
      "    else:\n",
      "        return -1\n",
      " 9/5:\n",
      "# expectation of V from taking action a in state s\n",
      "def psum(s, a, V):\n",
      "    if s > 100:\n",
      "        return 0\n",
      "    sum = 0\n",
      "    sum += 0.6*V[grid(\"\", a)+s]\n",
      "    sum += 0.1*V[grid(\"up\", a)+s] + 0.1*V[grid(\"down\", a)+s]\n",
      "    sum += 0.1*V[grid(\"left\", a)+s] + 0.1*V[grid(\"right\", a)+s]\n",
      "    return sum\n",
      " 9/6:\n",
      "def value_iteration(gamma, epsilon):\n",
      "    V = np.array([0.0 for i in range(states)])\n",
      "    policy = [0 for i in range(states)]\n",
      "    it = 0\n",
      "    vfuncs = [V.copy()]\n",
      "    Q = [[0 for i in range(16)] for j in range(states)]\n",
      "    while True:\n",
      "        it = it + 1\n",
      "        V_old = V.copy()\n",
      "        for s in S:\n",
      "            for a in A:\n",
      "                Q[s][a] = reward(s) + gamma*psum(s, a, V_old)\n",
      "            policy[s] = np.argmax(Q[s])\n",
      "            V[s] = Q[s][policy[s]]\n",
      "        vfuncs.append(V.copy())\n",
      "        if np.allclose(V, V_old, atol=epsilon):\n",
      "            break\n",
      "    return V, policy, it, vfuncs\n",
      " 9/7:\n",
      "V, policy, it, vfuncs = value_iteration(gamma=1, epsilon=0)\n",
      "print V\n",
      "print policy\n",
      "print \"iterations: \" + str(it)\n",
      " 9/8:\n",
      "# simulate trajectory of game using policy\n",
      "def simulate(policy, trials):\n",
      "    successes = 0\n",
      "    for i in range(trials):\n",
      "        score = 0\n",
      "        while score < 101:\n",
      "            r = random.random()\n",
      "            p = policy[score]\n",
      "            if r < 0.6:\n",
      "                score += grid(\"\", p)\n",
      "            elif r < 0.7:\n",
      "                score += grid(\"up\", p)\n",
      "            elif r < 0.8:\n",
      "                score += grid(\"down\", p)\n",
      "            elif r < 0.9:\n",
      "                score += grid(\"left\", p)\n",
      "            else:\n",
      "                score += grid(\"right\", p)\n",
      "        successes += 1 if score == 101 else 0\n",
      "    print float(successes)/trials\n",
      "    return\n",
      " 9/9:\n",
      "simulate(policy, 10000)\n",
      "simulate(policy, 10000)\n",
      "simulate(policy, 10000)\n",
      "9/10:\n",
      "# plot of value function over states\n",
      "def plot_states(V):\n",
      "    xs = [i for i in range(len(V))]\n",
      "    for i in range(len(V)):\n",
      "        plot.plot(xs, V, color=\"b\")\n",
      "9/11:\n",
      "plot.figure(figsize=(12,8))\n",
      "plot_states(V)\n",
      "plot.xlabel(\"state (score)\")\n",
      "plot.ylabel(\"value function\")\n",
      "9/12:\n",
      "# plot of value function over iterations\n",
      "def plot_vfuncs(vfuncs, n):\n",
      "    xs = [i for i in range(len(vfuncs))]\n",
      "    values = np.array(vfuncs).T\n",
      "    for i in range(n):\n",
      "        plot.plot(xs, values[i])\n",
      "9/13:\n",
      "plot_vfuncs(vfuncs, 1)\n",
      "plot.xlabel(\"iterations\")\n",
      "plot.ylabel(\"value function of state 0\")\n",
      "9/14:\n",
      "plot_vfuncs(vfuncs, states)\n",
      "plot.xlabel(\"iterations\")\n",
      "plot.ylabel(\"value functions of all states\")\n",
      "9/15:\n",
      "# states sorted by highest value function to lowest\n",
      "np.argsort(V)[::-1]\n",
      "9/16:\n",
      "# color plot of value function for each state over iterations - warmer colors means higher value\n",
      "def plot_vfuncs_color(vfuncs):\n",
      "    while len(vfuncs) < 24:\n",
      "        vfuncs.append(vfuncs[len(vfuncs)-1])\n",
      "    values = np.array(vfuncs).T\n",
      "    plot.imshow(values)\n",
      "9/17:\n",
      "plot.figure(figsize=(10,10))\n",
      "plot_vfuncs_color(vfuncs)\n",
      "10/1:\n",
      "import math\n",
      "import numpy as np\n",
      "import random\n",
      "import matplotlib.pyplot as plot\n",
      "10/2:\n",
      "states = 117+1\n",
      "S = [i for i in range(states)]\n",
      "A = [i for i in range(16)]\n",
      "g = [[7,12,1,14],[2,13,8,11],[16,3,10,5],[9,6,15,4]]\n",
      "10/3:\n",
      "def grid(dir, a):\n",
      "    i = a/4\n",
      "    j = a - 4*i\n",
      "    if dir == \"\":\n",
      "        return g[i][j]\n",
      "    elif dir == \"up\":\n",
      "        return 0 if i == 0 else g[i-1][j]\n",
      "    elif dir == \"down\":\n",
      "        return 0 if i == 3 else g[i+1][j]\n",
      "    elif dir == \"left\":\n",
      "        return 0 if j == 0 else g[i][j-1]\n",
      "    else:\n",
      "        return 0 if j == 3 else g[i][j+1]\n",
      "10/4:\n",
      "def reward(s):\n",
      "    if s < 101:\n",
      "        return 0\n",
      "    elif s == 101:\n",
      "        return 1\n",
      "    else:\n",
      "        return -1\n",
      "10/5:\n",
      "# expectation of V from taking action a in state s\n",
      "def psum(s, a, V):\n",
      "    if s > 100:\n",
      "        return 0\n",
      "    sum = 0\n",
      "    sum += 0.6*V[grid(\"\", a)+s]\n",
      "    sum += 0.1*V[grid(\"up\", a)+s] + 0.1*V[grid(\"down\", a)+s]\n",
      "    sum += 0.1*V[grid(\"left\", a)+s] + 0.1*V[grid(\"right\", a)+s]\n",
      "    return sum\n",
      "10/6:\n",
      "def value_iteration(gamma, epsilon):\n",
      "    V = np.array([0.0 for i in range(states)])\n",
      "    policy = [0 for i in range(states)]\n",
      "    it = 0\n",
      "    vfuncs = [V.copy()]\n",
      "    Q = [[0 for i in range(16)] for j in range(states)]\n",
      "    while True:\n",
      "        it = it + 1\n",
      "        V_old = V.copy()\n",
      "        for s in S:\n",
      "            for a in A:\n",
      "                Q[s][a] = reward(s) + gamma*psum(s, a, V_old)\n",
      "            policy[s] = np.argmax(Q[s])\n",
      "            V[s] = Q[s][policy[s]]\n",
      "        vfuncs.append(V.copy())\n",
      "        if np.allclose(V, V_old, atol=epsilon):\n",
      "            break\n",
      "    return V, policy, it, vfuncs\n",
      "10/7:\n",
      "V, policy, it, vfuncs = value_iteration(gamma=1, epsilon=0)\n",
      "print V\n",
      "print policy\n",
      "print \"iterations: \" + str(it)\n",
      "10/8:\n",
      "# simulate trajectory of game using policy\n",
      "def simulate(policy, trials):\n",
      "    successes = 0\n",
      "    for i in range(trials):\n",
      "        score = 0\n",
      "        while score < 101:\n",
      "            r = random.random()\n",
      "            p = policy[score]\n",
      "            if r < 0.6:\n",
      "                score += grid(\"\", p)\n",
      "            elif r < 0.7:\n",
      "                score += grid(\"up\", p)\n",
      "            elif r < 0.8:\n",
      "                score += grid(\"down\", p)\n",
      "            elif r < 0.9:\n",
      "                score += grid(\"left\", p)\n",
      "            else:\n",
      "                score += grid(\"right\", p)\n",
      "            print score\n",
      "        successes += 1 if score == 101 else 0\n",
      "    print float(successes)/trials\n",
      "    return\n",
      "10/9:\n",
      "simulate(policy, 10000)\n",
      "simulate(policy, 10000)\n",
      "simulate(policy, 10000)\n",
      "10/10:\n",
      "# simulate trajectory of game using policy\n",
      "def simulate(policy, trials):\n",
      "    successes = 0\n",
      "    for i in range(trials):\n",
      "        score = 0\n",
      "        while score < 101:\n",
      "            r = random.random()\n",
      "            p = policy[score]\n",
      "            if r < 0.6:\n",
      "                score += grid(\"\", p)\n",
      "            elif r < 0.7:\n",
      "                score += grid(\"up\", p)\n",
      "            elif r < 0.8:\n",
      "                score += grid(\"down\", p)\n",
      "            elif r < 0.9:\n",
      "                score += grid(\"left\", p)\n",
      "            else:\n",
      "                score += grid(\"right\", p)\n",
      "#             print score\n",
      "        successes += 1 if score == 101 else 0\n",
      "    print float(successes)/trials\n",
      "    return\n",
      "10/11:\n",
      "simulate(policy, 10000)\n",
      "simulate(policy, 10000)\n",
      "simulate(policy, 10000)\n",
      "10/12:\n",
      "# plot of value function over states\n",
      "def plot_states(V):\n",
      "    xs = [i for i in range(len(V))]\n",
      "    for i in range(len(V)):\n",
      "        plot.plot(xs, V, color=\"b\")\n",
      "10/13:\n",
      "plot.figure(figsize=(12,8))\n",
      "plot_states(V)\n",
      "plot.xlabel(\"state (score)\")\n",
      "plot.ylabel(\"value function\")\n",
      "10/14:\n",
      "# plot of value function over iterations\n",
      "def plot_vfuncs(vfuncs, n):\n",
      "    xs = [i for i in range(len(vfuncs))]\n",
      "    values = np.array(vfuncs).T\n",
      "    for i in range(n):\n",
      "        plot.plot(xs, values[i])\n",
      "10/15:\n",
      "plot_vfuncs(vfuncs, 1)\n",
      "plot.xlabel(\"iterations\")\n",
      "plot.ylabel(\"value function of state 0\")\n",
      "10/16:\n",
      "plot_vfuncs(vfuncs, states)\n",
      "plot.xlabel(\"iterations\")\n",
      "plot.ylabel(\"value functions of all states\")\n",
      "10/17:\n",
      "# states sorted by highest value function to lowest\n",
      "np.argsort(V)[::-1]\n",
      "10/18:\n",
      "# color plot of value function for each state over iterations - warmer colors means higher value\n",
      "def plot_vfuncs_color(vfuncs):\n",
      "    while len(vfuncs) < 24:\n",
      "        vfuncs.append(vfuncs[len(vfuncs)-1])\n",
      "    values = np.array(vfuncs).T\n",
      "    plot.imshow(values)\n",
      "10/19:\n",
      "plot.figure(figsize=(10,10))\n",
      "plot_vfuncs_color(vfuncs)\n",
      "11/1:\n",
      "import math\n",
      "import numpy as np\n",
      "import random\n",
      "import matplotlib.pyplot as plot\n",
      "11/2:\n",
      "states = 117+1\n",
      "S = [i for i in range(states)]\n",
      "A = [i for i in range(16)]\n",
      "g = [[7,12,1,14],[2,13,8,11],[16,3,10,5],[9,6,15,4]]\n",
      "11/3:\n",
      "def grid(dir, a):\n",
      "    i = a/4\n",
      "    j = a - 4*i\n",
      "    if dir == \"\":\n",
      "        return g[i][j]\n",
      "    elif dir == \"up\":\n",
      "        return 0 if i == 0 else g[i-1][j]\n",
      "    elif dir == \"down\":\n",
      "        return 0 if i == 3 else g[i+1][j]\n",
      "    elif dir == \"left\":\n",
      "        return 0 if j == 0 else g[i][j-1]\n",
      "    else:\n",
      "        return 0 if j == 3 else g[i][j+1]\n",
      "11/4:\n",
      "def reward(s):\n",
      "    if s < 101:\n",
      "        return 0\n",
      "    elif s == 101:\n",
      "        return 1\n",
      "    else:\n",
      "        return -1\n",
      "11/5:\n",
      "# expectation of V from taking action a in state s\n",
      "def psum(s, a, V):\n",
      "    if s > 100:\n",
      "        return 0\n",
      "    sum = 0\n",
      "    sum += 0.6*V[grid(\"\", a)+s]\n",
      "    sum += 0.1*V[grid(\"up\", a)+s] + 0.1*V[grid(\"down\", a)+s]\n",
      "    sum += 0.1*V[grid(\"left\", a)+s] + 0.1*V[grid(\"right\", a)+s]\n",
      "    return sum\n",
      "11/6:\n",
      "def value_iteration(gamma, epsilon):\n",
      "    V = np.array([0.0 for i in range(states)])\n",
      "    policy = [0 for i in range(states)]\n",
      "    it = 0\n",
      "    vfuncs = [V.copy()]\n",
      "    Q = [[0 for i in range(16)] for j in range(states)]\n",
      "    while True:\n",
      "        it = it + 1\n",
      "        V_old = V.copy()\n",
      "        for s in S:\n",
      "            for a in A:\n",
      "                Q[s][a] = reward(s) + gamma*psum(s, a, V_old)\n",
      "            policy[s] = np.argmax(Q[s])\n",
      "            V[s] = Q[s][policy[s]]\n",
      "        vfuncs.append(V.copy())\n",
      "        if np.allclose(V, V_old, atol=epsilon):\n",
      "            break\n",
      "    return V, policy, it, vfuncs\n",
      "11/7:\n",
      "V, policy, it, vfuncs = value_iteration(gamma=1, epsilon=0)\n",
      "print V\n",
      "print policy\n",
      "print \"iterations: \" + str(it)\n",
      "11/8:\n",
      "V99, policy99, it99, vfuncs99 = value_iteration(gamma=0.99, epsilon=0)\n",
      "print V99\n",
      "print policy99\n",
      "print \"iterations: \" + str(it99)\n",
      "11/9:\n",
      "# simulate trajectory of game using policy\n",
      "def simulate(policy, trials):\n",
      "    successes = 0\n",
      "    for i in range(trials):\n",
      "        score = 0\n",
      "        while score < 101:\n",
      "            r = random.random()\n",
      "            p = policy[score]\n",
      "            if r < 0.6:\n",
      "                score += grid(\"\", p)\n",
      "            elif r < 0.7:\n",
      "                score += grid(\"up\", p)\n",
      "            elif r < 0.8:\n",
      "                score += grid(\"down\", p)\n",
      "            elif r < 0.9:\n",
      "                score += grid(\"left\", p)\n",
      "            else:\n",
      "                score += grid(\"right\", p)\n",
      "#             print score\n",
      "        successes += 1 if score == 101 else 0\n",
      "    print float(successes)/trials\n",
      "    return\n",
      "11/10:\n",
      "simulate(policy, 10000)\n",
      "simulate(policy, 10000)\n",
      "simulate(policy, 10000)\n",
      "11/11:\n",
      "simulate(policy99, 10000)\n",
      "simulate(policy99, 10000)\n",
      "simulate(policy99, 10000)\n",
      "11/12:\n",
      "simulate(policy99, 10000)\n",
      "simulate(policy99, 10000)\n",
      "simulate(policy99, 10000)\n",
      "11/13:\n",
      "# plot of value function over states\n",
      "def plot_states(V):\n",
      "    xs = [i for i in range(len(V))]\n",
      "    for i in range(len(V)):\n",
      "        plot.plot(xs, V, color=\"b\")\n",
      "11/14:\n",
      "plot.figure(figsize=(12,8))\n",
      "plot_states(V)\n",
      "plot.xlabel(\"state (score)\")\n",
      "plot.ylabel(\"value function\")\n",
      "11/15:\n",
      "# plot of value function over iterations\n",
      "def plot_vfuncs(vfuncs, n):\n",
      "    xs = [i for i in range(len(vfuncs))]\n",
      "    values = np.array(vfuncs).T\n",
      "    for i in range(n):\n",
      "        plot.plot(xs, values[i])\n",
      "11/16:\n",
      "plot_vfuncs(vfuncs, 1)\n",
      "plot.xlabel(\"iterations\")\n",
      "plot.ylabel(\"value function of state 0\")\n",
      "11/17:\n",
      "plot_vfuncs(vfuncs, states)\n",
      "plot.xlabel(\"iterations\")\n",
      "plot.ylabel(\"value functions of all states\")\n",
      "11/18:\n",
      "# states sorted by highest value function to lowest\n",
      "np.argsort(V)[::-1]\n",
      "11/19:\n",
      "# color plot of value function for each state over iterations - warmer colors means higher value\n",
      "def plot_vfuncs_color(vfuncs):\n",
      "    while len(vfuncs) < 24:\n",
      "        vfuncs.append(vfuncs[len(vfuncs)-1])\n",
      "    values = np.array(vfuncs).T\n",
      "    plot.imshow(values)\n",
      "11/20:\n",
      "plot.figure(figsize=(10,10))\n",
      "plot_vfuncs_color(vfuncs)\n",
      "12/1:\n",
      "import math\n",
      "import numpy as np\n",
      "import random\n",
      "import matplotlib.pyplot as plot\n",
      "12/2:\n",
      "states = 117+1\n",
      "S = [i for i in range(states)]\n",
      "A = [i for i in range(16)]\n",
      "g = [[7,12,1,14],[2,13,8,11],[16,3,10,5],[9,6,15,4]]\n",
      "12/3:\n",
      "def grid(dir, a):\n",
      "    i = a/4\n",
      "    j = a - 4*i\n",
      "    if dir == \"\":\n",
      "        return g[i][j]\n",
      "    elif dir == \"up\":\n",
      "        return 0 if i == 0 else g[i-1][j]\n",
      "    elif dir == \"down\":\n",
      "        return 0 if i == 3 else g[i+1][j]\n",
      "    elif dir == \"left\":\n",
      "        return 0 if j == 0 else g[i][j-1]\n",
      "    else:\n",
      "        return 0 if j == 3 else g[i][j+1]\n",
      "12/4:\n",
      "def reward(s):\n",
      "    if s < 101:\n",
      "        return 0\n",
      "    elif s == 101:\n",
      "        return 1\n",
      "    else:\n",
      "        return -1\n",
      "12/5:\n",
      "# expectation of V from taking action a in state s\n",
      "def psum(s, a, V):\n",
      "    if s > 100:\n",
      "        return 0\n",
      "    sum = 0\n",
      "    sum += 0.6*V[grid(\"\", a)+s]\n",
      "    sum += 0.1*V[grid(\"up\", a)+s] + 0.1*V[grid(\"down\", a)+s]\n",
      "    sum += 0.1*V[grid(\"left\", a)+s] + 0.1*V[grid(\"right\", a)+s]\n",
      "    return sum\n",
      "12/6:\n",
      "def value_iteration(gamma, epsilon):\n",
      "    V = np.array([0.0 for i in range(states)])\n",
      "    policy = [0 for i in range(states)]\n",
      "    it = 0\n",
      "    vfuncs = [V.copy()]\n",
      "    Q = [[0 for i in range(16)] for j in range(states)]\n",
      "    while True:\n",
      "        it = it + 1\n",
      "        V_old = V.copy()\n",
      "        for s in S:\n",
      "            for a in A:\n",
      "                Q[s][a] = reward(s) + gamma*psum(s, a, V_old)\n",
      "            policy[s] = np.argmax(Q[s])\n",
      "            V[s] = Q[s][policy[s]]\n",
      "        vfuncs.append(V.copy())\n",
      "        if np.allclose(V, V_old, atol=epsilon):\n",
      "            break\n",
      "    return V, policy, it, vfuncs\n",
      "12/7:\n",
      "V, policy, it, vfuncs = value_iteration(gamma=1, epsilon=0)\n",
      "print V\n",
      "print policy\n",
      "print \"iterations: \" + str(it)\n",
      "12/8:\n",
      "V99, policy99, it99, vfuncs99 = value_iteration(gamma=0.99, epsilon=0)\n",
      "print V99\n",
      "print policy99\n",
      "print \"iterations: \" + str(it99)\n",
      "12/9:\n",
      "# simulate trajectory of game using policy\n",
      "def simulate(policy, trials):\n",
      "    successes = 0\n",
      "    for i in range(trials):\n",
      "        score = 0\n",
      "        while score < 101:\n",
      "            r = random.random()\n",
      "            p = policy[score]\n",
      "            if r < 0.6:\n",
      "                score += grid(\"\", p)\n",
      "            elif r < 0.7:\n",
      "                score += grid(\"up\", p)\n",
      "            elif r < 0.8:\n",
      "                score += grid(\"down\", p)\n",
      "            elif r < 0.9:\n",
      "                score += grid(\"left\", p)\n",
      "            else:\n",
      "                score += grid(\"right\", p)\n",
      "#             print score\n",
      "        successes += 1 if score == 101 else 0\n",
      "    print float(successes)/trials\n",
      "    return\n",
      "12/10:\n",
      "# plot of value function over states\n",
      "def plot_states(V):\n",
      "    xs = [i for i in range(len(V))]\n",
      "    for i in range(len(V)):\n",
      "        plot.plot(xs, V, color=\"b\")\n",
      "12/11:\n",
      "plot.figure(figsize=(12,8))\n",
      "plot_states(V)\n",
      "plot.xlabel(\"state (score)\")\n",
      "plot.ylabel(\"value function\")\n",
      "12/12:\n",
      "plot.figure(figsize=(12,8))\n",
      "plot_states(V99)\n",
      "plot.xlabel(\"state (score)\")\n",
      "plot.ylabel(\"value function\")\n",
      "12/13:\n",
      "V9, policy9, it9, vfuncs9 = value_iteration(gamma=0.9, epsilon=0)\n",
      "plot.figure(figsize=(12,8))\n",
      "plot_states(V99)\n",
      "plot.xlabel(\"state (score)\")\n",
      "plot.ylabel(\"value function\")\n",
      "12/14:\n",
      "plot.figure(figsize=(12,8))\n",
      "plot_states(V99)\n",
      "plot.xlabel(\"state (score)\")\n",
      "plot.ylabel(\"value function\")\n",
      "12/15:\n",
      "V, policy, it, vfuncs = value_iteration(gamma=1, epsilon=0)\n",
      "print V\n",
      "print policy\n",
      "print \"iterations: \" + str(it)\n",
      "12/16:\n",
      "simulate(policy, 10000)\n",
      "simulate(policy, 10000)\n",
      "simulate(policy, 10000)\n",
      "12/17:\n",
      "V99, policy99, it99, vfuncs99 = value_iteration(gamma=0.99, epsilon=0)\n",
      "print V99\n",
      "print policy99\n",
      "print \"iterations: \" + str(it99)\n",
      "12/18:\n",
      "# simulate trajectory of game using policy\n",
      "def simulate(policy, trials):\n",
      "    successes = 0\n",
      "    for i in range(trials):\n",
      "        score = 0\n",
      "        while score < 101:\n",
      "            r = random.random()\n",
      "            p = policy[score]\n",
      "            if r < 0.6:\n",
      "                score += grid(\"\", p)\n",
      "            elif r < 0.7:\n",
      "                score += grid(\"up\", p)\n",
      "            elif r < 0.8:\n",
      "                score += grid(\"down\", p)\n",
      "            elif r < 0.9:\n",
      "                score += grid(\"left\", p)\n",
      "            else:\n",
      "                score += grid(\"right\", p)\n",
      "#             print score\n",
      "        successes += 1 if score == 101 else 0\n",
      "    print float(successes)/trials\n",
      "    return\n",
      "12/19:\n",
      "simulate(policy99, 10000)\n",
      "simulate(policy99, 10000)\n",
      "simulate(policy99, 10000)\n",
      "12/20:\n",
      "simulate(policy99, 10000)\n",
      "simulate(policy99, 10000)\n",
      "simulate(policy99, 10000)\n",
      "12/21:\n",
      "# plot of value function over states\n",
      "def plot_states(V):\n",
      "    xs = [i for i in range(len(V))]\n",
      "    for i in range(len(V)):\n",
      "        plot.plot(xs, V, color=\"b\")\n",
      "12/22:\n",
      "plot.figure(figsize=(12,8))\n",
      "plot_states(V99)\n",
      "plot.xlabel(\"state (score)\")\n",
      "plot.ylabel(\"value function\")\n",
      "12/23:\n",
      "simulate(policy, 10000)\n",
      "simulate(policy, 10000)\n",
      "simulate(policy, 10000)\n",
      "12/24:\n",
      "V, policy, it, vfuncs = value_iteration(gamma=1, epsilon=0)\n",
      "print V\n",
      "print policy\n",
      "print \"iterations: \" + str(it)\n",
      "12/25:\n",
      "plot.figure(figsize=(12,8))\n",
      "plot_states(V99)\n",
      "plot.xlabel(\"state (score)\")76\n",
      "plot.ylabel(\"value function\")\n",
      "12/26:\n",
      "plot.figure(figsize=(12,8))\n",
      "plot_states(V99)\n",
      "plot.xlabel(\"state (score)\")\n",
      "plot.ylabel(\"value function\")\n",
      "12/27:\n",
      "plot.figure(figsize=(12,8))\n",
      "plot_states(V)\n",
      "plot.xlabel(\"state (score)\")\n",
      "plot.ylabel(\"value function\")\n",
      "12/28:\n",
      "plot_vfuncs(vfuncs99, 1)\n",
      "plot.xlabel(\"iterations\")\n",
      "plot.ylabel(\"value function of state 0\")\n",
      "12/29:\n",
      "# plot of value function over iterations\n",
      "def plot_vfuncs(vfuncs, n):\n",
      "    xs = [i for i in range(len(vfuncs))]\n",
      "    values = np.array(vfuncs).T\n",
      "    for i in range(n):\n",
      "        plot.plot(xs, values[i])\n",
      "12/30:\n",
      "plot_vfuncs(vfuncs99, 1)\n",
      "plot.xlabel(\"iterations\")\n",
      "plot.ylabel(\"value function of state 0\")\n",
      "12/31:\n",
      "plot_vfuncs(vfuncs, states)\n",
      "plot.xlabel(\"iterations\")\n",
      "plot.ylabel(\"value functions of all states\")\n",
      "12/32:\n",
      "# states sorted by highest value function to lowest\n",
      "np.argsort(V)[::-1]\n",
      "12/33:\n",
      "# color plot of value function for each state over iterations - warmer colors means higher value\n",
      "def plot_vfuncs_color(vfuncs):\n",
      "    while len(vfuncs) < 24:\n",
      "        vfuncs.append(vfuncs[len(vfuncs)-1])\n",
      "    values = np.array(vfuncs).T\n",
      "    plot.imshow(values)\n",
      "12/34:\n",
      "plot.figure(figsize=(10,10))\n",
      "plot_vfuncs_color(vfuncs)\n",
      "13/1:\n",
      "import sklearn\n",
      "from sklearn.naive_bayes import MultinomialNB\n",
      "mnb = MultinomialNB()\n",
      "x = \"-v_bag_of_words_5.csv\"\n",
      "import pandas as pd\n",
      "x = pd.read_csv(x)\n",
      "x.shape\n",
      "13/2:\n",
      "import sklearn\n",
      "from sklearn.naive_bayes import MultinomialNB\n",
      "mnb = MultinomialNB()\n",
      "x = \"-v_bag_of_words_5.csv\"\n",
      "import pandas as pd\n",
      "x = pd.read_csv(x, header=None)\n",
      "x.shape\n",
      "13/3:\n",
      "x = \"-v_bag_of_words_5.csv\"\n",
      "x = pd.read_csv(x, header=None)\n",
      "x.shape\n",
      "13/4:\n",
      "import sklearn\n",
      "from sklearn.naive_bayes import MultinomialNB\n",
      "mnb = MultinomialNB()\n",
      "\n",
      "import pandas as pd\n",
      "13/5:\n",
      "import sklearn\n",
      "from sklearn.naive_bayes import MultinomialNB\n",
      "mnb = MultinomialNB()\n",
      "import pandas as pd\n",
      "13/6:\n",
      "import sklearn\n",
      "from sklearn.naive_bayes import MultinomialNB\n",
      "import pandas as pd\n",
      "13/7: V\n",
      "13/8: mnb = MultinomialNB()\n",
      "13/9:\n",
      "x = \"-v_bag_of_words_5.csv\"\n",
      "x = pd.read_csv(x, header=None)\n",
      "x.shape\n",
      "13/10:\n",
      "y = open(\"-v_classes_5.txt\", \"r\")\n",
      "y = y.readlines()\n",
      "13/11:\n",
      "y = open(\"-v_classes_5.txt\", \"r\")\n",
      "y = y.readlines()\n",
      "y_train = [int(x) for x in y]\n",
      "13/12:\n",
      "y = open(\"-v_classes_5.txt\", \"r\")\n",
      "y = y.readlines()\n",
      "y_train = [int(x) for x in y]\n",
      "len(y_train)\n",
      "13/13: mnb.fit(x, y_train)\n",
      "13/14:\n",
      "x = \"-v_bag_of_words_5.csv\"\n",
      "x_train = pd.read_csv(x, header=None)\n",
      "x_train.shape\n",
      "13/15:\n",
      "y = open(\"-v_classes_5.txt\", \"r\")\n",
      "y = y.readlines()\n",
      "y_train = [int(x) for x in y]\n",
      "len(y_train)\n",
      "13/16: mnb.fit(x, y_train)\n",
      "13/17: mnb.fit(x_train, y_train)\n",
      "13/18: mnb.fit(x_train, y_train)\n",
      "13/19: mnb.get_params()\n",
      "13/20: mnb.predict(x_train)\n",
      "13/21: mnb.score(x_train, y_train)\n",
      "13/22:\n",
      "import sklearn\n",
      "from sklearn.naive_bayes import MultinomialNB\n",
      "import pand\n",
      "as as pd\n",
      "13/23:\n",
      "import sklearn\n",
      "from sklearn.naive_bayes import MultinomialNB\n",
      "import pandas as pd\n",
      "13/24: mnb = MultinomialNB()\n",
      "13/25:\n",
      "x = \"train_bag_of_words_5.csv\"\n",
      "x_train = pd.read_csv(x, header=None)\n",
      "x_train.shape\n",
      "13/26:\n",
      "y = open(\"train_classes_5.txt\", \"r\")\n",
      "y = y.readlines()\n",
      "y_train = [int(x) for x in y]\n",
      "len(y_train)\n",
      "13/27: mnb.fit(x_train, y_train)\n",
      "13/28: mnb.score(x_train, y_train)\n",
      "13/29:\n",
      "x2 = \"test_bag_of_words_5.csv\"\n",
      "x_test = pd.read_csv(x, header=None)\n",
      "x_test.shape\n",
      "13/30:\n",
      "x2 = \"test_bag_of_words_5.csv\"\n",
      "x_test = pd.read_csv(x2, header=None)\n",
      "x_test.shape\n",
      "13/31:\n",
      "y2 = open(\"test_classes_5.txt\", \"r\")\n",
      "y2 = y2.readlines()\n",
      "y_test = [int(x) for x in y2]\n",
      "len(y_test)\n",
      "13/32: mnb.score(x_test, y_test)\n",
      "13/33:\n",
      "x2 = \"test__bag_of_words_0.csv\"\n",
      "x_test = pd.read_csv(x2, header=None)\n",
      "x_test.shape\n",
      "13/34:\n",
      "y2 = open(\"test_classes_5.txt\", \"r\")\n",
      "y2 = y2.readlines()\n",
      "y_test = [int(x) for x in y2]\n",
      "len(y_test)\n",
      "13/35: mnb.score(x_test, y_test)\n",
      "13/36: cnb = ComplementNB()\n",
      "13/37:\n",
      "import sklearn\n",
      "from sklearn.naive_bayes import MultinomialNB, ComplementNB\n",
      "import pandas as pd\n",
      "13/38:\n",
      "import sklearn\n",
      "from sklearn.naive_bayes import MultinomialNB, ComplementNB\n",
      "import pandas as pd\n",
      "13/39:\n",
      "import sklearn\n",
      "from sklearn.naive_bayes import MultinomialNB, ComplementNB\n",
      "import pandas as pd\n",
      "13/40:\n",
      "import sklearn\n",
      "from sklearn.naive_bayes import MultinomialNB, ComplementNB\n",
      "import pandas as pd\n",
      "13/41: !pip install --user -U sklearn\n",
      "13/42: !pip install --user -U sklearn\n",
      "13/43:\n",
      "import sklearn\n",
      "from sklearn.naive_bayes import MultinomialNB, ComplementNB\n",
      "import pandas as pd\n",
      "13/44:\n",
      "import sys\n",
      "!{sys.executable} -m pip install sklearn\n",
      "13/45:\n",
      "import sklearn\n",
      "from sklearn.naive_bayes import MultinomialNB, ComplementNB\n",
      "import pandas as pd\n",
      "13/46:\n",
      "import sklearn\n",
      "from sklearn.naive_bayes import MultinomialNB, ComplementNB\n",
      "import pandas as pd\n",
      "13/47:\n",
      "import sys\n",
      "!{sys.executable} -m pip install -U sklearn\n",
      "13/48:\n",
      "import sklearn\n",
      "from sklearn.naive_bayes import MultinomialNB, ComplementNB\n",
      "import pandas as pd\n",
      "13/49:\n",
      "import sklearn\n",
      "from sklearn.naive_bayes import MultinomialNB\n",
      "import pandas as pd\n",
      "13/50: from sklearn.naive_bayes import ComplementNB\n",
      "13/51:\n",
      "import sys\n",
      "!{sys.executable} -m pip install --user -U sklearn\n",
      "14/1:\n",
      "import sys\n",
      "!{sys.executable} -m pip install --user -U sklearn\n",
      "14/2:\n",
      "import sklearn\n",
      "from sklearn.naive_bayes import MultinomialNB\n",
      "import pandas as pd\n",
      "14/3: from sklearn.naive_bayes import ComplementNB\n",
      "14/4: mnb = MultinomialNB()\n",
      "14/5:\n",
      "import sklearn\n",
      "from sklearn.naive_bayes import MultinomialNB\n",
      "import pandas as pd\n",
      "14/6: from sklearn.naive_bayes import ComplementNB\n",
      "14/7: mnb = MultinomialNB()\n",
      "14/8:\n",
      "x = \"train_bag_of_words_5.csv\"\n",
      "x_train = pd.read_csv(x, header=None)\n",
      "x_train.shape\n",
      "14/9:\n",
      "y = open(\"train_classes_5.txt\", \"r\")\n",
      "y = y.readlines()\n",
      "y_train = [int(x) for x in y]\n",
      "len(y_train)\n",
      "14/10:\n",
      "import sklearn\n",
      "from sklearn.naive_bayes import MultinomialNB, ComplementNB\n",
      "import pandas as pd\n",
      "14/11:\n",
      "import sklearn\n",
      "from sklearn.naive_bayes import MultinomialNB, ComplementNB\n",
      "import pandas as pd\n",
      "14/12: mnb = MultinomialNB()\n",
      "14/13:\n",
      "x = \"train_bag_of_words_5.csv\"\n",
      "x_train = pd.read_csv(x, header=None)\n",
      "x_train.shape\n",
      "14/14:\n",
      "y = open(\"train_classes_5.txt\", \"r\")\n",
      "y = y.readlines()\n",
      "y_train = [int(x) for x in y]\n",
      "len(y_train)\n",
      "14/15: mnb.fit(x_train, y_train)\n",
      "14/16: mnb.score(x_train, y_train)\n",
      "14/17:\n",
      "x2 = \"test__bag_of_words_0.csv\"\n",
      "x_test = pd.read_csv(x2, header=None)\n",
      "x_test.shape\n",
      "14/18:\n",
      "y2 = open(\"test_classes_5.txt\", \"r\")\n",
      "y2 = y2.readlines()\n",
      "y_test = [int(x) for x in y2]\n",
      "len(y_test)\n",
      "14/19: mnb.score(x_test, y_test)\n",
      "14/20: cnb = ComplementNB()\n",
      "14/21: cnb.fit(x_train, y_train)\n",
      "14/22: cnb_score(x_train, y_train)\n",
      "14/23: cnb.score(x_train, y_train)\n",
      "14/24: cnb.score(x_test, y_test)\n",
      "14/25: from sklearn.linear_model import LogisticRegression\n",
      "14/26: lr = LogisticRegression()\n",
      "14/27: lr.fit(x_train, y_train)\n",
      "14/28: lr = LogisticRegression(solver='lbfgs')\n",
      "14/29: lr.fit(x_train, y_train)\n",
      "14/30: lr.score(x_train, y_train)\n",
      "14/31: lr.score(x_test, y_test)\n",
      "14/32: from sklearn.svm import SVC\n",
      "14/33: svc = SVC\n",
      "14/34: svc = SVC()\n",
      "14/35: svm = SVC()\n",
      "14/36: svm.fit(x_train, y_train)\n",
      "14/37: svm = SVC(gamma='auto')\n",
      "14/38: svm.fit(x_train, y_train)\n",
      "14/39: svm.score(x_train, y_train)\n",
      "14/40: svm.score(x_test, y_test)\n",
      "14/41:\n",
      "from sklearn.svm import SVC\n",
      "svm = SVC(gamma='auto')\n",
      "svm.fit(x_train, y_train)\n",
      "svm.score(x_train, y_train)\n",
      "svm.score(x_test, y_test)\n",
      "14/42:\n",
      "from sklearn.svm import SVC\n",
      "svm = SVC(gamma='auto')\n",
      "svm.fit(x_train, y_train)\n",
      "svm.score(x_train, y_train)\n",
      "14/43: svm.score(x_test, y_test)\n",
      "14/44: lr = LogisticRegression(solver='lbfgs')\n",
      "14/45: lr.fit(x_train, y_train)\n",
      "14/46: lr.score(x_train, y_train)\n",
      "14/47: lr.score(x_test, y_test)\n",
      "14/48:\n",
      "lr_train = []\n",
      "lr_test = []\n",
      "for w in [0.01, 0.05, 0.1, 0.25, 0.5, 0.75, 0.9, 1]:\n",
      "    lr = LogisticRegression(solver='lbfgs', C=w)\n",
      "    lr.fit(x_train, y_train)\n",
      "    lr_train.append(lr.score(x_train, y_train))\n",
      "    lr_test.append(lr.score(x_test, y_test))\n",
      "14/49: import pyplot\n",
      "14/50: from matplotlib import pyplot\n",
      "14/51: from matplotlib import pyplot as plot\n",
      "14/52:\n",
      "lr_train = []\n",
      "lr_test = []\n",
      "ws = [0.01, 0.05, 0.1, 0.25, 0.5, 0.75, 0.9, 1]\n",
      "for w in ws:\n",
      "    lr = LogisticRegression(solver='lbfgs', C=w)\n",
      "    lr.fit(x_train, y_train)\n",
      "    lr_train.append(lr.score(x_train, y_train))\n",
      "    lr_test.append(lr.score(x_test, y_test))\n",
      "14/53: plot.plot(ws, lr_train)\n",
      "14/54: plot.plot(ws, lr_test)\n",
      "14/55: plot.plot(ws, lr_train, lr_test)\n",
      "14/56: plot.plot(ws, lr_train)\n",
      "14/57:\n",
      "plot.figure()\n",
      "plot.plot(ws, lr_train)\n",
      "plot.plot(ws, lr_test)\n",
      "14/58:\n",
      "lr_train = []\n",
      "lr_test = []\n",
      "ws = [0.01, 0.05, 0.1, 0.2, 0.25, 0.3, 0.4, 0.5, 0.75, 0.9, 1]\n",
      "for w in ws:\n",
      "    lr = LogisticRegression(solver='lbfgs', C=w)\n",
      "    lr.fit(x_train, y_train)\n",
      "    lr_train.append(lr.score(x_train, y_train))\n",
      "    lr_test.append(lr.score(x_test, y_test))\n",
      "14/59: from matplotlib import pyplot as plot\n",
      "14/60:\n",
      "plot.figure()\n",
      "plot.plot(ws, lr_train)\n",
      "plot.plot(ws, lr_test)\n",
      "14/61:\n",
      "lr_train = []\n",
      "lr_test = []\n",
      "ws = [0.01, 0.05, 0.1, 0.25, 0.5, 0.75, 0.9, 1]\n",
      "for w in ws:\n",
      "    lr = LogisticRegression(solver='lbfgs', C=w)\n",
      "    lr.fit(x_train, y_train)\n",
      "    lr_train.append(lr.score(x_train, y_train))\n",
      "    lr_test.append(lr.score(x_test, y_test))\n",
      "14/62: from matplotlib import pyplot as plot\n",
      "14/63:\n",
      "plot.figure()\n",
      "plot.plot(ws, lr_train)\n",
      "plot.plot(ws, lr_test)\n",
      "14/64: print lr_train[3]\n",
      "14/65:\n",
      "print lr_train[3]\n",
      "print lr_test[3]\n",
      "14/66:\n",
      "print lr_train[3]\n",
      "print lr_test[2]\n",
      "14/67:\n",
      "print lr_train[3]\n",
      "print lr_test[4]\n",
      "14/68:\n",
      "print lr_train[3]\n",
      "print lr_test[3]\n",
      "14/69:\n",
      "from sklearn.svm import LinearSVC\n",
      "svm = LinearSVC(gamma='auto')\n",
      "svm.fit(x_train, y_train)\n",
      "svm.score(x_train, y_train)\n",
      "14/70:\n",
      "from sklearn.svm import LinearSVC\n",
      "svm = LinearSVC(C=.01)\n",
      "svm.fit(x_train, y_train)\n",
      "svm.score(x_train, y_train)\n",
      "14/71: svm.score(x_test, y_test)\n",
      "14/72:\n",
      "from sklearn.svm import LinearSVC\n",
      "svm = LinearSVC(C=.001)\n",
      "svm.fit(x_train, y_train)\n",
      "svm.score(x_train, y_train)\n",
      "14/73: svm.score(x_test, y_test)\n",
      "14/74:\n",
      "from sklearn.svm import LinearSVC\n",
      "svm = LinearSVC(C=.1)\n",
      "svm.fit(x_train, y_train)\n",
      "svm.score(x_train, y_train)\n",
      "14/75: svm.score(x_test, y_test)\n",
      "14/76:\n",
      "from sklearn.svm import LinearSVC\n",
      "svm = LinearSVC(C=.2)\n",
      "svm.fit(x_train, y_train)\n",
      "svm.score(x_train, y_train)\n",
      "14/77: svm.score(x_test, y_test)\n",
      "14/78:\n",
      "from sklearn.svm import LinearSVC\n",
      "svm = LinearSVC(C=.25)\n",
      "svm.fit(x_train, y_train)\n",
      "svm.score(x_train, y_train)\n",
      "14/79: svm.score(x_test, y_test)\n",
      "14/80:\n",
      "from sklearn.svm import LinearSVC\n",
      "svm = LinearSVC(C=.5)\n",
      "svm.fit(x_train, y_train)\n",
      "svm.score(x_train, y_train)\n",
      "14/81: svm.score(x_test, y_test)\n",
      "14/82:\n",
      "from sklearn.svm import LinearSVC\n",
      "svm = LinearSVC(C=.25)\n",
      "svm.fit(x_train, y_train)\n",
      "svm.score(x_train, y_train)\n",
      "14/83: svm.score(x_test, y_test)\n",
      "14/84:\n",
      "trainWords = open(\"train_vocab_5.txt\", \"r\")\n",
      "trainWords = y2.readlines()\n",
      "14/85:\n",
      "trainWords = open(\"train_vocab_5.txt\", \"r\")\n",
      "trainWords = trainWords.readlines()\n",
      "14/86: sorted(zip(trainWords, lr.feature_importances_), key=operator.itemgetter(1))\n",
      "14/87: lr = LogisticRegression(solver='lbfgs', C=0.25)\n",
      "14/88: model = lr.fit(x_train, y_train)\n",
      "14/89: lr.score(x_train, y_train)\n",
      "14/90: lr.score(x_test, y_test)\n",
      "14/91: print model.get_support(indices=True)\n",
      "14/92: model = lr.fit(x_train, y_train)\n",
      "14/93: print model.get_support(indices=True)\n",
      "14/94: print model.coef_[0]\n",
      "14/95:\n",
      "coefs = model.coef_[0]\n",
      "top_three = np.argpartition(coefs, -3)[-3:]\n",
      "14/96:\n",
      "import sklearn\n",
      "from sklearn.naive_bayes import MultinomialNB, ComplementNB\n",
      "import pandas as pd\n",
      "import numpy as np\n",
      "14/97:\n",
      "coefs = model.coef_[0]\n",
      "top_three = np.argpartition(coefs, -3)[-3:]\n",
      "14/98:\n",
      "coefs = model.coef_[0]\n",
      "top_three = np.argpartition(coefs, -3)[-3:]\n",
      "print trainWords[top_three]\n",
      "14/99:\n",
      "coefs = model.coef_[0]\n",
      "top_three = np.argpartition(coefs, -3)[-3:]\n",
      "print trainWords[t] for t in top_three\n",
      "14/100:\n",
      "coefs = model.coef_[0]\n",
      "top_three = np.argpartition(coefs, -3)[-3:]\n",
      "for t in top_three:\n",
      "    print train_words[t]\n",
      "14/101:\n",
      "coefs = model.coef_[0]\n",
      "top_three = np.argpartition(coefs, -3)[-3:]\n",
      "for t in top_three:\n",
      "    print trainWords[t]\n",
      "14/102:\n",
      "coefs = model.coef_[0]\n",
      "top_three = np.argpartition(coefs, -3)[-5:]\n",
      "for t in top_three:\n",
      "    print trainWords[t]\n",
      "14/103:\n",
      "coefs = model.coef_[0]\n",
      "top_three = np.argpartition(coefs, -3)[-10:]\n",
      "for t in top_three:\n",
      "    print trainWords[t]\n",
      "14/104:\n",
      "coefs = model.coef_[0]\n",
      "top_three = np.argpartition(coefs, -5)[-10:]\n",
      "for t in top_three:\n",
      "    print trainWords[t]\n",
      "14/105:\n",
      "coefs = model.coef_[0]\n",
      "top_three = np.argpartition(coefs, -3)[-10:]\n",
      "for t in top_three:\n",
      "    print trainWords[t]\n",
      "14/106:\n",
      "coefs = model.coef_[0]\n",
      "top_three = np.argpartition(coefs, -1)[-10:]\n",
      "for t in top_three:\n",
      "    print trainWords[t]\n",
      "14/107:\n",
      "coefs = model.coef_[0]\n",
      "top_three = np.argpartition(coefs, -10)[-10:]\n",
      "for t in top_three:\n",
      "    print trainWords[t]\n",
      "14/108:\n",
      "coefs = model.coef_[0]\n",
      "top_three = np.argpartition(coefs, -3)[-3:]\n",
      "for t in top_three:\n",
      "    print trainWords[t]\n",
      "14/109:\n",
      "coefs = model.coef_[0]\n",
      "top_three = np.argpartition(coefs, -5)[-5:]\n",
      "for t in top_three:\n",
      "    print trainWords[t]\n",
      "14/110:\n",
      "coefs = model.coef_[0]\n",
      "sorted(zip(trainWords, coefs), key=operator.itemgetter(1))\n",
      "14/111:\n",
      "coefs = model.coef_[0]\n",
      "sorted(zip(trainWords, coefs))\n",
      "14/112:\n",
      "coefs = model.coef_[0]\n",
      "sorted(zip(trainWords, coefs), key=(lambda x: x[1]))\n",
      "14/113:\n",
      "coefs = model.coef_[0]\n",
      "tuples = sorted(zip(trainWords, coefs), key=(lambda x: x[1]))\n",
      "for i in range(10):\n",
      "    print tuples[i][0]\n",
      "14/114:\n",
      "for i in range(-10):\n",
      "    print tuples[i][0]\n",
      "14/115:\n",
      "coefs = model.coef_[0]\n",
      "tuples = sorted(zip(trainWords, coefs), key=(lambda x: x[1]))\n",
      "print tuples[:10][0]\n",
      "14/116:\n",
      "coefs = model.coef_[0]\n",
      "tuples = sorted(zip(trainWords, coefs), key=(lambda x: x[1]))\n",
      "print tuples[:10]\n",
      "14/117:\n",
      "coefs = model.coef_[0]\n",
      "tuples = sorted(zip(trainWords, coefs), key=(lambda x: x[1]))\n",
      "print tuples[:10, 0]\n",
      "14/118:\n",
      "coefs = model.coef_[0]\n",
      "tuples = sorted(zip(trainWords, coefs), key=(lambda x: x[1]))\n",
      "print tuples[:10]\n",
      "14/119:\n",
      "coefs = model.coef_[0]\n",
      "tuples = sorted(zip(trainWords, coefs), key=(lambda x: x[1]))\n",
      "print tuples[:10]\n",
      "print tuples[:-10]\n",
      "14/120:\n",
      "coefs = model.coef_[0]\n",
      "tuples = sorted(zip(trainWords, coefs), key=(lambda x: x[1]))\n",
      "print tuples[:10]\n",
      "print tuples[-10:]\n",
      "14/121:\n",
      "coefs = model.coef_[0]\n",
      "tuples = sorted(zip(trainWords, coefs), key=(lambda x: x[1]))\n",
      "print [i[0] for i in tuples[:10]]\n",
      "print tuples[-10:]\n",
      "14/122:\n",
      "coefs = model.coef_[0]\n",
      "tuples = sorted(zip(trainWords, coefs), key=(lambda x: x[1]))\n",
      "print [str(i[0]) for i in tuples[:10]]\n",
      "print [i[0] for i in tuples[-10:]]\n",
      "14/123:\n",
      "coefs = model.coef_[0]\n",
      "tuples = sorted(zip(trainWords, coefs), key=(lambda x: x[1]))\n",
      "print [i[0] for i in tuples[:10]]\n",
      "print [i[0] for i in tuples[-10:]]\n",
      "14/124: from sklearn.feature_extraction.text import TfidfTransformer\n",
      "14/125: tfidf = TfidfTransformer.transform(x_train)\n",
      "14/126: tfidf = TfidfTransformer()\n",
      "14/127: tfidf.transform(x_train)\n",
      "14/128: tfidf.fit_transform(x_train)\n",
      "14/129: x_train_tf = tfidf.fit_transform(x_train)\n",
      "14/130: lr2 = LogisticRegression(solver='lbfgs', C=0.25)\n",
      "14/131: lr2.fit(x_train_tf, y_train)\n",
      "14/132: lr2.score(x_test, y_test)\n",
      "14/133: lr2 = LogisticRegression(solver='lbfgs', C=0.5)\n",
      "14/134: lr2.fit(x_train_tf, y_train)\n",
      "14/135: lr2.score(x_test, y_test)\n",
      "14/136: lr2 = LogisticRegression(solver='lbfgs', C=0.6)\n",
      "14/137: lr2.fit(x_train_tf, y_train)\n",
      "14/138: lr2.score(x_test, y_test)\n",
      "14/139: lr2 = LogisticRegression(solver='lbfgs', C=1)\n",
      "14/140: lr2.fit(x_train_tf, y_train)\n",
      "14/141: lr2.score(x_test, y_test)\n",
      "14/142: lr2 = LogisticRegression(solver='lbfgs', C=.01)\n",
      "14/143: lr2.fit(x_train_tf, y_train)\n",
      "14/144: lr2.score(x_test, y_test)\n",
      "14/145: lr2 = LogisticRegression(solver='lbfgs', C=.9)\n",
      "14/146: lr2.fit(x_train_tf, y_train)\n",
      "14/147: lr2.score(x_test, y_test)\n",
      "14/148: lr2 = LogisticRegression(solver='lbfgs', C=1)\n",
      "14/149: lr2.fit(x_train_tf, y_train)\n",
      "14/150: lr2.score(x_test, y_test)\n",
      "14/151: lr2 = LogisticRegression(solver='lbfgs', C=1, max_iter=500)\n",
      "14/152: lr2.fit(x_train_tf, y_train)\n",
      "14/153: lr2.score(x_test, y_test)\n",
      "14/154: lr2 = LogisticRegression(solver='lbfgs', C=1, max_iter=1000)\n",
      "14/155: lr2.fit(x_train_tf, y_train)\n",
      "14/156: lr2.score(x_test, y_test)\n",
      "14/157: lr2 = LogisticRegression(solver='lbfgs', C=1)\n",
      "14/158: lr2.fit(x_train_tf, y_train)\n",
      "14/159: lr2.score(x_test, y_test)\n",
      "14/160: from sklearn.feature_selection import SelectKBest\n",
      "14/161: x_top50 = SelectKBest(chi2, k=50).fit_transform(x_train, y_train)\n",
      "14/162: from sklearn.feature_selection import SelectKBest, chi2\n",
      "14/163: x_top50 = SelectKBest(chi2, k=50).fit_transform(x_train, y_train)\n",
      "14/164: lr3 = LogisticRegression(solver='lbfgs', C=1)\n",
      "14/165: lr3.fit(x_top50, y_train)\n",
      "14/166:\n",
      "x_top50 = SelectKBest(chi2, k=50).fit_transform(x_train, y_train)\n",
      "x_top50.shape\n",
      "14/167: lr3.score(x_top50, y_train)\n",
      "14/168: model3 = lr3.fit(x_top50, y_train)\n",
      "14/169: lr3.score(x_top50, y_train)\n",
      "14/170:\n",
      "coefs = model3.coef_[0]\n",
      "tuples = sorted(zip(trainWords, coefs), key=(lambda x: x[1]))\n",
      "print [i[0] for i in tuples[:10]]\n",
      "print [i[0] for i in tuples[-10:]]\n",
      "14/171: lr3.score(x_test, y_test)\n",
      "14/172:\n",
      "x_top100 = SelectKBest(chi2, k=100).fit_transform(x_train, y_train)\n",
      "x_top100.shape\n",
      "14/173: lr3 = LogisticRegression(solver='lbfgs', C=1)\n",
      "14/174: model3 = lr3.fit(x_top100, y_train)\n",
      "14/175: lr3.score(x_top100, y_train)\n",
      "14/176: lr3.score(x_test, y_test)\n",
      "14/177: lr3 = LogisticRegression(solver='lbfgs', C=.5)\n",
      "14/178: model3 = lr3.fit(x_top100, y_train)\n",
      "14/179: lr3.score(x_top100, y_train)\n",
      "14/180: lr3 = LogisticRegression(solver='lbfgs', C=.9)\n",
      "14/181: model3 = lr3.fit(x_top100, y_train)\n",
      "14/182: lr3.score(x_top100, y_train)\n",
      "14/183: lr3 = LogisticRegression(solver='lbfgs', C=1)\n",
      "14/184: model3 = lr3.fit(x_top100, y_train)\n",
      "14/185: lr3.score(x_top100, y_train)\n",
      "14/186: from sklearn.feature_selection import SelectKBest, chi2, f_classif\n",
      "14/187:\n",
      "x_top100 = SelectKBest(f_classif, k=100).fit_transform(x_train, y_train)\n",
      "x_top100.shape\n",
      "14/188: lr3 = LogisticRegression(solver='lbfgs', C=1)\n",
      "14/189: model3 = lr3.fit(x_top100, y_train)\n",
      "14/190: lr3.score(x_top100, y_train)\n",
      "14/191:\n",
      "x_top100 = SelectKBest(chi2, k=100).fit_transform(x_train, y_train)\n",
      "x_top100.shape\n",
      "14/192: lr3 = LogisticRegression(solver='lbfgs', C=1)\n",
      "14/193: model3 = lr3.fit(x_top100, y_train)\n",
      "14/194: lr3.score(x_top100, y_train)\n",
      "14/195: selected_indices = x_top100.get_support(indices=True)\n",
      "14/196:\n",
      "selector = SelectKBest(chi2, k=100).fit(x_train, y_train)\n",
      "x_top100 = selector.transform(x_train)\n",
      "x_top100.shape\n",
      "14/197: lr3 = LogisticRegression(solver='lbfgs', C=1)\n",
      "14/198: model3 = lr3.fit(x_top100, y_train)\n",
      "14/199: lr3.score(x_top100, y_train)\n",
      "14/200: selected_indices = x_top100.get_support(indices=True)\n",
      "14/201: selected_indices = selector.get_support(indices=True)\n",
      "14/202:\n",
      "selected_indices = selector.get_support(indices=True)\n",
      "selected_indices\n",
      "14/203:\n",
      "selected_indices = selector.get_support(indices=True)\n",
      "for i in selected_indices:\n",
      "    print trainWords[i]\n",
      "14/204:\n",
      "selector = SelectKBest(chi2, k=10).fit(x_train, y_train)\n",
      "x_top100 = selector.transform(x_train)\n",
      "x_top100.shape\n",
      "14/205: lr3 = LogisticRegression(solver='lbfgs', C=1)\n",
      "14/206: model3 = lr3.fit(x_top100, y_train)\n",
      "14/207: lr3.score(x_top100, y_train)\n",
      "14/208:\n",
      "selected_indices = selector.get_support(indices=True)\n",
      "for i in selected_indices:\n",
      "    print trainWords[i]\n",
      "14/209:\n",
      "selector = SelectKBest(chi2, k=100).fit(x_train, y_train)\n",
      "x_top100 = selector.transform(x_train)\n",
      "x_top100.shape\n",
      "14/210: lr3 = LogisticRegression(solver='lbfgs', C=1)\n",
      "14/211: model3 = lr3.fit(x_top100, y_train)\n",
      "14/212: lr3.score(x_top100, y_train)\n",
      "14/213:\n",
      "selected_indices = selector.get_support(indices=True)\n",
      "for i in selected_indices:\n",
      "    print trainWords[i]\n",
      "14/214:\n",
      "selected_indices = selector.get_support(indices=True)\n",
      "# for i in selected_indices:\n",
      "#     print trainWords[i]\n",
      "14/215: x_test100 = x_test[i for i in selected_indices]\n",
      "14/216: x_test100 = x_test[selected_indices]\n",
      "14/217:\n",
      "x_test100 = x_test[selected_indices]\n",
      "x_test100.shape\n",
      "14/218:\n",
      "x_test100 = x_test[selected_indices]\n",
      "xs = x_test[[1,3,5]\n",
      "14/219:\n",
      "x_test100 = x_test[selected_indices]\n",
      "xs = x_test[[1,3,5]]\n",
      "14/220:\n",
      "x_test100 = x_test[selected_indices]\n",
      "xs = x_test[[1,3,5]]\n",
      "xs\n",
      "14/221:\n",
      "x_test100 = x_test[selected_indices]\n",
      "xs = x_test[[1,3,5]]\n",
      "xs.shape\n",
      "14/222: x_test100 = x_test[selected_indices]\n",
      "14/223: lr3.score(x_test, y_test)\n",
      "14/224:\n",
      "selector = SelectKBest(chi2, k=100).fit(x_train, y_train)\n",
      "x_top100 = selector.transform(x_train)\n",
      "x_top100.shape\n",
      "14/225: lr3 = LogisticRegression(solver='lbfgs', C=1)\n",
      "14/226: model3 = lr3.fit(x_top100, y_train)\n",
      "14/227: lr3.score(x_top100, y_train)\n",
      "14/228:\n",
      "selected_indices = selector.get_support(indices=True)\n",
      "# for i in selected_indices:\n",
      "#     print trainWords[i]\n",
      "14/229: x_test100 = x_test[selected_indices]\n",
      "14/230: lr3.score(x_test, y_test)\n",
      "14/231: lr3.score(x_test100, y_test)\n",
      "14/232: lr3 = LogisticRegression(solver='lbfgs', C=.25)\n",
      "14/233: model3 = lr3.fit(x_top100, y_train)\n",
      "14/234: lr3.score(x_top100, y_train)\n",
      "14/235:\n",
      "selected_indices = selector.get_support(indices=True)\n",
      "# for i in selected_indices:\n",
      "#     print trainWords[i]\n",
      "14/236: x_test100 = x_test[selected_indices]\n",
      "14/237: lr3.score(x_test100, y_test)\n",
      "14/238: lr3 = LogisticRegression(solver='lbfgs', C=1)\n",
      "14/239: model3 = lr3.fit(x_top100, y_train)\n",
      "14/240: lr3.score(x_top100, y_train)\n",
      "14/241:\n",
      "selected_indices = selector.get_support(indices=True)\n",
      "# for i in selected_indices:\n",
      "#     print trainWords[i]\n",
      "14/242: x_test100 = x_test[selected_indices]\n",
      "14/243: lr3.score(x_test100, y_test)\n",
      "14/244:\n",
      "selector = SelectKBest(chi2, k=50).fit(x_train, y_train)\n",
      "x_top100 = selector.transform(x_train)\n",
      "x_top100.shape\n",
      "14/245: lr3 = LogisticRegression(solver='lbfgs', C=1)\n",
      "14/246: model3 = lr3.fit(x_top100, y_train)\n",
      "14/247: lr3.score(x_top100, y_train)\n",
      "14/248:\n",
      "selected_indices = selector.get_support(indices=True)\n",
      "# for i in selected_indices:\n",
      "#     print trainWords[i]\n",
      "14/249: x_test100 = x_test[selected_indices]\n",
      "14/250: lr3.score(x_test100, y_test)\n",
      "14/251:\n",
      "selector = SelectKBest(chi2, k=500).fit(x_train, y_train)\n",
      "x_top100 = selector.transform(x_train)\n",
      "x_top100.shape\n",
      "14/252: lr3 = LogisticRegression(solver='lbfgs', C=1)\n",
      "14/253: model3 = lr3.fit(x_top100, y_train)\n",
      "14/254: lr3.score(x_top100, y_train)\n",
      "14/255:\n",
      "selected_indices = selector.get_support(indices=True)\n",
      "# for i in selected_indices:\n",
      "#     print trainWords[i]\n",
      "14/256: x_test100 = x_test[selected_indices]\n",
      "14/257: lr3.score(x_test100, y_test)\n",
      "14/258:\n",
      "selector = SelectKBest(chi2, k=400).fit(x_train, y_train)\n",
      "x_top100 = selector.transform(x_train)\n",
      "x_top100.shape\n",
      "14/259: lr3 = LogisticRegression(solver='lbfgs', C=1)\n",
      "14/260: model3 = lr3.fit(x_top100, y_train)\n",
      "14/261: lr3.score(x_top100, y_train)\n",
      "14/262:\n",
      "selected_indices = selector.get_support(indices=True)\n",
      "# for i in selected_indices:\n",
      "#     print trainWords[i]\n",
      "14/263: x_test100 = x_test[selected_indices]\n",
      "14/264: lr3.score(x_test100, y_test)\n",
      "14/265:\n",
      "selector = SelectKBest(chi2, k=100).fit(x_train, y_train)\n",
      "x_top100 = selector.transform(x_train)\n",
      "x_top100.shape\n",
      "14/266: lr3 = LogisticRegression(solver='lbfgs', C=1)\n",
      "14/267: model3 = lr3.fit(x_top100, y_train)\n",
      "14/268: lr3.score(x_top100, y_train)\n",
      "14/269:\n",
      "selected_indices = selector.get_support(indices=True)\n",
      "# for i in selected_indices:\n",
      "#     print trainWords[i]\n",
      "14/270: x_test100 = x_test[selected_indices]\n",
      "14/271: lr3.score(x_test100, y_test)\n",
      "14/272:\n",
      "for i in [20,50,100,150,200,250,300,350,400,450,500,540]:\n",
      "    print i\n",
      "14/273:\n",
      "topk = []\n",
      "topktest = []\n",
      "for i in [20,50,100,150,200,250,300,350,400,450,500,540]:\n",
      "    selector = SelectKBest(chi2, k=100).fit(x_train, y_train)\n",
      "    x_top = selector.transform(x_train)\n",
      "    lr_top = LogisticRegression(solver='lbfgs', C=1)\n",
      "    model_top = lr3.fit(x_top, y_train)\n",
      "    topk.append(lr_top.score(x_top, y_train))\n",
      "    selected = selector.get_support(indices=True)\n",
      "    topktest.append(lr_top.score(x_test[selected], y_test))\n",
      "14/274:\n",
      "topk = []\n",
      "topktest = []\n",
      "for i in [20,50,100,150,200,250,300,350,400,450,500,540]:\n",
      "    selector = SelectKBest(chi2, k=100).fit(x_train, y_train)\n",
      "    x_top = selector.transform(x_train)\n",
      "    lr_top = LogisticRegression(solver='lbfgs', C=1)\n",
      "    model_top = lr_top.fit(x_top, y_train)\n",
      "    topk.append(lr_top.score(x_top, y_train))\n",
      "    selected = selector.get_support(indices=True)\n",
      "    topktest.append(lr_top.score(x_test[selected], y_test))\n",
      "14/275:\n",
      "topk = []\n",
      "topktest = []\n",
      "ks = [20,50,100,150,200,250,300,350,400,450,500,540]\n",
      "for i in ks:\n",
      "    selector = SelectKBest(chi2, k=100).fit(x_train, y_train)\n",
      "    x_top = selector.transform(x_train)\n",
      "    lr_top = LogisticRegression(solver='lbfgs', C=1)\n",
      "    model_top = lr_top.fit(x_top, y_train)\n",
      "    topk.append(lr_top.score(x_top, y_train))\n",
      "    selected = selector.get_support(indices=True)\n",
      "    topktest.append(lr_top.score(x_test[selected], y_test))\n",
      "14/276:\n",
      "plot.figure()\n",
      "plot.plot(ks, topk)\n",
      "plot.plot(ks, topktest)\n",
      "14/277:\n",
      "topk = []\n",
      "topktest = []\n",
      "ks = [20,50,100,150,200,250,300,350,400,450,500,540]\n",
      "for i in ks:\n",
      "    selector = SelectKBest(chi2, k=i).fit(x_train, y_train)\n",
      "    x_top = selector.transform(x_train)\n",
      "    lr_top = LogisticRegression(solver='lbfgs', C=1)\n",
      "    model_top = lr_top.fit(x_top, y_train)\n",
      "    topk.append(lr_top.score(x_top, y_train))\n",
      "    selected = selector.get_support(indices=True)\n",
      "    topktest.append(lr_top.score(x_test[selected], y_test))\n",
      "14/278:\n",
      "plot.figure()\n",
      "plot.plot(ks, topk)\n",
      "plot.plot(ks, topktest)\n",
      "14/279:\n",
      "plot.figure()\n",
      "plot.plot(ks, topk)\n",
      "# plot.plot(ks, topktest)\n",
      "14/280:\n",
      "plot.figure()\n",
      "plot.plot(ks, topk)\n",
      "plot.plot(ks, topktest)\n",
      "14/281:\n",
      "plot.figure()\n",
      "plot.plot(ks, topk)\n",
      "plot.plot(ks, topktest)\n",
      "print topktest\n",
      "14/282:\n",
      "plot.figure()\n",
      "plot.plot(ks, topk)\n",
      "plot.plot(ks, topktest)\n",
      "print argmax(topktest)\n",
      "14/283:\n",
      "plot.figure()\n",
      "plot.plot(ks, topk)\n",
      "plot.plot(ks, topktest)\n",
      "print np.argmax(topktest)\n",
      "14/284:\n",
      "plot.figure()\n",
      "plot.plot(ks, topk)\n",
      "plot.plot(ks, topktest)\n",
      "print ks[np.argmax(topktest)]\n",
      "14/285:\n",
      "plot.figure()\n",
      "plot.plot(ks, topk)\n",
      "plot.plot(ks, topktest)\n",
      "print ks[np.argmax(topktest)], np.max(topktest)\n",
      "14/286:\n",
      "topk = []\n",
      "topktest = []\n",
      "ks = [20,50,100,150,200,250,300,330,340,350,360,370,400,450,500,540]\n",
      "for i in ks:\n",
      "    selector = SelectKBest(chi2, k=i).fit(x_train, y_train)\n",
      "    x_top = selector.transform(x_train)\n",
      "    lr_top = LogisticRegression(solver='lbfgs', C=1)\n",
      "    model_top = lr_top.fit(x_top, y_train)\n",
      "    topk.append(lr_top.score(x_top, y_train))\n",
      "    selected = selector.get_support(indices=True)\n",
      "    topktest.append(lr_top.score(x_test[selected], y_test))\n",
      "14/287:\n",
      "plot.figure()\n",
      "plot.plot(ks, topk)\n",
      "plot.plot(ks, topktest)\n",
      "print ks[np.argmax(topktest)], np.max(topktest)\n",
      "14/288:\n",
      "topk = []\n",
      "topktest = []\n",
      "ks = [20,50,100,150,200,250,300,330,335,340,345,350,400,450,500,540]\n",
      "for i in ks:\n",
      "    selector = SelectKBest(chi2, k=i).fit(x_train, y_train)\n",
      "    x_top = selector.transform(x_train)\n",
      "    lr_top = LogisticRegression(solver='lbfgs', C=1)\n",
      "    model_top = lr_top.fit(x_top, y_train)\n",
      "    topk.append(lr_top.score(x_top, y_train))\n",
      "    selected = selector.get_support(indices=True)\n",
      "    topktest.append(lr_top.score(x_test[selected], y_test))\n",
      "14/289:\n",
      "plot.figure()\n",
      "plot.plot(ks, topk)\n",
      "plot.plot(ks, topktest)\n",
      "print ks[np.argmax(topktest)], np.max(topktest)\n",
      "14/290:\n",
      "topk = []\n",
      "topktest = []\n",
      "ks = [20,50,100,150,200,250,300,330,333,335,337,340,350,400,450,500,540]\n",
      "for i in ks:\n",
      "    selector = SelectKBest(chi2, k=i).fit(x_train, y_train)\n",
      "    x_top = selector.transform(x_train)\n",
      "    lr_top = LogisticRegression(solver='lbfgs', C=1)\n",
      "    model_top = lr_top.fit(x_top, y_train)\n",
      "    topk.append(lr_top.score(x_top, y_train))\n",
      "    selected = selector.get_support(indices=True)\n",
      "    topktest.append(lr_top.score(x_test[selected], y_test))\n",
      "14/291:\n",
      "plot.figure()\n",
      "plot.plot(ks, topk)\n",
      "plot.plot(ks, topktest)\n",
      "print ks[np.argmax(topktest)], np.max(topktest)\n",
      "14/292:\n",
      "topk = []\n",
      "topktest = []\n",
      "ks = [20,50,100,150,200,250,300,330,334,335,336,340,350,400,450,500,540]\n",
      "for i in ks:\n",
      "    selector = SelectKBest(chi2, k=i).fit(x_train, y_train)\n",
      "    x_top = selector.transform(x_train)\n",
      "    lr_top = LogisticRegression(solver='lbfgs', C=1)\n",
      "    model_top = lr_top.fit(x_top, y_train)\n",
      "    topk.append(lr_top.score(x_top, y_train))\n",
      "    selected = selector.get_support(indices=True)\n",
      "    topktest.append(lr_top.score(x_test[selected], y_test))\n",
      "14/293:\n",
      "plot.figure()\n",
      "plot.plot(ks, topk)\n",
      "plot.plot(ks, topktest)\n",
      "print ks[np.argmax(topktest)], np.max(topktest)\n",
      "14/294:\n",
      "topk = []\n",
      "topktest = []\n",
      "ks = [20,50,100,150,200,250,300,330,335,340,350,400,450,500,540]\n",
      "for i in ks:\n",
      "    selector = SelectKBest(chi2, k=i).fit(x_train, y_train)\n",
      "    x_top = selector.transform(x_train)\n",
      "    lr_top = LogisticRegression(solver='lbfgs', C=1)\n",
      "    model_top = lr_top.fit(x_top, y_train)\n",
      "    topk.append(lr_top.score(x_top, y_train))\n",
      "    selected = selector.get_support(indices=True)\n",
      "    topktest.append(lr_top.score(x_test[selected], y_test))\n",
      "14/295:\n",
      "plot.figure()\n",
      "plot.plot(ks, topk)\n",
      "plot.plot(ks, topktest)\n",
      "print ks[np.argmax(topktest)], np.max(topktest)\n",
      "14/296: from sklearn.feature_selection import SelectKBest, chi2\n",
      "14/297:\n",
      "selector = SelectKBest(chi2, k=335).fit(x_train, y_train)\n",
      "x_topk = selector.transform(x_train)\n",
      "x_topk.shape\n",
      "14/298: lr3 = LogisticRegression(solver='lbfgs', C=1)\n",
      "14/299: model3 = lr3.fit(x_topk, y_train)\n",
      "14/300: lr3.score(x_topk, y_train)\n",
      "14/301:\n",
      "selected_indices = selector.get_support(indices=True)\n",
      "# for i in selected_indices:\n",
      "#     print trainWords[i]\n",
      "14/302: x_test100 = x_test[selected_indices]\n",
      "14/303: lr3.score(x_test100, y_test)\n",
      "14/304:\n",
      "topk = []\n",
      "topktest = []\n",
      "ks = [20,50,100,150,200,250,300,330,335,340,350,400,450,500,540]\n",
      "for i in ks:\n",
      "    selector = SelectKBest(chi2, k=i).fit(x_train, y_train)\n",
      "    x_top = selector.transform(x_train)\n",
      "    lr_top = LogisticRegression(solver='lbfgs', C=1)\n",
      "    model_top = lr_top.fit(x_top, y_train)\n",
      "    topk.append(lr_top.score(x_top, y_train))\n",
      "    selected = selector.get_support(indices=True)\n",
      "    topktest.append(lr_top.score(x_test[selected], y_test))\n",
      "14/305:\n",
      "plot.figure()\n",
      "plot.plot(ks, topk)\n",
      "plot.plot(ks, topktest)\n",
      "print ks[np.argmax(topktest)], np.max(topktest)\n",
      "14/306: x_testk = x_test[selected_indices]\n",
      "14/307: lr3.score(x_test100, y_test)\n",
      "15/1:\n",
      "import nltk, re, pprint\n",
      "from nltk import word_tokenize\n",
      "from nltk.corpus import stopwords\n",
      "from os import listdir\n",
      "from os.path import isfile, isdir, join\n",
      "import numpy\n",
      "import re\n",
      "import sys\n",
      "import getopt\n",
      "import codecs\n",
      "import time\n",
      "import os\n",
      "import csv\n",
      "\n",
      "chars = ['{','}','#','%','&','\\(','\\)','\\[','\\]','<','>',',', '!', '.', ';',\n",
      "'?', '*', '\\\\', '\\/', '~', '_','|','=','+','^',':','\\\"','\\'','@','-']\n",
      "\n",
      "def stem(word):\n",
      "   regexp = r'^(.*?)(ing|ly|ed|ious|ies|ive|es|s|ment)?$'\n",
      "   stem, suffix = re.findall(regexp, word)[0]\n",
      "   return stem\n",
      "\n",
      "def unique(a):\n",
      "   \"\"\" return the list with duplicate elements removed \"\"\"\n",
      "   return list(set(a))\n",
      "\n",
      "def intersect(a, b):\n",
      "   \"\"\" return the intersection of two lists \"\"\"\n",
      "   return list(set(a) & set(b))\n",
      "\n",
      "def union(a, b):\n",
      "   \"\"\" return the union of two lists \"\"\"\n",
      "   return list(set(a) | set(b))\n",
      "\n",
      "def get_files(mypath):\n",
      "   return [ f for f in listdir(mypath) if isfile(join(mypath,f)) ]\n",
      "\n",
      "def get_dirs(mypath):\n",
      "   return [ f for f in listdir(mypath) if isdir(join(mypath,f)) ]\n",
      "\n",
      "# Reading a bag of words file back into python. The number and order\n",
      "# of sentences should be the same as in the *samples_class* file.\n",
      "def read_bagofwords_dat(myfile):\n",
      "  bagofwords = numpy.genfromtxt('myfile.csv',delimiter=',')\n",
      "  return bagofwords\n",
      "\n",
      "def tokenize_corpus(path, train=True):\n",
      "\n",
      "  porter = nltk.PorterStemmer() # also lancaster stemmer\n",
      "  wnl = nltk.WordNetLemmatizer()\n",
      "  stopWords = stopwords.words(\"english\")\n",
      "  classes = []\n",
      "  samples = []\n",
      "  docs = []\n",
      "  if train == True:\n",
      "    words = {}\n",
      "  f = open(path, 'r')\n",
      "  lines = f.readlines()\n",
      "\n",
      "  for line in lines:\n",
      "    classes.append(line.rsplit()[-1])\n",
      "    samples.append(line.rsplit()[0])\n",
      "    raw = line.decode('latin1')\n",
      "    raw = ' '.join(raw.rsplit()[1:-1])\n",
      "    # remove noisy characters; tokenize\n",
      "    raw = re.sub('[%s]' % ''.join(chars), ' ', raw)\n",
      "    tokens = word_tokenize(raw)\n",
      "    tokens = [w.lower() for w in tokens]\n",
      "    tokens = [w for w in tokens if w not in stopWords]\n",
      "    tokens = [wnl.lemmatize(t) for t in tokens]\n",
      "    tokens = [porter.stem(t) for t in tokens]\n",
      "    if train == True:\n",
      "     for t in tokens:\n",
      "         try:\n",
      "             words[t] = words[t]+1\n",
      "         except:\n",
      "             words[t] = 1\n",
      "    docs.append(tokens)\n",
      "\n",
      "  if train == True:\n",
      "     return(docs, classes, samples, words)\n",
      "  else:\n",
      "     return(docs, classes, samples)\n",
      "\n",
      "\n",
      "def wordcount_filter(words, num=5):\n",
      "   keepset = []\n",
      "   for k in words.keys():\n",
      "       if(words[k] > num):\n",
      "           keepset.append(k)\n",
      "   print \"Vocab length:\", len(keepset)\n",
      "   return(sorted(set(keepset)))\n",
      "\n",
      "\n",
      "def find_wordcounts(docs, vocab):\n",
      "   bagofwords = numpy.zeros(shape=(len(docs),len(vocab)), dtype=numpy.uint8)\n",
      "   vocabIndex={}\n",
      "   for i in range(len(vocab)):\n",
      "      vocabIndex[vocab[i]]=i\n",
      "\n",
      "   for i in range(len(docs)):\n",
      "       doc = docs[i]\n",
      "\n",
      "       for t in doc:\n",
      "          index_t=vocabIndex.get(t)\n",
      "          if index_t>=0:\n",
      "             bagofwords[i,index_t]=bagofwords[i,index_t]+1\n",
      "\n",
      "   print \"Finished find_wordcounts for:\", len(docs), \"docs\"\n",
      "   print bagofwords\n",
      "   return(bagofwords)\n",
      "\n",
      "\n",
      "def main(argv):\n",
      "\n",
      "  start_time = time.time()\n",
      "\n",
      "  path = ''\n",
      "  outputf = 'out'\n",
      "  vocabf = ''\n",
      "\n",
      "  try:\n",
      "   opts, args = getopt.getopt(argv,\"p:o:v:\",[\"path=\",\"ofile=\",\"vocabfile=\"])\n",
      "  except getopt.GetoptError:\n",
      "    print 'Usage: \\n python preprocessSentences.py -p <path> -o <outputfile> -v <vocabulary>'\n",
      "    sys.exit(2)\n",
      "  for opt, arg in opts:\n",
      "    if opt == '-h':\n",
      "      print 'Usage: \\n python preprocessSentences.py -p <path> -o <outputfile> -v <vocabulary>'\n",
      "      sys.exit()\n",
      "    elif opt in (\"-p\", \"--path\"):\n",
      "      path = arg\n",
      "    elif opt in (\"-o\", \"--ofile\"):\n",
      "      outputf = arg\n",
      "    elif opt in (\"-v\", \"--vocabfile\"):\n",
      "      vocabf = arg\n",
      "\n",
      "  traintxt = path+\"/test.txt\"\n",
      "  print 'Path:', path\n",
      "  print 'Training data:', traintxt\n",
      "\n",
      "  # Tokenize training data (if training vocab doesn't already exist):\n",
      "  if (not vocabf):\n",
      "    word_count_threshold = 5\n",
      "    (docs, classes, samples, words) = tokenize_corpus(traintxt, train=True)\n",
      "    vocab = wordcount_filter(words, num=word_count_threshold)\n",
      "    # Write new vocab file\n",
      "    vocabf = outputf+\"_vocab_\"+str(word_count_threshold)+\".txt\"\n",
      "    outfile = codecs.open(path+\"/\"+vocabf, 'w',\"utf-8-sig\")\n",
      "    outfile.write(\"\\n\".join(vocab))\n",
      "    outfile.close()\n",
      "  else:\n",
      "    word_count_threshold = 0\n",
      "    (docs, classes, samples) = tokenize_corpus(traintxt, train=False)\n",
      "    vocabfile = open(path+\"/\"+vocabf, 'r')\n",
      "    vocab = [line.rstrip('\\n') for line in vocabfile]\n",
      "    vocabfile.close()\n",
      "\n",
      "  print 'Vocabulary file:', path+\"/\"+vocabf\n",
      "\n",
      "  # Get bag of words:\n",
      "  bow = find_wordcounts(docs, vocab)\n",
      "  # Check: sum over docs to check if any zero word counts\n",
      "  print \"Doc with smallest number of words in vocab has:\", min(numpy.sum(bow, axis=1))\n",
      "\n",
      "  # Write bow file\n",
      "  with open(path+\"/\"+outputf+\"_bag_of_words_\"+str(word_count_threshold)+\".csv\", \"wb\") as f:\n",
      "    writer = csv.writer(f)\n",
      "    writer.writerows(bow)\n",
      "\n",
      "  # Runtime\n",
      "  print 'Runtime:', str(time.time() - start_time)\n",
      "\n",
      "if __name__ == \"__main__\":\n",
      "  main(sys.argv[1:])\n",
      "14/308:\n",
      "x = \"stop_bag_of_words_5.csv\"\n",
      "x_train = pd.read_csv(x, header=None)\n",
      "x_train.shape\n",
      "14/309:\n",
      "x = \"stopt_bag_of_words_5.csv\"\n",
      "x_train = pd.read_csv(x, header=None)\n",
      "x_train.shape\n",
      "14/310:\n",
      "topk = []\n",
      "topktest = []\n",
      "ks = [20,50,100,150,200,250,300,330,335,340,350,400,450,500,540]\n",
      "for i in ks:\n",
      "    selector = SelectKBest(chi2, k=i).fit(x_train, y_train)\n",
      "    x_top = selector.transform(x_train)\n",
      "    lr_top = LogisticRegression(solver='lbfgs', C=1)\n",
      "    model_top = lr_top.fit(x_top, y_train)\n",
      "    topk.append(lr_top.score(x_top, y_train))\n",
      "    selected = selector.get_support(indices=True)\n",
      "    topktest.append(lr_top.score(x_test[selected], y_test))\n",
      "14/311:\n",
      "x = \"stop_bag_of_words_5.csv\"\n",
      "x_test = pd.read_csv(x, header=None)\n",
      "x_test.shape\n",
      "14/312:\n",
      "x = \"stop_bag_of_words_0.csv\"\n",
      "x_test = pd.read_csv(x, header=None)\n",
      "x_test.shape\n",
      "14/313:\n",
      "x = \"stopt_bag_of_words_5.csv\"\n",
      "x_train = pd.read_csv(x, header=None)\n",
      "x_train.shape\n",
      "14/314:\n",
      "x = \"stop_bag_of_words_0.csv\"\n",
      "x_test = pd.read_csv(x, header=None)\n",
      "x_test.shape\n",
      "14/315:\n",
      "x = \"stopt_bag_of_words_5.csv\"\n",
      "x_train = pd.read_csv(x, header=None)\n",
      "x_train.shape\n",
      "14/316:\n",
      "topk = []\n",
      "topktest = []\n",
      "ks = [20,50,100,150,200,250,300,330,335,340,350,400,450,500,540]\n",
      "for i in ks:\n",
      "    selector = SelectKBest(chi2, k=i).fit(x_train, y_train)\n",
      "    x_top = selector.transform(x_train)\n",
      "    lr_top = LogisticRegression(solver='lbfgs', C=1)\n",
      "    model_top = lr_top.fit(x_top, y_train)\n",
      "    topk.append(lr_top.score(x_top, y_train))\n",
      "    selected = selector.get_support(indices=True)\n",
      "    topktest.append(lr_top.score(x_test[selected], y_test))\n",
      "14/317:\n",
      "plot.figure()\n",
      "plot.plot(ks, topk)\n",
      "plot.plot(ks, topktest)\n",
      "print ks[np.argmax(topktest)], np.max(topktest)\n",
      "14/318:\n",
      "topk = []\n",
      "topktest = []\n",
      "ks = [20,50,100,150,200,250,300,350,390,400,410,450,500,540]\n",
      "for i in ks:\n",
      "    selector = SelectKBest(chi2, k=i).fit(x_train, y_train)\n",
      "    x_top = selector.transform(x_train)\n",
      "    lr_top = LogisticRegression(solver='lbfgs', C=1)\n",
      "    model_top = lr_top.fit(x_top, y_train)\n",
      "    topk.append(lr_top.score(x_top, y_train))\n",
      "    selected = selector.get_support(indices=True)\n",
      "    topktest.append(lr_top.score(x_test[selected], y_test))\n",
      "14/319:\n",
      "plot.figure()\n",
      "plot.plot(ks, topk)\n",
      "plot.plot(ks, topktest)\n",
      "print ks[np.argmax(topktest)], np.max(topktest)\n",
      "14/320:\n",
      "plot.figure()\n",
      "plot.plot(ks, topk)\n",
      "plot.plot(ks, topktest)\n",
      "print ks[np.argmax(topktest)], np.max(topktest)\n",
      "14/321:\n",
      "topk = []\n",
      "topktest = []\n",
      "ks = [20,50,100,150,200,250,300,350,400,450,500,540]\n",
      "for i in ks:\n",
      "    selector = SelectKBest(chi2, k=i).fit(x_train, y_train)\n",
      "    x_top = selector.transform(x_train)\n",
      "    lr_top = LogisticRegression(solver='lbfgs', C=.5)\n",
      "    model_top = lr_top.fit(x_top, y_train)\n",
      "    topk.append(lr_top.score(x_top, y_train))\n",
      "    selected = selector.get_support(indices=True)\n",
      "    topktest.append(lr_top.score(x_test[selected], y_test))\n",
      "14/322:\n",
      "plot.figure()\n",
      "plot.plot(ks, topk)\n",
      "plot.plot(ks, topktest)\n",
      "print ks[np.argmax(topktest)], np.max(topktest)\n",
      "14/323:\n",
      "topk = []\n",
      "topktest = []\n",
      "ks = [20,50,100,150,200,250,300,350,400,450,500,540]\n",
      "for i in ks:\n",
      "    selector = SelectKBest(chi2, k=i).fit(x_train, y_train)\n",
      "    x_top = selector.transform(x_train)\n",
      "    lr_top = LogisticRegression(solver='lbfgs', C=.05)\n",
      "    model_top = lr_top.fit(x_top, y_train)\n",
      "    topk.append(lr_top.score(x_top, y_train))\n",
      "    selected = selector.get_support(indices=True)\n",
      "    topktest.append(lr_top.score(x_test[selected], y_test))\n",
      "14/324:\n",
      "plot.figure()\n",
      "plot.plot(ks, topk)\n",
      "plot.plot(ks, topktest)\n",
      "print ks[np.argmax(topktest)], np.max(topktest)\n",
      "14/325:\n",
      "topk = []\n",
      "topktest = []\n",
      "ks = [20,50,100,150,200,250,300,350,400,450,500,540]\n",
      "for i in ks:\n",
      "    selector = SelectKBest(chi2, k=i).fit(x_train, y_train)\n",
      "    x_top = selector.transform(x_train)\n",
      "    lr_top = LogisticRegression(solver='lbfgs', C=.9)\n",
      "    model_top = lr_top.fit(x_top, y_train)\n",
      "    topk.append(lr_top.score(x_top, y_train))\n",
      "    selected = selector.get_support(indices=True)\n",
      "    topktest.append(lr_top.score(x_test[selected], y_test))\n",
      "14/326:\n",
      "plot.figure()\n",
      "plot.plot(ks, topk)\n",
      "plot.plot(ks, topktest)\n",
      "print ks[np.argmax(topktest)], np.max(topktest)\n",
      "14/327:\n",
      "topk = []\n",
      "topktest = []\n",
      "ks = [20,50,100,150,200,250,300,350,400,450,500,540]\n",
      "for i in ks:\n",
      "    selector = SelectKBest(chi2, k=i).fit(x_train, y_train)\n",
      "    x_top = selector.transform(x_train)\n",
      "    lr_top = LogisticRegression(solver='lbfgs', C=1)\n",
      "    model_top = lr_top.fit(x_top, y_train)\n",
      "    topk.append(lr_top.score(x_top, y_train))\n",
      "    selected = selector.get_support(indices=True)\n",
      "    topktest.append(lr_top.score(x_test[selected], y_test))\n",
      "14/328:\n",
      "plot.figure()\n",
      "plot.plot(ks, topk)\n",
      "plot.plot(ks, topktest)\n",
      "print ks[np.argmax(topktest)], np.max(topktest)\n",
      "14/329:\n",
      "plot.figure()\n",
      "plot.plot(ks, topk)\n",
      "plot.plot(ks, topktest)\n",
      "print ks[np.argmax(topk)], np.max(topk)\n",
      "print ks[np.argmax(topktest)], np.max(topktest)\n",
      "14/330:\n",
      "plot.figure()\n",
      "plot.plot(ks, topk)\n",
      "plot.plot(ks, topktest)\n",
      "print ks[np.argmax(topktest)], np.max(topktest)\n",
      "14/331:\n",
      "from sklearn.svm import LinearSVC\n",
      "svm = LinearSVC(C=.25)\n",
      "svm.fit(x_train, y_train)\n",
      "svm.score(x_train, y_train)\n",
      "svm.score(x_test, y_test)\n",
      "14/332: svm.score(x_test, y_test)\n",
      "14/333:\n",
      "x = \"stop_bag_of_words_0.csv\"\n",
      "x_test = pd.read_csv(x, header=None)\n",
      "x_test.shape\n",
      "14/334:\n",
      "x = \"stopt_bag_of_words_5.csv\"\n",
      "x_train = pd.read_csv(x, header=None)\n",
      "x_train.shape\n",
      "14/335:\n",
      "topk = []\n",
      "topktest = []\n",
      "ks = [20,50,100,150,200,250,300,350,400,450,500,540]\n",
      "for i in ks:\n",
      "    selector = SelectKBest(chi2, k=i).fit(x_train, y_train)\n",
      "    x_top = selector.transform(x_train)\n",
      "    lr_top = LogisticRegression(solver='lbfgs', C=1)\n",
      "    model_top = lr_top.fit(x_top, y_train)\n",
      "    topk.append(lr_top.score(x_top, y_train))\n",
      "    selected = selector.get_support(indices=True)\n",
      "    topktest.append(lr_top.score(x_test[selected], y_test))\n",
      "14/336:\n",
      "plot.figure()\n",
      "plot.plot(ks, topk)\n",
      "plot.plot(ks, topktest)\n",
      "print ks[np.argmax(topktest)], np.max(topktest)\n",
      "14/337:\n",
      "from sklearn.svm import LinearSVC\n",
      "svm = LinearSVC(C=.25)\n",
      "svm.fit(x_train, y_train)\n",
      "svm.score(x_train, y_train)\n",
      "14/338: svm.score(x_test, y_test)\n",
      "14/339:\n",
      "from sklearn.svm import LinearSVC\n",
      "svm = LinearSVC(C=.5)\n",
      "svm.fit(x_train, y_train)\n",
      "svm.score(x_train, y_train)\n",
      "14/340: svm.score(x_test, y_test)\n",
      "14/341:\n",
      "from sklearn.svm import LinearSVC\n",
      "svm = LinearSVC(C=1)\n",
      "svm.fit(x_train, y_train)\n",
      "svm.score(x_train, y_train)\n",
      "14/342: svm.score(x_test, y_test)\n",
      "14/343:\n",
      "from sklearn.svm import LinearSVC\n",
      "svm = LinearSVC(C=.05)\n",
      "svm.fit(x_train, y_train)\n",
      "svm.score(x_train, y_train)\n",
      "14/344: svm.score(x_test, y_test)\n",
      "14/345:\n",
      "from sklearn.svm import LinearSVC\n",
      "svm = LinearSVC(C=.01)\n",
      "svm.fit(x_train, y_train)\n",
      "svm.score(x_train, y_train)\n",
      "14/346: svm.score(x_test, y_test)\n",
      "14/347:\n",
      "from sklearn.svm import LinearSVC\n",
      "svm = LinearSVC(C=0)\n",
      "svm.fit(x_train, y_train)\n",
      "svm.score(x_train, y_train)\n",
      "14/348: svm.score(x_test, y_test)\n",
      "14/349:\n",
      "from sklearn.svm import LinearSVC\n",
      "svm = LinearSVC(C=0.001)\n",
      "svm.fit(x_train, y_train)\n",
      "svm.score(x_train, y_train)\n",
      "14/350: svm.score(x_test, y_test)\n",
      "14/351:\n",
      "from sklearn.svm import LinearSVC\n",
      "svm = LinearSVC(C=0.01)\n",
      "svm.fit(x_train, y_train)\n",
      "svm.score(x_train, y_train)\n",
      "14/352: svm.score(x_test, y_test)\n",
      "14/353:\n",
      "from sklearn.svm import LinearSVC\n",
      "svm = LinearSVC(C=0.1)\n",
      "svm.fit(x_train, y_train)\n",
      "svm.score(x_train, y_train)\n",
      "14/354: svm.score(x_test, y_test)\n",
      "14/355:\n",
      "from sklearn.svm import LinearSVC\n",
      "svm = LinearSVC(C=0.2)\n",
      "svm.fit(x_train, y_train)\n",
      "svm.score(x_train, y_train)\n",
      "14/356: svm.score(x_test, y_test)\n",
      "14/357:\n",
      "from sklearn.svm import LinearSVC\n",
      "svm = LinearSVC(C=0.7)\n",
      "svm.fit(x_train, y_train)\n",
      "svm.score(x_train, y_train)\n",
      "14/358: svm.score(x_test, y_test)\n",
      "14/359:\n",
      "from sklearn.svm import LinearSVC\n",
      "svm = LinearSVC(C=0.75)\n",
      "svm.fit(x_train, y_train)\n",
      "svm.score(x_train, y_train)\n",
      "14/360: svm.score(x_test, y_test)\n",
      "14/361:\n",
      "from sklearn.svm import LinearSVC\n",
      "svm = LinearSVC(C=0.8)\n",
      "svm.fit(x_train, y_train)\n",
      "svm.score(x_train, y_train)\n",
      "14/362: svm.score(x_test, y_test)\n",
      "14/363:\n",
      "from sklearn.svm import LinearSVC\n",
      "svm = LinearSVC(C=0.9)\n",
      "svm.fit(x_train, y_train)\n",
      "svm.score(x_train, y_train)\n",
      "14/364: svm.score(x_test, y_test)\n",
      "14/365:\n",
      "from sklearn.svm import LinearSVC\n",
      "svm = LinearSVC(C=0.6)\n",
      "svm.fit(x_train, y_train)\n",
      "svm.score(x_train, y_train)\n",
      "14/366: svm.score(x_test, y_test)\n",
      "14/367:\n",
      "from sklearn.svm import LinearSVC\n",
      "svm = LinearSVC(C=0.7)\n",
      "svm.fit(x_train, y_train)\n",
      "svm.score(x_train, y_train)\n",
      "14/368: svm.score(x_test, y_test)\n",
      "14/369: np.max(x_train)\n",
      "14/370:\n",
      "x = \"temp\"_bag_of_words_0.csv\"\n",
      "x_test = pd.read_csv(x, header=None)\n",
      "x_test.shape\n",
      "14/371:\n",
      "x = \"temp\"_bag_of_words_5.csv\"\n",
      "x_test = pd.read_csv(x, header=None)\n",
      "x_test.shape\n",
      "14/372:\n",
      "x = \"temp_bag_of_words_5.csv\"\n",
      "x_test = pd.read_csv(x, header=None)\n",
      "x_test.shape\n",
      "14/373:\n",
      "x = \"temp_bag_of_words_5.csv\"\n",
      "x_test = pd.read_csv(x, header=None)\n",
      "x_test.shape\n",
      "14/374:\n",
      "x = \"temp_bag_of_words_5.csv\"\n",
      "x_test = pd.read_csv(x, header=None)\n",
      "x_test.shape\n",
      "14/375:\n",
      "topk = []\n",
      "topktest = []\n",
      "ks = [100,300,500,700]\n",
      "for i in ks:\n",
      "    selector = SelectKBest(chi2, k=i).fit(x_train, y_train)\n",
      "    x_top = selector.transform(x_train)\n",
      "    lr_top = LogisticRegression(solver='lbfgs', C=1)\n",
      "    model_top = lr_top.fit(x_top, y_train)\n",
      "    topk.append(lr_top.score(x_top, y_train))\n",
      "    selected = selector.get_support(indices=True)\n",
      "    topktest.append(lr_top.score(x_test[selected], y_test))\n",
      "14/376:\n",
      "topk = []\n",
      "topktest = []\n",
      "ks = [100,300,500,700]\n",
      "for i in ks:\n",
      "    selector = SelectKBest(chi2, k=i).fit(x_train, y_train)\n",
      "    x_top = selector.transform(x_train)\n",
      "    lr_top = LogisticRegression(solver='lbfgs', C=1)\n",
      "    model_top = lr_top.fit(x_top, y_train)\n",
      "    topk.append(lr_top.score(x_top, y_train))\n",
      "#     selected = selector.get_support(indices=True)\n",
      "#     topktest.append(lr_top.score(x_test[selected], y_test))\n",
      "14/377:\n",
      "topk = []\n",
      "topktest = []\n",
      "ks = [100,300,500,700]\n",
      "for i in ks:\n",
      "    selector = SelectKBest(chi2, k=i).fit(x_test, y_train)\n",
      "    x_top = selector.transform(x_test)\n",
      "    lr_top = LogisticRegression(solver='lbfgs', C=1)\n",
      "    model_top = lr_top.fit(x_top, y_train)\n",
      "    topk.append(lr_top.score(x_top, y_train))\n",
      "#     selected = selector.get_support(indices=True)\n",
      "#     topktest.append(lr_top.score(x_test[selected], y_test))\n",
      "14/378:\n",
      "plot.figure()\n",
      "plot.plot(ks, topk)\n",
      "plot.plot(ks, topktest)\n",
      "print ks[np.argmax(topktest)], np.max(topktest)\n",
      "14/379:\n",
      "x = \"temp_bag_of_words_5.csv\"\n",
      "x_train = pd.read_csv(x, header=None)\n",
      "x_train.shape\n",
      "14/380:\n",
      "topk = []\n",
      "topktest = []\n",
      "ks = [100,300,500,700]\n",
      "for i in ks:\n",
      "    selector = SelectKBest(chi2, k=i).fit(x_train, y_train)\n",
      "    x_top = selector.transform(x_train)\n",
      "    lr_top = LogisticRegression(solver='lbfgs', C=1)\n",
      "    model_top = lr_top.fit(x_top, y_train)\n",
      "    topk.append(lr_top.score(x_top, y_train))\n",
      "    selected = selector.get_support(indices=True)\n",
      "    topktest.append(lr_top.score(x_test[selected], y_test))\n",
      "14/381:\n",
      "x2 = \"temp2_bag_of_words_5.csv\"\n",
      "x_test = pd.read_csv(x2, header=None)\n",
      "x_test.shape\n",
      "14/382:\n",
      "x2 = \"temp2_bag_of_words_5.csv\"\n",
      "x_test = pd.read_csv(x2, header=None)\n",
      "x_test.shape\n",
      "14/383:\n",
      "topk = []\n",
      "topktest = []\n",
      "ks = [100,300,500,700]\n",
      "for i in ks:\n",
      "    selector = SelectKBest(chi2, k=i).fit(x_train, y_train)\n",
      "    x_top = selector.transform(x_train)\n",
      "    lr_top = LogisticRegression(solver='lbfgs', C=1)\n",
      "    model_top = lr_top.fit(x_top, y_train)\n",
      "    topk.append(lr_top.score(x_top, y_train))\n",
      "    selected = selector.get_support(indices=True)\n",
      "    topktest.append(lr_top.score(x_test[selected], y_test))\n",
      "14/384:\n",
      "x2 = \"temp2_bag_of_words_0.csv\"\n",
      "x_test = pd.read_csv(x2, header=None)\n",
      "x_test.shape\n",
      "14/385:\n",
      "x2 = \"temp2_bag_of_words_0.csv\"\n",
      "x_test = pd.read_csv(x2, header=None)\n",
      "x_test.shape\n",
      "14/386:\n",
      "x = \"temp_bag_of_words_5.csv\"\n",
      "x_train = pd.read_csv(x, header=None)\n",
      "x_train.shape\n",
      "14/387:\n",
      "x2 = \"temp2_bag_of_words_0.csv\"\n",
      "x_test = pd.read_csv(x2, header=None)\n",
      "x_test.shape\n",
      "14/388:\n",
      "y2 = open(\"temp2_vocab_5.txt\", \"r\")\n",
      "y2 = y2.readlines()\n",
      "len(y2)\n",
      "14/389:\n",
      "x = \"temp2_bag_of_words_5.csv\"\n",
      "x_train = pd.read_csv(x, header=None)\n",
      "x_train.shape\n",
      "14/390:\n",
      "x2 = \"temp_bag_of_words_0.csv\"\n",
      "x_test = pd.read_csv(x2, header=None)\n",
      "x_test.shape\n",
      "14/391:\n",
      "x2 = \"temp_bag_of_words_0.csv\"\n",
      "x_test = pd.read_csv(x2, header=None)\n",
      "x_test.shape\n",
      "14/392:\n",
      "topk = []\n",
      "topktest = []\n",
      "ks = [100,300,500,700]\n",
      "for i in ks:\n",
      "    selector = SelectKBest(chi2, k=i).fit(x_train, y_train)\n",
      "    x_top = selector.transform(x_train)\n",
      "    lr_top = LogisticRegression(solver='lbfgs', C=1)\n",
      "    model_top = lr_top.fit(x_top, y_train)\n",
      "    topk.append(lr_top.score(x_top, y_train))\n",
      "    selected = selector.get_support(indices=True)\n",
      "    topktest.append(lr_top.score(x_test[selected], y_test))\n",
      "14/393:\n",
      "plot.figure()\n",
      "plot.plot(ks, topk)\n",
      "plot.plot(ks, topktest)\n",
      "print ks[np.argmax(topktest)], np.max(topktest)\n",
      "14/394:\n",
      "topk = []\n",
      "topktest = []\n",
      "ks = [100,300,500,700,900,1100,1300,1500]\n",
      "for i in ks:\n",
      "    selector = SelectKBest(chi2, k=i).fit(x_train, y_train)\n",
      "    x_top = selector.transform(x_train)\n",
      "    lr_top = LogisticRegression(solver='lbfgs', C=1)\n",
      "    model_top = lr_top.fit(x_top, y_train)\n",
      "    topk.append(lr_top.score(x_top, y_train))\n",
      "    selected = selector.get_support(indices=True)\n",
      "    topktest.append(lr_top.score(x_test[selected], y_test))\n",
      "14/395:\n",
      "plot.figure()\n",
      "plot.plot(ks, topk)\n",
      "plot.plot(ks, topktest)\n",
      "print ks[np.argmax(topktest)], np.max(topktest)\n",
      "14/396:\n",
      "topk = []\n",
      "topktest = []\n",
      "ks = [1500,2000,2500,5000]\n",
      "for i in ks:\n",
      "    selector = SelectKBest(chi2, k=i).fit(x_train, y_train)\n",
      "    x_top = selector.transform(x_train)\n",
      "    lr_top = LogisticRegression(solver='lbfgs', C=1)\n",
      "    model_top = lr_top.fit(x_top, y_train)\n",
      "    topk.append(lr_top.score(x_top, y_train))\n",
      "    selected = selector.get_support(indices=True)\n",
      "    topktest.append(lr_top.score(x_test[selected], y_test))\n",
      "14/397:\n",
      "plot.figure()\n",
      "plot.plot(ks, topk)\n",
      "plot.plot(ks, topktest)\n",
      "print ks[np.argmax(topktest)], np.max(topktest)\n",
      "14/398:\n",
      "topk = []\n",
      "topktest = []\n",
      "ks = [1500,2000,2500,5000,7500,10000]\n",
      "for i in ks:\n",
      "    selector = SelectKBest(chi2, k=i).fit(x_train, y_train)\n",
      "    x_top = selector.transform(x_train)\n",
      "    lr_top = LogisticRegression(solver='lbfgs', C=1)\n",
      "    model_top = lr_top.fit(x_top, y_train)\n",
      "    topk.append(lr_top.score(x_top, y_train))\n",
      "    selected = selector.get_support(indices=True)\n",
      "    topktest.append(lr_top.score(x_test[selected], y_test))\n",
      "14/399:\n",
      "plot.figure()\n",
      "plot.plot(ks, topk)\n",
      "plot.plot(ks, topktest)\n",
      "print ks[np.argmax(topktest)], np.max(topktest)\n",
      "14/400:\n",
      "topk = []\n",
      "topktest = []\n",
      "ks = [1000,5000,7500,10000,12500,15000]\n",
      "for i in ks:\n",
      "    selector = SelectKBest(chi2, k=i).fit(x_train, y_train)\n",
      "    x_top = selector.transform(x_train)\n",
      "    lr_top = LogisticRegression(solver='lbfgs', C=1)\n",
      "    model_top = lr_top.fit(x_top, y_train)\n",
      "    topk.append(lr_top.score(x_top, y_train))\n",
      "    selected = selector.get_support(indices=True)\n",
      "    topktest.append(lr_top.score(x_test[selected], y_test))\n",
      "14/401:\n",
      "plot.figure()\n",
      "plot.plot(ks, topk)\n",
      "plot.plot(ks, topktest)\n",
      "print ks[np.argmax(topktest)], np.max(topktest)\n",
      "14/402:\n",
      "topk = []\n",
      "topktest = []\n",
      "ks = [1000,5000,7500,10000,12500]\n",
      "for i in ks:\n",
      "    selector = SelectKBest(chi2, k=i).fit(x_train, y_train)\n",
      "    x_top = selector.transform(x_train)\n",
      "    lr_top = LogisticRegression(solver='lbfgs', C=1)\n",
      "    model_top = lr_top.fit(x_top, y_train)\n",
      "    topk.append(lr_top.score(x_top, y_train))\n",
      "    selected = selector.get_support(indices=True)\n",
      "    topktest.append(lr_top.score(x_test[selected], y_test))\n",
      "14/403:\n",
      "plot.figure()\n",
      "plot.plot(ks, topk)\n",
      "plot.plot(ks, topktest)\n",
      "print ks[np.argmax(topktest)], np.max(topktest)\n",
      "14/404: plot.plot(ks, topktest)\n",
      "14/405:\n",
      "trainWords = open(\"temp2_vocab_5.txt\", \"r\")\n",
      "trainWords = trainWords.readlines()\n",
      "coefs = model_top.coef_[0]\n",
      "tuples = sorted(zip(trainWords, coefs), key=(lambda x: x[1]))\n",
      "print [i[0] for i in tuples[:10]]\n",
      "print [i[0] for i in tuples[-10:]]\n",
      "14/406:\n",
      "trainWords = open(\"temp2_vocab_5.txt\", \"r\")\n",
      "trainWords = trainWords.readlines()\n",
      "coefs = model_top.coef_[0]\n",
      "tuples = sorted(zip(trainWords, coefs), key=(lambda x: x[1]))\n",
      "print [i[0], i[1] for i in tuples[:10]]\n",
      "print [i[0] for i in tuples[-10:]]\n",
      "14/407:\n",
      "trainWords = open(\"temp2_vocab_5.txt\", \"r\")\n",
      "trainWords = trainWords.readlines()\n",
      "coefs = model_top.coef_[0]\n",
      "tuples = sorted(zip(trainWords, coefs), key=(lambda x: x[1]))\n",
      "print [(i[0], i[1]) for i in tuples[:10]]\n",
      "print [i[0] for i in tuples[-10:]]\n",
      "14/408:\n",
      "trainWords = open(\"temp2_vocab_5.txt\", \"r\")\n",
      "trainWords = trainWords.readlines()\n",
      "coefs = model_top.coef_[0]\n",
      "tuples = sorted(zip(trainWords, coefs), key=(lambda x: x[1]))\n",
      "print [(i[0], i[1]) for i in tuples[:10]]\n",
      "print [(i[0], i[1]) for i in tuples[-10:]]\n",
      "14/409: print np.max(coefs)\n",
      "14/410: print np.mean(coefs)\n",
      "14/411: print np.max(coefs)\n",
      "14/412:\n",
      "from sklearn.svm import LinearSVC\n",
      "svm = LinearSVC(C=0.7)\n",
      "svm.fit(x_train, y_train)\n",
      "svm.score(x_train, y_train)\n",
      "14/413: svm.score(x_test, y_test)\n",
      "14/414:\n",
      "from sklearn.svm import LinearSVC\n",
      "svm = LinearSVC(C=1)\n",
      "svm.fit(x_train, y_train)\n",
      "svm.score(x_train, y_train)\n",
      "14/415: svm.score(x_test, y_test)\n",
      "14/416:\n",
      "from sklearn.svm import LinearSVC\n",
      "svm = LinearSVC(C=.01)\n",
      "svm.fit(x_train, y_train)\n",
      "svm.score(x_train, y_train)\n",
      "14/417: svm.score(x_test, y_test)\n",
      "14/418:\n",
      "from sklearn.svm import LinearSVC\n",
      "svm = LinearSVC(C=.1)\n",
      "svm.fit(x_train, y_train)\n",
      "svm.score(x_train, y_train)\n",
      "14/419: svm.score(x_test, y_test)\n",
      "14/420:\n",
      "from sklearn.svm import LinearSVC\n",
      "svm = LinearSVC(C=.6)\n",
      "svm.fit(x_train, y_train)\n",
      "svm.score(x_train, y_train)\n",
      "14/421: svm.score(x_test, y_test)\n",
      "14/422:\n",
      "x3 = \"temp3_bag_of_words_5.csv\"\n",
      "x_train = pd.read_csv(x, header=None)\n",
      "x_train.shape\n",
      "14/423:\n",
      "x4 = \"temp4_bag_of_words_0.csv\"\n",
      "x_test = pd.read_csv(x2, header=None)\n",
      "x_test.shape\n",
      "14/424:\n",
      "x3 = \"temp3_bag_of_words_5.csv\"\n",
      "x_train = pd.read_csv(x3, header=None)\n",
      "x_train.shape\n",
      "14/425:\n",
      "x4 = \"temp4_bag_of_words_0.csv\"\n",
      "x_test = pd.read_csv(x4, header=None)\n",
      "x_test.shape\n",
      "14/426:\n",
      "topk = []\n",
      "topktest = []\n",
      "ks = [1000,5000,7500,10000,12500]\n",
      "for i in ks:\n",
      "    selector = SelectKBest(chi2, k=i).fit(x_train, y_train)\n",
      "    x_top = selector.transform(x_train)\n",
      "    lr_top = LogisticRegression(solver='lbfgs', C=1)\n",
      "    model_top = lr_top.fit(x_top, y_train)\n",
      "    topk.append(lr_top.score(x_top, y_train))\n",
      "    selected = selector.get_support(indices=True)\n",
      "    topktest.append(lr_top.score(x_test[selected], y_test))\n",
      "14/427:\n",
      "topk = []\n",
      "topktest = []\n",
      "ks = [100,250,500,750,1000,1100]\n",
      "for i in ks:\n",
      "    selector = SelectKBest(chi2, k=i).fit(x_train, y_train)\n",
      "    x_top = selector.transform(x_train)\n",
      "    lr_top = LogisticRegression(solver='lbfgs', C=1)\n",
      "    model_top = lr_top.fit(x_top, y_train)\n",
      "    topk.append(lr_top.score(x_top, y_train))\n",
      "    selected = selector.get_support(indices=True)\n",
      "    topktest.append(lr_top.score(x_test[selected], y_test))\n",
      "14/428:\n",
      "plot.figure()\n",
      "plot.plot(ks, topk)\n",
      "plot.plot(ks, topktest)\n",
      "print ks[np.argmax(topktest)], np.max(topktest)\n",
      "14/429:\n",
      "trainWords = open(\"temp3_vocab_5.txt\", \"r\")\n",
      "trainWords = trainWords.readlines()\n",
      "coefs = model_top.coef_[0]\n",
      "tuples = sorted(zip(trainWords, coefs), key=(lambda x: x[1]))\n",
      "print [(i[0], i[1]) for i in tuples[:10]]\n",
      "print [(i[0], i[1]) for i in tuples[-10:]]\n",
      "14/430:\n",
      "topk = []\n",
      "topktest = []\n",
      "ks = [100,250,500,750,800,1000,1100]\n",
      "for i in ks:\n",
      "    selector = SelectKBest(chi2, k=i).fit(x_train, y_train)\n",
      "    x_top = selector.transform(x_train)\n",
      "    lr_top = LogisticRegression(solver='lbfgs', C=1)\n",
      "    model_top = lr_top.fit(x_top, y_train)\n",
      "    topk.append(lr_top.score(x_top, y_train))\n",
      "    selected = selector.get_support(indices=True)\n",
      "    topktest.append(lr_top.score(x_test[selected], y_test))\n",
      "14/431:\n",
      "plot.figure()\n",
      "plot.plot(ks, topk)\n",
      "plot.plot(ks, topktest)\n",
      "print ks[np.argmax(topktest)], np.max(topktest)\n",
      "14/432:\n",
      "topk = []\n",
      "topktest = []\n",
      "ks = [100,250,500,700,750,1000,1100]\n",
      "for i in ks:\n",
      "    selector = SelectKBest(chi2, k=i).fit(x_train, y_train)\n",
      "    x_top = selector.transform(x_train)\n",
      "    lr_top = LogisticRegression(solver='lbfgs', C=1)\n",
      "    model_top = lr_top.fit(x_top, y_train)\n",
      "    topk.append(lr_top.score(x_top, y_train))\n",
      "    selected = selector.get_support(indices=True)\n",
      "    topktest.append(lr_top.score(x_test[selected], y_test))\n",
      "14/433:\n",
      "plot.figure()\n",
      "plot.plot(ks, topk)\n",
      "plot.plot(ks, topktest)\n",
      "print ks[np.argmax(topktest)], np.max(topktest)\n",
      "14/434:\n",
      "topk = []\n",
      "topktest = []\n",
      "ks = [100,250,500,600,700,750,1000,1100]\n",
      "for i in ks:\n",
      "    selector = SelectKBest(chi2, k=i).fit(x_train, y_train)\n",
      "    x_top = selector.transform(x_train)\n",
      "    lr_top = LogisticRegression(solver='lbfgs', C=1)\n",
      "    model_top = lr_top.fit(x_top, y_train)\n",
      "    topk.append(lr_top.score(x_top, y_train))\n",
      "    selected = selector.get_support(indices=True)\n",
      "    topktest.append(lr_top.score(x_test[selected], y_test))\n",
      "14/435:\n",
      "plot.figure()\n",
      "plot.plot(ks, topk)\n",
      "plot.plot(ks, topktest)\n",
      "print ks[np.argmax(topktest)], np.max(topktest)\n",
      "14/436:\n",
      "x3 = \"temp3_bag_of_words_5.csv\"\n",
      "x_train = pd.read_csv(x3, header=None)\n",
      "x_train.shape\n",
      "14/437:\n",
      "x4 = \"temp4_bag_of_words_0.csv\"\n",
      "x_test = pd.read_csv(x4, header=None)\n",
      "x_test.shape\n",
      "14/438:\n",
      "topk = []\n",
      "topktest = []\n",
      "ks = [100,250,500,600,700,750,1000,1100]\n",
      "for i in ks:\n",
      "    selector = SelectKBest(chi2, k=i).fit(x_train, y_train)\n",
      "    x_top = selector.transform(x_train)\n",
      "    lr_top = LogisticRegression(solver='lbfgs', C=1)\n",
      "    model_top = lr_top.fit(x_top, y_train)\n",
      "    topk.append(lr_top.score(x_top, y_train))\n",
      "    selected = selector.get_support(indices=True)\n",
      "    topktest.append(lr_top.score(x_test[selected], y_test))\n",
      "14/439:\n",
      "plot.figure()\n",
      "plot.plot(ks, topk)\n",
      "plot.plot(ks, topktest)\n",
      "print ks[np.argmax(topktest)], np.max(topktest)\n",
      "14/440:\n",
      "topk = []\n",
      "topktest = []\n",
      "ks = [100,250,500,750,1000,1250,1500,1750,2000]\n",
      "for i in ks:\n",
      "    selector = SelectKBest(chi2, k=i).fit(x_train, y_train)\n",
      "    x_top = selector.transform(x_train)\n",
      "    lr_top = LogisticRegression(solver='lbfgs', C=1)\n",
      "    model_top = lr_top.fit(x_top, y_train)\n",
      "    topk.append(lr_top.score(x_top, y_train))\n",
      "    selected = selector.get_support(indices=True)\n",
      "    topktest.append(lr_top.score(x_test[selected], y_test))\n",
      "14/441:\n",
      "plot.figure()\n",
      "plot.plot(ks, topk)\n",
      "plot.plot(ks, topktest)\n",
      "print ks[np.argmax(topktest)], np.max(topktest)\n",
      "14/442:\n",
      "topk = []\n",
      "topktest = []\n",
      "ks = [100,250,500,750,1000,1200,1250,1300,1500,1750,2000]\n",
      "for i in ks:\n",
      "    selector = SelectKBest(chi2, k=i).fit(x_train, y_train)\n",
      "    x_top = selector.transform(x_train)\n",
      "    lr_top = LogisticRegression(solver='lbfgs', C=1)\n",
      "    model_top = lr_top.fit(x_top, y_train)\n",
      "    topk.append(lr_top.score(x_top, y_train))\n",
      "    selected = selector.get_support(indices=True)\n",
      "    topktest.append(lr_top.score(x_test[selected], y_test))\n",
      "14/443:\n",
      "plot.figure()\n",
      "plot.plot(ks, topk)\n",
      "plot.plot(ks, topktest)\n",
      "print ks[np.argmax(topktest)], np.max(topktest)\n",
      "14/444:\n",
      "topk = []\n",
      "topktest = []\n",
      "ks = [100,250,500,750,1000,1250,1500,1750,2000]\n",
      "for i in ks:\n",
      "    selector = SelectKBest(chi2, k=i).fit(x_train, y_train)\n",
      "    x_top = selector.transform(x_train)\n",
      "    lr_top = LogisticRegression(solver='lbfgs', C=1)\n",
      "    model_top = lr_top.fit(x_top, y_train)\n",
      "    topk.append(lr_top.score(x_top, y_train))\n",
      "    selected = selector.get_support(indices=True)\n",
      "    topktest.append(lr_top.score(x_test[selected], y_test))\n",
      "14/445:\n",
      "plot.figure()\n",
      "plot.plot(ks, topk)\n",
      "plot.plot(ks, topktest)\n",
      "print ks[np.argmax(topktest)], np.max(topktest)\n",
      "14/446:\n",
      "topk = []\n",
      "topktest = []\n",
      "ks = [100,250,500,750,1000,1250,1500,1750,2000,2500,3000,3500,4000]\n",
      "for i in ks:\n",
      "    selector = SelectKBest(chi2, k=i).fit(x_train, y_train)\n",
      "    x_top = selector.transform(x_train)\n",
      "    lr_top = LogisticRegression(solver='lbfgs', C=1)\n",
      "    model_top = lr_top.fit(x_top, y_train)\n",
      "    topk.append(lr_top.score(x_top, y_train))\n",
      "    selected = selector.get_support(indices=True)\n",
      "    topktest.append(lr_top.score(x_test[selected], y_test))\n",
      "14/447:\n",
      "x3 = \"temp3_bag_of_words_5.csv\"\n",
      "x_train = pd.read_csv(x3, header=None)\n",
      "x_train.shape\n",
      "14/448:\n",
      "x4 = \"temp4_bag_of_words_0.csv\"\n",
      "x_test = pd.read_csv(x4, header=None)\n",
      "x_test.shape\n",
      "14/449:\n",
      "topk = []\n",
      "topktest = []\n",
      "ks = [100,250,500,750,1000,1250,1500,1750,2000,2500,3000,3500,4000]\n",
      "for i in ks:\n",
      "    selector = SelectKBest(chi2, k=i).fit(x_train, y_train)\n",
      "    x_top = selector.transform(x_train)\n",
      "    lr_top = LogisticRegression(solver='lbfgs', C=1)\n",
      "    model_top = lr_top.fit(x_top, y_train)\n",
      "    topk.append(lr_top.score(x_top, y_train))\n",
      "    selected = selector.get_support(indices=True)\n",
      "    topktest.append(lr_top.score(x_test[selected], y_test))\n",
      "14/450:\n",
      "plot.figure()\n",
      "plot.plot(ks, topk)\n",
      "plot.plot(ks, topktest)\n",
      "print ks[np.argmax(topktest)], np.max(topktest)\n",
      "14/451:\n",
      "topk = []\n",
      "topktest = []\n",
      "ks = [100,500,1000,1500,2000,2250,2500,2750,3000,3500,4000]\n",
      "for i in ks:\n",
      "    selector = SelectKBest(chi2, k=i).fit(x_train, y_train)\n",
      "    x_top = selector.transform(x_train)\n",
      "    lr_top = LogisticRegression(solver='lbfgs', C=1)\n",
      "    model_top = lr_top.fit(x_top, y_train)\n",
      "    topk.append(lr_top.score(x_top, y_train))\n",
      "    selected = selector.get_support(indices=True)\n",
      "    topktest.append(lr_top.score(x_test[selected], y_test))\n",
      "14/452:\n",
      "plot.figure()\n",
      "plot.plot(ks, topk)\n",
      "plot.plot(ks, topktest)\n",
      "print ks[np.argmax(topktest)], np.max(topktest)\n",
      "14/453:\n",
      "topk = []\n",
      "topktest = []\n",
      "ks = [100,500,1000,1500,2000,2450,2500,2550,3000,3500,4000]\n",
      "for i in ks:\n",
      "    selector = SelectKBest(chi2, k=i).fit(x_train, y_train)\n",
      "    x_top = selector.transform(x_train)\n",
      "    lr_top = LogisticRegression(solver='lbfgs', C=1)\n",
      "    model_top = lr_top.fit(x_top, y_train)\n",
      "    topk.append(lr_top.score(x_top, y_train))\n",
      "    selected = selector.get_support(indices=True)\n",
      "    topktest.append(lr_top.score(x_test[selected], y_test))\n",
      "14/454:\n",
      "plot.figure()\n",
      "plot.plot(ks, topk)\n",
      "plot.plot(ks, topktest)\n",
      "print ks[np.argmax(topktest)], np.max(topktest)\n",
      "14/455:\n",
      "topk = []\n",
      "topktest = []\n",
      "ks = [100,500,1000,1500,2000,2500,3000,3500,4000]\n",
      "for i in ks:\n",
      "    selector = SelectKBest(chi2, k=i).fit(x_train, y_train)\n",
      "    x_top = selector.transform(x_train)\n",
      "    lr_top = LogisticRegression(solver='lbfgs', C=1)\n",
      "    model_top = lr_top.fit(x_top, y_train)\n",
      "    topk.append(lr_top.score(x_top, y_train))\n",
      "    selected = selector.get_support(indices=True)\n",
      "    topktest.append(lr_top.score(x_test[selected], y_test))\n",
      "14/456:\n",
      "plot.figure()\n",
      "plot.plot(ks, topk)\n",
      "plot.plot(ks, topktest)\n",
      "print ks[np.argmax(topktest)], np.max(topktest)\n",
      "14/457:\n",
      "from sklearn.svm import LinearSVC\n",
      "svm = LinearSVC(C=0.7)\n",
      "svm.fit(x_train, y_train)\n",
      "svm.score(x_train, y_train)\n",
      "14/458: svm.score(x_test, y_test)\n",
      "14/459:\n",
      "from sklearn.svm import LinearSVC\n",
      "svm = LinearSVC(C=0.5)\n",
      "svm.fit(x_train, y_train)\n",
      "svm.score(x_train, y_train)\n",
      "14/460: svm.score(x_test, y_test)\n",
      "14/461:\n",
      "from sklearn.svm import LinearSVC\n",
      "svm = LinearSVC(C=0.05)\n",
      "svm.fit(x_train, y_train)\n",
      "svm.score(x_train, y_train)\n",
      "14/462: svm.score(x_test, y_test)\n",
      "14/463:\n",
      "from sklearn.svm import LinearSVC\n",
      "svm = LinearSVC(C=1)\n",
      "svm.fit(x_train, y_train)\n",
      "svm.score(x_train, y_train)\n",
      "14/464: svm.score(x_test, y_test)\n",
      "14/465:\n",
      "from sklearn.svm import LinearSVC\n",
      "svm = LinearSVC(C=.01)\n",
      "svm.fit(x_train, y_train)\n",
      "svm.score(x_train, y_train)\n",
      "14/466: svm.score(x_test, y_test)\n",
      "14/467:\n",
      "x5 = \"temp5_bag_of_words_5.csv\"\n",
      "x_train = pd.read_csv(x5, header=None)\n",
      "x_train.shape\n",
      "14/468:\n",
      "x6 = \"temp6_bag_of_words_0.csv\"\n",
      "x_test = pd.read_csv(x6, header=None)\n",
      "x_test.shape\n",
      "14/469:\n",
      "topk = []\n",
      "topktest = []\n",
      "ks = [1000,5000,7500,10000,12500]\n",
      "for i in ks:\n",
      "    selector = SelectKBest(chi2, k=i).fit(x_train, y_train)\n",
      "    x_top = selector.transform(x_train)\n",
      "    lr_top = LogisticRegression(solver='lbfgs', C=1)\n",
      "    model_top = lr_top.fit(x_top, y_train)\n",
      "    topk.append(lr_top.score(x_top, y_train))\n",
      "    selected = selector.get_support(indices=True)\n",
      "    topktest.append(lr_top.score(x_test[selected], y_test))\n",
      "14/470:\n",
      "plot.figure()\n",
      "plot.plot(ks, topk)\n",
      "plot.plot(ks, topktest)\n",
      "print ks[np.argmax(topktest)], np.max(topktest)\n",
      "14/471:\n",
      "topk = []\n",
      "topktest = []\n",
      "ks = [100,500,1000,1500,2000,2500,3000,3500,4000]\n",
      "for i in ks:\n",
      "    selector = SelectKBest(chi2, k=i).fit(x_train, y_train)\n",
      "    x_top = selector.transform(x_train)\n",
      "    lr_top = LogisticRegression(solver='lbfgs', C=.1)\n",
      "    model_top = lr_top.fit(x_top, y_train)\n",
      "    topk.append(lr_top.score(x_top, y_train))\n",
      "    selected = selector.get_support(indices=True)\n",
      "    topktest.append(lr_top.score(x_test[selected], y_test))\n",
      "14/472:\n",
      "plot.figure()\n",
      "plot.plot(ks, topk)\n",
      "plot.plot(ks, topktest)\n",
      "print ks[np.argmax(topktest)], np.max(topktest)\n",
      "14/473:\n",
      "topk = []\n",
      "topktest = []\n",
      "ks = [100,500,1000,1500,2000,2500,3000,3500,4000]\n",
      "for i in ks:\n",
      "    selector = SelectKBest(chi2, k=i).fit(x_train, y_train)\n",
      "    x_top = selector.transform(x_train)\n",
      "    lr_top = LogisticRegression(solver='lbfgs', C=.8)\n",
      "    model_top = lr_top.fit(x_top, y_train)\n",
      "    topk.append(lr_top.score(x_top, y_train))\n",
      "    selected = selector.get_support(indices=True)\n",
      "    topktest.append(lr_top.score(x_test[selected], y_test))\n",
      "14/474:\n",
      "plot.figure()\n",
      "plot.plot(ks, topk)\n",
      "plot.plot(ks, topktest)\n",
      "print ks[np.argmax(topktest)], np.max(topktest)\n",
      "14/475:\n",
      "topk = []\n",
      "topktest = []\n",
      "ks = [100,500,1000,1500,2000,2500,3000,3500,4000]\n",
      "for i in ks:\n",
      "    selector = SelectKBest(chi2, k=i).fit(x_train, y_train)\n",
      "    x_top = selector.transform(x_train)\n",
      "    lr_top = LogisticRegression(solver='lbfgs', C=1)\n",
      "    model_top = lr_top.fit(x_top, y_train)\n",
      "    topk.append(lr_top.score(x_top, y_train))\n",
      "    selected = selector.get_support(indices=True)\n",
      "    topktest.append(lr_top.score(x_test[selected], y_test))\n",
      "14/476:\n",
      "plot.figure()\n",
      "plot.plot(ks, topk)\n",
      "plot.plot(ks, topktest)\n",
      "print ks[np.argmax(topktest)], np.max(topktest)\n",
      "14/477:\n",
      "topk = []\n",
      "topktest = []\n",
      "ks = [1000,5000,7500,10000,12500]\n",
      "for i in ks:\n",
      "    selector = SelectKBest(chi2, k=i).fit(x_train, y_train)\n",
      "    x_top = selector.transform(x_train)\n",
      "    lr_top = LogisticRegression(solver='lbfgs', C=.1)\n",
      "    model_top = lr_top.fit(x_top, y_train)\n",
      "    topk.append(lr_top.score(x_top, y_train))\n",
      "    selected = selector.get_support(indices=True)\n",
      "    topktest.append(lr_top.score(x_test[selected], y_test))\n",
      "14/478:\n",
      "plot.figure()\n",
      "plot.plot(ks, topk)\n",
      "plot.plot(ks, topktest)\n",
      "print ks[np.argmax(topktest)], np.max(topktest)\n",
      "14/479:\n",
      "topk = []\n",
      "topktest = []\n",
      "ks = [1000,5000,7500,10000,12500]\n",
      "for i in ks:\n",
      "    selector = SelectKBest(chi2, k=i).fit(x_train, y_train)\n",
      "    x_top = selector.transform(x_train)\n",
      "    lr_top = LogisticRegression(solver='lbfgs', C=.8)\n",
      "    model_top = lr_top.fit(x_top, y_train)\n",
      "    topk.append(lr_top.score(x_top, y_train))\n",
      "    selected = selector.get_support(indices=True)\n",
      "    topktest.append(lr_top.score(x_test[selected], y_test))\n",
      "14/480:\n",
      "plot.figure()\n",
      "plot.plot(ks, topk)\n",
      "plot.plot(ks, topktest)\n",
      "print ks[np.argmax(topktest)], np.max(topktest)\n",
      "14/481:\n",
      "topk = []\n",
      "topktest = []\n",
      "ks = [1000,5000,7500,10000,12500]\n",
      "for i in ks:\n",
      "    selector = SelectKBest(chi2, k=i).fit(x_train, y_train)\n",
      "    x_top = selector.transform(x_train)\n",
      "    lr_top = LogisticRegression(solver='lbfgs', C=1)\n",
      "    model_top = lr_top.fit(x_top, y_train)\n",
      "    topk.append(lr_top.score(x_top, y_train))\n",
      "    selected = selector.get_support(indices=True)\n",
      "    topktest.append(lr_top.score(x_test[selected], y_test))\n",
      "14/482:\n",
      "plot.figure()\n",
      "plot.plot(ks, topk)\n",
      "plot.plot(ks, topktest)\n",
      "print ks[np.argmax(topktest)], np.max(topktest)\n",
      "14/483:\n",
      "x = \"stop_bag_of_words_0.csv\"\n",
      "x_test = pd.read_csv(x, header=None)\n",
      "x_test.shape\n",
      "14/484:\n",
      "x = \"stopt_bag_of_words_5.csv\"\n",
      "x_train = pd.read_csv(x, header=None)\n",
      "x_train.shape\n",
      "14/485:\n",
      "topk = []\n",
      "topktest = []\n",
      "ks = [20,50,100,150,200,250,300,350,400,450,500,540]\n",
      "for i in ks:\n",
      "    selector = SelectKBest(chi2, k=i).fit(x_train, y_train)\n",
      "    x_top = selector.transform(x_train)\n",
      "    lr_top = LogisticRegression(solver='lbfgs', C=1)\n",
      "    model_top = lr_top.fit(x_top, y_train)\n",
      "    topk.append(lr_top.score(x_top, y_train))\n",
      "    selected = selector.get_support(indices=True)\n",
      "    topktest.append(lr_top.score(x_test[selected], y_test))\n",
      "14/486:\n",
      "plot.figure()\n",
      "plot.plot(ks, topk)\n",
      "plot.plot(ks, topktest)\n",
      "print ks[np.argmax(topktest)], np.max(topktest)\n",
      "14/487:\n",
      "from sklearn.svm import LinearSVC\n",
      "svm = LinearSVC(C=0.7)\n",
      "svm.fit(x_train, y_train)\n",
      "svm.score(x_train, y_train)\n",
      "14/488: svm.score(x_test, y_test)\n",
      "14/489:\n",
      "x = \"temp2_bag_of_words_5.csv\"\n",
      "x_train = pd.read_csv(x, header=None)\n",
      "x_train.shape\n",
      "14/490:\n",
      "x2 = \"temp_bag_of_words_0.csv\"\n",
      "x_test = pd.read_csv(x2, header=None)\n",
      "x_test.shape\n",
      "14/491:\n",
      "topk = []\n",
      "topktest = []\n",
      "ks = [1000,5000,7500,10000,12500]\n",
      "for i in ks:\n",
      "    selector = SelectKBest(chi2, k=i).fit(x_train, y_train)\n",
      "    x_top = selector.transform(x_train)\n",
      "    lr_top = LogisticRegression(solver='lbfgs', C=1)\n",
      "    model_top = lr_top.fit(x_top, y_train)\n",
      "    topk.append(lr_top.score(x_top, y_train))\n",
      "    selected = selector.get_support(indices=True)\n",
      "    topktest.append(lr_top.score(x_test[selected], y_test))\n",
      "14/492:\n",
      "plot.figure()\n",
      "plot.plot(ks, topk)\n",
      "plot.plot(ks, topktest)\n",
      "print ks[np.argmax(topktest)], np.max(topktest)\n",
      "14/493: plot.plot(ks, topktest)\n",
      "14/494:\n",
      "trainWords = open(\"temp2_vocab_5.txt\", \"r\")\n",
      "trainWords = trainWords.readlines()\n",
      "coefs = model_top.coef_[0]\n",
      "tuples = sorted(zip(trainWords, coefs), key=(lambda x: x[1]))\n",
      "print [(i[0], i[1]) for i in tuples[:10]]\n",
      "print [(i[0], i[1]) for i in tuples[-10:]]\n",
      "14/495:\n",
      "x3 = \"temp3_bag_of_words_5.csv\"\n",
      "x_train = pd.read_csv(x3, header=None)\n",
      "x_train.shape\n",
      "14/496:\n",
      "x4 = \"temp4_bag_of_words_0.csv\"\n",
      "x_test = pd.read_csv(x4, header=None)\n",
      "x_test.shape\n",
      "14/497:\n",
      "topk = []\n",
      "topktest = []\n",
      "ks = [100,500,1000,1500,2000,2500,3000,3500,4000]\n",
      "for i in ks:\n",
      "    selector = SelectKBest(chi2, k=i).fit(x_train, y_train)\n",
      "    x_top = selector.transform(x_train)\n",
      "    lr_top = LogisticRegression(solver='lbfgs', C=1)\n",
      "    model_top = lr_top.fit(x_top, y_train)\n",
      "    topk.append(lr_top.score(x_top, y_train))\n",
      "    selected = selector.get_support(indices=True)\n",
      "    topktest.append(lr_top.score(x_test[selected], y_test))\n",
      "14/498:\n",
      "plot.figure()\n",
      "plot.plot(ks, topk)\n",
      "plot.plot(ks, topktest)\n",
      "print ks[np.argmax(topktest)], np.max(topktest)\n",
      "14/499:\n",
      "x5 = \"temp5_bag_of_words_5.csv\"\n",
      "x_train = pd.read_csv(x5, header=None)\n",
      "x_train.shape\n",
      "14/500:\n",
      "x6 = \"temp6_bag_of_words_0.csv\"\n",
      "x_test = pd.read_csv(x6, header=None)\n",
      "x_test.shape\n",
      "14/501:\n",
      "topk = []\n",
      "topktest = []\n",
      "ks = [1000,5000,7500,10000,12500]\n",
      "for i in ks:\n",
      "    selector = SelectKBest(chi2, k=i).fit(x_train, y_train)\n",
      "    x_top = selector.transform(x_train)\n",
      "    lr_top = LogisticRegression(solver='lbfgs', C=1)\n",
      "    model_top = lr_top.fit(x_top, y_train)\n",
      "    topk.append(lr_top.score(x_top, y_train))\n",
      "    selected = selector.get_support(indices=True)\n",
      "    topktest.append(lr_top.score(x_test[selected], y_test))\n",
      "14/502:\n",
      "plot.figure()\n",
      "plot.plot(ks, topk)\n",
      "plot.plot(ks, topktest)\n",
      "print ks[np.argmax(topktest)], np.max(topktest)\n",
      "14/503:\n",
      "x = \"temp2_bag_of_words_5.csv\"\n",
      "x_train = pd.read_csv(x, header=None)\n",
      "x_train.shape\n",
      "14/504:\n",
      "x2 = \"temp_bag_of_words_0.csv\"\n",
      "x_test = pd.read_csv(x2, header=None)\n",
      "x_test.shape\n",
      "14/505:\n",
      "selector = SelectKBest(chi2, k=i).fit(x_train, y_train)\n",
      "x_top = selector.transform(x_train)\n",
      "lr_top = LogisticRegression(solver='lbfgs', C=1)\n",
      "model_top = lr_top.fit(x_top, y_train)\n",
      "topk.append(lr_top.score(x_top, y_train))\n",
      "selected = selector.get_support(indices=True)\n",
      "topktest.append(lr_top.score(x_test[selected], y_test))\n",
      "14/506:\n",
      "selector = SelectKBest(chi2, k=i).fit(x_train, y_train)\n",
      "x_top = selector.transform(x_train)\n",
      "lr_top = LogisticRegression(solver='lbfgs', C=1)\n",
      "model_top = lr_top.fit(x_top, y_train)\n",
      "lr_top.score(x_top, y_train)\n",
      "selected = selector.get_support(indices=True)\n",
      "lr_top.score(x_test[selected], y_test)\n",
      "14/507:\n",
      "selector = SelectKBest(chi2, k=10000).fit(x_train, y_train)\n",
      "x_top = selector.transform(x_train)\n",
      "lr_top = LogisticRegression(solver='lbfgs', C=1)\n",
      "model_top = lr_top.fit(x_top, y_train)\n",
      "lr_top.score(x_top, y_train)\n",
      "selected = selector.get_support(indices=True)\n",
      "lr_top.score(x_test[selected], y_test)\n",
      "14/508:\n",
      "selector = SelectKBest(chi2, k=10100).fit(x_train, y_train)\n",
      "x_top = selector.transform(x_train)\n",
      "lr_top = LogisticRegression(solver='lbfgs', C=1)\n",
      "model_top = lr_top.fit(x_top, y_train)\n",
      "lr_top.score(x_top, y_train)\n",
      "selected = selector.get_support(indices=True)\n",
      "lr_top.score(x_test[selected], y_test)\n",
      "14/509:\n",
      "selector = SelectKBest(chi2, k=12000).fit(x_train, y_train)\n",
      "x_top = selector.transform(x_train)\n",
      "lr_top = LogisticRegression(solver='lbfgs', C=1)\n",
      "model_top = lr_top.fit(x_top, y_train)\n",
      "lr_top.score(x_top, y_train)\n",
      "selected = selector.get_support(indices=True)\n",
      "lr_top.score(x_test[selected], y_test)\n",
      "14/510:\n",
      "selector = SelectKBest(chi2, k=9000).fit(x_train, y_train)\n",
      "x_top = selector.transform(x_train)\n",
      "lr_top = LogisticRegression(solver='lbfgs', C=1)\n",
      "model_top = lr_top.fit(x_top, y_train)\n",
      "lr_top.score(x_top, y_train)\n",
      "selected = selector.get_support(indices=True)\n",
      "lr_top.score(x_test[selected], y_test)\n",
      "14/511:\n",
      "selector = SelectKBest(chi2, k=8000).fit(x_train, y_train)\n",
      "x_top = selector.transform(x_train)\n",
      "lr_top = LogisticRegression(solver='lbfgs', C=1)\n",
      "model_top = lr_top.fit(x_top, y_train)\n",
      "lr_top.score(x_top, y_train)\n",
      "selected = selector.get_support(indices=True)\n",
      "lr_top.score(x_test[selected], y_test)\n",
      "14/512:\n",
      "selector = SelectKBest(chi2, k=6000).fit(x_train, y_train)\n",
      "x_top = selector.transform(x_train)\n",
      "lr_top = LogisticRegression(solver='lbfgs', C=1)\n",
      "model_top = lr_top.fit(x_top, y_train)\n",
      "lr_top.score(x_top, y_train)\n",
      "selected = selector.get_support(indices=True)\n",
      "lr_top.score(x_test[selected], y_test)\n",
      "14/513:\n",
      "selector = SelectKBest(chi2, k=4000).fit(x_train, y_train)\n",
      "x_top = selector.transform(x_train)\n",
      "lr_top = LogisticRegression(solver='lbfgs', C=1)\n",
      "model_top = lr_top.fit(x_top, y_train)\n",
      "lr_top.score(x_top, y_train)\n",
      "selected = selector.get_support(indices=True)\n",
      "lr_top.score(x_test[selected], y_test)\n",
      "14/514:\n",
      "selector = SelectKBest(chi2, k=2000).fit(x_train, y_train)\n",
      "x_top = selector.transform(x_train)\n",
      "lr_top = LogisticRegression(solver='lbfgs', C=1)\n",
      "model_top = lr_top.fit(x_top, y_train)\n",
      "lr_top.score(x_top, y_train)\n",
      "selected = selector.get_support(indices=True)\n",
      "lr_top.score(x_test[selected], y_test)\n",
      "14/515:\n",
      "selector = SelectKBest(chi2, k=8000).fit(x_train, y_train)\n",
      "x_top = selector.transform(x_train)\n",
      "lr_top = LogisticRegression(solver='lbfgs', C=1)\n",
      "model_top = lr_top.fit(x_top, y_train)\n",
      "lr_top.score(x_top, y_train)\n",
      "selected = selector.get_support(indices=True)\n",
      "lr_top.score(x_test[selected], y_test)\n",
      "14/516:\n",
      "selector = SelectKBest(chi2, k=9000).fit(x_train, y_train)\n",
      "x_top = selector.transform(x_train)\n",
      "lr_top = LogisticRegression(solver='lbfgs', C=1)\n",
      "model_top = lr_top.fit(x_top, y_train)\n",
      "lr_top.score(x_top, y_train)\n",
      "selected = selector.get_support(indices=True)\n",
      "lr_top.score(x_test[selected], y_test)\n",
      "14/517:\n",
      "selector = SelectKBest(chi2, k=9500).fit(x_train, y_train)\n",
      "x_top = selector.transform(x_train)\n",
      "lr_top = LogisticRegression(solver='lbfgs', C=1)\n",
      "model_top = lr_top.fit(x_top, y_train)\n",
      "lr_top.score(x_top, y_train)\n",
      "selected = selector.get_support(indices=True)\n",
      "lr_top.score(x_test[selected], y_test)\n",
      "14/518:\n",
      "selector = SelectKBest(chi2, k=9900).fit(x_train, y_train)\n",
      "x_top = selector.transform(x_train)\n",
      "lr_top = LogisticRegression(solver='lbfgs', C=1)\n",
      "model_top = lr_top.fit(x_top, y_train)\n",
      "lr_top.score(x_top, y_train)\n",
      "selected = selector.get_support(indices=True)\n",
      "lr_top.score(x_test[selected], y_test)\n",
      "14/519:\n",
      "selector = SelectKBest(chi2, k=8800).fit(x_train, y_train)\n",
      "x_top = selector.transform(x_train)\n",
      "lr_top = LogisticRegression(solver='lbfgs', C=1)\n",
      "model_top = lr_top.fit(x_top, y_train)\n",
      "lr_top.score(x_top, y_train)\n",
      "selected = selector.get_support(indices=True)\n",
      "lr_top.score(x_test[selected], y_test)\n",
      "14/520:\n",
      "selector = SelectKBest(chi2, k=9000).fit(x_train, y_train)\n",
      "x_top = selector.transform(x_train)\n",
      "lr_top = LogisticRegression(solver='lbfgs', C=1)\n",
      "model_top = lr_top.fit(x_top, y_train)\n",
      "lr_top.score(x_top, y_train)\n",
      "selected = selector.get_support(indices=True)\n",
      "lr_top.score(x_test[selected], y_test)\n",
      "14/521:\n",
      "selector = SelectKBest(chi2, k=9000).fit(x_train, y_train)\n",
      "x_top = selector.transform(x_train)\n",
      "lr_top = LogisticRegression(solver='lbfgs', C=1)\n",
      "model_top = lr_top.fit(x_top, y_train)\n",
      "selected = selector.get_support(indices=True)\n",
      "lr_top.score(x_test[selected], y_test)\n",
      "14/522: from sklearn.metrics import roc_curve\n",
      "14/523: fpr, tpr, _ = roc_curve(lr_top.predict(x_train,x_test,drop_intermediate=False)\n",
      "14/524: fpr, tpr, _ = roc_curve(lr_top.predict(x_train),x_test,drop_intermediate=False)\n",
      "14/525: fpr, tpr, _ = roc_curve(lr_top.predict(x_train[selected]),x_test,drop_intermediate=False)\n",
      "14/526: fpr, tpr, _ = roc_curve(lr_top.predict(x_train[selected]),x_test[selected],drop_intermediate=False)\n",
      "14/527: fpr, tpr, _ = roc_curve(lr_top.predict(x_test[selected]),y_test,drop_intermediate=False)\n",
      "14/528: plot.plot(fpr, tpr)\n",
      "14/529: fpr\n",
      "14/530:\n",
      "plot.figure()\n",
      "plot.plot(fpr, tpr)\n",
      "14/531: fpr, tpr, _ = roc_curve(lr_top.predict_proba(x_test[selected])[:,1],y_test,drop_intermediate=False)\n",
      "14/532: plot.plot(fpr, tpr)\n",
      "14/533: fpr, tpr, _ = roc_curve(lr_top.predict_proba(x_test[selected])[:,1],y_test)\n",
      "14/534: fpr, tpr, _ = roc_curve(y_test, lr_top.predict_proba(x_test[selected])[:,1])\n",
      "14/535: plot.plot(fpr, tpr)\n",
      "14/536:\n",
      "plot.figure()\n",
      "plot.plot(fpr, tpr)\n",
      "14/537: fpr3, tpr3, _ = roc_curve(y_test, lr3.predict_proba(x_testk)[:,1])\n",
      "14/538:\n",
      "plot.figure()\n",
      "plot.plot(fpr, tpr)\n",
      "plot.plot(fpr3, tpr3)\n",
      "14/539: # Logistic Regression, CountVectorizer, 2-word sequence, top K=9000, stopwords\n",
      "14/540: # Using TfidfVectorizer\n",
      "14/541: # Logistic regression, CountVectorizer, 2-word sequences\n",
      "14/542: Logistic Regression, CountVectorizer, 2-word sequences, min_df=3\n",
      "14/543: # Logistic Regression, CountVectorizer, 2-word sequences, min_df=3\n",
      "14/544: # Logistic regression, top K\n",
      "14/545: # SVM, top K\n",
      "14/546: # ROC curve: logistic regression with (1,2) vs 1 word sequences\n",
      "14/547: # ROC curve: logistic regression vs SVM vs MNB\n",
      "14/548: # Logistic regression: top K vs all\n",
      "14/549: # ROC curve: logistic regression: few stopwords vs all\n",
      "14/550: fpr2, tpr2, _ = roc_curve(y_test, svm.predict_proba(x_test)[:,1])\n",
      "14/551: fpr2, tpr2, _ = roc_curve(y_test, mnb.predict_proba(x_test)[:,1])\n",
      "14/552:\n",
      "mnb.fit(x_train, y_train)\n",
      "fpr2, tpr2, _ = roc_curve(y_test, mnb.predict_proba(x_test)[:,1])\n",
      "14/553:\n",
      "plot.figure()\n",
      "plot.plot(fpr, tpr) # logistic regression, top K + 2-word sequences\n",
      "plot.plot(fpr2, tpr2)\n",
      "14/554:\n",
      "plot.figure()\n",
      "plot.plot(fpr, tpr) # logistic regression, top K + 2-word sequences\n",
      "# plot.plot(fpr2, tpr2)\n",
      "14/555:\n",
      "plot.figure()\n",
      "plot.plot(fpr, tpr) # logistic regression, top K + 2-word sequences\n",
      "plot.plot(fpr2, tpr2)\n",
      "14/556:\n",
      "svm = LinearSVC(C=0.7, probability=True)\n",
      "svm.fit(x_train, y_train)\n",
      "14/557:\n",
      "svm = LinearSVC(C=0.7)\n",
      "svm.fit(x_train, y_train)\n",
      "14/558:\n",
      "mnb.fit(x_train, y_train)\n",
      "fpr_mnb, tpr_mnb, _ = roc_curve(y_test, mnb.predict_proba(x_test)[:,1])\n",
      "14/559:\n",
      "svm = LinearSVC(C=0.7)\n",
      "svm.fit(x_train, y_train)\n",
      "fpr_svm, tpr_svm, _ = roc_curve(y_test, svm.decision_function(x_test)[:,1])\n",
      "14/560:\n",
      "svm = LinearSVC(C=0.7)\n",
      "svm.fit(x_train, y_train)\n",
      "fpr_svm, tpr_svm, _ = roc_curve(y_test, svm.decision_function(x_test))\n",
      "14/561:\n",
      "plot.figure()\n",
      "plot.plot(fpr, tpr) # logistic regression, top K + 2-word sequences\n",
      "plot.plot(fpr_mnb, tpr_mnb) # MNB\n",
      "plot.plot(fpr_svm, tpr_svm)\n",
      "14/562:\n",
      "plot.figure()\n",
      "plot.plot(fpr, tpr) # logistic regression, top K + 2-word sequences\n",
      "plot.plot(fpr_mnb, tpr_mnb) # MNB\n",
      "plot.plot(fpr_svm, tpr_svm) # SVM\n",
      "14/563: svm.score(x_test, y_test)\n",
      "14/564:\n",
      "svm = LinearSVC(C=1)\n",
      "svm.fit(x_train, y_train)\n",
      "fpr_svm, tpr_svm, _ = roc_curve(y_test, svm.decision_function(x_test))\n",
      "14/565: svm.score(x_test, y_test)\n",
      "14/566:\n",
      "plot.figure()\n",
      "plot.plot(fpr, tpr) # logistic regression, top K + 2-word sequences\n",
      "plot.plot(fpr_mnb, tpr_mnb) # MNB\n",
      "plot.plot(fpr_svm, tpr_svm) # SVM\n",
      "14/567:\n",
      "svm = LinearSVC(C=.1)\n",
      "svm.fit(x_train, y_train)\n",
      "fpr_svm, tpr_svm, _ = roc_curve(y_test, svm.decision_function(x_test))\n",
      "14/568: svm.score(x_test, y_test)\n",
      "14/569:\n",
      "svm = LinearSVC(C=.5)\n",
      "svm.fit(x_train, y_train)\n",
      "fpr_svm, tpr_svm, _ = roc_curve(y_test, svm.decision_function(x_test))\n",
      "14/570: svm.score(x_test, y_test)\n",
      "14/571:\n",
      "svm = LinearSVC(C=.5)\n",
      "svm.fit(x_train, y_train)\n",
      "fpr_svm, tpr_svm, _ = roc_curve(y_test, svm.decision_function(x_test))\n",
      "14/572: svm.score(x_test, y_test)\n",
      "14/573:\n",
      "plot.figure()\n",
      "plot.plot(fpr, tpr) # logistic regression, top K + 2-word sequences\n",
      "plot.plot(fpr_mnb, tpr_mnb) # MNB\n",
      "plot.plot(fpr_svm, tpr_svm) # SVM\n",
      "14/574:\n",
      "lr_all = LogisticRegression(solver='lbfgs', C=1)\n",
      "model_all = lr_all.fit(x_train, y_train)\n",
      "lr_all.score(x_test, y_test)\n",
      "14/575:\n",
      "lr_all = LogisticRegression(solver='lbfgs', C=1)\n",
      "model_all = lr_all.fit(x_train, y_train)\n",
      "lr_all.score(x_test, y_test)\n",
      "fpr_all, tpr_all, _ = roc_curve(y_test, lr_all.predict(x_test))\n",
      "14/576:\n",
      "plot.figure()\n",
      "plot.plot(fpr, tpr) # logistic regression, top K + 2-word sequences\n",
      "plot.plot(fpr_mnb, tpr_mnb) # MNB\n",
      "14/577:\n",
      "plot.figure()\n",
      "plot.plot(fpr, tpr) # logistic regression, top K + 2-word sequences\n",
      "plot.plot(fpr_all, tpr_all) # logistic regression, all\n",
      "14/578:\n",
      "lr_all = LogisticRegression(solver='lbfgs', C=1)\n",
      "model_all = lr_all.fit(x_train, y_train)\n",
      "lr_all.score(x_test, y_test)\n",
      "fpr_all, tpr_all, _ = roc_curve(y_test, lr_all.predict_proba(x_test))\n",
      "14/579:\n",
      "lr_all = LogisticRegression(solver='lbfgs', C=1)\n",
      "model_all = lr_all.fit(x_train, y_train)\n",
      "lr_all.score(x_test, y_test)\n",
      "fpr_all, tpr_all, _ = roc_curve(y_test, lr_all.predict_proba(x_test)[:,1])\n",
      "14/580:\n",
      "plot.figure()\n",
      "plot.plot(fpr, tpr) # logistic regression, top K + 2-word sequences\n",
      "plot.plot(fpr_all, tpr_all) # logistic regression, all\n",
      "14/581:\n",
      "xs = \"tempstop_bag_of_words_5.csv\"\n",
      "xs_train = pd.read_csv(x, header=None)\n",
      "xs_train.shape\n",
      "14/582:\n",
      "xst = \"tempstop_bag_of_words_0.csv\"\n",
      "xs_test = pd.read_csv(x2, header=None)\n",
      "xs_test.shape\n",
      "14/583:\n",
      "lr_alls = LogisticRegression(solver='lbfgs', C=1)\n",
      "model_alls = lr_all.fit(xs_train, y_train)\n",
      "lr_alls.score(xs_test, y_test)\n",
      "fpr_alls, tpr_alls, _ = roc_curve(y_test, lr_alls.predict_proba(xs_test)[:,1])\n",
      "14/584:\n",
      "lr_alls = LogisticRegression(solver='lbfgs', C=1)\n",
      "model_alls = lr_alls.fit(xs_train, y_train)\n",
      "lr_alls.score(xs_test, y_test)\n",
      "fpr_alls, tpr_alls, _ = roc_curve(y_test, lr_alls.predict_proba(xs_test)[:,1])\n",
      "14/585:\n",
      "plot.figure()\n",
      "plot.plot(fpr, tpr) # logistic regression, top K + 2-word sequences\n",
      "fpr_alls, tpr_alls, _ = roc_curve(y_test, lr_alls.predict_proba(xs_test)[:,1])\n",
      "14/586:\n",
      "plot.figure()\n",
      "plot.plot(fpr, tpr) # logistic regression, top K + 2-word sequences\n",
      "plot.plot(fpr_alls, tpr_alls)\n",
      "14/587:\n",
      "selector = SelectKBest(chi2, k=9000).fit(xs_train, y_train)\n",
      "xs_top = selector.transform(xs_train)\n",
      "lrs_top = LogisticRegression(solver='lbfgs', C=1)\n",
      "models_top = lrs_top.fit(xs_top, y_train)\n",
      "selected = selector.get_support(indices=True)\n",
      "lrs_top.score(xs_test[selected], y_test)\n",
      "fpr_alls, tpr_alls, _ = roc_curve(y_test, lr_alls.predict_proba(xs_test)[:,1])\n",
      "14/588:\n",
      "plot.figure()\n",
      "plot.plot(fpr, tpr) # logistic regression, top K + 2-word sequences\n",
      "plot.plot(fpr_alls, tpr_alls) # logistic regression, top K + all stop words + 2-word sequences\n",
      "14/589:\n",
      "topk = []\n",
      "topktest = []\n",
      "ks = [1000,5000,7500,9000,10000,12500]\n",
      "for i in ks:\n",
      "    selector = SelectKBest(chi2, k=i).fit(x_train, y_train)\n",
      "    x_top = selector.transform(x_train)\n",
      "    lr_top = LogisticRegression(solver='lbfgs', C=1)\n",
      "    model_top = lr_top.fit(x_top, y_train)\n",
      "    topk.append(lr_top.score(x_top, y_train))\n",
      "    selected = selector.get_support(indices=True)\n",
      "    topktest.append(lr_top.score(x_test[selected], y_test))\n",
      "14/590:\n",
      "plot.figure()\n",
      "plot.plot(ks, topk)\n",
      "plot.plot(ks, topktest)\n",
      "print ks[np.argmax(topktest)], np.max(topktest)\n",
      "14/591:\n",
      "topk = []\n",
      "topktest = []\n",
      "ks = [1000,5000,7500,8500,9000,9500,10000,12500]\n",
      "for i in ks:\n",
      "    selector = SelectKBest(chi2, k=i).fit(x_train, y_train)\n",
      "    x_top = selector.transform(x_train)\n",
      "    lr_top = LogisticRegression(solver='lbfgs', C=1)\n",
      "    model_top = lr_top.fit(x_top, y_train)\n",
      "    topk.append(lr_top.score(x_top, y_train))\n",
      "    selected = selector.get_support(indices=True)\n",
      "    topktest.append(lr_top.score(x_test[selected], y_test))\n",
      "14/592:\n",
      "plot.figure()\n",
      "plot.plot(ks, topk)\n",
      "plot.plot(ks, topktest)\n",
      "print ks[np.argmax(topktest)], np.max(topktest)\n",
      "14/593:\n",
      "plot.figure()\n",
      "plot.plot(fpr, tpr, label=\"limited stop words\") # logistic regression, top K + 2-word sequences\n",
      "plot.plot(fpr_alls, tpr_alls, label=\"all stop words\") # logistic regression, top K + all stop words + 2-word sequences\n",
      "14/594:\n",
      "plot.figure()\n",
      "plot.ax(fpr, tpr, label=\"limited stop words\") # logistic regression, top K + 2-word sequences\n",
      "plot.plot(fpr_alls, tpr_alls, label=\"all stop words\") # logistic regression, top K + all stop words + 2-word sequences\n",
      "14/595:\n",
      "plot.figure()\n",
      "plot.plot(fpr, tpr, label=\"limited stop words\") # logistic regression, top K + 2-word sequences\n",
      "plot.plot(fpr_alls, tpr_alls, label=\"all stop words\") # logistic regression, top K + all stop words + 2-word sequences\n",
      "14/596:\n",
      "plot.figure()\n",
      "plot.plot(fpr, tpr, label=\"limited stop words\") # logistic regression, top K + 2-word sequences\n",
      "plot.plot(fpr_alls, tpr_alls, label=\"all stop words\") # logistic regression, top K + all stop words + 2-word sequences\n",
      "plot.legend()\n",
      "14/597:\n",
      "plot.figure()\n",
      "plot.plot(fpr, tpr, label=\"limited stop words\") # logistic regression, top K + 2-word sequences\n",
      "plot.plot(fpr_alls, tpr_alls, label=\"all stop words\") # logistic regression, top K + all stop words + 2-word sequences\n",
      "plot.legend()\n",
      "plot.xlabel(\"FPR\")\n",
      "plot.ylabel(\"TPR\")\n",
      "14/598:\n",
      "plot.figure()\n",
      "plot.plot(fpr, tpr, label=) # logistic regression, top K + 2-word sequences\n",
      "plot.plot(fpr_all, tpr_all) # logistic regression, all + 2-word sequences\n",
      "plot.legend()\n",
      "plot.xlabel(\"FPR\")\n",
      "plot.ylabel(\"TPR\")\n",
      "14/599:\n",
      "plot.figure()\n",
      "plot.plot(fpr, tpr, label=\"top\") # logistic regression, top K + 2-word sequences\n",
      "plot.plot(fpr_all, tpr_all) # logistic regression, all + 2-word sequences\n",
      "plot.legend()\n",
      "plot.xlabel(\"FPR\")\n",
      "plot.ylabel(\"TPR\")\n",
      "14/600:\n",
      "plot.figure()\n",
      "plot.plot(fpr, tpr, label=\"top K features\") # logistic regression, top K + 2-word sequences\n",
      "plot.plot(fpr_all, tpr_all, label=\"all features\") # logistic regression, all + 2-word sequences\n",
      "plot.legend()\n",
      "plot.xlabel(\"FPR\")\n",
      "plot.ylabel(\"TPR\")\n",
      "14/601:\n",
      "plot.figure()\n",
      "plot.plot(fpr, tpr, label=\"logistic regression\") # logistic regression, top K + 2-word sequences\n",
      "plot.plot(fpr_mnb, tpr_mnb, label=\"multinomial naive bayes\") # MNB\n",
      "plot.plot(fpr_svm, tpr_svm, label=\"support vector machine\") # SVM\n",
      "plot.legend()\n",
      "plot.xlabel(\"FPR\")\n",
      "plot.ylabel(\"TPR\")\n",
      "14/602:\n",
      "plot.figure()\n",
      "plot.plot(fpr, tpr, label=\"2-word features (bigrams)\") # logistic regression, top K + 2-word sequences\n",
      "plot.plot(fpr3, tpr3, label=\"1-word\") # logistic regression, top K\n",
      "plot.legend()\n",
      "plot.xlabel(\"FPR\")\n",
      "plot.ylabel(\"TPR\")\n",
      "14/603: lr_top.predict(x_test[selected]).shape\n",
      "14/604: lr_top.predict(x_train[selected]).shape\n",
      "14/605: lr_top.predict(x_train[selected])\n",
      "14/606: lr_top.predict(x_train[selected])[0:25]\n",
      "14/607: lr_top.predict(x_train[selected])[0:402]\n",
      "14/608: lr3.predict(x_train[selected])[0:402]\n",
      "14/609: lr3.predict(x_train)[0:402]\n",
      "14/610:\n",
      "# original preprocessor\n",
      "x = \"train_bag_of_words_5.csv\"\n",
      "x_train_orig = pd.read_csv(x, header=None)\n",
      "x_train_orig.shape\n",
      "14/611: lr3.predict(x_train_orig)[0:402]\n",
      "14/612: lr3 = LogisticRegression(solver='lbfgs', C=1)\n",
      "14/613:\n",
      "selector = SelectKBest(chi2, k=335).fit(x_train_orig, y_train)\n",
      "x_topk = selector.transform(x_train_orig)\n",
      "x_topk.shape\n",
      "14/614: lr3 = LogisticRegression(solver='lbfgs', C=1)\n",
      "14/615: model3 = lr3.fit(x_topk, y_train)\n",
      "14/616: lr3.score(x_topk, y_train)\n",
      "14/617: lr3.predict(x_train_orig)[0:402]\n",
      "14/618:\n",
      "x_train_orig.shape\n",
      "lr3.predict(x_train_orig)[0:402]\n",
      "14/619: x_train_orig.shape\n",
      "14/620: lr3.predict(x_topk)[0:402]\n",
      "14/621: lr3.predict(x_topk)[22]\n",
      "14/622: lr3.predict(x_topk)0:25]\n",
      "14/623: lr3.predict(x_topk)[0:25]\n",
      "14/624: lr3.predict(x_topk)[0:50]\n",
      "14/625: lr3.predict(x_topk)[65]\n",
      "14/626: lr_top.predict(x_test[selected])[65]\n",
      "14/627: x_twitter = pd.read_csv(\"twitter.csv\", header=None)\n",
      "16/1: import networkx as nx\n",
      "16/2:\n",
      "import networkx as nx\n",
      "import csv\n",
      "16/3:\n",
      "with open('title.principals.tsv') as tsvfile:\n",
      "    reader = csv.reader(tsvfile, delimiter='\\t')\n",
      "    for row in reader:\n",
      "        print(row)\n",
      "17/1: G = nx.Graph()\n",
      "17/2:\n",
      "import networkx as nx\n",
      "import csv\n",
      "17/3: G = nx.Graph()\n",
      "17/4:\n",
      "with open('title.principals.tsv') as tsvfile:\n",
      "    reader = csv.reader(tsvfile, delimiter='\\t')\n",
      "    i = 0\n",
      "    for row in reader:\n",
      "        if row[3] == 'actor':\n",
      "            row\n",
      "        if i < 10:\n",
      "            print(row)\n",
      "            i++\n",
      "17/5:\n",
      "with open('title.principals.tsv') as tsvfile:\n",
      "    reader = csv.reader(tsvfile, delimiter='\\t')\n",
      "    i = 0\n",
      "    for row in reader:\n",
      "        if row[3] == 'actor':\n",
      "            row\n",
      "        if i < 10:\n",
      "            print(row)\n",
      "            i = i + 1\n",
      "17/6:\n",
      "with open('title.principals.tsv') as tsvfile:\n",
      "    reader = csv.reader(tsvfile, delimiter='\\t')\n",
      "    i = 0\n",
      "    prev = \"\"\n",
      "    actors = []\n",
      "    for row in reader:\n",
      "        if row[3] == 'actor':\n",
      "            G.add_node(row[2])\n",
      "            actors.append(row[2])\n",
      "        else:\n",
      "            continue\n",
      "        if row[0] != prev:\n",
      "            for a1 in actors:\n",
      "                for a2 in actors:\n",
      "                    G.add_edge(a1, a2)\n",
      "            actors = []\n",
      "        prev = row[0]\n",
      "        if i < 20:\n",
      "            print(row)\n",
      "            i = i + 1\n",
      "        else:\n",
      "            break\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "17/7: list(G.nodes)\n",
      "17/8:\n",
      "with open('title.principals.tsv') as tsvfile:\n",
      "    reader = csv.reader(tsvfile, delimiter='\\t')\n",
      "    i = 0\n",
      "    prev = \"\"\n",
      "    actors = []\n",
      "    for row in reader:\n",
      "        if row[3] == 'actor':\n",
      "            G.add_node(row[2])\n",
      "            actors.append(row[2])\n",
      "        else:\n",
      "            continue\n",
      "        if row[0] != prev:\n",
      "            for a1 in actors:\n",
      "                for a2 in actors:\n",
      "                    G.add_edge(a1, a2)\n",
      "            actors = []\n",
      "        prev = row[0]\n",
      "        if i < 20:\n",
      "            print(row)\n",
      "            i = i + 1\n",
      "        else:\n",
      "            break\n",
      "17/9: list(G.edges)\n",
      "17/10:\n",
      "with open('title.principals.tsv') as tsvfile:\n",
      "    reader = csv.reader(tsvfile, delimiter='\\t')\n",
      "    i = 0\n",
      "    prev = \"\"\n",
      "    actors = []\n",
      "    for row in reader:\n",
      "        if row[3] == 'actor':\n",
      "            G.add_node(row[2])\n",
      "            actors.append(row[2])\n",
      "        else:\n",
      "            continue\n",
      "        if row[0] != prev:\n",
      "            for a1 in actors:\n",
      "                for a2 in actors:\n",
      "                    if a1 == a2:\n",
      "                        continue\n",
      "                    G.add_edge(a1, a2)\n",
      "            actors = []\n",
      "        prev = row[0]\n",
      "        if i < 20:\n",
      "            print(row)\n",
      "            i = i + 1\n",
      "        else:\n",
      "            break\n",
      "17/11: list(G.nodes)\n",
      "17/12: list(G.edges)\n",
      "17/13:\n",
      "with open('title.principals.tsv') as tsvfile:\n",
      "    reader = csv.reader(tsvfile, delimiter='\\t')\n",
      "    i = 0\n",
      "    prev = \"\"\n",
      "    actors = []\n",
      "    for row in reader:\n",
      "        if row[3] == 'actor':\n",
      "            G.add_node(row[2])\n",
      "            actors.append(row[2])\n",
      "        else:\n",
      "            continue\n",
      "        if row[0] != prev:\n",
      "            for a1 in actors:\n",
      "                for a2 in actors:\n",
      "                    if a1 == a2:\n",
      "                        continue\n",
      "                    G.add_edge(a1, a2)\n",
      "            actors = []\n",
      "        prev = row[0]\n",
      "        if i < 20:\n",
      "            print(row)\n",
      "            i = i + 1\n",
      "        else:\n",
      "            break\n",
      "17/14: list(G.nodes)\n",
      "17/15: list(G.edges)\n",
      "17/16:\n",
      "G = nx.Graph()\n",
      "with open('title.principals.tsv') as tsvfile:\n",
      "    reader = csv.reader(tsvfile, delimiter='\\t')\n",
      "    i = 0\n",
      "    prev = \"\"\n",
      "    actors = []\n",
      "    for row in reader:\n",
      "        if row[3] == 'actor':\n",
      "            G.add_node(row[2])\n",
      "            actors.append(row[2])\n",
      "        else:\n",
      "            continue\n",
      "        if row[0] != prev:\n",
      "            for a1 in actors:\n",
      "                for a2 in actors:\n",
      "                    if a1 == a2:\n",
      "                        continue\n",
      "                    G.add_edge(a1, a2)\n",
      "            actors = []\n",
      "        prev = row[0]\n",
      "        if i < 20:\n",
      "            print(row)\n",
      "            i = i + 1\n",
      "        else:\n",
      "            break\n",
      "17/17: list(G.nodes)\n",
      "17/18: list(G.edges)\n",
      "17/19:\n",
      "G = nx.Graph()\n",
      "with open('title.principals.tsv') as tsvfile:\n",
      "    reader = csv.reader(tsvfile, delimiter='\\t')\n",
      "    i = 0\n",
      "    prev = \"\"\n",
      "    actors = []\n",
      "    for row in reader:\n",
      "        if row[3] == 'actor':\n",
      "            G.add_node(row[2])\n",
      "            actors.append(row[2])\n",
      "        else:\n",
      "            continue\n",
      "        if row[0] != prev:\n",
      "            for a1 in actors:\n",
      "                for a2 in actors:\n",
      "                    if a1 == a2:\n",
      "                        continue\n",
      "                    G.add_edge(a1, a2)\n",
      "            actors = []\n",
      "        prev = row[0]\n",
      "        if i < 20:\n",
      "            print(row)\n",
      "            i = i + 1\n",
      "17/20:\n",
      "G = nx.Graph()\n",
      "with open('title.principals.tsv') as tsvfile:\n",
      "    reader = csv.reader(tsvfile, delimiter='\\t')\n",
      "    i = 0\n",
      "    prev = \"\"\n",
      "    actors = []\n",
      "    for row in reader:\n",
      "        if row[3] == 'actor':\n",
      "            G.add_node(row[2])\n",
      "            actors.append(row[2])\n",
      "        else:\n",
      "            continue\n",
      "        if row[0] != prev:\n",
      "            for a1 in actors:\n",
      "                for a2 in actors:\n",
      "                    if a1 == a2:\n",
      "                        continue\n",
      "                    G.add_edge(a1, a2)\n",
      "            actors = []\n",
      "        prev = row[0]\n",
      "        if i < 200:\n",
      "            print(row)\n",
      "            i = i + 1\n",
      "        else:\n",
      "            break\n",
      "17/21: list(G.nodes)\n",
      "17/22: list(G.edges)\n",
      "17/23: G = nx.petersen_graph()\n",
      "17/24:\n",
      "import networkx as nx\n",
      "import csv\n",
      "import matplotlib.pyplot as plot\n",
      "17/25: nx.draw(G, with_labels=True, font_weight='bold')\n",
      "17/26:\n",
      "import networkx as nx\n",
      "import csv\n",
      "import matplotlib.pyplot as plot\n",
      "17/27:\n",
      "G = nx.Graph()\n",
      "with open('title.principals.tsv') as tsvfile:\n",
      "    reader = csv.reader(tsvfile, delimiter='\\t')\n",
      "    i = 0\n",
      "    prev = \"\"\n",
      "    actors = []\n",
      "    for row in reader:\n",
      "        if row[3] == 'actor':\n",
      "            G.add_node(row[2])\n",
      "            actors.append(row[2])\n",
      "        else:\n",
      "            continue\n",
      "        if row[0] != prev:\n",
      "            for a1 in actors:\n",
      "                for a2 in actors:\n",
      "                    if a1 == a2:\n",
      "                        continue\n",
      "                    G.add_edge(a1, a2)\n",
      "            actors = []\n",
      "        prev = row[0]\n",
      "        if i < 200:\n",
      "            print(row)\n",
      "            i = i + 1\n",
      "        else:\n",
      "            break\n",
      "17/28: list(G.nodes)\n",
      "17/29: list(G.edges)\n",
      "17/30: nx.draw(G, with_labels=True, font_weight='bold')\n",
      "17/31: nx.draw(G, with_labels=False, font_weight='bold')\n",
      "17/32:\n",
      "with open('title.basics.tsv') as tsvfile:\n",
      "    reader = csv.reader(tsvfile, delimiter='\\t')\n",
      "    for row in reader:\n",
      "        if i < 200:\n",
      "            print(row)\n",
      "            i = i + 1\n",
      "        else:\n",
      "            break\n",
      "17/33:\n",
      "with open('title.basics.tsv') as tsvfile:\n",
      "    reader = csv.reader(tsvfile, delimiter='\\t')\n",
      "    i = 0\n",
      "    for row in reader:\n",
      "        if i < 200:\n",
      "            print(row)\n",
      "            i = i + 1\n",
      "        else:\n",
      "            break\n",
      "17/34:\n",
      "with open('name.basics.tsv') as tsvfile:\n",
      "    reader = csv.reader(tsvfile, delimiter='\\t')\n",
      "    i = 0\n",
      "    for row in reader:\n",
      "        if i < 200:\n",
      "            print(row)\n",
      "            i = i + 1\n",
      "        else:\n",
      "            break\n",
      "17/35:\n",
      "dict = {}\n",
      "with open('name.basics.tsv') as tsvfile:\n",
      "    reader = csv.reader(tsvfile, delimiter='\\t')\n",
      "    i = 0\n",
      "    for row in reader:\n",
      "        if i < 200:\n",
      "            print(row)\n",
      "            i = i + 1\n",
      "        else:\n",
      "#             break\n",
      "        if i > 1:\n",
      "            dict[row[0]] = row[1]\n",
      "17/36:\n",
      "dict = {}\n",
      "with open('name.basics.tsv') as tsvfile:\n",
      "    reader = csv.reader(tsvfile, delimiter='\\t')\n",
      "    i = 0\n",
      "    for row in reader:\n",
      "        if i < 200:\n",
      "            print(row)\n",
      "            i = i + 1\n",
      "        if i > 1:\n",
      "            dict[row[0]] = row[1]\n",
      "17/37:\n",
      "G = nx.Graph()\n",
      "with open('title.principals.tsv') as tsvfile:\n",
      "    reader = csv.reader(tsvfile, delimiter='\\t')\n",
      "    i = 0\n",
      "    prev = \"\"\n",
      "    actors = []\n",
      "    for row in reader:\n",
      "        if row[3] == 'actor':\n",
      "            G.add_node(dict[row[2]])\n",
      "            actors.append(dict[row[2]])\n",
      "        else:\n",
      "            continue\n",
      "        if row[0] != prev:\n",
      "            for a1 in actors:\n",
      "                for a2 in actors:\n",
      "                    if a1 == a2:\n",
      "                        continue\n",
      "                    G.add_edge(a1, a2)\n",
      "            actors = []\n",
      "        prev = row[0]\n",
      "        if i < 50:\n",
      "            print(row)\n",
      "            i = i + 1\n",
      "        else:\n",
      "            break\n",
      "17/38: list(G.nodes)\n",
      "17/39: list(G.edges)\n",
      "17/40:\n",
      "G = nx.Graph()\n",
      "with open('title.principals.tsv') as tsvfile:\n",
      "    reader = csv.reader(tsvfile, delimiter='\\t')\n",
      "    i = 0\n",
      "    prev = \"\"\n",
      "    actors = []\n",
      "    for row in reader:\n",
      "        if row[3] == 'actor':\n",
      "            G.add_node(dict[row[2]])\n",
      "            actors.append(dict[row[2]])\n",
      "        else:\n",
      "            continue\n",
      "        if row[0] != prev:\n",
      "            for a1 in actors:\n",
      "                for a2 in actors:\n",
      "                    if a1 == a2:\n",
      "                        continue\n",
      "                    G.add_edge(a1, a2)\n",
      "            actors = []\n",
      "        prev = row[0]\n",
      "        if i < 50:\n",
      "            print(row[0], dict[row[2]])\n",
      "            i = i + 1\n",
      "        else:\n",
      "            break\n",
      "17/41:\n",
      "G = nx.Graph()\n",
      "with open('title.principals.tsv') as tsvfile:\n",
      "    reader = csv.reader(tsvfile, delimiter='\\t')\n",
      "    i = 0\n",
      "    prev = \"\"\n",
      "    actors = []\n",
      "    for row in reader:\n",
      "        if row[3] == 'actor':\n",
      "            G.add_node(dict[row[2]])\n",
      "        else:\n",
      "            continue\n",
      "        if row[0] != prev:\n",
      "            for a1 in actors:\n",
      "                for a2 in actors:\n",
      "                    if a1 == a2:\n",
      "                        continue\n",
      "                    G.add_edge(a1, a2)\n",
      "            actors = [dict[row[2]]]\n",
      "        else:\n",
      "            actors.append(dict[row[2]])\n",
      "        prev = row[0]\n",
      "        if i < 50:\n",
      "            print(row[0], dict[row[2]])\n",
      "            i = i + 1\n",
      "        else:\n",
      "            break\n",
      "17/42: list(G.nodes)\n",
      "17/43: list(G.edges)\n",
      "17/44:\n",
      "G = nx.Graph()\n",
      "with open('title.principals.tsv') as tsvfile:\n",
      "    reader = csv.reader(tsvfile, delimiter='\\t')\n",
      "    i = 0\n",
      "    prev = \"\"\n",
      "    actors = []\n",
      "    for row in reader:\n",
      "        if row[3] == 'actor':\n",
      "            G.add_node(dict[row[2]])\n",
      "        else:\n",
      "            continue\n",
      "        if row[0] != prev:\n",
      "            for a1 in actors:\n",
      "                for a2 in actors:\n",
      "                    if a1 == a2:\n",
      "                        continue\n",
      "                    G.add_edge(a1, a2)\n",
      "            actors = [dict[row[2]]]\n",
      "        else:\n",
      "            actors.append(dict[row[2]])\n",
      "        prev = row[0]\n",
      "#         if i < 50:\n",
      "#             print(row[0], dict[row[2]])\n",
      "#             i = i + 1\n",
      "#         else:\n",
      "#             break\n",
      "17/45:\n",
      "dict = {}\n",
      "with open('name.basics.tsv') as tsvfile:\n",
      "    reader = csv.reader(tsvfile, delimiter='\\t')\n",
      "    i = 0\n",
      "    for row in reader:\n",
      "        if i < 200:\n",
      "#             print(row)\n",
      "            i = i + 1\n",
      "        if i > 1:\n",
      "            dict[row[0]] = row[1]\n",
      "17/46:\n",
      "G = nx.Graph()\n",
      "with open('title.principals.tsv') as tsvfile:\n",
      "    reader = csv.reader(tsvfile, delimiter='\\t')\n",
      "    i = 0\n",
      "    prev = \"\"\n",
      "    actors = []\n",
      "    for row in reader:\n",
      "        if row[3] == 'actor':\n",
      "            G.add_node(dict[row[2]])\n",
      "        else:\n",
      "            continue\n",
      "        if row[0] != prev:\n",
      "            for a1 in actors:\n",
      "                for a2 in actors:\n",
      "                    if a1 == a2:\n",
      "                        continue\n",
      "                    G.add_edge(a1, a2)\n",
      "            actors = [dict[row[2]]]\n",
      "        else:\n",
      "            actors.append(dict[row[2]])\n",
      "        prev = row[0]\n",
      "#         if i < 50:\n",
      "#             print(row[0], dict[row[2]])\n",
      "#             i = i + 1\n",
      "#         else:\n",
      "#             break\n",
      "17/47:\n",
      "G = nx.Graph()\n",
      "with open('title.principals.tsv') as tsvfile:\n",
      "    reader = csv.reader(tsvfile, delimiter='\\t')\n",
      "    i = 0\n",
      "    prev = \"\"\n",
      "    actors = []\n",
      "    for row in reader:\n",
      "        if row[3] == 'actor':\n",
      "            G.add_node(dict[row[2]])\n",
      "        else:\n",
      "            continue\n",
      "        if row[0] != prev:\n",
      "            for a1 in actors:\n",
      "                for a2 in actors:\n",
      "                    if a1 == a2:\n",
      "                        continue\n",
      "                    G.add_edge(a1, a2)\n",
      "            actors = [dict[row[2]]]\n",
      "        else:\n",
      "            actors.append(dict[row[2]])\n",
      "        prev = row[0]\n",
      "#         if i < 50:\n",
      "#             print(row[0], dict[row[2]])\n",
      "#             i = i + 1\n",
      "#         else:\n",
      "#             break\n",
      "17/48:\n",
      "dict = {}\n",
      "with open('name.basics.tsv') as tsvfile:\n",
      "    reader = csv.reader(tsvfile, delimiter='\\t')\n",
      "    i = 0\n",
      "    for row in reader:\n",
      "        if i < 200:\n",
      "            print(row)\n",
      "            i = i + 1\n",
      "        if i > 1:\n",
      "            dict[row[0]] = row[1]\n",
      "17/49:\n",
      "G = nx.Graph()\n",
      "with open('title.principals.tsv') as tsvfile:\n",
      "    reader = csv.reader(tsvfile, delimiter='\\t')\n",
      "    i = 0\n",
      "    prev = \"\"\n",
      "    actors = []\n",
      "    for row in reader:\n",
      "        if row[3] == 'actor':\n",
      "            G.add_node(dict[row[2]])\n",
      "        else:\n",
      "            continue\n",
      "        if row[0] != prev:\n",
      "            for a1 in actors:\n",
      "                for a2 in actors:\n",
      "                    if a1 == a2:\n",
      "                        continue\n",
      "                    G.add_edge(a1, a2)\n",
      "            actors = [dict[row[2]]]\n",
      "        else:\n",
      "            actors.append(dict[row[2]])\n",
      "        prev = row[0]\n",
      "#         if i < 50:\n",
      "#             print(row[0], dict[row[2]])\n",
      "#             i = i + 1\n",
      "#         else:\n",
      "#             break\n",
      "17/50:\n",
      "G = nx.Graph()\n",
      "with open('title.principals.tsv') as tsvfile:\n",
      "    reader = csv.reader(tsvfile, delimiter='\\t')\n",
      "    i = 0\n",
      "    prev = \"\"\n",
      "    actors = []\n",
      "    for row in reader:\n",
      "        if row[3] == 'actor' and row[2] in dict:\n",
      "            G.add_node(dict[row[2]])\n",
      "        else:\n",
      "            continue\n",
      "        if row[0] != prev:\n",
      "            for a1 in actors:\n",
      "                for a2 in actors:\n",
      "                    if a1 == a2:\n",
      "                        continue\n",
      "                    G.add_edge(a1, a2)\n",
      "            actors = [dict[row[2]]]\n",
      "        else:\n",
      "            actors.append(dict[row[2]])\n",
      "        prev = row[0]\n",
      "#         if i < 50:\n",
      "#             print(row[0], dict[row[2]])\n",
      "#             i = i + 1\n",
      "#         else:\n",
      "#             break\n",
      "17/51: list(G.nodes)\n",
      "17/52: list(G.edges)\n",
      "17/53: nx.draw(G, with_labels=False, font_weight='bold')\n",
      "17/54: degree_centrality = G.degree_centrality()\n",
      "17/55: degree_centrality = nx.degree_centrality(G)\n",
      "17/56: degree_centrality\n",
      "17/57: sort(degree_centrality)\n",
      "17/58: sorted(degree_centrality)\n",
      "17/59: sorted(degree_centrality, key=lambda (k,v): v)\n",
      "17/60: sorted(degree_centrality, key=degree_centrality.get)\n",
      "17/61: sorted(degree_centrality, key=degree_centrality.get, reverse=True)\n",
      "17/62: eigenvector_centrality = nx.eigenvector_centrality(G)\n",
      "17/63: eigenvector_centrality = nx.eigenvector_centrality(G)\n",
      "17/64: nx.draw(G, with_labels=False, font_weight='bold')\n",
      "17/65: closeness_centrality = nx.closeness_centrality(G)\n",
      "17/66: len(G.nodes)\n",
      "17/67: len(G.edges)\n",
      "17/68:\n",
      "from bs4 import BeautifulSoup\n",
      "\n",
      "with open(\"https://www.boxofficemojo.com/people/?view=Actor&p=.htm\") as fp:\n",
      "    soup = BeautifulSoup(fp)\n",
      "\n",
      "soup = BeautifulSoup(\"<html>data</html>\")\n",
      "17/69:\n",
      "from bs4 import BeautifulSoup\n",
      "import urllib2\n",
      "17/70:\n",
      "page = urllib2.urlopen(\"https://www.boxofficemojo.com/people/?view=Actor&p=.htm\")\n",
      "soup = BeautifulSoup(page, html.parser)\n",
      "17/71:\n",
      "page = urllib2.urlopen(\"https://www.boxofficemojo.com/people/?view=Actor&p=.htm\")\n",
      "soup = BeautifulSoup(page, 'html.parser')\n",
      "17/72: soup.find_all('tr')\n",
      "17/73: soup.find_all('tr')[0]\n",
      "17/74: soup.find_all('tr')[1]\n",
      "17/75: soup.find_all('tr')[2]\n",
      "17/76: soup = soup.find_all('tr')[2]\n",
      "17/77:\n",
      "soup = soup.find_all('tr')[2]\n",
      "soup\n",
      "17/78:\n",
      "soup = soup.find_all('tr')[1]\n",
      "soup\n",
      "17/79:\n",
      "page = urllib2.urlopen(\"https://www.boxofficemojo.com/people/?view=Actor&p=.htm\")\n",
      "soup = BeautifulSoup(page, 'html.parser')\n",
      "17/80:\n",
      "soup = soup.find_all('tr')[1]\n",
      "soup\n",
      "17/81:\n",
      "page = urllib2.urlopen(\"https://www.boxofficemojo.com/people/?view=Actor&p=.htm\")\n",
      "soup = BeautifulSoup(page, 'html.parser')\n",
      "17/82:\n",
      "soup = soup.find_all('tr')[1]\n",
      "soup\n",
      "17/83: soup.find_all('tr')[1]\n",
      "17/84:\n",
      "page = urllib2.urlopen(\"https://www.boxofficemojo.com/people/?view=Actor&p=.htm\")\n",
      "soup = BeautifulSoup(page, 'html.parser')\n",
      "17/85: soup.find_all('tr')[1]\n",
      "17/86: soup.find_all('tr')[2]\n",
      "17/87: soup.find_all('tr')[2].find_all('tr')\n",
      "17/88:\n",
      "page = urllib2.urlopen(\"https://www.boxofficemojo.com/people/?view=Actor&p=.htm\")\n",
      "soup = BeautifulSoup(page, 'html.parser')\n",
      "17/89: actors = soup.find_all('tr')[2].find_all('tr')\n",
      "17/90: actors\n",
      "17/91: len(actors)\n",
      "17/92: actors\n",
      "17/93: actors[1:]\n",
      "17/94: actors = soup.find_all('tr')[2].find_all('tr')[1:]\n",
      "17/95: actors\n",
      "17/96:\n",
      "a1 = actors[0]\n",
      "a1.find_all('b').find_all('b')\n",
      "17/97:\n",
      "a1 = actors[0]\n",
      "a1.find('b').find('b')\n",
      "17/98:\n",
      "a1 = actors[0]\n",
      "a1.find('b').find('b').strip()\n",
      "17/99:\n",
      "a1 = actors[0]\n",
      "a1.find('b').find('b').strip()\n",
      "17/100:\n",
      "a1 = actors[0]\n",
      "a1.find('b').find('b')\n",
      "17/101:\n",
      "a1 = actors[0]\n",
      "a1.find('b').find('b').text\n",
      "17/102: a1.find('td', attrs={'align':'right'})\n",
      "17/103: a1.find('td', attrs={'align':'right'}).text\n",
      "17/104: a1.find('td', attrs={'align':'right'}).text[1:]\n",
      "17/105: a1.find('td', attrs={'align':'right'}).text[1:]+1\n",
      "17/106: a1.find('td', attrs={'align':'right'}).text[1:]\n",
      "17/107: int(a1.find('td', attrs={'align':'right'}).text[1:])\n",
      "17/108: float(a1.find('td', attrs={'align':'right'}).text[1:])\n",
      "17/109:\n",
      "a1 = actors[0]\n",
      "a1.find('b').find('b').text == \"Amy Adams\"\n",
      "17/110:\n",
      "a1 = actors[0]\n",
      "a1.find('b').find('b').text\n",
      "17/111: float(str(a1.find('td', attrs={'align':'right'}).text[1:]))\n",
      "17/112: str(a1.find('td', attrs={'align':'right'}).text[1:])\n",
      "17/113: float(\"2401.3\")\n",
      "17/114: float(str(a1.find('td', attrs={'align':'right'}).text[1:]))\n",
      "17/115: str(a1.find('td', attrs={'align':'right'}).text[1:])\n",
      "17/116: str(a1.find('td', attrs={'align':'right'}).text[1:]).replace(\",\", \"\")\n",
      "17/117: float(str(a1.find('td', attrs={'align':'right'}).text[1:]).replace(\",\", \"\"))\n",
      "17/118:\n",
      "boxofficedict = {}\n",
      "for actor in actors:\n",
      "    name = str(actor.find('b').find('b').text)\n",
      "    boxoffice = float(str(actor.find('td', attrs={'align':'right'}).text[1:]).replace(\",\", \"\"))\n",
      "    boxofficedict[name] = boxoffice\n",
      "17/119:\n",
      "boxofficedict = {}\n",
      "for actor in actors:\n",
      "    name = str(actor.find('b').find('b').text)\n",
      "    boxoffice = str(actor.find('td', attrs={'align':'right'}).text[1:]).replace(\",\", \"\")\n",
      "    if \"k\" in boxoffice:\n",
      "        boxoffice = float(boxoffice.replace(\"k\", \"\"))/1000\n",
      "    boxofficedict[name] = float(boxoffice)\n",
      "17/120: boxofficedict\n",
      "17/121: boxofficedict = {}\n",
      "17/122:\n",
      "def addtodict(actors):\n",
      "    for actor in actors:\n",
      "    name = str(actor.find('b').find('b').text)\n",
      "    boxoffice = str(actor.find('td', attrs={'align':'right'}).text[1:]).replace(\",\", \"\")\n",
      "    if \"k\" in boxoffice:\n",
      "        boxoffice = float(boxoffice.replace(\"k\", \"\"))/1000\n",
      "    boxofficedict[name] = float(boxoffice)\n",
      "17/123:\n",
      "def addtodict(actors):\n",
      "    for actor in actors:\n",
      "        name = str(actor.find('b').find('b').text)\n",
      "        boxoffice = str(actor.find('td', attrs={'align':'right'}).text[1:]).replace(\",\", \"\")\n",
      "        if \"k\" in boxoffice:\n",
      "            boxoffice = float(boxoffice.replace(\"k\", \"\"))/1000\n",
      "        boxofficedict[name] = float(boxoffice)\n",
      "17/124:\n",
      "page2 = urllib2.urlopen(\"https://www.boxofficemojo.com/people/?view=Actor&p=.htm\")\n",
      "soup2 = BeautifulSoup(page, 'html.parser')\n",
      "actors2 = soup.find_all('tr')[2].find_all('tr')[1:]\n",
      "17/125:\n",
      "page2 = urllib2.urlopen(\"https://www.boxofficemojo.com/people/?view=Actor&pagenum=2&sort=person&order=ASC&p=.htm\")\n",
      "soup2 = BeautifulSoup(page, 'html.parser')\n",
      "actors2 = soup.find_all('tr')[2].find_all('tr')[1:]\n",
      "17/126:\n",
      "page2 = urllib2.urlopen(\"https://www.boxofficemojo.com/people/?view=Actor&pagenum=2&sort=person&order=ASC&p=.htm\")\n",
      "soup2 = BeautifulSoup(page, 'html.parser')\n",
      "actors2 = soup.find_all('tr')[2].find_all('tr')[1:]\n",
      "17/127:\n",
      "page3 = urllib2.urlopen(\"https://www.boxofficemojo.com/people/?view=Actor&pagenum=3&sort=person&order=ASC&p=.htm\")\n",
      "soup3 = BeautifulSoup(page, 'html.parser')\n",
      "actors3 = soup.find_all('tr')[2].find_all('tr')[1:]\n",
      "17/128:\n",
      "addtodict(actors)\n",
      "addtodict(actors2)\n",
      "addtodict(actors3)\n",
      "17/129: len(boxofficedict)\n",
      "17/130: boxofficedict = {}\n",
      "17/131:\n",
      "def addtodict(actors):\n",
      "    for actor in actors:\n",
      "        name = str(actor.find('b').find('b').text)\n",
      "        boxoffice = str(actor.find('td', attrs={'align':'right'}).text[1:]).replace(\",\", \"\")\n",
      "        if \"k\" in boxoffice:\n",
      "            boxoffice = float(boxoffice.replace(\"k\", \"\"))/1000\n",
      "        boxofficedict[name] = float(boxoffice)\n",
      "17/132: boxofficedict\n",
      "17/133:\n",
      "page2 = urllib2.urlopen(\"https://www.boxofficemojo.com/people/?view=Actor&pagenum=2&sort=person&order=ASC&p=.htm\")\n",
      "soup2 = BeautifulSoup(page, 'html.parser')\n",
      "actors2 = soup.find_all('tr')[2].find_all('tr')[1:]\n",
      "17/134:\n",
      "page3 = urllib2.urlopen(\"https://www.boxofficemojo.com/people/?view=Actor&pagenum=3&sort=person&order=ASC&p=.htm\")\n",
      "soup3 = BeautifulSoup(page, 'html.parser')\n",
      "actors3 = soup.find_all('tr')[2].find_all('tr')[1:]\n",
      "17/135:\n",
      "addtodict(actors)\n",
      "addtodict(actors2)\n",
      "addtodict(actors3)\n",
      "17/136: len(boxofficedict)\n",
      "17/137: boxofficedict\n",
      "17/138: actors2\n",
      "17/139:\n",
      "page2 = urllib2.urlopen(\"https://www.boxofficemojo.com/people/?view=Actor&pagenum=2&sort=person&order=ASC&p=.htm\")\n",
      "soup2 = BeautifulSoup(page, 'html.parser')\n",
      "actors2 = soup2.find_all('tr')[2].find_all('tr')[1:]\n",
      "17/140:\n",
      "page2 = urllib2.urlopen(\"https://www.boxofficemojo.com/people/?view=Actor&pagenum=2&sort=person&order=ASC&p=.htm\")\n",
      "soup2 = BeautifulSoup(page2, 'html.parser')\n",
      "actors2 = soup2.find_all('tr')[2].find_all('tr')[1:]\n",
      "17/141:\n",
      "page3 = urllib2.urlopen(\"https://www.boxofficemojo.com/people/?view=Actor&pagenum=3&sort=person&order=ASC&p=.htm\")\n",
      "soup3 = BeautifulSoup(page3, 'html.parser')\n",
      "actors3 = soup.find_all('tr')[2].find_all('tr')[1:]\n",
      "17/142:\n",
      "addtodict(actors)\n",
      "addtodict(actors2)\n",
      "addtodict(actors3)\n",
      "17/143: len(boxofficedict)\n",
      "17/144: boxofficedict\n",
      "17/145: actors3\n",
      "17/146:\n",
      "page3 = urllib2.urlopen(\"https://www.boxofficemojo.com/people/?view=Actor&pagenum=3&sort=person&order=ASC&p=.htm\")\n",
      "soup3 = BeautifulSoup(page3, 'html.parser')\n",
      "actors3 = soup3.find_all('tr')[2].find_all('tr')[1:]\n",
      "17/147: len(boxofficedict)\n",
      "17/148:\n",
      "addtodict(actors)\n",
      "addtodict(actors2)\n",
      "addtodict(actors3)\n",
      "17/149: boxofficedict = {}\n",
      "17/150:\n",
      "def addtodict(actors):\n",
      "    for actor in actors:\n",
      "        name = actor.find('b').find('b').text\n",
      "        boxoffice = str(actor.find('td', attrs={'align':'right'}).text[1:]).replace(\",\", \"\")\n",
      "        if \"k\" in boxoffice:\n",
      "            boxoffice = float(boxoffice.replace(\"k\", \"\"))/1000\n",
      "        boxofficedict[name] = float(boxoffice)\n",
      "17/151: boxofficedict\n",
      "17/152:\n",
      "page2 = urllib2.urlopen(\"https://www.boxofficemojo.com/people/?view=Actor&pagenum=2&sort=person&order=ASC&p=.htm\")\n",
      "soup2 = BeautifulSoup(page2, 'html.parser')\n",
      "actors2 = soup2.find_all('tr')[2].find_all('tr')[1:]\n",
      "17/153:\n",
      "page3 = urllib2.urlopen(\"https://www.boxofficemojo.com/people/?view=Actor&pagenum=3&sort=person&order=ASC&p=.htm\")\n",
      "soup3 = BeautifulSoup(page3, 'html.parser')\n",
      "actors3 = soup3.find_all('tr')[2].find_all('tr')[1:]\n",
      "17/154:\n",
      "addtodict(actors)\n",
      "addtodict(actors2)\n",
      "addtodict(actors3)\n",
      "17/155: len(boxofficedict)\n",
      "17/156: boxofficedict\n",
      "17/157:\n",
      "G = nx.Graph()\n",
      "with open('title.principals.tsv') as tsvfile:\n",
      "    reader = csv.reader(tsvfile, delimiter='\\t')\n",
      "    i = 0\n",
      "    prev = \"\"\n",
      "    actors = []\n",
      "    for row in reader:\n",
      "        if row[3] == 'actor' and row[2] in dict and row[2] in boxofficedict:\n",
      "            i = i + 1\n",
      "            G.add_node(dict[row[2]])\n",
      "        else:\n",
      "            continue\n",
      "        if row[0] != prev:\n",
      "            for a1 in actors:\n",
      "                for a2 in actors:\n",
      "                    if a1 == a2:\n",
      "                        continue\n",
      "                    G.add_edge(a1, a2)\n",
      "            actors = [dict[row[2]]]\n",
      "        else:\n",
      "            actors.append(dict[row[2]])\n",
      "        prev = row[0]\n",
      "#         if i < 50:\n",
      "#             print(row[0], dict[row[2]])\n",
      "#             i = i + 1\n",
      "#         else:\n",
      "#             break\n",
      "print i\n",
      "17/158: \"Aaron Eckhart\" in boxofficedict\n",
      "17/159: boxofficedict\n",
      "17/160:\n",
      "G = nx.Graph()\n",
      "with open('title.principals.tsv') as tsvfile:\n",
      "    reader = csv.reader(tsvfile, delimiter='\\t')\n",
      "    i = 0\n",
      "    prev = \"\"\n",
      "    actors = []\n",
      "    for row in reader:\n",
      "        if row[3] == 'actor' and row[2] in dict:\n",
      "            if row[2] in boxofficedict:\n",
      "                print \"asdf\"\n",
      "            i = i + 1\n",
      "            G.add_node(dict[row[2]])\n",
      "        else:\n",
      "            continue\n",
      "        if row[0] != prev:\n",
      "            for a1 in actors:\n",
      "                for a2 in actors:\n",
      "                    if a1 == a2:\n",
      "                        continue\n",
      "                    G.add_edge(a1, a2)\n",
      "            actors = [dict[row[2]]]\n",
      "        else:\n",
      "            actors.append(dict[row[2]])\n",
      "        prev = row[0]\n",
      "print i\n",
      "17/161:\n",
      "G = nx.Graph()\n",
      "with open('title.principals.tsv') as tsvfile:\n",
      "    reader = csv.reader(tsvfile, delimiter='\\t')\n",
      "    i = 0\n",
      "    prev = \"\"\n",
      "    actors = []\n",
      "    for row in reader:\n",
      "        if row[3] == 'actor' and row[2] in dict and dict[row[2]] in boxofficedict:\n",
      "            i = i + 1\n",
      "            G.add_node(dict[row[2]])\n",
      "        else:\n",
      "            continue\n",
      "        if row[0] != prev:\n",
      "            for a1 in actors:\n",
      "                for a2 in actors:\n",
      "                    if a1 == a2:\n",
      "                        continue\n",
      "                    G.add_edge(a1, a2)\n",
      "            actors = [dict[row[2]]]\n",
      "        else:\n",
      "            actors.append(dict[row[2]])\n",
      "        prev = row[0]\n",
      "print i\n",
      "17/162:\n",
      "page0 = urllib2.urlopen(\"https://en.wikipedia.org/wiki/List_of_actors_with_Academy_Award_nominations\")\n",
      "soup0 = BeautifulSoup(page0, 'html.parser')\n",
      "17/163:\n",
      "page1 = urllib2.urlopen(\"https://www.boxofficemojo.com/people/?view=Actor&p=.htm\")\n",
      "soup = BeautifulSoup(page1, 'html.parser')\n",
      "17/164: soup0.find_all('tr')\n",
      "17/165: soup0.find_all('tr').find_all('a')\n",
      "17/166: soup0.find_all('tr')\n",
      "17/167: soup0.find_all('tr')[10:]\n",
      "17/168: soup0.find_all('tr')[11:]\n",
      "17/169: actors0 = soup1.find_all('tr')[2].find_all('tr')[1:]\n",
      "17/170: soup0.find_all('tr')[11:][:12]\n",
      "17/171: soup0.find_all('tr')[11:][:-12]\n",
      "17/172: soup0.find_all('tr')[11:][:-11]\n",
      "17/173: actors0 = soup0.find_all('tr')[11:][:-11]\n",
      "17/174: actors0\n",
      "17/175: actors0[0].find('a')\n",
      "17/176: actors0[0].find('a').text\n",
      "17/177: academydict = {}\n",
      "17/178:\n",
      "academylist = []\n",
      "for actor in actors0:\n",
      "    academylist.append(actor.find('a').text)\n",
      "17/179: academylist\n",
      "17/180:\n",
      "G = nx.Graph()\n",
      "with open('title.principals.tsv') as tsvfile:\n",
      "    reader = csv.reader(tsvfile, delimiter='\\t')\n",
      "    i = 0\n",
      "    prev = \"\"\n",
      "    actors = []\n",
      "    for row in reader:\n",
      "        if row[3] == 'actor' and row[2] in dict and dict[row[2]] in boxofficedict:\n",
      "            i = i + 1\n",
      "            G.add_node(dict[row[2]])\n",
      "        else:\n",
      "            continue\n",
      "        if row[0] != prev:\n",
      "            for a1 in actors:\n",
      "                for a2 in actors:\n",
      "                    if a1 == a2:\n",
      "                        continue\n",
      "                    G.add_edge(a1, a2)\n",
      "            actors = [dict[row[2]]]\n",
      "        else:\n",
      "            actors.append(dict[row[2]])\n",
      "        prev = row[0]\n",
      "print i\n",
      "17/181: list(G.nodes)\n",
      "17/182: len(G.nodes)\n",
      "17/183: list(G.edges)\n",
      "17/184: len(G.edges)\n",
      "17/185: nx.draw(G, with_labels=False, font_weight='bold')\n",
      "17/186: degree_centrality = nx.degree_centrality(G)\n",
      "17/187: sorted(degree_centrality, key=degree_centrality.get, reverse=True)\n",
      "17/188: eigenvector_centrality = nx.eigenvector_centrality(G)\n",
      "17/189: sorted(eigenvector_centrality, key=eigenvector_centrality.get, reverse=True)\n",
      "17/190: closeness_centrality = nx.closeness_centrality(G)\n",
      "17/191: sorted(closeness_centrality, key=closeness_centrality.get, reverse=True)\n",
      "17/192: betweenness_centrality = nx.betweenness_centrality(G)\n",
      "17/193:\n",
      "G = nx.Graph()\n",
      "with open('title.principals.tsv') as tsvfile:\n",
      "    reader = csv.reader(tsvfile, delimiter='\\t')\n",
      "    i = 0\n",
      "    prev = \"\"\n",
      "    actors = []\n",
      "    for row in reader:\n",
      "        if (row[3] == 'actor' or row[3] == 'actress') and row[2] in dict and dict[row[2]] in boxofficedict:\n",
      "            i = i + 1\n",
      "            G.add_node(dict[row[2]])\n",
      "        else:\n",
      "            continue\n",
      "        if row[0] != prev:\n",
      "            for a1 in actors:\n",
      "                for a2 in actors:\n",
      "                    if a1 == a2:\n",
      "                        continue\n",
      "                    G.add_edge(a1, a2)\n",
      "            actors = [dict[row[2]]]\n",
      "        else:\n",
      "            actors.append(dict[row[2]])\n",
      "        prev = row[0]\n",
      "print i\n",
      "17/194:\n",
      "G = nx.Graph()\n",
      "with open('title.principals.tsv') as tsvfile:\n",
      "    reader = csv.reader(tsvfile, delimiter='\\t')\n",
      "    prev = \"\"\n",
      "    actors = []\n",
      "    for row in reader:\n",
      "        if (row[3] == 'actor' or row[3] == 'actress') and row[2] in dict and dict[row[2]] in boxofficedict:\n",
      "            G.add_node(dict[row[2]])\n",
      "        else:\n",
      "            continue\n",
      "        if row[0] != prev:\n",
      "            for a1 in actors:\n",
      "                for a2 in actors:\n",
      "                    if a1 == a2:\n",
      "                        continue\n",
      "                    G.add_edge(a1, a2)\n",
      "            actors = [dict[row[2]]]\n",
      "        else:\n",
      "            actors.append(dict[row[2]])\n",
      "        prev = row[0]\n",
      "17/195: list(G.nodes)\n",
      "17/196: len(G.nodes)\n",
      "17/197: list(G.edges)\n",
      "17/198: len(G.edges)\n",
      "17/199: nx.draw(G, with_labels=False, font_weight='bold')\n",
      "17/200: degree_centrality = nx.degree_centrality(G)\n",
      "17/201: nx.draw(G, with_labels=True, font_weight='bold')\n",
      "17/202: nx.draw(G, with_labels=True)\n",
      "17/203: degree_centrality = nx.degree_centrality(G)\n",
      "17/204: sorted(degree_centrality, key=degree_centrality.get, reverse=True)\n",
      "17/205: eigenvector_centrality = nx.eigenvector_centrality(G)\n",
      "17/206: sorted(eigenvector_centrality, key=eigenvector_centrality.get, reverse=True)\n",
      "17/207: closeness_centrality = nx.closeness_centrality(G)\n",
      "17/208: sorted(closeness_centrality, key=closeness_centrality.get, reverse=True)\n",
      "17/209: betweenness_centrality = nx.betweenness_centrality(G)\n",
      "17/210: sorted(betweenness_centrality, key=betweenness_centrality.get, reverse=True)\n",
      "17/211: plot.draw_networkx(G)\n",
      "17/212: plot.draw(G)\n",
      "17/213:\n",
      "import networkx as nx\n",
      "import csv\n",
      "import matplotlib.pyplot as plot\n",
      "import matplotlib\n",
      "17/214: matplotlib.draw(G)\n",
      "17/215: matplotlib.draw_networkx(G)\n",
      "17/216:\n",
      "import networkx as nx\n",
      "import csv\n",
      "import matplotlib.pyplot as plot\n",
      "17/217:\n",
      "degree_centralities = []\n",
      "actorlist = sorted(G.nodes)\n",
      "for actor in actorlist:\n",
      "    degree_centralities.append(degree_centrality.get(actor))\n",
      "17/218: degree_centralities\n",
      "17/219: actorlist\n",
      "17/220:\n",
      "boxofficegross = []\n",
      "for actor in boxofficedict:\n",
      "    boxofficegross.append(boxofficedict[actor])\n",
      "17/221: boxofficegross\n",
      "17/222: plot.plot(degree_centralities, boxofficegross)\n",
      "17/223:\n",
      "boxofficegross = []\n",
      "for actor in actorlist:\n",
      "    boxofficegross.append(boxofficedict[actor])\n",
      "17/224: boxofficegross\n",
      "17/225: plot.plot(degree_centralities, boxofficegross)\n",
      "17/226: plot.plot(degree_centralities, boxofficegross, \"ro\")\n",
      "17/227: from sklearn.linear_model import LinearRegression\n",
      "17/228: reg = LinearRegression.fit(degree_centralities, boxofficegross)\n",
      "17/229:\n",
      "\n",
      "reg = LinearRegression().fit(degree_centralities, boxofficegross)\n",
      "17/230: reg = LinearRegression().fit(degree_centralities.T, boxofficegross)\n",
      "17/231:\n",
      "from sklearn.linear_model import LinearRegression\n",
      "import numpy as np\n",
      "17/232: reg = LinearRegression().fit(np.array(degree_centralities), np.array(boxofficegross))\n",
      "17/233: reg = LinearRegression().fit(np.array(degree_centralities).reshape(-1,1), np.array(boxofficegross))\n",
      "17/234:\n",
      "x = np.array(degree_centralities).reshape(-1,1)\n",
      "y = np.array(boxofficegross)\n",
      "reg = LinearRegression().fit(x, y)\n",
      "17/235: reg.score(x, y)\n",
      "17/236:\n",
      "G = nx.Graph()\n",
      "Gw = nx.Graph()\n",
      "Gm = nx.Graph()\n",
      "\n",
      "with open('title.principals.tsv') as tsvfile:\n",
      "    reader = csv.reader(tsvfile, delimiter='\\t')\n",
      "    prev = \"\"\n",
      "    actors = []\n",
      "    for row in reader:\n",
      "        if row[3] == 'actor' and row[2] in dict and dict[row[2]] in boxofficedict:\n",
      "            Gm.add_node(dict[row[2]])\n",
      "            G.add_node(dict[row[2]])G.add_node(dict[row[2]])\n",
      "        elif row[3] == 'actress' and row[2] in dict and dict[row[2]] in boxofficedict:\n",
      "            Gw.add_node(dict[row[2]])\n",
      "            G.add_node(dict[row[2]])\n",
      "        else:\n",
      "            continue\n",
      "        if row[0] != prev:\n",
      "            for a1 in actors:\n",
      "                for a2 in actors:\n",
      "                    if a1 == a2:\n",
      "                        continue\n",
      "                    G.add_edge(a1, a2)\n",
      "                    if a1 in Gw.nodes and a2 in Gw.nodes:\n",
      "                        Gw.add_edge(a1, a2)\n",
      "                    elif a1 in Gm.nodes and a2 in Gm.nodes:\n",
      "                        Gm.add_edge(a1, a2)\n",
      "            actors = [dict[row[2]]]\n",
      "        else:\n",
      "            actors.append(dict[row[2]])\n",
      "        prev = row[0]\n",
      "17/237:\n",
      "G = nx.Graph()\n",
      "Gw = nx.Graph()\n",
      "Gm = nx.Graph()\n",
      "\n",
      "with open('title.principals.tsv') as tsvfile:\n",
      "    reader = csv.reader(tsvfile, delimiter='\\t')\n",
      "    prev = \"\"\n",
      "    actors = []\n",
      "    for row in reader:\n",
      "        if row[3] == 'actor' and row[2] in dict and dict[row[2]] in boxofficedict:\n",
      "            Gm.add_node(dict[row[2]])\n",
      "            G.add_node(dict[row[2]])\n",
      "        elif row[3] == 'actress' and row[2] in dict and dict[row[2]] in boxofficedict:\n",
      "            Gw.add_node(dict[row[2]])\n",
      "            G.add_node(dict[row[2]])\n",
      "        else:\n",
      "            continue\n",
      "        if row[0] != prev:\n",
      "            for a1 in actors:\n",
      "                for a2 in actors:\n",
      "                    if a1 == a2:\n",
      "                        continue\n",
      "                    G.add_edge(a1, a2)\n",
      "                    if a1 in Gw.nodes and a2 in Gw.nodes:\n",
      "                        Gw.add_edge(a1, a2)\n",
      "                    elif a1 in Gm.nodes and a2 in Gm.nodes:\n",
      "                        Gm.add_edge(a1, a2)\n",
      "            actors = [dict[row[2]]]\n",
      "        else:\n",
      "            actors.append(dict[row[2]])\n",
      "        prev = row[0]\n",
      "17/238: list(Gw.nodes)\n",
      "17/239: len(Gw.nodes)\n",
      "17/240: nx.draw(Gw)\n",
      "17/241: nx.draw(Gm)\n",
      "17/242: actorw = sorted(Gw.nodes)\n",
      "17/243:\n",
      "actorw = sorted(Gw.nodes)\n",
      "actorm = sorted(Gm.nodes)\n",
      "17/244:\n",
      "actorw = sorted(Gw.nodes)\n",
      "actorm = sorted(Gm.nodes)\n",
      "17/245:\n",
      "degree_centralities = []\n",
      "actorlist = [a for a in actorw, a for a in actorm]\n",
      "for actor in actorlist:\n",
      "    degree_centralities.append(degree_centrality.get(actor))\n",
      "17/246:\n",
      "degree_centralities = []\n",
      "actorlist = [a for a in actorw]\n",
      "for actor in actorlist:\n",
      "    degree_centralities.append(degree_centrality.get(actor))\n",
      "17/247:\n",
      "degree_centralities = []\n",
      "actorlist = [a for a in actorw]\n",
      "actorlist.append(a for a in actorm)\n",
      "for actor in actorlist:\n",
      "    degree_centralities.append(degree_centrality.get(actor))\n",
      "17/248: actorlist\n",
      "17/249:\n",
      "degree_centralities = []\n",
      "actorlist = [a for a in actorw]\n",
      "for a in actorm:\n",
      "    actorlist.append(a)\n",
      "for actor in actorlist:\n",
      "    degree_centralities.append(degree_centrality.get(actor))\n",
      "17/250: actorlist\n",
      "17/251: len(actorw)\n",
      "17/252:\n",
      "degree_centralities = []\n",
      "actorlist = [a for a in actorw]\n",
      "for a in actorm:\n",
      "    actorlist.append(a)\n",
      "for actor in actorlist:\n",
      "    degree_centralities.append(degree_centrality.get(actor))\n",
      "17/253: len(actorw)\n",
      "17/254: actorlist\n",
      "17/255: degree_centralities\n",
      "17/256:\n",
      "boxofficegross = []\n",
      "for actor in actorlist:\n",
      "    boxofficegross.append(boxofficedict[actor])\n",
      "17/257: boxofficegross\n",
      "17/258: plot.plot(degree_centralities, boxofficegross, \"ro\")\n",
      "17/259:\n",
      "x = np.array(degree_centralities).reshape(-1,1)\n",
      "y = np.array(boxofficegross)\n",
      "reg = LinearRegression().fit(x, y)\n",
      "17/260: plot.plot(degree_centralities[:313], boxofficegross[:313], \"ro\")\n",
      "17/261: plot.plot(degree_centralities[313:], boxofficegross[313:], \"ro\")\n",
      "17/262:\n",
      "xw = np.array(degree_centralities[:313]).reshape(-1,1)\n",
      "yw = np.array(boxofficegross[:313])\n",
      "regw = LinearRegression().fit(xw, yw)\n",
      "17/263:\n",
      "xw = np.array(degree_centralities[:313]).reshape(-1,1)\n",
      "yw = np.array(boxofficegross[:313])\n",
      "regw = LinearRegression().fit(xw, yw)\n",
      "regw.score(xw, yw)\n",
      "17/264:\n",
      "xm = np.array(degree_centralities[313:]).reshape(-1,1)\n",
      "ym = np.array(boxofficegross[313:])\n",
      "regm = LinearRegression().fit(xm, ym)\n",
      "regm.score(xm, ym)\n",
      "17/265:\n",
      "academynominated = []\n",
      "for a in actorlist:\n",
      "    if a in academylist:\n",
      "        academynominated.append(1)\n",
      "    else:\n",
      "        academynominated.append(0)\n",
      "17/266:\n",
      "from sklearn.linear_model import LinearRegression, LogisticRegression\n",
      "import numpy as np\n",
      "17/267: log = LogisticRegression().fit(x, academynominated)\n",
      "17/268: log = LogisticRegression(solver='lbfgs').fit(x, academynominated)\n",
      "17/269: log.score(x, academynominated)\n",
      "18/1: x_twitter = pd.read_csv(\"twitter.csv\", header=0)\n",
      "18/2:\n",
      "import sklearn\n",
      "from sklearn.naive_bayes import MultinomialNB, ComplementNB\n",
      "import pandas as pd\n",
      "import numpy as np\n",
      "18/3: x_twitter = pd.read_csv(\"twitter.csv\", header=0)\n",
      "18/4: x_twitter = pd.read_csv(\"twitter.csv\", header=0)\n",
      "18/5: len(x_twitter)\n",
      "18/6: x_twitter.shape\n",
      "18/7: x_twitter[0]\n",
      "18/8: x_twitter\n",
      "18/9: x_twitter = pd.read_csv(\"twitter.csv\", header=0, skiprows[0,2,4,5,6])\n",
      "18/10: x_twitter = pd.read_csv(\"twitter.csv\", header=0, skiprows=[0,2,4,5,6])\n",
      "18/11: x_twitter = pd.read_csv(\"twitter.csv\", header=0, skiprows=[0,2,4,5,6], low_memory=False)\n",
      "18/12: x_twitter.shape\n",
      "18/13: x_twitter\n",
      "18/14: x_twitter = pd.read_csv(\"twitter.csv\", header=0, skipcols=[0,2,4,5,6], low_memory=False)\n",
      "18/15: x_twitter = pd.read_csv(\"twitter.csv\", header=0, usecols=[1,3], low_memory=False)\n",
      "18/16: x_twitter.shape\n",
      "18/17: x_twitter\n",
      "18/18:\n",
      "for row in x_twitter:\n",
      "    print row\n",
      "18/19:\n",
      "for row in x_twitter:\n",
      "    print row[0]\n",
      "18/20:\n",
      "for row in list(x_twitter):\n",
      "    print row[0]\n",
      "18/21:\n",
      "for i in range(len(x_twitter)):\n",
      "    print x_twitter.ix(i,0)\n",
      "18/22:\n",
      "for i in range(len(x_twitter)):\n",
      "    print x_twitter.ix[i,0]\n",
      "18/23:\n",
      "x_twitter = x_twitter.iterrows()\n",
      "for i in range(len(x_twitter)):\n",
      "    print x_twitter[i]\n",
      "18/24:\n",
      "x_twitter = x_twitter.itertuples()\n",
      "for row in x_twitter:\n",
      "    print row\n",
      "18/25:\n",
      "for row in x_twitter.itertuples():\n",
      "    print row\n",
      "18/26: list(x_twitter)\n",
      "18/27:\n",
      "for i in range(x_twitter.shape[0]):\n",
      "    print x_twitter[i]\n",
      "18/28: x_twitter = pd.read_csv(\"twitter.csv\", header=0, usecols=[1,3], low_memory=False)\n",
      "18/29:\n",
      "for i in range(x_twitter.shape[0]):\n",
      "    print x_twitter[i]\n",
      "18/30:\n",
      "for row in x_twitter.itertuples():\n",
      "    print row\n",
      "18/31:\n",
      "k = 0\n",
      "for row in x_twitter.itertuples():\n",
      "    k = k + 1\n",
      "    if k > 10:\n",
      "        break\n",
      "    print row\n",
      "18/32:\n",
      "k = 0\n",
      "for row in x_twitter.itertuples():\n",
      "    k = k + 1\n",
      "    if k > 10:\n",
      "        break\n",
      "    print row[0]\n",
      "18/33:\n",
      "k = 0\n",
      "for row in x_twitter.itertuples():\n",
      "    k = k + 1\n",
      "    if k > 10:\n",
      "        break\n",
      "    print row[1]\n",
      "18/34:\n",
      "k = 0\n",
      "for row in x_twitter.itertuples():\n",
      "    k = k + 1\n",
      "    if k > 10:\n",
      "        break\n",
      "    print row[0] + \"\\t\" + row[2] + \"\\t\" + row[1]\n",
      "18/35:\n",
      "k = 0\n",
      "for row in x_twitter.itertuples():\n",
      "    k = k + 1\n",
      "    if k > 10:\n",
      "        break\n",
      "    print str(row[0]) + \"\\t\" + row[2] + \"\\t\" + str(row[1])\n",
      "17/270:\n",
      "x_train = x[0:213, 313:734]\n",
      "x_train\n",
      "17/271:\n",
      "x_train = x[0:213] + x[313:734]\n",
      "x_train\n",
      "17/272:\n",
      "x_train = list(x[0:213]) + list(x[313:734])\n",
      "x_train\n",
      "17/273:\n",
      "x_train = list(x)\n",
      "# x_train = list(x[0:213]) + list(x[313:734])\n",
      "x_train\n",
      "17/274:\n",
      "x_train = list(x)\n",
      "# x_train = list(x[0:213]) + list(x[313:734])\n",
      "x_train\n",
      "17/275:\n",
      "x_train = x[:,0]\n",
      "# x_train = list(x[0:213]) + list(x[313:734])\n",
      "x_train\n",
      "17/276:\n",
      "x_train = x[:,0]\n",
      "x_train = x_train[0:213] + x_train[313:734]\n",
      "x_train\n",
      "17/277:\n",
      "x_train = x[:,0]\n",
      "x_train = np.concatenate(x_train[0:213], x_train[313:734])\n",
      "17/278: x_train = np.concatenate(x[0:213], x[313:734])\n",
      "17/279: x_train = np.concatenate(x[0:213], x[313:734])\n",
      "17/280: x_train = np.concatenate((x[0:213], x[313:734]))\n",
      "17/281: x_train.shape\n",
      "17/282:\n",
      "x_train = np.concatenate((x[0:213], x[313:734]))\n",
      "x_test = np.concatenate((x[213:313], x[734:]))\n",
      "17/283:\n",
      "lr = LogisticRegression(solver='lbfgs').fit(x_train, academynominated)\n",
      "lr.score(x_train, academynominated)\n",
      "17/284:\n",
      "x_train = np.concatenate((x[0:213], x[313:734]))\n",
      "x_test = np.concatenate((x[213:313], x[734:]))\n",
      "y_train = np.concatenate((academynominated[0:213], academynominated[313:734]))\n",
      "y_test = np.concatenate((academynominated[213:313], academynominated[734:]))\n",
      "17/285:\n",
      "lr = LogisticRegression(solver='lbfgs').fit(x_train, academynominated)\n",
      "lr.score(x_train, academynominated)\n",
      "17/286:\n",
      "lr = LogisticRegression(solver='lbfgs').fit(x_train, y_train)\n",
      "lr.score(x_train, y_train)\n",
      "17/287: lr.score(x_test, y_test)\n",
      "17/288:\n",
      "lr = LogisticRegression(solver='lbfgs', C=1).fit(x_train, y_train)\n",
      "lr.score(x_train, y_train)\n",
      "17/289: lr.score(x_test, y_test)\n",
      "17/290:\n",
      "lr = LogisticRegression(solver='lbfgs', C=.5).fit(x_train, y_train)\n",
      "lr.score(x_train, y_train)\n",
      "17/291: lr.score(x_test, y_test)\n",
      "17/292:\n",
      "lr = LogisticRegression(solver='lbfgs', C=.1).fit(x_train, y_train)\n",
      "lr.score(x_train, y_train)\n",
      "17/293: lr.score(x_test, y_test)\n",
      "17/294:\n",
      "lr = LogisticRegression(solver='lbfgs', C=.01).fit(x_train, y_train)\n",
      "lr.score(x_train, y_train)\n",
      "17/295: lr.score(x_test, y_test)\n",
      "17/296: nx.write_gexf(G, \"graph.gexf\")\n",
      "17/297:\n",
      "from sklearn.linear_model import LinearRegression, LogisticRegression\n",
      "import numpy as np\n",
      "from scipy import stats\n",
      "17/298: slope, intercept, r_value, p_value, std_err = stats.linregress(x,y)\n",
      "17/299:\n",
      "lr = LogisticRegression(solver='lbfgs).fit(x_train, y_train)\n",
      "lr.score(x_train, y_train)\n",
      "17/300:\n",
      "lr = LogisticRegression(solver='lbfgs').fit(x_train, y_train)\n",
      "lr.score(x_train, y_train)\n",
      "17/301: lr.score(x_test, y_test)\n",
      "17/302:\n",
      "x.shape\n",
      "y.shape\n",
      "17/303: x.shape, y.shape\n",
      "17/304: y.reshape(-1,1)\n",
      "17/305:\n",
      "y.reshape(-1,1)\n",
      "y.shape\n",
      "17/306: y.reshape(-1,1).shape\n",
      "17/307: slope, intercept, r_value, p_value, std_err = stats.linregress(x,y.reshape(-1,1))\n",
      "17/308: slope, intercept, r_value, p_value, std_err = stats.linregress(np.array(x),y)\n",
      "17/309: slope, intercept, r_value, p_value, std_err = stats.linregress(np.array(degree_centralities),y)\n",
      "17/310: slope, intercept, r_value, p_value, std_err\n",
      "17/311:\n",
      "x = np.array(degree_centralities).reshape(-1,1)\n",
      "y = np.array(boxofficegross)\n",
      "slope, intercept, r_value, p_value, std_err = stats.linregress(np.array(degree_centralities),y)\n",
      "17/312: slope, intercept, r_value, p_value, std_err\n",
      "17/313:\n",
      "x = np.array(degree_centralities)\n",
      "y = np.array(boxofficegross)\n",
      "slope, intercept, r_value, p_value, std_err = stats.linregress(x,y)\n",
      "17/314: slope, intercept, r_value, p_value, std_err\n",
      "17/315:\n",
      "xw = np.array(degree_centralities[:313])\n",
      "yw = np.array(boxofficegross[:313])\n",
      "slope, intercept, r_value, p_value, std_err = stats.linregress(xw,yw)\n",
      "17/316:\n",
      "xw = np.array(degree_centralities[:313])\n",
      "yw = np.array(boxofficegross[:313])\n",
      "slope, intercept, r_value, p_value, std_err = stats.linregress(xw,yw)\n",
      "slope, intercept, r_value, p_value, std_err\n",
      "17/317:\n",
      "x = np.array(degree_centralities)\n",
      "y = np.array(boxofficegross)\n",
      "slope, intercept, r_value, p_value, std_err = stats.linregress(x,y)\n",
      "slope, intercept, r_value, p_value, std_err\n",
      "17/318:\n",
      "xw = np.array(degree_centralities[:313])\n",
      "yw = np.array(boxofficegross[:313])\n",
      "slope, intercept, r_value, p_value, std_err = stats.linregress(xw,yw)\n",
      "slope, intercept, r_value, p_value, std_err\n",
      "17/319:\n",
      "xm = np.array(degree_centralities[313:])\n",
      "ym = np.array(boxofficegross[313:])\n",
      "slope, intercept, r_value, p_value, std_err = stats.linregress(xm,ym)\n",
      "slope, intercept, r_value, p_value, std_err\n",
      "17/320:  0.614319472312018* 0.614319472312018\n",
      "17/321:  0.6511147814286344* 0.6511147814286344\n",
      "17/322:\n",
      "plot.figure()\n",
      "plot.plot(x, y)\n",
      "17/323:\n",
      "plot.figure()\n",
      "plot.plot(x, y, \"ro\")\n",
      "17/324:\n",
      "plot.figure()\n",
      "plot.plot(x, y, \"o\")\n",
      "yfit = [a + b * xi for xi in X]\n",
      "plt.plot(X, yfit)\n",
      "17/325:\n",
      "plot.figure()\n",
      "plot.plot(x, y, \"o\")\n",
      "yfit = [intercept + slope * xi for xi in x]\n",
      "plt.plot(x, yfit)\n",
      "17/326:\n",
      "plot.figure()\n",
      "plot.plot(x, y, \"o\")\n",
      "yfit = [intercept + slope * xi for xi in x]\n",
      "plot.plot(x, yfit)\n",
      "17/327:\n",
      "x = np.array(degree_centralities)\n",
      "y = np.array(boxofficegross)\n",
      "slope, intercept, r_value, p_value, std_err = stats.linregress(x,y)\n",
      "slope, intercept, r_value, p_value, std_err\n",
      "17/328:\n",
      "plot.figure()\n",
      "plot.plot(x, y, \"o\")\n",
      "yfit = [intercept + slope * xi for xi in x]\n",
      "plot.plot(x, yfit, \"r\")\n",
      "17/329:\n",
      "plot.figure()\n",
      "plot.plot(x, y, \"o\")\n",
      "yfit = [intercept + slope * xi for xi in x]\n",
      "plot.plot(x, yfit, \"blue\")\n",
      "17/330:\n",
      "plot.figure()\n",
      "plot.plot(x, y, \"o\")\n",
      "yfit = [intercept + slope * xi for xi in x]\n",
      "plot.plot(x, yfit, \"r\")\n",
      "17/331: plot.plot(x_train, y_train)\n",
      "17/332: plot.plot(x_train, y_train, \"o\")\n",
      "17/333:\n",
      "x = np.array(eigenvector_centralities)\n",
      "y = np.array(boxofficegross)\n",
      "slope, intercept, r_value, p_value, std_err = stats.linregress(x,y)\n",
      "slope, intercept, r_value, p_value, std_err\n",
      "17/334:\n",
      "eigenvector_centralities = []\n",
      "for actor in actorlist:\n",
      "    eigenvector_centralities.append(eigenvector_centrality.get(actor))\n",
      "17/335:\n",
      "x_e = np.array(eigenvector_centralities)\n",
      "y = np.array(boxofficegross)\n",
      "slope, intercept, r_value, p_value, std_err = stats.linregress(x_e,y)\n",
      "slope, intercept, r_value, p_value, std_err\n",
      "17/336:\n",
      "betweenness_centralities = []\n",
      "for actor in actorlist:\n",
      "    betweenness_centralities.append(betweenness_centrality.get(actor))\n",
      "x_b = np.array(betweenness_centralities)\n",
      "y = np.array(boxofficegross)\n",
      "slope, intercept, r_value, p_value, std_err = stats.linregress(x_b,y)\n",
      "slope, intercept, r_value, p_value, std_err\n",
      "17/337:\n",
      "closeness_centralities = []\n",
      "for actor in actorlist:\n",
      "    closeness_centralities.append(closeness_centrality.get(actor))\n",
      "x_c = np.array(closeness_centralities)\n",
      "y = np.array(boxofficegross)\n",
      "slope, intercept, r_value, p_value, std_err = stats.linregress(x_c,y)\n",
      "slope, intercept, r_value, p_value, std_err\n",
      "17/338:\n",
      "log = LogisticRegression(solver='lbfgs').fit(x_e, academynominated)\n",
      "log.score(x_e, academynominated)\n",
      "17/339:\n",
      "\n",
      "log = LogisticRegression(solver='lbfgs').fit(x_e.reshape(-1,1), academynominated)\n",
      "log.score(x_e, academynominated)\n",
      "17/340:\n",
      "\n",
      "log = LogisticRegression(solver='lbfgs').fit(x_e.reshape(-1,1), academynominated)\n",
      "log.score(x_e.reshape(-1,1), academynominated)\n",
      "17/341:\n",
      "log = LogisticRegression(solver='lbfgs').fit(x_e.reshape(-1,1), academynominated)\n",
      "log.score(x_e.reshape(-1,1), academynominated)\n",
      "17/342:\n",
      "log = LogisticRegression(solver='lbfgs').fit(x_b.reshape(-1,1), academynominated)\n",
      "log.score(x_b.reshape(-1,1), academynominated)\n",
      "17/343:\n",
      "log = LogisticRegression(solver='lbfgs').fit(x_c.reshape(-1,1), academynominated)\n",
      "log.score(x_c.reshape(-1,1), academynominated)\n",
      "17/344:\n",
      "G = nx.Graph()\n",
      "Gw = nx.Graph()\n",
      "Gm = nx.Graph()\n",
      "moviecountdict = {}\n",
      "\n",
      "with open('title.principals.tsv') as tsvfile:\n",
      "    reader = csv.reader(tsvfile, delimiter='\\t')\n",
      "    prev = \"\"\n",
      "    actors = []\n",
      "    for row in reader:\n",
      "        if row[3] == 'actor' and row[2] in dict and dict[row[2]] in boxofficedict:\n",
      "            Gm.add_node(dict[row[2]])\n",
      "            G.add_node(dict[row[2]])\n",
      "            moviecountdict[dict[row[2]]] += 1 if dict[row[2]] in moviecountdict else moviecountdict[dict[row[2]]] = 1\n",
      "        elif row[3] == 'actress' and row[2] in dict and dict[row[2]] in boxofficedict:\n",
      "            Gw.add_node(dict[row[2]])\n",
      "            G.add_node(dict[row[2]])\n",
      "            moviecountdict[dict[row[2]]] += 1 if dict[row[2]] in moviecountdict else moviecountdict[dict[row[2]]] = 1\n",
      "        else:\n",
      "            continue\n",
      "        if row[0] != prev:\n",
      "            for a1 in actors:\n",
      "                for a2 in actors:\n",
      "                    if a1 == a2:\n",
      "                        continue\n",
      "                    G.add_edge(a1, a2)\n",
      "                    if a1 in Gw.nodes and a2 in Gw.nodes:\n",
      "                        Gw.add_edge(a1, a2)\n",
      "                    elif a1 in Gm.nodes and a2 in Gm.nodes:\n",
      "                        Gm.add_edge(a1, a2)\n",
      "            actors = [dict[row[2]]]\n",
      "        else:\n",
      "            actors.append(dict[row[2]])\n",
      "        prev = row[0]\n",
      "17/345:\n",
      "G = nx.Graph()\n",
      "Gw = nx.Graph()\n",
      "Gm = nx.Graph()\n",
      "moviecountdict = {}\n",
      "\n",
      "with open('title.principals.tsv') as tsvfile:\n",
      "    reader = csv.reader(tsvfile, delimiter='\\t')\n",
      "    prev = \"\"\n",
      "    actors = []\n",
      "    for row in reader:\n",
      "        if row[3] == 'actor' and row[2] in dict and dict[row[2]] in boxofficedict:\n",
      "            Gm.add_node(dict[row[2]])\n",
      "            G.add_node(dict[row[2]])\n",
      "            if dict[row[2]] in moviecountdict:\n",
      "                moviecountdict[dict[row[2]]] += 1\n",
      "            else:\n",
      "                moviecountdict[dict[row[2]]] = 1\n",
      "        elif row[3] == 'actress' and row[2] in dict and dict[row[2]] in boxofficedict:\n",
      "            Gw.add_node(dict[row[2]])\n",
      "            G.add_node(dict[row[2]])\n",
      "            if dict[row[2]] in moviecountdict:\n",
      "                moviecountdict[dict[row[2]]] += 1\n",
      "            else:\n",
      "                moviecountdict[dict[row[2]]] = 1\n",
      "            continue\n",
      "        if row[0] != prev:\n",
      "            for a1 in actors:\n",
      "                for a2 in actors:\n",
      "                    if a1 == a2:\n",
      "                        continue\n",
      "                    G.add_edge(a1, a2)\n",
      "                    if a1 in Gw.nodes and a2 in Gw.nodes:\n",
      "                        Gw.add_edge(a1, a2)\n",
      "                    elif a1 in Gm.nodes and a2 in Gm.nodes:\n",
      "                        Gm.add_edge(a1, a2)\n",
      "            actors = [dict[row[2]]]\n",
      "        else:\n",
      "            actors.append(dict[row[2]])\n",
      "        prev = row[0]\n",
      "17/346:\n",
      "G = nx.Graph()\n",
      "Gw = nx.Graph()\n",
      "Gm = nx.Graph()\n",
      "moviecountdict = {}\n",
      "\n",
      "with open('title.principals.tsv') as tsvfile:\n",
      "    reader = csv.reader(tsvfile, delimiter='\\t')\n",
      "    prev = \"\"\n",
      "    actors = []\n",
      "    for row in reader:\n",
      "        if row[3] == 'actor' and row[2] in dict and dict[row[2]] in boxofficedict:\n",
      "            Gm.add_node(dict[row[2]])\n",
      "            G.add_node(dict[row[2]])\n",
      "            if dict[row[2]] in moviecountdict:\n",
      "                moviecountdict[dict[row[2]]] += 1\n",
      "            else:\n",
      "                moviecountdict[dict[row[2]]] = 1\n",
      "        elif row[3] == 'actress' and row[2] in dict and dict[row[2]] in boxofficedict:\n",
      "            Gw.add_node(dict[row[2]])\n",
      "            G.add_node(dict[row[2]])\n",
      "            if dict[row[2]] in moviecountdict:\n",
      "                moviecountdict[dict[row[2]]] += 1\n",
      "            else:\n",
      "                moviecountdict[dict[row[2]]] = 1\n",
      "        else:\n",
      "            continue\n",
      "        if row[0] != prev:\n",
      "            for a1 in actors:\n",
      "                for a2 in actors:\n",
      "                    if a1 == a2:\n",
      "                        continue\n",
      "                    G.add_edge(a1, a2)\n",
      "                    if a1 in Gw.nodes and a2 in Gw.nodes:\n",
      "                        Gw.add_edge(a1, a2)\n",
      "                    elif a1 in Gm.nodes and a2 in Gm.nodes:\n",
      "                        Gm.add_edge(a1, a2)\n",
      "            actors = [dict[row[2]]]\n",
      "        else:\n",
      "            actors.append(dict[row[2]])\n",
      "        prev = row[0]\n",
      "17/347: moviecountdict\n",
      "17/348:\n",
      "moviecounts = []\n",
      "for actor in actorlist:\n",
      "    moviecounts.append(moviecountdict.get(actor))\n",
      "17/349:\n",
      "xc = np.array(moviecounts)\n",
      "yc = np.array(boxofficegross)\n",
      "slope, intercept, r_value, p_value, std_err = stats.linregress(xc,yc)\n",
      "slope, intercept, r_value, p_value, std_err\n",
      "17/350: import statsmodels.api as sm\n",
      "17/351:\n",
      "df = pd.DataFrame(\n",
      "    {\n",
      "        'degree_centralities': degree_centralities,\n",
      "        'moviecount': moviecounts\n",
      "    })\n",
      "17/352:\n",
      "import statsmodels.api as sm\n",
      "import pandas as pd\n",
      "17/353:\n",
      "df = pd.DataFrame(\n",
      "    {\n",
      "        'degree_centralities': degree_centralities,\n",
      "        'moviecount': moviecounts\n",
      "    })\n",
      "17/354: X = df['degree_centralities', 'moviecount']\n",
      "17/355: X = df[['degree_centralities', 'moviecount']]\n",
      "17/356:\n",
      "X = sm.add_constant(X) # adding a constant\n",
      "Y = boxofficegross\n",
      "model = sm.OLS(Y, X).fit()\n",
      "predictions = model.predict(X) \n",
      " \n",
      "print_model = model.summary()\n",
      "print(print_model)\n",
      "17/357:\n",
      "df = pd.DataFrame(\n",
      "    {\n",
      "        'degree_centralities': degree_centralities,\n",
      "        'eigenvector_centralities': eigenvector_centralities,\n",
      "        'betweenness_centralities': eigenvector_centralities,\n",
      "        'closeness_centralities': eigenvector_centralities,\n",
      "        'moviecount': moviecounts,\n",
      "        'boxofficegross': boxofficegross,\n",
      "        'academynominated': academynominated\n",
      "    })\n",
      "17/358:\n",
      "X1 = sm.add_constant(X1) # adding a constant\n",
      "Y1 = academynominated\n",
      "model = sm.OLS(Y1, X1).fit()\n",
      "predictions = model.predict(X1) \n",
      " \n",
      "print_model = model.summary()\n",
      "print(print_model)\n",
      "17/359:\n",
      "X1 = sm.add_constant(X) # adding a constant\n",
      "Y1 = academynominated\n",
      "model = sm.OLS(Y1, X1).fit()\n",
      "predictions = model.predict(X1) \n",
      " \n",
      "print_model = model.summary()\n",
      "print(print_model)\n",
      "17/360:\n",
      "X = sm.add_constant(X) # adding a constant\n",
      "Y = boxofficegross\n",
      "model = sm.OLS(Y, X).fit()\n",
      "predictions = model.predict(X) \n",
      " \n",
      "print_model = model.summary()\n",
      "print(print_model)\n",
      "17/361: X = df[['degree_centralities', 'eigenvector_centralities', 'moviecount']]\n",
      "17/362:\n",
      "X = sm.add_constant(X) # adding a constant\n",
      "Y = boxofficegross\n",
      "model = sm.OLS(Y, X).fit()\n",
      "predictions = model.predict(X) \n",
      " \n",
      "print_model = model.summary()\n",
      "print(print_model)\n",
      "17/363: X = df[['eigenvector_centralities', 'degree_centralities', 'moviecount']]\n",
      "17/364:\n",
      "X = sm.add_constant(X) # adding a constant\n",
      "Y = boxofficegross\n",
      "model = sm.OLS(Y, X).fit()\n",
      "predictions = model.predict(X) \n",
      " \n",
      "print_model = model.summary()\n",
      "print(print_model)\n",
      "17/365: X = df[['degree_centralities', 'eigenvector_centralities', 'moviecount']]\n",
      "17/366: X = df[['degree_centralities', 'eigenvector_centralities', 'moviecount', 'academynominated']]\n",
      "17/367:\n",
      "X = sm.add_constant(X) # adding a constant\n",
      "Y = boxofficegross\n",
      "model = sm.OLS(Y, X).fit()\n",
      "predictions = model.predict(X) \n",
      " \n",
      "print_model = model.summary()\n",
      "print(print_model)\n",
      "17/368: X = df[['degree_centralities', 'eigenvector_centralities', 'moviecount']]\n",
      "17/369:\n",
      "X = sm.add_constant(X) # adding a constant\n",
      "Y = boxofficegross\n",
      "model = sm.OLS(Y, X).fit()\n",
      "predictions = model.predict(X) \n",
      " \n",
      "print_model = model.summary()\n",
      "print(print_model)\n",
      "18/36:\n",
      "twitter = \"twitter5000_bag_of_words_5.csv\"\n",
      "twitter_test = pd.read_csv(twitter, header=None)\n",
      "twitter_test.shape\n",
      "17/370: plot.plot(moviecounts, boxofficegross, \"o\")\n",
      "17/371:\n",
      "plot.figure()\n",
      "plot.plot(moviecounts, boxofficegross, \"o\")\n",
      "plot.plot(moviecounts, [slope*x + intercept for x in moviecounts])\n",
      "17/372:\n",
      "plot.figure()\n",
      "plot.plot(x, y, \"o\")\n",
      "yfit = [intercept + slope * xi for xi in x]\n",
      "plot.plot(x, yfit)\n",
      "17/373:\n",
      "x = np.array(degree_centralities)\n",
      "y = np.array(boxofficegross)\n",
      "slope, intercept, r_value, p_value, std_err = stats.linregress(x,y)\n",
      "slope, intercept, r_value, p_value, std_err\n",
      "17/374:\n",
      "plot.figure()\n",
      "plot.plot(x, y, \"o\")\n",
      "yfit = [intercept + slope * xi for xi in x]\n",
      "plot.plot(x, yfit)\n",
      "17/375: X = df[['degree_centralities', 'moviecount']]\n",
      "17/376:\n",
      "X = sm.add_constant(X) # adding a constant\n",
      "Y = boxofficegross\n",
      "model = sm.OLS(Y, X).fit()\n",
      "predictions = model.predict(X) \n",
      " \n",
      "print_model = model.summary()\n",
      "print(print_model)\n",
      "18/37:\n",
      "twitter = \"twitter5000_bag_of_words_5.csv\"\n",
      "twitter_train = pd.read_csv(twitter, header=None)\n",
      "twitter_train.shape\n",
      "18/38:\n",
      "twittertest = \"twittertest_bag_of_words_0.csv\"\n",
      "twitter_test = pd.read_csv(twittertest, header=None)\n",
      "twitter_test.shape\n",
      "18/39:\n",
      "twitter_y = open(\"twittertest_classes_0.txt\", \"r\")\n",
      "twitter_y = twitter_y.readlines()\n",
      "twitter_y = [int(x) for x in twitter_y]\n",
      "len(twitter_y)\n",
      "18/40:\n",
      "twitter_y = open(\"twittertest_classes_0.txt\", \"r\")\n",
      "twitter_y = twitter_y.readlines()\n",
      "twitter_y_ = [int(x) for x in twitter_y]\n",
      "len(twitter_y)\n",
      "18/41:\n",
      "twitter_y = open(\"twittertest_classes_0.txt\", \"r\")\n",
      "twitter_y = twitter_y.readlines()\n",
      "twitter_y\n",
      "# twitter_y_ = [int(x) for x in twitter_y]\n",
      "# len(twitter_y)\n",
      "18/42:\n",
      "twitter_y = open(\"twittertest_classes_0.txt\", \"r\")\n",
      "twitter_y = twitter_y.readlines()\n",
      "twitter_y\n",
      "[int(x) for x in twitter_y]\n",
      "# len(twitter_y)\n",
      "18/43:\n",
      "twitter_y = open(\"twittertest_classes_0.txt\", \"r\")\n",
      "twitter_y = twitter_y.readlines()\n",
      "twitter_y\n",
      "[int(x.strip()) for x in twitter_y]\n",
      "# len(twitter_y)\n",
      "18/44:\n",
      "twitter_y = open(\"twittertest_classes_0.txt\", \"r\")\n",
      "twitter_y = twitter_y.readlines()\n",
      "int(twitter_y[0])\n",
      "# [int(x) for x in twitter_y]\n",
      "# len(twitter_y)\n",
      "18/45:\n",
      "twitter_y = open(\"twittertest_classes_0.txt\", \"r\")\n",
      "twitter_y = twitter_y.readlines()\n",
      "int(twitter_y[0])\n",
      "[int(x) for x in twitter_y]\n",
      "# len(twitter_y)\n",
      "18/46:\n",
      "twitter_y = open(\"twittertest_classes_0.txt\", \"r\")\n",
      "twitter_y = twitter_y.readlines()\n",
      "int(twitter_y[0])\n",
      "for x in twitter_y:\n",
      "    if x != \"0\" and x != \"1\":\n",
      "        print x, \"asdf\"\n",
      "# [int(x) for x in twitter_y]\n",
      "# len(twitter_y)\n",
      "18/47:\n",
      "twitter_y = open(\"twittertest_classes_0.txt\", \"r\")\n",
      "twitter_y = twitter_y.readlines()\n",
      "int(twitter_y[1])\n",
      "for x in twitter_y:\n",
      "    if x != \"0\" and x != \"1\":\n",
      "        print x, \"asdf\"\n",
      "# [int(x) for x in twitter_y]\n",
      "# len(twitter_y)\n",
      "18/48:\n",
      "twitter_y = open(\"twittertest_classes_0.txt\", \"r\")\n",
      "twitter_y = twitter_y.readlines()\n",
      "int(twitter_y[1])\n",
      "for x in twitter_y:\n",
      "    if x != 0 and x != 1:\n",
      "        print x, \"asdf\"\n",
      "# [int(x) for x in twitter_y]\n",
      "# len(twitter_y)\n",
      "18/49:\n",
      "twitter_y = open(\"twittertest_classes_0.txt\", \"r\")\n",
      "twitter_y = twitter_y.readlines()\n",
      "int(twitter_y[1])\n",
      "for x in twitter_y:\n",
      "    print x\n",
      "# [int(x) for x in twitter_y]\n",
      "# len(twitter_y)\n",
      "18/50:\n",
      "twitter_y = open(\"twittertest_classes_0.txt\", \"r\")\n",
      "twitter_y = twitter_y.readlines()\n",
      "print twitter_y[1]\n",
      "# for x in twitter_y:\n",
      "#     print x\n",
      "# [int(x) for x in twitter_y]\n",
      "# len(twitter_y)\n",
      "18/51:\n",
      "twitter_y = open(\"twittertest_classes_0.txt\", \"r\")\n",
      "twitter_y = twitter_y.readlines()\n",
      "print twitter_y[1]\n",
      "j = 0\n",
      "for x in twitter_y:\n",
      "    j = j + 1\n",
      "#     print x\n",
      "print j\n",
      "# [int(x) for x in twitter_y]\n",
      "# len(twitter_y)\n",
      "18/52:\n",
      "twitter_y = open(\"twittertest_classes_0.txt\", \"r\")\n",
      "twitter_y = twitter_y.readlines()\n",
      "print twitter_y[1]\n",
      "j = 0\n",
      "for x in twitter_y:\n",
      "    j = j + 1\n",
      "#   print int(x)\n",
      "print j\n",
      "# [int(x) for x in twitter_y]\n",
      "# len(twitter_y)\n",
      "18/53:\n",
      "twitter_y = open(\"twittertest_classes_0.txt\", \"r\")\n",
      "twitter_y = twitter_y.readlines()\n",
      "print twitter_y[1]\n",
      "j = 0\n",
      "for x in twitter_y:\n",
      "    j = j + 1\n",
      "  print int(x)\n",
      "print j\n",
      "# [int(x) for x in twitter_y]\n",
      "# len(twitter_y)\n",
      "18/54:\n",
      "twitter_y = open(\"twittertest_classes_0.txt\", \"r\")\n",
      "twitter_y = twitter_y.readlines()\n",
      "print twitter_y[1]\n",
      "j = 0\n",
      "for x in twitter_y:\n",
      "    j = j + 1\n",
      "    print int(x)\n",
      "print j\n",
      "# [int(x) for x in twitter_y]\n",
      "# len(twitter_y)\n",
      "18/55:\n",
      "twitter_y = open(\"twittertest_classes_0.txt\", \"r\")\n",
      "twitter_y = twitter_y.readlines()\n",
      "twitter_ys = []\n",
      "for x in twitter_y:\n",
      "    twitter_ys.append(int(x))\n",
      "18/56:\n",
      "twitter_y = open(\"twittertest_classes_0.txt\", \"r\")\n",
      "twitter_y = twitter_y.readlines()\n",
      "twitter_ys = []\n",
      "print twitter_y[len(twitter_y)-1]\n",
      "for x in twitter_y:\n",
      "    twitter_ys.append(int(x))\n",
      "18/57:\n",
      "twitter_y = open(\"twittertest_classes_0.txt\", \"r\")\n",
      "twitter_y = twitter_y.readlines()\n",
      "twitter_ys = []\n",
      "# print twitter_y[len(twitter_y)-1]\n",
      "for x in twitter_y:\n",
      "    print int(x)\n",
      "#     twitter_ys.append(int(x))\n",
      "18/58:\n",
      "twitter_y = open(\"twittertest_classes_0.txt\", \"r\")\n",
      "twitter_y = twitter_y.readlines()\n",
      "twitter_ys = []\n",
      "# print twitter_y[len(twitter_y)-1]\n",
      "for x in twitter_y:\n",
      "    print int(x)\n",
      "    twitter_ys.append(int(x))\n",
      "18/59:\n",
      "twitter_y = open(\"twittertest_classes_0.txt\", \"r\")\n",
      "twitter_y = twitter_y.readlines()\n",
      "twitter_ys = []\n",
      "# print twitter_y[len(twitter_y)-1]\n",
      "for x in twitter_y:\n",
      "    twitter_ys.append(int(x))\n",
      "18/60:\n",
      "twitter_y = open(\"twittertest_classes_0.txt\", \"r\")\n",
      "twitter_y = twitter_y.readlines()\n",
      "twitter_ys = []\n",
      "# print twitter_y[len(twitter_y)-1]\n",
      "for x in twitter_y:\n",
      "    print int(x)\n",
      "    twitter_ys.append(int(x))\n",
      "18/61:\n",
      "twitter_y = open(\"twittertest_classes_0.txt\", \"r\")\n",
      "twitter_y = twitter_y.readlines()\n",
      "twitter_ys = []\n",
      "count = 0\n",
      "for x in twitter_y:\n",
      "    count += 1\n",
      "    print int(x), count\n",
      "    twitter_ys.append(int(x))\n",
      "18/62:\n",
      "twitter_y = open(\"twittertest_classes_0.txt\", \"r\")\n",
      "twitter_y = twitter_y.readlines()\n",
      "twitter_ys = []\n",
      "count = 0\n",
      "for x in twitter_y:\n",
      "    count += 1\n",
      "    if count == \"0\":\n",
      "        print \"asdf\"\n",
      "#     print int(x), count\n",
      "    twitter_ys.append(int(x))\n",
      "18/63:\n",
      "twitter_y = open(\"twittertest_classes_0.txt\", \"r\")\n",
      "twitter_y = twitter_y.readlines()\n",
      "twitter_ys = []\n",
      "count = 0\n",
      "for x in twitter_y:\n",
      "    count += 1\n",
      "    if count == '0':\n",
      "        print \"asdf\"\n",
      "#     print int(x), count\n",
      "    twitter_ys.append(int(x))\n",
      "18/64:\n",
      "twitter_y = open(\"twittertest_classes_0.txt\", \"r\")\n",
      "twitter_y = twitter_y.readlines()\n",
      "twitter_ys = []\n",
      "count = 0\n",
      "for x in twitter_y:\n",
      "    count += 1\n",
      "    if x == \"0\":\n",
      "        print \"asdf\"\n",
      "#     print int(x), count\n",
      "    twitter_ys.append(int(x))\n",
      "18/65:\n",
      "twitter_y = open(\"twittertest_classes_0.txt\", \"r\")\n",
      "twitter_y = twitter_y.readlines()\n",
      "twitter_ys = []\n",
      "count = 0\n",
      "for x in twitter_y:\n",
      "    count += 1\n",
      "    if count == 4890:\n",
      "        print x\n",
      "#     print int(x), count\n",
      "    twitter_ys.append(int(x))\n",
      "18/66:\n",
      "twitter_y = open(\"twittertest_classes_0.txt\", \"r\")\n",
      "twitter_y = twitter_y.readlines()\n",
      "twitter_ys = []\n",
      "count = 0\n",
      "for x in twitter_y:\n",
      "    count += 1\n",
      "    if count == 4890:\n",
      "        print x\n",
      "#     print int(x), count\n",
      "    twitter_ys?.append(int(x))\n",
      "18/67:\n",
      "twitter_y = open(\"twittertest_classes_0.txt\", \"r\")\n",
      "twitter_y = twitter_y.readlines()\n",
      "twitter_ys = []\n",
      "count = 0\n",
      "for x in twitter_y:\n",
      "    count += 1\n",
      "    if count == 4890:\n",
      "        print x\n",
      "#     print int(x), count\n",
      "    twitter_ys.append(int(x))\n",
      "18/68:\n",
      "twitter_y = open(\"twittertest_classes_0.txt\", \"r\")\n",
      "twitter_y = twitter_y.readlines()\n",
      "twitter_ys = []\n",
      "count = 0\n",
      "for x in twitter_y:\n",
      "    count += 1\n",
      "    if count == 4890:\n",
      "        print x\n",
      "#     print int(x), count\n",
      "#     twitter_ys.append(int(x))\n",
      "18/69:\n",
      "twitter_y = open(\"twittertest_classes_0.txt\", \"r\")\n",
      "twitter_y = twitter_y.readlines()\n",
      "twitter_ys = []\n",
      "count = 0\n",
      "for x in twitter_y:\n",
      "    count += 1\n",
      "    if count == 4980:\n",
      "        print x\n",
      "#     print int(x), count\n",
      "#     twitter_ys.append(int(x))\n",
      "18/70:\n",
      "twitter_y = open(\"twittertest_classes_0.txt\", \"r\")\n",
      "twitter_y = twitter_y.readlines()\n",
      "twitter_ys = []\n",
      "count = 0\n",
      "for x in twitter_y:\n",
      "    count += 1\n",
      "    if x == \"field!\":\n",
      "        print x\n",
      "#     print int(x), count\n",
      "#     twitter_ys.append(int(x))\n",
      "18/71:\n",
      "twitter_y = open(\"twittertest_classes_0.txt\", \"r\")\n",
      "twitter_y = twitter_y.readlines()\n",
      "twitter_ys = []\n",
      "count = 0\n",
      "for x in twitter_y:\n",
      "    count += 1\n",
      "    if x == \"field!\":\n",
      "        print x\n",
      "#     print int(x), count\n",
      "#     twitter_ys.append(int(x))\n",
      "18/72:\n",
      "twitter_y = open(\"twittertest_classes_0.txt\", \"r\")\n",
      "twitter_y = twitter_y.readlines()\n",
      "twitter_ys = []\n",
      "count = 0\n",
      "for x in twitter_y:\n",
      "    count += 1\n",
      "    if count == 4290:\n",
      "        print x\n",
      "    if x == \"field!\":\n",
      "        print x\n",
      "#     print int(x), count\n",
      "#     twitter_ys.append(int(x))\n",
      "18/73:\n",
      "twitter_y = open(\"twittertest_classes_0.txt\", \"r\")\n",
      "twitter_y = twitter_y.readlines()\n",
      "twitter_ys = []\n",
      "count = 0\n",
      "for x in twitter_y:\n",
      "    count += 1\n",
      "    if count == 4290:\n",
      "        x = 0\n",
      "#     print int(x), count\n",
      "#     twitter_ys.append(int(x))\n",
      "18/74:\n",
      "twitter_y = open(\"twittertest_classes_0.txt\", \"r\")\n",
      "twitter_y = twitter_y.readlines()\n",
      "twitter_ys = []\n",
      "count = 0\n",
      "for x in twitter_y:\n",
      "    count += 1\n",
      "    if count == 4290:\n",
      "        x = 0\n",
      "#     print int(x), count\n",
      "    twitter_ys.append(int(x))\n",
      "18/75:\n",
      "twitter_y = open(\"twittertest_classes_0.txt\", \"r\")\n",
      "twitter_y = twitter_y.readlines()\n",
      "twitter_ys = []\n",
      "count = 0\n",
      "for x in twitter_y:\n",
      "    count += 1\n",
      "    if count == 4290:\n",
      "        x = 0\n",
      "    print int(x), count\n",
      "    twitter_ys.append(int(x))\n",
      "18/76:\n",
      "twitter_y = open(\"twittertest_classes_0.txt\", \"r\")\n",
      "twitter_y = twitter_y.readlines()\n",
      "twitter_ys = []\n",
      "count = 0\n",
      "for x in twitter_y:\n",
      "    count += 1\n",
      "    # fixing the dataset errors\n",
      "    if count == 4290 || count == 5182:\n",
      "        x = 0\n",
      "    print int(x), count\n",
      "    twitter_ys.append(int(x))\n",
      "18/77:\n",
      "twitter_y = open(\"twittertest_classes_0.txt\", \"r\")\n",
      "twitter_y = twitter_y.readlines()\n",
      "twitter_ys = []\n",
      "count = 0\n",
      "for x in twitter_y:\n",
      "    count += 1\n",
      "    # fixing the dataset errors\n",
      "    if count == 4290 or count == 5182:\n",
      "        x = 0\n",
      "    print int(x), count\n",
      "    twitter_ys.append(int(x))\n",
      "18/78:\n",
      "twitter_y = open(\"twittertest_classes_0.txt\", \"r\")\n",
      "twitter_y = twitter_y.readlines()\n",
      "twitter_ys = []\n",
      "count = 0\n",
      "for x in twitter_y:\n",
      "    count += 1\n",
      "    # fixing the dataset errors\n",
      "    if count == 4290 or count == 5182:\n",
      "        x = 0\n",
      "#     print int(x), count\n",
      "    twitter_ys.append(int(x))\n",
      "18/79:\n",
      "twitter_y = open(\"twittertest_classes_0.txt\", \"r\")\n",
      "twitter_y = twitter_y.readlines()\n",
      "twitter_ys = []\n",
      "count = 0\n",
      "for x in twitter_y:\n",
      "    count += 1\n",
      "    # fixing the dataset errors\n",
      "    if count == 4290 or count == 5182:\n",
      "        x = 0\n",
      "    twitter_ys.append(int(x))\n",
      "18/80: lrs_top.score(twitter_test, twitter_y)\n",
      "18/81:\n",
      "selector = SelectKBest(chi2, k=9000).fit(xs_train, y_train)\n",
      "xs_top = selector.transform(xs_train)\n",
      "lrs_top = LogisticRegression(solver='lbfgs', C=1)\n",
      "models_top = lrs_top.fit(xs_top, y_train)\n",
      "selected = selector.get_support(indices=True)\n",
      "lrs_top.score(xs_test[selected], y_test)\n",
      "fpr_alls, tpr_alls, _ = roc_curve(y_test, lr_alls.predict_proba(xs_test)[:,1])\n",
      "18/82:\n",
      "import sklearn\n",
      "from sklearn.naive_bayes import MultinomialNB, ComplementNB\n",
      "import pandas as pd\n",
      "import numpy as np\n",
      "18/83: mnb = MultinomialNB()\n",
      "18/84:\n",
      "# original preprocessor\n",
      "x = \"train_bag_of_words_5.csv\"\n",
      "x_train_orig = pd.read_csv(x, header=None)\n",
      "x_train_orig.shape\n",
      "18/85:\n",
      "y = open(\"train_classes_5.txt\", \"r\")\n",
      "y = y.readlines()\n",
      "y_train = [int(x) for x in y]\n",
      "len(y_train)\n",
      "18/86: mnb.fit(x_train, y_train)\n",
      "18/87: mnb.score(x_train, y_train)\n",
      "18/88:\n",
      "x2 = \"test__bag_of_words_0.csv\"\n",
      "x_test = pd.read_csv(x2, header=None)\n",
      "x_test.shape\n",
      "18/89:\n",
      "y2 = open(\"test_classes_5.txt\", \"r\")\n",
      "y2 = y2.readlines()\n",
      "y_test = [int(x) for x in y2]\n",
      "len(y_test)\n",
      "18/90: mnb.score(x_test, y_test)\n",
      "18/91: mnb.fit(x_train_orig, y_train)\n",
      "18/92: mnb.score(x_train_orig, y_train)\n",
      "18/93:\n",
      "x2 = \"test__bag_of_words_0.csv\"\n",
      "x_test = pd.read_csv(x2, header=None)\n",
      "x_test.shape\n",
      "18/94:\n",
      "y2 = open(\"test_classes_5.txt\", \"r\")\n",
      "y2 = y2.readlines()\n",
      "y_test = [int(x) for x in y2]\n",
      "len(y_test)\n",
      "18/95: mnb.score(x_test, y_test)\n",
      "18/96: cnb = ComplementNB()\n",
      "18/97: cnb.fit(x_train, y_train)\n",
      "18/98: cnb.score(x_train, y_train)\n",
      "18/99: cnb.fit(x_train_orig, y_train)\n",
      "18/100: cnb.score(x_train_orig, y_train)\n",
      "18/101: cnb.score(x_test, y_test)\n",
      "18/102:\n",
      "from sklearn.svm import LinearSVC\n",
      "svm = LinearSVC(C=.25)\n",
      "svm.fit(x_train, y_train)\n",
      "svm.score(x_train, y_train)\n",
      "18/103: svm.score(x_test, y_test)\n",
      "18/104:\n",
      "from sklearn.svm import LinearSVC\n",
      "svm = LinearSVC(C=.25)\n",
      "svm.fit(x_train_orig, y_train)\n",
      "svm.score(x_train_orig, y_train)\n",
      "18/105: svm.score(x_test, y_test)\n",
      "18/106: from sklearn.linear_model import LogisticRegression\n",
      "18/107: lr = LogisticRegression(solver='lbfgs', C=0.25)\n",
      "18/108: model = lr.fit(x_train_orig, y_train)\n",
      "18/109: lr.score(x_train_orig, y_train)\n",
      "18/110: lr.score(x_test, y_test)\n",
      "18/111:\n",
      "lr_train = []\n",
      "lr_test = []\n",
      "ws = [0.01, 0.05, 0.1, 0.25, 0.5, 0.75, 0.9, 1]\n",
      "for w in ws:\n",
      "    lr = LogisticRegression(solver='lbfgs', C=w)\n",
      "    lr.fit(x_train, y_train)\n",
      "    lr_train.append(lr.score(x_train, y_train))\n",
      "    lr_test.append(lr.score(x_test, y_test))\n",
      "18/112:\n",
      "lr_train = []\n",
      "lr_test = []\n",
      "ws = [0.01, 0.05, 0.1, 0.25, 0.5, 0.75, 0.9, 1]\n",
      "for w in ws:\n",
      "    lr = LogisticRegression(solver='lbfgs', C=w)\n",
      "    lr.fit(x_train_orig, y_train)\n",
      "    lr_train.append(lr.score(x_train_orig, y_train))\n",
      "    lr_test.append(lr.score(x_test, y_test))\n",
      "18/113: from matplotlib import pyplot as plot\n",
      "18/114:\n",
      "plot.figure()\n",
      "plot.plot(ws, lr_train)\n",
      "plot.plot(ws, lr_test)\n",
      "18/115:\n",
      "print lr_train[3]\n",
      "print lr_test[3]\n",
      "18/116:\n",
      "trainWords = open(\"train_vocab_5.txt\", \"r\")\n",
      "trainWords = trainWords.readlines()\n",
      "18/117:\n",
      "coefs = model.coef_[0]\n",
      "tuples = sorted(zip(trainWords, coefs), key=(lambda x: x[1]))\n",
      "print [i[0] for i in tuples[:10]]\n",
      "print [i[0] for i in tuples[-10:]]\n",
      "18/118: from sklearn.feature_extraction.text import TfidfTransformer\n",
      "18/119: tfidf = TfidfTransformer()\n",
      "18/120: x_train_tf = tfidf.fit_transform(x_train)\n",
      "18/121: lr2 = LogisticRegression(solver='lbfgs', C=1)\n",
      "18/122: lr2.fit(x_train_tf, y_train)\n",
      "18/123: from sklearn.feature_selection import SelectKBest, chi2\n",
      "18/124:\n",
      "selector = SelectKBest(chi2, k=335).fit(x_train_orig, y_train)\n",
      "x_topk = selector.transform(x_train_orig)\n",
      "x_topk.shape\n",
      "18/125: lr_select.score(x_topk, y_train)\n",
      "18/126: lr_select = LogisticRegression(solver='lbfgs', C=1)\n",
      "18/127: model_select = lr_select.fit(x_topk, y_train)\n",
      "18/128: lr_select.score(x_topk, y_train)\n",
      "18/129: lr_select.predict(x_topk)[65]\n",
      "18/130:\n",
      "selected_indices = selector.get_support(indices=True)\n",
      "# for i in selected_indices:\n",
      "#     print trainWords[i]\n",
      "18/131: x_testk = x_test[selected_indices]\n",
      "18/132: lr3.score(x_test100, y_test)\n",
      "18/133: lr_select.score(x_test100, y_test)\n",
      "18/134: lr_select.score(x_testk, y_test)\n",
      "18/135:\n",
      "topk = []\n",
      "topktest = []\n",
      "ks = [20,50,100,150,200,250,300,330,335,340,350,400,450,500,540]\n",
      "for i in ks:\n",
      "    selector = SelectKBest(chi2, k=i).fit(x_train, y_train)\n",
      "    x_top = selector.transform(x_train)\n",
      "    lr_top = LogisticRegression(solver='lbfgs', C=1)\n",
      "    model_top = lr_top.fit(x_top, y_train)\n",
      "    topk.append(lr_top.score(x_top, y_train))\n",
      "    selected = selector.get_support(indices=True)\n",
      "    topktest.append(lr_top.score(x_test[selected], y_test))\n",
      "18/136:\n",
      "topk = []\n",
      "topktest = []\n",
      "ks = [20,50,100,150,200,250,300,330,335,340,350,400,450,500,540]\n",
      "for i in ks:\n",
      "    selector = SelectKBest(chi2, k=i).fit(x_train_orig, y_train)\n",
      "    x_top = selector.transform(x_train_orig)\n",
      "    lr_top = LogisticRegression(solver='lbfgs', C=1)\n",
      "    model_top = lr_top.fit(x_top, y_train)\n",
      "    topk.append(lr_top.score(x_top, y_train))\n",
      "    selected = selector.get_support(indices=True)\n",
      "    topktest.append(lr_top.score(x_test[selected], y_test))\n",
      "18/137:\n",
      "plot.figure()\n",
      "plot.plot(ks, topk)\n",
      "plot.plot(ks, topktest)\n",
      "print ks[np.argmax(topktest)], np.max(topktest)\n",
      "18/138:\n",
      "x = \"stop_bag_of_words_0.csv\"\n",
      "x_test = pd.read_csv(x, header=None)\n",
      "x_test.shape\n",
      "18/139:\n",
      "x = \"stopt_bag_of_words_5.csv\"\n",
      "x_train = pd.read_csv(x, header=None)\n",
      "x_train.shape\n",
      "18/140:\n",
      "topk = []\n",
      "topktest = []\n",
      "ks = [20,50,100,150,200,250,300,350,400,450,500,540]\n",
      "for i in ks:\n",
      "    selector = SelectKBest(chi2, k=i).fit(x_train, y_train)\n",
      "    x_top = selector.transform(x_train)\n",
      "    lr_top = LogisticRegression(solver='lbfgs', C=1)\n",
      "    model_top = lr_top.fit(x_top, y_train)\n",
      "    topk.append(lr_top.score(x_top, y_train))\n",
      "    selected = selector.get_support(indices=True)\n",
      "    topktest.append(lr_top.score(x_test[selected], y_test))\n",
      "18/141:\n",
      "plot.figure()\n",
      "plot.plot(ks, topk)\n",
      "plot.plot(ks, topktest)\n",
      "print ks[np.argmax(topktest)], np.max(topktest)\n",
      "18/142: # SVM, top K\n",
      "18/143:\n",
      "from sklearn.svm import LinearSVC\n",
      "svm = LinearSVC(C=0.7)\n",
      "svm.fit(x_train, y_train)\n",
      "svm.score(x_train, y_train)\n",
      "18/144: svm.score(x_test, y_test)\n",
      "18/145: # Logistic regression, CountVectorizer, 2-word sequences\n",
      "18/146:\n",
      "x = \"temp2_bag_of_words_5.csv\"\n",
      "x_train = pd.read_csv(x, header=None)\n",
      "x_train.shape\n",
      "18/147:\n",
      "topk = []\n",
      "topktest = []\n",
      "ks = [1000,5000,7500,10000,12500]\n",
      "for i in ks:\n",
      "    selector = SelectKBest(chi2, k=i).fit(x_train, y_train)\n",
      "    x_top = selector.transform(x_train)\n",
      "    lr_top = LogisticRegression(solver='lbfgs', C=1)\n",
      "    model_top = lr_top.fit(x_top, y_train)\n",
      "    topk.append(lr_top.score(x_top, y_train))\n",
      "    selected = selector.get_support(indices=True)\n",
      "    topktest.append(lr_top.score(x_test[selected], y_test))\n",
      "18/148:\n",
      "topk = []\n",
      "topktest = []\n",
      "ks = [1000,5000,7500,10000,12500]\n",
      "for i in ks:\n",
      "    selector = SelectKBest(chi2, k=i).fit(x_train, y_train)\n",
      "    x_top = selector.transform(x_train)\n",
      "    lr_top = LogisticRegression(solver='lbfgs', C=1)\n",
      "    model_top = lr_top.fit(x_top, y_train)\n",
      "    topk.append(lr_top.score(x_top, y_train))\n",
      "    selected = selector.get_support(indices=True)\n",
      "    topktest.append(lr_top.score(x_test[selected], y_test))\n",
      "18/149:\n",
      "topk = []\n",
      "topktest = []\n",
      "ks = [1000,5000,7500,10000,12500]\n",
      "for i in ks:\n",
      "    selector = SelectKBest(chi2, k=i).fit(x_train, y_train)\n",
      "    x_top = selector.transform(x_train)\n",
      "    lr_top = LogisticRegression(solver='lbfgs', C=1)\n",
      "    model_top = lr_top.fit(x_top, y_train)\n",
      "    topk.append(lr_top.score(x_top, y_train))\n",
      "    selected = selector.get_support(indices=True)\n",
      "    topktest.append(lr_top.score(x_test[selected], y_test))\n",
      "18/150:\n",
      "x = \"temp2_bag_of_words_5.csv\"\n",
      "x_train = pd.read_csv(x, header=None)\n",
      "x_train.shape\n",
      "18/151:\n",
      "x2 = \"temp_bag_of_words_0.csv\"\n",
      "x_test = pd.read_csv(x2, header=None)\n",
      "x_test.shape\n",
      "18/152:\n",
      "topk = []\n",
      "topktest = []\n",
      "ks = [1000,5000,7500,10000,12500]\n",
      "for i in ks:\n",
      "    selector = SelectKBest(chi2, k=i).fit(x_train, y_train)\n",
      "    x_top = selector.transform(x_train)\n",
      "    lr_top = LogisticRegression(solver='lbfgs', C=1)\n",
      "    model_top = lr_top.fit(x_top, y_train)\n",
      "    topk.append(lr_top.score(x_top, y_train))\n",
      "    selected = selector.get_support(indices=True)\n",
      "    topktest.append(lr_top.score(x_test[selected], y_test))\n",
      "18/153:\n",
      "plot.figure()\n",
      "plot.plot(ks, topk)\n",
      "plot.plot(ks, topktest)\n",
      "print ks[np.argmax(topktest)], np.max(topktest)\n",
      "18/154: plot.plot(ks, topktest)\n",
      "18/155:\n",
      "trainWords = open(\"temp2_vocab_5.txt\", \"r\")\n",
      "trainWords = trainWords.readlines()\n",
      "coefs = model_top.coef_[0]\n",
      "tuples = sorted(zip(trainWords, coefs), key=(lambda x: x[1]))\n",
      "print [(i[0], i[1]) for i in tuples[:10]]\n",
      "print [(i[0], i[1]) for i in tuples[-10:]]\n",
      "18/156: # Logistic Regression, CountVectorizer, 2-word sequences, min_df=3\n",
      "18/157:\n",
      "x3 = \"temp3_bag_of_words_5.csv\"\n",
      "x_train = pd.read_csv(x3, header=None)\n",
      "x_train.shape\n",
      "18/158:\n",
      "x4 = \"temp4_bag_of_words_0.csv\"\n",
      "x_test = pd.read_csv(x4, header=None)\n",
      "x_test.shape\n",
      "18/159:\n",
      "topk = []\n",
      "topktest = []\n",
      "ks = [100,500,1000,1500,2000,2500,3000,3500,4000]\n",
      "for i in ks:\n",
      "    selector = SelectKBest(chi2, k=i).fit(x_train, y_train)\n",
      "    x_top = selector.transform(x_train)\n",
      "    lr_top = LogisticRegression(solver='lbfgs', C=1)\n",
      "    model_top = lr_top.fit(x_top, y_train)\n",
      "    topk.append(lr_top.score(x_top, y_train))\n",
      "    selected = selector.get_support(indices=True)\n",
      "    topktest.append(lr_top.score(x_test[selected], y_test))\n",
      "18/160:\n",
      "plot.figure()\n",
      "plot.plot(ks, topk)\n",
      "plot.plot(ks, topktest)\n",
      "print ks[np.argmax(topktest)], np.max(topktest)\n",
      "18/161:\n",
      "x5 = \"temp5_bag_of_words_5.csv\"\n",
      "x_train = pd.read_csv(x5, header=None)\n",
      "x_train.shape\n",
      "18/162:\n",
      "x6 = \"temp6_bag_of_words_0.csv\"\n",
      "x_test = pd.read_csv(x6, header=None)\n",
      "x_test.shape\n",
      "18/163:\n",
      "plot.figure()\n",
      "plot.plot(ks, topk)\n",
      "plot.plot(ks, topktest)\n",
      "print ks[np.argmax(topktest)], np.max(topktest)\n",
      "18/164:\n",
      "topk = []\n",
      "topktest = []\n",
      "ks = [1000,5000,7500,8500,9000,9500,10000,12500]\n",
      "for i in ks:\n",
      "    selector = SelectKBest(chi2, k=i).fit(x_train, y_train)\n",
      "    x_top = selector.transform(x_train)\n",
      "    lr_top = LogisticRegression(solver='lbfgs', C=1)\n",
      "    model_top = lr_top.fit(x_top, y_train)\n",
      "    topk.append(lr_top.score(x_top, y_train))\n",
      "    selected = selector.get_support(indices=True)\n",
      "    topktest.append(lr_top.score(x_test[selected], y_test))\n",
      "18/165:\n",
      "plot.figure()\n",
      "plot.plot(ks, topk)\n",
      "plot.plot(ks, topktest)\n",
      "print ks[np.argmax(topktest)], np.max(topktest)\n",
      "18/166:\n",
      "x6 = \"temp6_bag_of_words_0.csv\"\n",
      "x_test = pd.read_csv(x6, header=None)\n",
      "x_test.shape\n",
      "18/167:\n",
      "topk = []\n",
      "topktest = []\n",
      "ks = [1000,5000,7500,8500,9000,9500,10000,12500]\n",
      "for i in ks:\n",
      "    selector = SelectKBest(chi2, k=i).fit(x_train, y_train)\n",
      "    x_top = selector.transform(x_train)\n",
      "    lr_top = LogisticRegression(solver='lbfgs', C=1)\n",
      "    model_top = lr_top.fit(x_top, y_train)\n",
      "    topk.append(lr_top.score(x_top, y_train))\n",
      "    selected = selector.get_support(indices=True)\n",
      "    topktest.append(lr_top.score(x_test[selected], y_test))\n",
      "18/168:\n",
      "plot.figure()\n",
      "plot.plot(ks, topk)\n",
      "plot.plot(ks, topktest)\n",
      "print ks[np.argmax(topktest)], np.max(topktest)\n",
      "18/169:\n",
      "x5 = \"temp5_bag_of_words_5.csv\"\n",
      "x_train = pd.read_csv(x5, header=None)\n",
      "x_train.shape|\n",
      "18/170:\n",
      "x5 = \"temp5_bag_of_words_5.csv\"\n",
      "x_train = pd.read_csv(x5, header=None)\n",
      "x_train.shape\n",
      "18/171:\n",
      "x6 = \"temp6_bag_of_words_0.csv\"\n",
      "x_test = pd.read_csv(x6, header=None)\n",
      "x_test.shape\n",
      "18/172:\n",
      "topk = []\n",
      "topktest = []\n",
      "ks = [1000,5000,7500,8500,9000,9500,10000,12500]\n",
      "for i in ks:\n",
      "    selector = SelectKBest(chi2, k=i).fit(x_train, y_train)\n",
      "    x_top = selector.transform(x_train)\n",
      "    lr_top = LogisticRegression(solver='lbfgs', C=1)\n",
      "    model_top = lr_top.fit(x_top, y_train)\n",
      "    topk.append(lr_top.score(x_top, y_train))\n",
      "    selected = selector.get_support(indices=True)\n",
      "    topktest.append(lr_top.score(x_test[selected], y_test))\n",
      "18/173:\n",
      "plot.figure()\n",
      "plot.plot(ks, topk)\n",
      "plot.plot(ks, topktest)\n",
      "print ks[np.argmax(topktest)], np.max(topktest)\n",
      "18/174:\n",
      "x = \"temp2_bag_of_words_5.csv\"\n",
      "x_train = pd.read_csv(x, header=None)\n",
      "x_train.shape\n",
      "18/175:\n",
      "x2 = \"temp_bag_of_words_0.csv\"\n",
      "x_test = pd.read_csv(x2, header=None)\n",
      "x_test.shape\n",
      "18/176:\n",
      "selector = SelectKBest(chi2, k=9000).fit(x_train, y_train)\n",
      "x_top = selector.transform(x_train)\n",
      "lr_top = LogisticRegression(solver='lbfgs', C=1)\n",
      "model_top = lr_top.fit(x_top, y_train)\n",
      "selected = selector.get_support(indices=True)\n",
      "lr_top.score(x_test[selected], y_test)\n",
      "18/177:\n",
      "lr_top = LogisticRegression(solver='lbfgs', C=1)\n",
      "model_top = lr_top.fit(x_top, y_train)\n",
      "selected = selector.get_support(indices=True)\n",
      "lr_top.score(x_test[selected], y_test)\n",
      "18/178: from sklearn.metrics import roc_curve\n",
      "18/179: fpr, tpr, _ = roc_curve(y_test, lr_top.predict_proba(x_test[selected])[:,1])\n",
      "18/180: plot.plot(fpr, tpr)\n",
      "18/181: # ROC curve: logistic regression with (1,2) vs 1 word sequences\n",
      "18/182: fpr3, tpr3, _ = roc_curve(y_test, lr3.predict_proba(x_testk)[:,1])\n",
      "18/183: fpr3, tpr3, _ = roc_curve(y_test, lr_select.predict_proba(x_testk)[:,1])\n",
      "18/184:\n",
      "plot.figure()\n",
      "plot.plot(fpr, tpr, label=\"2-word features (bigrams)\") # logistic regression, top K + 2-word sequences\n",
      "plot.plot(fpr3, tpr3, label=\"1-word\") # logistic regression, top K\n",
      "plot.legend()\n",
      "plot.xlabel(\"FPR\")\n",
      "plot.ylabel(\"TPR\")\n",
      "18/185: # ROC curve: logistic regression vs MNB vs SVM\n",
      "18/186:\n",
      "mnb.fit(x_train, y_train)\n",
      "fpr_mnb, tpr_mnb, _ = roc_curve(y_test, mnb.predict_proba(x_test)[:,1])\n",
      "18/187:\n",
      "svm = LinearSVC(C=.5)\n",
      "svm.fit(x_train, y_train)\n",
      "fpr_svm, tpr_svm, _ = roc_curve(y_test, svm.decision_function(x_test))\n",
      "18/188: svm.score(x_test, y_test)\n",
      "18/189:\n",
      "plot.figure()\n",
      "plot.plot(fpr, tpr, label=\"logistic regression\") # logistic regression, top K + 2-word sequences\n",
      "plot.plot(fpr_mnb, tpr_mnb, label=\"multinomial naive bayes\") # MNB\n",
      "plot.plot(fpr_svm, tpr_svm, label=\"support vector machine\") # SVM\n",
      "plot.legend()\n",
      "plot.xlabel(\"FPR\")\n",
      "plot.ylabel(\"TPR\")\n",
      "18/190: # ROC curve: logistic regression: top K vs all\n",
      "18/191:\n",
      "lr_all = LogisticRegression(solver='lbfgs', C=1)\n",
      "model_all = lr_all.fit(x_train, y_train)\n",
      "lr_all.score(x_test, y_test)\n",
      "fpr_all, tpr_all, _ = roc_curve(y_test, lr_all.predict_proba(x_test)[:,1])\n",
      "18/192:\n",
      "plot.figure()\n",
      "plot.plot(fpr, tpr, label=\"top K features\") # logistic regression, top K + 2-word sequences\n",
      "plot.plot(fpr_all, tpr_all, label=\"all features\") # logistic regression, all + 2-word sequences\n",
      "plot.legend()\n",
      "plot.xlabel(\"FPR\")\n",
      "plot.ylabel(\"TPR\")\n",
      "18/193: # ROC curve: logistic regression: few stopwords vs all\n",
      "18/194:\n",
      "xs = \"tempstop_bag_of_words_5.csv\"\n",
      "xs_train = pd.read_csv(x, header=None)\n",
      "xs_train.shape\n",
      "18/195:\n",
      "xst = \"tempstop_bag_of_words_0.csv\"\n",
      "xs_test = pd.read_csv(x2, header=None)\n",
      "xs_test.shape\n",
      "18/196:\n",
      "selector = SelectKBest(chi2, k=9000).fit(xs_train, y_train)\n",
      "xs_top = selector.transform(xs_train)\n",
      "lrs_top = LogisticRegression(solver='lbfgs', C=1)\n",
      "models_top = lrs_top.fit(xs_top, y_train)\n",
      "selected = selector.get_support(indices=True)\n",
      "lrs_top.score(xs_test[selected], y_test)\n",
      "fpr_alls, tpr_alls, _ = roc_curve(y_test, lr_alls.predict_proba(xs_test)[:,1])\n",
      "18/197:\n",
      "selector = SelectKBest(chi2, k=9000).fit(xs_train, y_train)\n",
      "xs_top = selector.transform(xs_train)\n",
      "lrs_top = LogisticRegression(solver='lbfgs', C=1)\n",
      "models_top = lrs_top.fit(xs_top, y_train)\n",
      "selected = selector.get_support(indices=True)\n",
      "lrs_top.score(xs_test[selected], y_test)\n",
      "fpr_alls, tpr_alls, _ = roc_curve(y_test, lr_all.predict_proba(xs_test)[:,1])\n",
      "18/198:\n",
      "selector = SelectKBest(chi2, k=9000).fit(xs_train, y_train)\n",
      "xs_top = selector.transform(xs_train)\n",
      "lrs_top = LogisticRegression(solver='lbfgs', C=1)\n",
      "models_top = lrs_top.fit(xs_top, y_train)\n",
      "selected = selector.get_support(indices=True)\n",
      "lrs_top.score(xs_test[selected], y_test)\n",
      "fpr_alls, tpr_alls, _ = roc_curve(y_test, lrs_top.predict_proba(xs_test)[:,1])\n",
      "18/199:\n",
      "selector = SelectKBest(chi2, k=9000).fit(xs_train, y_train)\n",
      "xs_top = selector.transform(xs_train)\n",
      "lrs_top = LogisticRegression(solver='lbfgs', C=1)\n",
      "models_top = lrs_top.fit(xs_top, y_train)\n",
      "selected = selector.get_support(indices=True)\n",
      "lrs_top.score(xs_test[selected], y_test)\n",
      "fpr_alls, tpr_alls, _ = roc_curve(y_test, lrs_top.predict_proba(xs_test[selected])[:,1])\n",
      "18/200:\n",
      "plot.figure()\n",
      "plot.plot(fpr, tpr, label=\"limited stop words\") # logistic regression, top K + 2-word sequences\n",
      "plot.plot(fpr_alls, tpr_alls, label=\"all stop words\") # logistic regression, top K + all stop words + 2-word sequences\n",
      "plot.legend()\n",
      "plot.xlabel(\"FPR\")\n",
      "plot.ylabel(\"TPR\")\n",
      "18/201:\n",
      "selector = SelectKBest(chi2, k=9000).fit(xs_train, y_train)\n",
      "xs_top = selector.transform(xs_train)\n",
      "lrs_top = LogisticRegression(solver='lbfgs', C=1)\n",
      "models_top = lrs_top.fit(xs_top, y_train)\n",
      "selected = selector.get_support(indices=True)\n",
      "lrs_top.score(xs_test[selected], y_test)\n",
      "18/202:\n",
      "selector = SelectKBest(chi2, k=9000).fit(xs_train, y_train)\n",
      "xs_top = selector.transform(xs_train)\n",
      "lrs_top = LogisticRegression(solver='lbfgs', C=1)\n",
      "models_top = lrs_top.fit(xs_top, y_train)\n",
      "selected = selector.get_support(indices=True)\n",
      "lrs_top.score(xs_test[selected], y_test)\n",
      "fpr_alls, tpr_alls, _ = roc_curve(y_test, lrs_top.predict_proba(xs_test[selected])[:,1])\n",
      "18/203: lrs_top.score(twitter_test, twitter_y)\n",
      "18/204: lrs_top.score(twitter_test[selected], twitter_y)\n",
      "18/205:\n",
      "selector = SelectKBest(chi2, k=9000).fit(x_train, y_train)\n",
      "x_top = selector.transform(x_train)\n",
      "lr_top = LogisticRegression(solver='lbfgs', C=1)\n",
      "model_top = lr_top.fit(x_top, y_train)\n",
      "selected_top = selector.get_support(indices=True)\n",
      "lr_top.score(x_test[selected_top], y_test)\n",
      "18/206: lr_top.score(twitter_test[selected_top], twitter_y)\n",
      "18/207:\n",
      "twitter_train_y = open(\"twitter5000_classes_5.txt\", \"r\")\n",
      "twitter_train_y = twitter_train_y.readlines()\n",
      "twitter_train_ys = []\n",
      "count = 0\n",
      "for x in twitter_train_y:\n",
      "    count += 1\n",
      "    # fixing the dataset errors\n",
      "    if count == 4290 or count == 5182:\n",
      "        x = 0\n",
      "    twitter_train_ys.append(int(x))\n",
      "18/208:\n",
      "lr_twitter = LogisticRegression(solver='lbfgs', C=1)\n",
      "model_twitter = lr_twitter.fit(twitter_train, twitter_train_y)\n",
      "lr_twitter.score(twitter_train, twitter_train_y)\n",
      "18/209: twitter_train_y.shape\n",
      "18/210: lr_top.score(x_test[selected_top], y_test)\n",
      "18/211: lr_twitter.score(twitter_test, twitter_y)\n",
      "18/212:\n",
      "lr_twitter = LogisticRegression(solver='lbfgs', C=1)\n",
      "model_twitter = lr_twitter.fit(twitter_train, twitter_train_ys)\n",
      "lr_twitter.score(twitter_train, twitter_train_y)\n",
      "18/213:\n",
      "lr_twitter = LogisticRegression(solver='lbfgs', C=1)\n",
      "model_twitter = lr_twitter.fit(twitter_train, twitter_train_ys)\n",
      "lr_twitter.score(twitter_train, twitter_train_ys)\n",
      "18/214: lr_top.score(twitter_test[selected_top], twitter_ys)\n",
      "18/215:\n",
      "lr_twitter = LogisticRegression(solver='lbfgs', C=1)\n",
      "model_twitter = lr_twitter.fit(twitter_train[:4500], twitter_train_ys[:4500])\n",
      "lr_twitter.score(twitter_train[:4500], twitter_train_ys[:4500])\n",
      "18/216: lr_twitter.score(twitter_train[4500:], twitter_train_ys[4500:])\n",
      "18/217: from sklearn.metrics import confusion_matrix\n",
      "18/218:\n",
      "#Prepare the relevant testing and training data\n",
      "\n",
      "testReviewsFile = open(\"test.txt\")\n",
      "testReviews = testReviewsFile.readlines()\n",
      "testReviews = [a.strip() for a in testReviews]\n",
      "\n",
      "trainFile = open(\"train_out_vocab_5.txt\")\n",
      "trainWords = trainFile.readlines()\n",
      "trainWords = [a.strip() for a in trainWords]\n",
      "\n",
      "\n",
      "x1 = \"train_bag_of_words_5.csv\"\n",
      "x_train = pd.read_csv(x1, names=trainWords)\n",
      "y1 = open(\"train_classes_5.txt\", \"r\")\n",
      "y1 = y1.readlines()\n",
      "y_train = [int(a) for a in y1]\n",
      "\n",
      "x2 = \"test_bag_of_words_0.csv\"\n",
      "x_test = pd.read_csv(x2, names=trainWords)\n",
      "y2 = open(\"test_classes_0.txt\", \"r\")\n",
      "y2 = y2.readlines()\n",
      "y_test = [int(a) for a in y2]\n",
      "18/219:\n",
      "#Finding the top elements in logistic regression\n",
      "lr = LogisticRegression(solver='lbfgs', C=0.25)\n",
      "model = lr.fit(x_train_orig, y_train)\n",
      "weights = model.coef_[0]\n",
      "topTwenty = np.argpartition(weights, -20)[-20:]\n",
      "topTwentyWords = [x_train.columns.values[a] for a in topTwenty]\n",
      "18/220:\n",
      "# Find what kinds of reviews are misclassified with logistic regression\n",
      "logisticModelAnswers = model.predict(x_test)\n",
      "logisticModelProbabilities = model.predict_proba(x_test)[:,1]\n",
      "#0 if predicted positive and is positive or if predicted negative and is negative. \n",
      "correct = [1 if a == b else 0 for (a, b) in zip(logisticModelAnswers, y_train)]\n",
      "for index, element in enumerate(correct):\n",
      "    if element == 1:\n",
      "        logisticModelProbabilities[index] = 10\n",
      "#1 if predicted negative and is positive, 1 if predicted positive and is negative, -1 if predicted negative and is positive\n",
      "misCalculated = [a - b for (a, b) in zip(logisticModelAnswers, y_train)]\n",
      "degreeWrong = [a - b for (a, b) in zip(logisticModelProbabilities, y_train)]\n",
      "reallyWrong = [(a,b) for (a,b) in enumerate(degreeWrong) if abs(b) >= .8 and b <= abs(1)]\n",
      "\n",
      "#Count the number of top words in the logistic regression ones that were really misclassified. \n",
      "predictedPositiveButNegativeIndicies = [int(a) for (a,b) in reallyWrong if b > 0]\n",
      "\n",
      "count = 0\n",
      "for index in predictedPositiveButNegativeIndicies:\n",
      "    for word in topTwentyWords:\n",
      "        if word in testReviews[index]:\n",
      "            count += 1\n",
      "print(count)\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "#print(misCalculated)\n",
      "18/221:\n",
      "x_test_orig = pd.read_csv(\"test__bag_of_words_0.csv\", header=None)\n",
      "x_test_orig.shape\n",
      "18/222:\n",
      "# Find what kinds of reviews are misclassified with logistic regression\n",
      "logisticModelAnswers = model.predict(x_test_orig)\n",
      "logisticModelProbabilities = model.predict_proba(x_test_orig)[:,1]\n",
      "#0 if predicted positive and is positive or if predicted negative and is negative. \n",
      "correct = [1 if a == b else 0 for (a, b) in zip(logisticModelAnswers, y_train)]\n",
      "for index, element in enumerate(correct):\n",
      "    if element == 1:\n",
      "        logisticModelProbabilities[index] = 10\n",
      "#1 if predicted negative and is positive, 1 if predicted positive and is negative, -1 if predicted negative and is positive\n",
      "misCalculated = [a - b for (a, b) in zip(logisticModelAnswers, y_train)]\n",
      "degreeWrong = [a - b for (a, b) in zip(logisticModelProbabilities, y_train)]\n",
      "reallyWrong = [(a,b) for (a,b) in enumerate(degreeWrong) if abs(b) >= .8 and b <= abs(1)]\n",
      "\n",
      "#Count the number of top words in the logistic regression ones that were really misclassified. \n",
      "predictedPositiveButNegativeIndicies = [int(a) for (a,b) in reallyWrong if b > 0]\n",
      "\n",
      "count = 0\n",
      "for index in predictedPositiveButNegativeIndicies:\n",
      "    for word in topTwentyWords:\n",
      "        if word in testReviews[index]:\n",
      "            count += 1\n",
      "print(count)\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "#print(misCalculated)\n",
      "18/223:\n",
      "testReviewsFile = open(\"test.txt\")\n",
      "testReviews = testReviewsFile.readlines()\n",
      "testReviews = [a.strip() for a in testReviews]\n",
      "18/224:\n",
      "# Find what kinds of reviews are misclassified with logistic regression\n",
      "logisticModelAnswers = model.predict(x_test_orig)\n",
      "logisticModelProbabilities = model.predict_proba(x_test_orig)[:,1]\n",
      "#0 if predicted positive and is positive or if predicted negative and is negative. \n",
      "correct = [1 if a == b else 0 for (a, b) in zip(logisticModelAnswers, y_train)]\n",
      "for index, element in enumerate(correct):\n",
      "    if element == 1:\n",
      "        logisticModelProbabilities[index] = 10\n",
      "#1 if predicted negative and is positive, 1 if predicted positive and is negative, -1 if predicted negative and is positive\n",
      "misCalculated = [a - b for (a, b) in zip(logisticModelAnswers, y_train)]\n",
      "degreeWrong = [a - b for (a, b) in zip(logisticModelProbabilities, y_train)]\n",
      "reallyWrong = [(a,b) for (a,b) in enumerate(degreeWrong) if abs(b) >= .8 and b <= abs(1)]\n",
      "\n",
      "#Count the number of top words in the logistic regression ones that were really misclassified. \n",
      "predictedPositiveButNegativeIndicies = [int(a) for (a,b) in reallyWrong if b > 0]\n",
      "\n",
      "count = 0\n",
      "for index in predictedPositiveButNegativeIndicies:\n",
      "    for word in topTwentyWords:\n",
      "        if word in testReviews[index]:\n",
      "            count += 1\n",
      "print(count)\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "#print(misCalculated)\n",
      "18/225:\n",
      "testReviewsFile = open(\"test.txt\")\n",
      "testReviews = testReviewsFile.readlines()\n",
      "testReviews = [a.strip() for a in testReviews]]\n",
      "len(testReviews)\n",
      "18/226:\n",
      "testReviewsFile = open(\"data/test.txt\")\n",
      "testReviews = testReviewsFile.readlines()\n",
      "testReviews = [a.strip() for a in testReviews]]\n",
      "len(testReviews)\n",
      "18/227:\n",
      "testReviewsFile = open(\"test.txt\")\n",
      "testReviews = testReviewsFile.readlines()\n",
      "testReviews = [a.strip() for a in testReviews]]\n",
      "len(testReviews)\n",
      "18/228:\n",
      "testReviewsFile = open(\"test.txt\")\n",
      "testReviews = testReviewsFile.readlines()\n",
      "testReviews = [a.strip() for a in testReviews]\n",
      "len(testReviews)\n",
      "18/229:\n",
      "# Find what kinds of reviews are misclassified with logistic regression\n",
      "logisticModelAnswers = model.predict(x_test_orig)\n",
      "logisticModelProbabilities = model.predict_proba(x_test_orig)[:,1]\n",
      "#0 if predicted positive and is positive or if predicted negative and is negative. \n",
      "correct = [1 if a == b else 0 for (a, b) in zip(logisticModelAnswers, y_train)]\n",
      "for index, element in enumerate(correct):\n",
      "    if element == 1:\n",
      "        logisticModelProbabilities[index] = 10\n",
      "#1 if predicted negative and is positive, 1 if predicted positive and is negative, -1 if predicted negative and is positive\n",
      "misCalculated = [a - b for (a, b) in zip(logisticModelAnswers, y_train)]\n",
      "degreeWrong = [a - b for (a, b) in zip(logisticModelProbabilities, y_train)]\n",
      "reallyWrong = [(a,b) for (a,b) in enumerate(degreeWrong) if abs(b) >= .8 and b <= abs(1)]\n",
      "\n",
      "#Count the number of top words in the logistic regression ones that were really misclassified. \n",
      "predictedPositiveButNegativeIndicies = [int(a) for (a,b) in reallyWrong if b > 0]\n",
      "\n",
      "count = 0\n",
      "for index in predictedPositiveButNegativeIndicies:\n",
      "    for word in topTwentyWords:\n",
      "        if word in testReviews[index]:\n",
      "            count += 1\n",
      "print(count)\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "#print(misCalculated)\n",
      "18/230:\n",
      "# Find what kinds of reviews are misclassified with logistic regression\n",
      "logisticModelAnswers = model.predict(x_test_orig)\n",
      "logisticModelProbabilities = model.predict_proba(x_test_orig)[:,1]\n",
      "# 0 if predicted positive and is positive or if predicted negative and is negative. \n",
      "correct = [1 if a == b else 0 for (a, b) in zip(logisticModelAnswers, y_train)]\n",
      "for index, element in enumerate(correct):\n",
      "    if element == 1:\n",
      "        logisticModelProbabilities[index] = 10\n",
      "# 1 if predicted negative and is positive, 1 if predicted positive and is negative, -1 if predicted negative and is positive\n",
      "misCalculated = [a - b for (a, b) in zip(logisticModelAnswers, y_train)]\n",
      "degreeWrong = [a - b for (a, b) in zip(logisticModelProbabilities, y_train)]\n",
      "reallyWrong = [(a,b) for (a,b) in enumerate(degreeWrong) if abs(b) >= .8 and b <= abs(1)]\n",
      "\n",
      "# Count the number of top words in the logistic regression ones that were really misclassified. \n",
      "predictedPositiveButNegativeIndicies = [int(a) for (a,b) in reallyWrong if b > 0]\n",
      "\n",
      "count = 0\n",
      "for index in predictedPositiveButNegativeIndicies:\n",
      "    for word in topTwentyWords:\n",
      "        if word in testReviews[index]:\n",
      "            count += 1\n",
      "print(count)\n",
      "\n",
      "# print(misCalculated)\n",
      "18/231:\n",
      "testReviewsFile = open(\"test.txt\")\n",
      "testReviews = testReviewsFile.readlines()\n",
      "testReviews = [a.strip() for a in testReviews]\n",
      "testReviews\n",
      "18/232:\n",
      "testReviewsFile = open(\"test.txt\")\n",
      "testReviews = testReviewsFile.readlines()\n",
      "testReviews = [a.strip() for a in testReviews]\n",
      "18/233:\n",
      "# Now look at what kinds of reviews are misclassified by multinominal naive bayes\n",
      "from sklearn.naive_bayes import MultinomialNB\n",
      "clf = MultinomialNB()\n",
      "model = clf.fit(testReviews, trainWords)\n",
      "18/234:\n",
      "# Find what kinds of reviews are misclassified with logistic regression\n",
      "logisticModelAnswers = model.predict(x_test_orig)\n",
      "logisticModelProbabilities = model.predict_proba(x_test_orig)[:,1]\n",
      "# 0 if predicted p\n",
      "ositive and is positive or if predicted negative and is negative. \n",
      "correct = [1 if a == b else 0 for (a, b) in zip(logisticModelAnswers, y_train)]\n",
      "for index, element in enumerate(correct):\n",
      "    if element == 1:\n",
      "        logisticModelProbabilities[index] = 10\n",
      "# 1 if predicted negative and is positive, 1 if predicted positive and is negative, -1 if predicted negative and is positive\n",
      "misCalculated = [a - b for (a, b) in zip(logisticModelAnswers, y_train)]\n",
      "degreeWrong = [a - b for (a, b) in zip(logisticModelProbabilities, y_train)]\n",
      "reallyWrong = [(a,b) for (a,b) in enumerate(degreeWrong) if abs(b) >= .8 and b <= abs(1)]\n",
      "\n",
      "# Count the number of top words in the logistic regression ones that were really misclassified. \n",
      "predictedPositiveButNegativeIndicies = [int(a) for (a,b) in reallyWrong if b > 0]\n",
      "\n",
      "count = 0\n",
      "for index in predictedPositiveButNegativeIndicies:\n",
      "    for word in topTwentyWords:\n",
      "        print word, testReviews[index]\n",
      "        if word in testReviews[index]:\n",
      "            count += 1\n",
      "print(count)\n",
      "\n",
      "# print(misCalculated)\n",
      "18/235:\n",
      "# Find what kinds of reviews are misclassified with logistic regression\n",
      "logisticModelAnswers = model.predict(x_test_orig)\n",
      "logisticModelProbabilities = model.predict_proba(x_test_orig)[:,1]\n",
      "# 0 if predicted positive and is positive or if predicted negative and is negative. \n",
      "correct = [1 if a == b else 0 for (a, b) in zip(logisticModelAnswers, y_train)]\n",
      "for index, element in enumerate(correct):\n",
      "    if element == 1:\n",
      "        logisticModelProbabilities[index] = 10\n",
      "# 1 if predicted negative and is positive, 1 if predicted positive and is negative, -1 if predicted negative and is positive\n",
      "misCalculated = [a - b for (a, b) in zip(logisticModelAnswers, y_train)]\n",
      "degreeWrong = [a - b for (a, b) in zip(logisticModelProbabilities, y_train)]\n",
      "reallyWrong = [(a,b) for (a,b) in enumerate(degreeWrong) if abs(b) >= .8 and b <= abs(1)]\n",
      "\n",
      "# Count the number of top words in the logistic regression ones that were really misclassified. \n",
      "predictedPositiveButNegativeIndicies = [int(a) for (a,b) in reallyWrong if b > 0]\n",
      "\n",
      "count = 0\n",
      "for index in predictedPositiveButNegativeIndicies:\n",
      "    for word in topTwentyWords:\n",
      "        print word, testReviews[index]\n",
      "        if word in testReviews[index]:\n",
      "            count += 1\n",
      "print(count)\n",
      "\n",
      "# print(misCalculated)\n",
      "18/236:\n",
      "# Find what kinds of reviews are misclassified with logistic regression\n",
      "logisticModelAnswers = model.predict(x_test_orig)\n",
      "logisticModelProbabilities = model.predict_proba(x_test_orig)[:,1]\n",
      "# 0 if predicted positive and is positive or if predicted negative and is negative. \n",
      "correct = [1 if a == b else 0 for (a, b) in zip(logisticModelAnswers, y_train)]\n",
      "for index, element in enumerate(correct):\n",
      "    if element == 1:\n",
      "        logisticModelProbabilities[index] = 10\n",
      "# 1 if predicted negative and is positive, 1 if predicted positive and is negative, -1 if predicted negative and is positive\n",
      "misCalculated = [a - b for (a, b) in zip(logisticModelAnswers, y_train)]\n",
      "degreeWrong = [a - b for (a, b) in zip(logisticModelProbabilities, y_train)]\n",
      "reallyWrong = [(a,b) for (a,b) in enumerate(degreeWrong) if abs(b) >= .8 and b <= abs(1)]\n",
      "\n",
      "# Count the number of top words in the logistic regression ones that were really misclassified. \n",
      "predictedPositiveButNegativeIndicies = [int(a) for (a,b) in reallyWrong if b > 0]\n",
      "\n",
      "count = 0\n",
      "for index in predictedPositiveButNegativeIndicies:\n",
      "    for word in topTwentyWords:\n",
      "        if str(word) in testReviews[index]:\n",
      "            count += 1\n",
      "print(count)\n",
      "\n",
      "# print(misCalculated)\n",
      "18/237: tn, fp, fn, tp = confusion_matrix(y_test, lr_top.predict_proba(x_test[selected])[:,1]).ravel()\n",
      "18/238: tn, fp, fn, tp = confusion_matrix(y_test, lr_top.predict(x_test[selected])[:,1]).ravel()\n",
      "18/239: tn, fp, fn, tp = confusion_matrix(y_test, lr_top.predict(x_test[selected])).ravel()\n",
      "18/240: tn, fp, fn, tp\n",
      "18/241: (tn + tp)/(tn + tp + fp + fn)\n",
      "18/242: 1.0*(tn + tp)/(tn + tp + fp + fn)\n",
      "18/243: float(tn + tp)/(tn + tp + fp + fn)\n",
      "18/244:\n",
      "topk = []\n",
      "topktest = []\n",
      "ks = [20,50,100,150,200,250,300,330,335,340,350,400,450,500,540]\n",
      "for i in ks:\n",
      "    selector = SelectKBest(chi2, k=i).fit(x_train_orig, y_train)\n",
      "    x_top = selector.transform(x_train)\n",
      "    mnb = MultinomialNB()\n",
      "    model_top = mnb.fit(x_top, y_train)\n",
      "    topk.append(mnb.score(x_top, y_train))\n",
      "    selected = selector.get_support(indices=True)\n",
      "    topktest.append(mnb.score(x_test[selected], y_test))\n",
      "\n",
      "plot.figure()\n",
      "plot.plot(ks, topk)\n",
      "plot.plot(ks, topktest)\n",
      "plot.show()\n",
      "\n",
      "print(ks[np.argmax(topktest)], np.max(topktest))\n",
      "18/245:\n",
      "topk = []\n",
      "topktest = []\n",
      "ks = [20,50,100,150,200,250,300,330,335,340,350,400,450,500,540]\n",
      "for i in ks:\n",
      "    selector = SelectKBest(chi2, k=i).fit(x_train_orig, y_train)\n",
      "    x_top = selector.transform(x_train_orig)\n",
      "    mnb = MultinomialNB()\n",
      "    model_top = mnb.fit(x_top, y_train)\n",
      "    topk.append(mnb.score(x_top, y_train))\n",
      "    selected = selector.get_support(indices=True)\n",
      "    topktest.append(mnb.score(x_test[selected], y_test))\n",
      "\n",
      "plot.figure()\n",
      "plot.plot(ks, topk)\n",
      "plot.plot(ks, topktest)\n",
      "plot.show()\n",
      "\n",
      "print(ks[np.argmax(topktest)], np.max(topktest))\n",
      "18/246:\n",
      "from sklearn.ensemble import ExtraTreesClassifier\n",
      "import operator\n",
      "import pandas\n",
      "\n",
      "trainFile = open(\"train_out_vocab_5.txt\")\n",
      "trainWords = list(trainFile)\n",
      "for i, w in enumerate(trainWords):\n",
      "    trainWords[i] = w.replace(\"\\n\", \"\")\n",
      "print(trainWords)\n",
      "\n",
      "trainBagOfWords = pandas.read_csv(\"train_out_bag_of_words_5.csv\", names=trainWords)\n",
      "trainLabels = pandas.read_csv(\"train_out_classes_5.txt\", names=[\"class\"])\n",
      "testBagOfWords = pandas.read_csv(\"test_out_bag_of_words_0.csv\", names=trainWords)\n",
      "testLabels = pandas.read_csv(\"test_out_classes_0.txt\", names=[\"class\"])\n",
      "#print(bagOfWords)\n",
      "#print(labels)\n",
      "trainArrayBag = trainBagOfWords.values\n",
      "trainArrayLabels = trainLabels.values[:, 0]\n",
      "testArrayBag = testBagOfWords.values\n",
      "testArrayLabels = testLabels.values[:, 0]\n",
      "#print(arrayBag)\n",
      "#print(arrayLabels)\n",
      "model = ExtraTreesClassifier()\n",
      "fit = model.fit(trainArrayBag, trainArrayLabels)\n",
      "#dictData = dict(enumerate(model.feature_importances_))\n",
      "#sortedDict = sorted(dictData.items(), key=operator.itemgetter(1))\n",
      "#print(sortedDict)\n",
      "print(fit.score(trainArrayBag, trainArrayLabels))\n",
      "print(fit.score(testArrayBag, testArrayLabels))\n",
      "\n",
      "sortedDict = sorted(zip(trainWords, model.feature_importances_), key=operator.itemgetter(1))\n",
      "print(sortedDict)\n",
      "18/247:\n",
      "from sklearn.ensemble import ExtraTreesClassifier\n",
      "import operator\n",
      "import pandas\n",
      "\n",
      "trainFile = open(\"train_out_vocab_5.txt\")\n",
      "trainWords = list(trainFile)\n",
      "for i, w in enumerate(trainWords):\n",
      "    trainWords[i] = w.replace(\"\\n\", \"\")\n",
      "print(trainWords)\n",
      "\n",
      "trainBagOfWords = pandas.read_csv(\"train_out_bag_of_words_5.csv\", names=trainWords)\n",
      "trainLabels = pandas.read_csv(\"train_out_classes_5.txt\", names=[\"class\"])\n",
      "testBagOfWords = pandas.read_csv(\"test_out_bag_of_words_0.csv\", names=trainWords)\n",
      "testLabels = pandas.read_csv(\"test_out_classes_0.txt\", names=[\"class\"])\n",
      "#print(bagOfWords)\n",
      "#print(labels)\n",
      "trainArrayBag = trainBagOfWords.values\n",
      "trainArrayLabels = trainLabels.values[:, 0]\n",
      "testArrayBag = testBagOfWords.values\n",
      "testArrayLabels = testLabels.values[:, 0]\n",
      "#print(arrayBag)\n",
      "#print(arrayLabels)\n",
      "model = ExtraTreesClassifier()\n",
      "fit = model.fit(trainArrayBag, trainArrayLabels)\n",
      "#dictData = dict(enumerate(model.feature_importances_))\n",
      "#sortedDict = sorted(dictData.items(), key=operator.itemgetter(1))\n",
      "#print(sortedDict)\n",
      "print(fit.score(trainArrayBag, trainArrayLabels))\n",
      "print(fit.score(testArrayBag, testArrayLabels))\n",
      "\n",
      "sortedDict = sorted(zip(trainWords, model.feature_importances_), key=operator.itemgetter(1))\n",
      "print(sortedDict)\n",
      "18/248:\n",
      "from sklearn.ensemble import ExtraTreesClassifier\n",
      "import operator\n",
      "import pandas\n",
      "\n",
      "trainFile = open(\"train_vocab_5.txt\")\n",
      "trainWords = list(trainFile)\n",
      "for i, w in enumerate(trainWords):\n",
      "    trainWords[i] = w.replace(\"\\n\", \"\")\n",
      "print(trainWords)\n",
      "\n",
      "trainBagOfWords = pandas.read_csv(\"train_bag_of_words_5.csv\", names=trainWords)\n",
      "trainLabels = pandas.read_csv(\"train_classes_5.txt\", names=[\"class\"])\n",
      "testBagOfWords = pandas.read_csv(\"test_bag_of_words_0.csv\", names=trainWords)\n",
      "testLabels = pandas.read_csv(\"test_classes_0.txt\", names=[\"class\"])\n",
      "#print(bagOfWords)\n",
      "#print(labels)\n",
      "trainArrayBag = trainBagOfWords.values\n",
      "trainArrayLabels = trainLabels.values[:, 0]\n",
      "testArrayBag = testBagOfWords.values\n",
      "testArrayLabels = testLabels.values[:, 0]\n",
      "#print(arrayBag)\n",
      "#print(arrayLabels)\n",
      "model = ExtraTreesClassifier()\n",
      "fit = model.fit(trainArrayBag, trainArrayLabels)\n",
      "#dictData = dict(enumerate(model.feature_importances_))\n",
      "#sortedDict = sorted(dictData.items(), key=operator.itemgetter(1))\n",
      "#print(sortedDict)\n",
      "print(fit.score(trainArrayBag, trainArrayLabels))\n",
      "print(fit.score(testArrayBag, testArrayLabels))\n",
      "\n",
      "sortedDict = sorted(zip(trainWords, model.feature_importances_), key=operator.itemgetter(1))\n",
      "print(sortedDict)\n",
      "18/249:\n",
      "from sklearn.ensemble import ExtraTreesClassifier\n",
      "import operator\n",
      "import pandas\n",
      "\n",
      "trainFile = open(\"train_vocab_5.txt\")\n",
      "trainWords = list(trainFile)\n",
      "for i, w in enumerate(trainWords):\n",
      "    trainWords[i] = w.replace(\"\\n\", \"\")\n",
      "print(trainWords)\n",
      "\n",
      "trainBagOfWords = pandas.read_csv(\"train_bag_of_words_5.csv\", names=trainWords)\n",
      "trainLabels = pandas.read_csv(\"train_classes_5.txt\", names=[\"class\"])\n",
      "testBagOfWords = pandas.read_csv(\"test__bag_of_words_0.csv\", names=trainWords)\n",
      "testLabels = pandas.read_csv(\"test__classes_0.txt\", names=[\"class\"])\n",
      "#print(bagOfWords)\n",
      "#print(labels)\n",
      "trainArrayBag = trainBagOfWords.values\n",
      "trainArrayLabels = trainLabels.values[:, 0]\n",
      "testArrayBag = testBagOfWords.values\n",
      "testArrayLabels = testLabels.values[:, 0]\n",
      "#print(arrayBag)\n",
      "#print(arrayLabels)\n",
      "model = ExtraTreesClassifier()\n",
      "fit = model.fit(trainArrayBag, trainArrayLabels)\n",
      "#dictData = dict(enumerate(model.feature_importances_))\n",
      "#sortedDict = sorted(dictData.items(), key=operator.itemgetter(1))\n",
      "#print(sortedDict)\n",
      "print(fit.score(trainArrayBag, trainArrayLabels))\n",
      "print(fit.score(testArrayBag, testArrayLabels))\n",
      "\n",
      "sortedDict = sorted(zip(trainWords, model.feature_importances_), key=operator.itemgetter(1))\n",
      "print(sortedDict)\n",
      "18/250:\n",
      "selector = SelectKBest(chi2, k=9000).fit(xs_train, y_train)\n",
      "xs_top = selector.transform(xs_train)\n",
      "lrs_top = LogisticRegression(solver='lbfgs', C=1)\n",
      "models_top = lrs_top.fit(xs_top, y_train)\n",
      "selected = selector.get_support(indices=True)\n",
      "lrs_top.score(xs_test[selected], y_test)\n",
      "18/251:\n",
      "xs = \"tempstop_bag_of_words_5.csv\"\n",
      "xs_train = pd.read_csv(xs, header=None)\n",
      "xs_train.shape\n",
      "18/252:\n",
      "xst = \"tempstop_bag_of_words_0.csv\"\n",
      "xs_test = pd.read_csv(xst, header=None)\n",
      "xs_test.shape\n",
      "18/253:\n",
      "selector = SelectKBest(chi2, k=9000).fit(xs_train, y_train)\n",
      "xs_top = selector.transform(xs_train)\n",
      "lrs_top = LogisticRegression(solver='lbfgs', C=1)\n",
      "models_top = lrs_top.fit(xs_top, y_train)\n",
      "selected = selector.get_support(indices=True)\n",
      "lrs_top.score(xs_test[selected], y_test)\n",
      "18/254: fpr_alls, tpr_alls, _ = roc_curve(y_test, lrs_top.predict_proba(xs_test[selected])[:,1])\n",
      "18/255:\n",
      "plot.figure()\n",
      "plot.plot(fpr, tpr, label=\"limited stop words\") # logistic regression, top K + 2-word sequences\n",
      "plot.plot(fpr_alls, tpr_alls, label=\"all stop words\") # logistic regression, top K + all stop words + 2-word sequences\n",
      "plot.legend()\n",
      "plot.xlabel(\"FPR\")\n",
      "plot.ylabel(\"TPR\")\n",
      "18/256:\n",
      "trainWordsT = open(\"twitter_vocab_5.txt\", \"r\")\n",
      "trainWordsT = trainWordsT.readlines()\n",
      "coefsT = model_twitter.coef_[0]\n",
      "tuplesT = sorted(zip(trainWordsT, coefsT), key=(lambda x: x[1]))\n",
      "print [(i[0], i[1]) for i in tuples[:10]]\n",
      "print [(i[0], i[1]) for i in tuples[-10:]]\n",
      "18/257:\n",
      "trainWordsT = open(\"twitter5000_vocab_5.txt\", \"r\")\n",
      "trainWordsT = trainWordsT.readlines()\n",
      "coefsT = model_twitter.coef_[0]\n",
      "tuplesT = sorted(zip(trainWordsT, coefsT), key=(lambda x: x[1]))\n",
      "print [(i[0], i[1]) for i in tuples[:10]]\n",
      "print [(i[0], i[1]) for i in tuples[-10:]]\n",
      "18/258: from sklearn.metrics import roc_auc_score\n",
      "18/259: roc_auc_score(y_test, lr_top.predict_proba(x_test[selected])[:,1])\n",
      "18/260: fpr, tpr, _ = roc_curve(y_test, lr_top.predict_proba(x_test[selected])[:,1])\n",
      "18/261: plot.plot(fpr, tpr)\n",
      "18/262:\n",
      "topk = []\n",
      "topktest = []\n",
      "ks = [100,500,1000,1500,2000,2500,3000,3500,4000]\n",
      "for i in ks:\n",
      "    selector = SelectKBest(chi2, k=i).fit(x_train, y_train)\n",
      "    x_top = selector.transform(x_train)\n",
      "    lr_top = LogisticRegression(solver='lbfgs', C=1)\n",
      "    model_top = lr_top.fit(x_top, y_train)\n",
      "    topk.append(lr_top.score(x_top, y_train))\n",
      "    selected = selector.get_support(indices=True)\n",
      "    topktest.append(lr_top.score(x_test[selected], y_test))\n",
      "18/263:\n",
      "plot.figure()\n",
      "plot.plot(ks, topk)\n",
      "plot.plot(ks, topktest)\n",
      "print ks[np.argmax(topktest)], np.max(topktest)\n",
      "18/264:\n",
      "topk = []\n",
      "topktest = []\n",
      "ks = [1000,5000,7500,8500,9000,9500,10000,12500]\n",
      "for i in ks:\n",
      "    selector = SelectKBest(chi2, k=i).fit(x_train, y_train)\n",
      "    x_top = selector.transform(x_train)\n",
      "    lr_top = LogisticRegression(solver='lbfgs', C=1)\n",
      "    model_top = lr_top.fit(x_top, y_train)\n",
      "    topk.append(lr_top.score(x_top, y_train))\n",
      "    selected = selector.get_support(indices=True)\n",
      "    topktest.append(lr_top.score(x_test[selected], y_test))\n",
      "18/265:\n",
      "plot.figure()\n",
      "plot.plot(ks, topk)\n",
      "plot.plot(ks, topktest)\n",
      "print ks[np.argmax(topktest)], np.max(topktest)\n",
      "18/266: fpr, tpr, _ = roc_curve(y_test, lr_top.predict_proba(x_test[selected])[:,1])\n",
      "18/267: plot.plot(fpr, tpr)\n",
      "18/268: from sklearn.metrics import roc_auc_score\n",
      "18/269: roc_auc_score(y_test, lr_top.predict_proba(x_test[selected])[:,1])\n",
      "18/270:\n",
      "# original preprocessor\n",
      "x = \"train_bag_of_words_5.csv\"\n",
      "x_train_orig = pd.read_csv(x, header=None)\n",
      "x_train_orig.shape\n",
      "18/271:\n",
      "y = open(\"train_classes_5.txt\", \"r\")\n",
      "y = y.readlines()\n",
      "y_train = [int(x) for x in y]\n",
      "len(y_train)\n",
      "18/272: mnb.fit(x_train_orig, y_train)\n",
      "18/273: mnb.score(x_train_orig, y_train)\n",
      "18/274:\n",
      "topk = []\n",
      "topktest = []\n",
      "ks = [20,50,100,150,200,250,300,330,335,340,350,400,450,500,540]\n",
      "for i in ks:\n",
      "    selector = SelectKBest(chi2, k=i).fit(x_train_orig, y_train)\n",
      "    x_top = selector.transform(x_train_orig)\n",
      "    mnb = MultinomialNB()\n",
      "    model_top = mnb.fit(x_top, y_train)\n",
      "    topk.append(mnb.score(x_top, y_train))\n",
      "    selected = selector.get_support(indices=True)\n",
      "    topktest.append(mnb.score(x_test[selected], y_test))\n",
      "\n",
      "plot.figure()\n",
      "plot.plot(ks, topk)\n",
      "plot.plot(ks, topktest)\n",
      "plot.show()\n",
      "\n",
      "print(ks[np.argmax(topktest)], np.max(topktest))\n",
      "18/275:\n",
      "x2 = \"test__bag_of_words_0.csv\"\n",
      "x_test = pd.read_csv(x2, header=None)\n",
      "x_test.shape\n",
      "18/276:\n",
      "topk = []\n",
      "topktest = []\n",
      "ks = [20,50,100,150,200,250,300,330,335,340,350,400,450,500,540]\n",
      "for i in ks:\n",
      "    selector = SelectKBest(chi2, k=i).fit(x_train_orig, y_train)\n",
      "    x_top = selector.transform(x_train_orig)\n",
      "    mnb = MultinomialNB()\n",
      "    model_top = mnb.fit(x_top, y_train)\n",
      "    topk.append(mnb.score(x_top, y_train))\n",
      "    selected = selector.get_support(indices=True)\n",
      "    topktest.append(mnb.score(x_test[selected], y_test))\n",
      "\n",
      "plot.figure()\n",
      "plot.plot(ks, topk)\n",
      "plot.plot(ks, topktest)\n",
      "plot.show()\n",
      "\n",
      "print(ks[np.argmax(topktest)], np.max(topktest))\n",
      "18/277:\n",
      "y2 = open(\"test_classes_5.txt\", \"r\")\n",
      "y2 = y2.readlines()\n",
      "y_test = [int(x) for x in y2]\n",
      "len(y_test)\n",
      "18/278:\n",
      "topk = []\n",
      "topktest = []\n",
      "ks = [20,50,100,150,200,250,300,330,335,340,350,400,450,500,540]\n",
      "for i in ks:\n",
      "    selector = SelectKBest(chi2, k=i).fit(x_train_orig, y_train)\n",
      "    x_top = selector.transform(x_train_orig)\n",
      "    mnb = MultinomialNB()\n",
      "    model_top = mnb.fit(x_top, y_train)\n",
      "    topk.append(mnb.score(x_top, y_train))\n",
      "    selected = selector.get_support(indices=True)\n",
      "    topktest.append(mnb.score(x_test[selected], y_test))\n",
      "\n",
      "plot.figure()\n",
      "plot.plot(ks, topk)\n",
      "plot.plot(ks, topktest)\n",
      "plot.show()\n",
      "\n",
      "print(ks[np.argmax(topktest)], np.max(topktest))\n",
      "18/279: mnb.score(x_test, y_test)\n",
      "18/280:\n",
      "x2 = \"test__bag_of_words_0.csv\"\n",
      "x_test = pd.read_csv(x2, header=None)\n",
      "x_test.shape\n",
      "18/281:\n",
      "y2 = open(\"test_classes_5.txt\", \"r\")\n",
      "y2 = y2.readlines()\n",
      "y_test = [int(x) for x in y2]\n",
      "len(y_test)\n",
      "18/282: mnb.score(x_test, y_test)\n",
      "18/283:\n",
      "# original preprocessor\n",
      "x = \"train_bag_of_words_5.csv\"\n",
      "x_train_orig = pd.read_csv(x, header=None)\n",
      "x_train_orig.shape\n",
      "18/284:\n",
      "y = open(\"train_classes_5.txt\", \"r\")\n",
      "y = y.readlines()\n",
      "y_train = [int(x) for x in y]\n",
      "len(y_train)\n",
      "18/285: mnb.fit(x_train_orig, y_train)\n",
      "18/286: mnb.score(x_train_orig, y_train)\n",
      "18/287:\n",
      "x2 = \"test__bag_of_words_0.csv\"\n",
      "x_test = pd.read_csv(x2, header=None)\n",
      "x_test.shape\n",
      "18/288:\n",
      "y2 = open(\"test_classes_5.txt\", \"r\")\n",
      "y2 = y2.readlines()\n",
      "y_test = [int(x) for x in y2]\n",
      "len(y_test)\n",
      "18/289: mnb.score(x_test, y_test)\n",
      "18/290:\n",
      "import sklearn\n",
      "from sklearn.naive_bayes import MultinomialNB, ComplementNB\n",
      "import pandas as pd\n",
      "import numpy as np\n",
      "from sklearn.metrics import roc_auc_score\n",
      "18/291: roc_auc_score(y_test, mnb.predict_proba(x_test)[:,1])\n",
      "18/292:\n",
      "import sklearn\n",
      "from sklearn.naive_bayes import MultinomialNB, ComplementNB\n",
      "import pandas as pd\n",
      "import numpy as np\n",
      "from sklearn.metrics import roc_auc_score\n",
      "from sklearn.metrics import confusion_matrix\n",
      "18/293:\n",
      "import sklearn\n",
      "from sklearn.naive_bayes import MultinomialNB, ComplementNB\n",
      "import pandas as pd\n",
      "import numpy as np\n",
      "from sklearn.metrics import roc_auc_score, confusion_matrix\n",
      "18/294: tn, fp, fn, tp = confusion_matrix(y_test, lr_top.predict(x_test[selected])).ravel()\n",
      "18/295: tn, fp, fn, tp = confusion_matrix(y_test, mnb.predict(x_test)).ravel()\n",
      "18/296:\n",
      "tn, fp, fn, tp = confusion_matrix(y_test, mnb.predict(x_test)).ravel()\n",
      "tn, fp, fn, tp\n",
      "18/297:\n",
      "def precisionrecall(tn, fp, fn, tp):\n",
      "    return (float(tp)/(tp+fp)), (float(tp)/(tp+fn))\n",
      "18/298: precisionrecall(tn, fp, fn, tp)\n",
      "18/299: cnb.fit(x_train_orig, y_train)\n",
      "18/300: cnb.score(x_train_orig, y_train)\n",
      "18/301: cnb.score(x_test, y_test)\n",
      "18/302: roc_auc_score(y_test, cnb.predict_proba(x_test)[:,1])\n",
      "18/303:\n",
      "def precisionrecall(y_test, y_pred):\n",
      "    tn, fp, fn, tp = confusion_matrix(y_test, y_pred).ravel()\n",
      "    return (float(tp)/(tp+fp)), (float(tp)/(tp+fn))\n",
      "18/304: precisionrecall(tn, fp, fn, tp)\n",
      "18/305: precisionrecall(y_test, mnb.predict(x_test))\n",
      "18/306: precisionrecall(y_test, cnb.predict(x_test))\n",
      "18/307:\n",
      "import sklearn\n",
      "from sklearn.naive_bayes import MultinomialNB, ComplementNB, BernoulliNB\n",
      "import pandas as pd\n",
      "import numpy as np\n",
      "from sklearn.metrics import roc_auc_score, confusion_matrix\n",
      "18/308:\n",
      "bnb = BernoulliNB()\n",
      "bnb.fit(x_train_orig, y_train)\n",
      "bnb.score(x_train_orig, y_train)\n",
      "18/309: bnb.score(x_test, y_test)\n",
      "18/310:\n",
      "import sklearn\n",
      "from sklearn.naive_bayes import MultinomialNB, ComplementNB, BernoulliNB, GaussianNB\n",
      "import pandas as pd\n",
      "import numpy as np\n",
      "from sklearn.metrics import roc_auc_score, confusion_matrix\n",
      "18/311:\n",
      "bnb = GaussianNB()\n",
      "bnb.fit(x_train_orig, y_train)\n",
      "bnb.score(x_train_orig, y_train)\n",
      "18/312: bnb.score(x_test, y_test)\n",
      "18/313:\n",
      "bnb = BernoulliNB()\n",
      "bnb.fit(x_train_orig, y_train)\n",
      "bnb.score(x_train_orig, y_train)\n",
      "18/314: bnb.score(x_test, y_test)\n",
      "18/315:\n",
      "import sklearn\n",
      "from sklearn.naive_bayes import MultinomialNB, ComplementNB, BernoulliNB\n",
      "import pandas as pd\n",
      "import numpy as np\n",
      "from sklearn.metrics import roc_auc_score, confusion_matrix\n",
      "18/316: from sklearn.tree import DecisionTreeClassifier\n",
      "18/317:\n",
      "from sklearn.tree import DecisionTreeClassifier\n",
      "dtc = DecisionTreeClassifier(random_state=0)\n",
      "dtc.fit(x_train_orig, y_train)\n",
      "dtc.score(x_train_orig, y_train)\n",
      "18/318: dtc.score(x_test, y_test)\n",
      "18/319: roc_auc_score(y_test, bnb.predict_proba(x_test)[:,1])\n",
      "18/320: precisionrecall(y_test, bnb.predict(x_test))\n",
      "18/321: roc_auc_score(y_test, dtc.predict_proba(x_test)[:,1])\n",
      "18/322: precisionrecall(y_test, dtc.predict(x_test))\n",
      "18/323: roc_auc_score(y_test, svm.predict_proba(x_test)[:,1])\n",
      "18/324: precisionrecall(y_test, svm.predict(x_test))\n",
      "18/325: roc_auc_score(y_test, svm.decision_function(x_test)[:,1])\n",
      "18/326: roc_auc_score(y_test, svm.decision_function(x_test)\n",
      "18/327: roc_auc_score(y_test, svm.decision_function(x_test))\n",
      "18/328: svm.score(x_test, y_test)\n",
      "18/329:\n",
      "from sklearn.svm import LinearSVC\n",
      "svm = LinearSVC(C=.25)\n",
      "svm.fit(x_train_orig, y_train)\n",
      "svm.score(x_train_orig, y_train)\n",
      "18/330: svm.score(x_test, y_test)\n",
      "18/331: roc_auc_score(y_test, svm.decision_function(x_test))\n",
      "18/332: precisionrecall(y_test, svm.predict(x_test))\n",
      "18/333: lr = LogisticRegression(solver='lbfgs', C=0.25)\n",
      "18/334: model = lr.fit(x_train_orig, y_train)\n",
      "18/335: lr.score(x_train_orig, y_train)\n",
      "18/336: lr.score(x_test, y_test)\n",
      "18/337:\n",
      "trainWords = open(\"train_vocab_5.txt\", \"r\")\n",
      "trainWords = trainWords.readlines()\n",
      "18/338:\n",
      "# seeing the \"best\" features - most predictive words\n",
      "coefs = model.coef_[0]\n",
      "tuples = sorted(zip(trainWords, coefs), key=(lambda x: x[1]))\n",
      "print [i[0] for i in tuples[:10]]\n",
      "print [i[0] for i in tuples[-10:]]\n",
      "18/339:\n",
      "from sklearn.ensemble import ExtraTreesClassifier\n",
      "etc = ExtraTreesClassifier()\n",
      "etc.fit(x_train_orig, y_train)\n",
      "etc.score(x_train_orig, y_train)\n",
      "18/340: etc.score(x_test, y_test)\n",
      "18/341: roc_auc_score(y_test, etc.predict_proba(x_test)[:,1])\n",
      "18/342: precisionrecall(y_test, etc.predict(x_test))\n",
      "18/343: roc_auc_score(y_test, lr.predict_proba(x_test)[:,1])\n",
      "18/344: precisionrecall(y_test, lr.predict(x_test))\n",
      "18/345: # x_twitter\n",
      "18/346:\n",
      "trainWords = open(\"temp2_vocab_5.txt\", \"r\")\n",
      "trainWords = trainWords.readlines()\n",
      "coefs = model_top.coef_[0]\n",
      "tuples = sorted(zip(trainWords, coefs), key=(lambda x: x[1]))\n",
      "print [(i[0], i[1]) for i in tuples[:10]]\n",
      "print [(i[0], i[1]) for i in tuples[-10:]]\n",
      "18/347:\n",
      "topk = []\n",
      "topktest = []\n",
      "ks = [1000,5000,7500,10000,12500]\n",
      "for i in ks:\n",
      "    selector = SelectKBest(chi2, k=i).fit(x_train, y_train)\n",
      "    x_top = selector.transform(x_train)\n",
      "    lr_top = LogisticRegression(solver='lbfgs', C=1)\n",
      "    model_top = lr_top.fit(x_top, y_train)\n",
      "    topk.append(lr_top.score(x_top, y_train))\n",
      "    selected = selector.get_support(indices=True)\n",
      "    topktest.append(lr_top.score(x_test[selected], y_test))\n",
      "18/348:\n",
      "x = \"temp2_bag_of_words_5.csv\"\n",
      "x_train = pd.read_csv(x, header=None)\n",
      "x_train.shape\n",
      "18/349:\n",
      "x2 = \"temp_bag_of_words_0.csv\"\n",
      "x_test = pd.read_csv(x2, header=None)\n",
      "x_test.shape\n",
      "18/350:\n",
      "topk = []\n",
      "topktest = []\n",
      "ks = [1000,5000,7500,10000,12500]\n",
      "for i in ks:\n",
      "    selector = SelectKBest(chi2, k=i).fit(x_train, y_train)\n",
      "    x_top = selector.transform(x_train)\n",
      "    lr_top = LogisticRegression(solver='lbfgs', C=1)\n",
      "    model_top = lr_top.fit(x_top, y_train)\n",
      "    topk.append(lr_top.score(x_top, y_train))\n",
      "    selected = selector.get_support(indices=True)\n",
      "    topktest.append(lr_top.score(x_test[selected], y_test))\n",
      "18/351:\n",
      "trainWords = open(\"temp2_vocab_5.txt\", \"r\")\n",
      "trainWords = trainWords.readlines()\n",
      "coefs = model_top.coef_[0]\n",
      "tuples = sorted(zip(trainWords, coefs), key=(lambda x: x[1]))\n",
      "print [(i[0], i[1]) for i in tuples[:10]]\n",
      "print [(i[0], i[1]) for i in tuples[-10:]]\n",
      "18/352: roc_auc_score(y_test, lr_select.predict_proba(x_topk)[:,1])\n",
      "18/353: model_select = lr_select.fit(x_topk, y_train)\n",
      "18/354: lr_select.score(x_topk, y_train)\n",
      "18/355: lr_select.predict(x_topk)[65]\n",
      "18/356:\n",
      "selected_indices = selector.get_support(indices=True)\n",
      "# for i in selected_indices:\n",
      "#     print trainWords[i]\n",
      "18/357: x_testk = x_test[selected_indices]\n",
      "18/358: lr_select.score(x_testk, y_test)\n",
      "18/359: roc_auc_score(y_test, lr_select.predict_proba(x_topk)[:,1])\n",
      "18/360: roc_auc_score(y_test, lr_select.predict_proba(x_testk)[:,1])\n",
      "18/361: x_testk = x_test[selected_indices]\n",
      "18/362: lr_select.score(x_testk, y_test)\n",
      "18/363:\n",
      "x2 = \"test__bag_of_words_0.csv\"\n",
      "x_test = pd.read_csv(x2, header=None)\n",
      "x_test.shape\n",
      "18/364: x_testk = x_test[selected_indices]\n",
      "18/365: lr_select.score(x_testk, y_test)\n",
      "18/366: etc.score(x_test, y_test)\n",
      "18/367: roc_auc_score(y_test, etc.predict_proba(x_test)[:,1])\n",
      "18/368: precisionrecall(y_test, etc.predict(x_test))\n",
      "18/369:\n",
      "from sklearn.svm import LinearSVC\n",
      "svm = LinearSVC(C=.25)\n",
      "svm.fit(x_train_orig, y_train)\n",
      "svm.score(x_train_orig, y_train)\n",
      "18/370: svm.score(x_test, y_test)\n",
      "18/371: roc_auc_score(y_test, svm.decision_function(x_test))\n",
      "18/372: precisionrecall(y_test, svm.predict(x_test))\n",
      "18/373: from sklearn.linear_model import LogisticRegression\n",
      "18/374: lr = LogisticRegression(solver='lbfgs', C=0.25)\n",
      "18/375: model = lr.fit(x_train_orig, y_train)\n",
      "18/376: lr.score(x_train_orig, y_train)\n",
      "18/377: lr.score(x_test, y_test)\n",
      "18/378: roc_auc_score(y_test, lr.predict_proba(x_test)[:,1])\n",
      "18/379: precisionrecall(y_test, lr.predict(x_test))\n",
      "18/380:\n",
      "# finding the best regularization penalty\n",
      "lr_train = []\n",
      "lr_test = []\n",
      "ws = [0.01, 0.05, 0.1, 0.25, 0.5, 0.75, 0.9, 1]\n",
      "for w in ws:\n",
      "    lr = LogisticRegression(solver='lbfgs', C=w)\n",
      "    lr.fit(x_train_orig, y_train)\n",
      "    lr_train.append(lr.score(x_train_orig, y_train))\n",
      "    lr_test.append(lr.score(x_test, y_test))\n",
      "18/381: from matplotlib import pyplot as plot\n",
      "18/382:\n",
      "plot.figure()\n",
      "plot.plot(ws, lr_train)\n",
      "plot.plot(ws, lr_test)\n",
      "18/383:\n",
      "print lr_train[3]\n",
      "print lr_test[3]\n",
      "18/384:\n",
      "trainWords = open(\"train_vocab_5.txt\", \"r\")\n",
      "trainWords = trainWords.readlines()\n",
      "18/385:\n",
      "# seeing the \"best\" features - most predictive words\n",
      "coefs = model.coef_[0]\n",
      "tuples = sorted(zip(trainWords, coefs), key=(lambda x: x[1]))\n",
      "print [i[0] for i in tuples[:10]]\n",
      "print [i[0] for i in tuples[-10:]]\n",
      "18/386: from sklearn.feature_extraction.text import TfidfTransformer\n",
      "18/387: # Logistic regression, top K\n",
      "18/388: from sklearn.feature_selection import SelectKBest, chi2\n",
      "18/389:\n",
      "selector = SelectKBest(chi2, k=335).fit(x_train_orig, y_train)\n",
      "x_topk = selector.transform(x_train_orig)\n",
      "x_topk.shape\n",
      "18/390: lr_select = LogisticRegression(solver='lbfgs', C=1)\n",
      "18/391: model_select = lr_select.fit(x_topk, y_train)\n",
      "18/392: lr_select.score(x_topk, y_train)\n",
      "18/393: lr_select.predict(x_topk)[65]\n",
      "18/394:\n",
      "selected_indices = selector.get_support(indices=True)\n",
      "# for i in selected_indices:\n",
      "#     print trainWords[i]\n",
      "18/395: x_testk = x_test[selected_indices]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "18/396: lr_select.score(x_testk, y_test)\n",
      "18/397: roc_auc_score(y_test, lr_select.predict_proba(x_testk)[:,1])\n",
      "18/398: precisionrecall(y_test, lr_select.predict(x_testk))\n",
      "18/399:\n",
      "x = \"stop_bag_of_words_0.csv\"\n",
      "x_tests = pd.read_csv(x, header=None)\n",
      "x_tests.shape\n",
      "18/400:\n",
      "x = \"stopt_bag_of_words_5.csv\"\n",
      "x_trains = pd.read_csv(x, header=None)\n",
      "x_trains.shape\n",
      "18/401:\n",
      "topk = []\n",
      "topktest = []\n",
      "ks = [20,50,100,150,200,250,300,350,400,450,500,540]\n",
      "for i in ks:\n",
      "    selector = SelectKBest(chi2, k=i).fit(x_trains, y_train)\n",
      "    x_top = selector.transform(x_trains)\n",
      "    lr_top = LogisticRegression(solver='lbfgs', C=1)\n",
      "    model_top = lr_top.fit(x_top, y_train)\n",
      "    topk.append(lr_top.score(x_top, y_train))\n",
      "    selected = selector.get_support(indices=True)\n",
      "    topktest.append(lr_top.score(x_tests[selected], y_test))\n",
      "18/402:\n",
      "plot.figure()\n",
      "plot.plot(ks, topk)\n",
      "plot.plot(ks, topktest)\n",
      "print ks[np.argmax(topktest)], np.max(topktest)\n",
      "18/403:\n",
      "selector = SelectKBest(chi2, k=400).fit(x_trains, y_train)\n",
      "x_top = selector.transform(x_trains)\n",
      "lr_top = LogisticRegression(solver='lbfgs', C=1)\n",
      "model_top = lr_top.fit(x_top, y_train)\n",
      "topk.append(lr_top.score(x_top, y_train))\n",
      "selected = selector.get_support(indices=True)\n",
      "18/404:\n",
      "selector = SelectKBest(chi2, k=400).fit(x_trains, y_train)\n",
      "x_top = selector.transform(x_trains)\n",
      "lr_top = LogisticRegression(solver='lbfgs', C=1)\n",
      "model_top = lr_top.fit(x_top, y_train)\n",
      "topk.append(lr_top.score(x_top, y_train))\n",
      "selected = selector.get_support(indices=True)\n",
      "lr_top.score(x_top, y_train)\n",
      "18/405: lr_top.score(x_test[selected], y_test)\n",
      "18/406: lr_top.score(x_tests[selected], y_test)\n",
      "18/407: roc_auc_score(y_test, lr_top.predict_proba(x_tests[selected])[:,1])\n",
      "18/408: precisionrecall(y_test, lr_top.predict(x_tests[selected]))\n",
      "18/409:\n",
      "x = \"temp2_bag_of_words_5.csv\"\n",
      "x_train = pd.read_csv(x, header=None)\n",
      "x_train.shape\n",
      "18/410:\n",
      "x2 = \"temp_bag_of_words_0.csv\"\n",
      "x_test = pd.read_csv(x2, header=None)\n",
      "x_test.shape\n",
      "18/411:\n",
      "topk = []\n",
      "topktest = []\n",
      "ks = [1000,5000,7500,10000,12500]\n",
      "for i in ks:\n",
      "    selector = SelectKBest(chi2, k=i).fit(x_train, y_train)\n",
      "    x_top = selector.transform(x_train)\n",
      "    lr_top = LogisticRegression(solver='lbfgs', C=1)\n",
      "    model_top = lr_top.fit(x_top, y_train)\n",
      "    topk.append(lr_top.score(x_top, y_train))\n",
      "    selected = selector.get_support(indices=True)\n",
      "    topktest.append(lr_top.score(x_test[selected], y_test))\n",
      "18/412:\n",
      "plot.figure()\n",
      "plot.plot(ks, topk)\n",
      "plot.plot(ks, topktest)\n",
      "print ks[np.argmax(topktest)], np.max(topktest)\n",
      "18/413: plot.plot(ks, topktest)\n",
      "18/414:\n",
      "trainWords = open(\"temp2_vocab_5.txt\", \"r\")\n",
      "trainWords = trainWords.readlines()\n",
      "coefs = model_top.coef_[0]\n",
      "tuples = sorted(zip(trainWords, coefs), key=(lambda x: x[1]))\n",
      "print [(i[0], i[1]) for i in tuples[:10]]\n",
      "print [(i[0], i[1]) for i in tuples[-10:]]\n",
      "18/415: # Logistic Regression, bigrams, selectKbest\n",
      "18/416:\n",
      "topk = []\n",
      "topktest = []\n",
      "ks = [1000,5000,7500,8500,9000,9500,10000,12500]\n",
      "for i in ks:\n",
      "    selector = SelectKBest(chi2, k=i).fit(x_train, y_train)\n",
      "    x_top = selector.transform(x_train)\n",
      "    lr_top = LogisticRegression(solver='lbfgs', C=1)\n",
      "    model_top = lr_top.fit(x_top, y_train)\n",
      "    topk.append(lr_top.score(x_top, y_train))\n",
      "    selected = selector.get_support(indices=True)\n",
      "    topktest.append(lr_top.score(x_test[selected], y_test))\n",
      "18/417:\n",
      "plot.figure()\n",
      "plot.plot(ks, topk)\n",
      "plot.plot(ks, topktest)\n",
      "print ks[np.argmax(topktest)], np.max(topktest)\n",
      "18/418:\n",
      "selector = SelectKBest(chi2, k=9000).fit(x_train, y_train)\n",
      "x_top = selector.transform(x_train)\n",
      "lr_top = LogisticRegression(solver='lbfgs', C=1)\n",
      "model_top = lr_top.fit(x_top, y_train)\n",
      "selected_top = selector.get_support(indices=True)\n",
      "lr_top.score(x_test[selected_top], y_test)\n",
      "18/419:\n",
      "selector = SelectKBest(chi2, k=9000).fit(x_train, y_train)\n",
      "x_top = selector.transform(x_train)\n",
      "lr_final = LogisticRegression(solver='lbfgs', C=1)\n",
      "model_final = lr_final.fit(x_top, y_train)\n",
      "selected_top = selector.get_support(indices=True)\n",
      "lr_final.score(x_test[selected_top], y_test)\n",
      "18/420: roc_auc_score(y_test, lr_final.predict_proba(x_test[selected_top])[:,1])\n",
      "18/421: lr_final.score(x_top, y_train)\n",
      "18/422: precisionrecall(y_test, lr_final.predict(x_test[selected_top]))\n",
      "18/423:\n",
      "xt = \"trigrams_bag_of_words_5.csv\"\n",
      "xt_train = pd.read_csv(xt, header=None)\n",
      "xt_train.shape\n",
      "18/424:\n",
      "xt2 = \"trigrams_bag_of_words_0.csv\"\n",
      "xt_test = pd.read_csv(xt2, header=None)\n",
      "xt_test.shape\n",
      "18/425:\n",
      "topk = []\n",
      "topktest = []\n",
      "ks = [1000,5000,10000,15000,20000,25000,30000]\n",
      "for i in ks:\n",
      "    selector = SelectKBest(chi2, k=i).fit(x_train, y_train)\n",
      "    x_top = selector.transform(x_train)\n",
      "    lr_top = LogisticRegression(solver='lbfgs', C=1)\n",
      "    model_top = lr_top.fit(x_top, y_train)\n",
      "    topk.append(lr_top.score(x_top, y_train))\n",
      "    selected = selector.get_support(indices=True)\n",
      "    topktest.append(lr_top.score(x_test[selected], y_test))\n",
      "18/426:\n",
      "topk = []\n",
      "topktest = []\n",
      "ks = [1000,5000,10000,15000,20000,25000,30000]\n",
      "for i in ks:\n",
      "    selector = SelectKBest(chi2, k=i).fit(xt_train, y_train)\n",
      "    x_top = selector.transform(xt_train)\n",
      "    lr_top = LogisticRegression(solver='lbfgs', C=1)\n",
      "    model_top = lr_top.fit(x_top, y_train)\n",
      "    topk.append(lr_top.score(x_top, y_train))\n",
      "    selected = selector.get_support(indices=True)\n",
      "    topktest.append(lr_top.score(xt_test[selected], y_test))\n",
      "18/427:\n",
      "plot.figure()\n",
      "plot.plot(ks, topk)\n",
      "plot.plot(ks, topktest)\n",
      "print ks[np.argmax(topktest)], np.max(topktest)\n",
      "18/428:\n",
      "topk = []\n",
      "topktest = []\n",
      "ks = [1000,5000,9000,10000,15000,20000,25000]\n",
      "for i in ks:\n",
      "    selector = SelectKBest(chi2, k=i).fit(xt_train, y_train)\n",
      "    x_top = selector.transform(xt_train)\n",
      "    lr_top = LogisticRegression(solver='lbfgs', C=1)\n",
      "    model_top = lr_top.fit(x_top, y_train)\n",
      "    topk.append(lr_top.score(x_top, y_train))\n",
      "    selected = selector.get_support(indices=True)\n",
      "    topktest.append(lr_top.score(xt_test[selected], y_test))\n",
      "18/429:\n",
      "plot.figure()\n",
      "plot.plot(ks, topk)\n",
      "plot.plot(ks, topktest)\n",
      "print ks[np.argmax(topktest)], np.max(topktest)\n",
      "18/430: roc_auc_score(y_test, lr_top.predict_proba(xt_test[selected_top])[:,1])\n",
      "18/431: roc_auc_score(y_test, lr_top.predict_proba(xt_test[selected])[:,1])\n",
      "18/432: lr_top.score(x_top)\n",
      "18/433: lr_top.score(x_top, y_train)\n",
      "18/434: lr_top.score(xt_test[selected], y_test)\n",
      "18/435: precisionrecall(y_test, lr_top.predict(xt_test[selected]))\n",
      "18/436: fpr3, tpr3, _ = roc_curve(y_test, lr_top.predict_proba(xt_test[selected])[:,1])\n",
      "18/437: fpr3, tprt, _ = roc_curve(y_test, lr_select.predict_proba(x_testk)[:,1])\n",
      "18/438: fpr3, tpr3, _ = roc_curve(y_test, lr_top.predict_proba(x_testk)[:,1])\n",
      "18/439: fpr3, tpr3, _ = roc_curve(y_test, lr_select.predict_proba(x_testk)[:,1])\n",
      "18/440: fprt, tprt, _ = roc_curve(y_test, lr_top.predict_proba(xt_test[selected])[:,1])\n",
      "18/441: fprt, tprt, _ = roc_curve(y_test, lr_top.predict_proba(xt_test[selected])[:,1])\n",
      "18/442: fpr, tpr, _ = roc_curve(y_test, lr_final.predict_proba(x_test[selected])[:,1])\n",
      "18/443: fpr, tpr, _ = roc_curve(y_test, lr_final.predict_proba(x_test[selected_top])[:,1])\n",
      "18/444: plot.plot(fpr, tpr)\n",
      "18/445: from sklearn.metrics import roc_auc_score\n",
      "18/446: roc_auc_score(y_test, lr_top.predict_proba(x_test[selected])[:,1])\n",
      "18/447: roc_auc_score(y_test, lr_top.predict_proba(x_test[selected_top])[:,1])\n",
      "18/448: roc_auc_score(y_test, lr_final.predict_proba(x_test[selected_top])[:,1])\n",
      "18/449: # ROC curve: logistic regression with (1,2) vs 1 word sequences\n",
      "18/450: fpr3, tpr3, _ = roc_curve(y_test, lr_select.predict_proba(x_testk)[:,1])\n",
      "18/451:\n",
      "plot.figure()\n",
      "plot.plot(fprt, tprt, label=\"3-word features (trigrams)\")\n",
      "plot.plot(fpr, tpr, label=\"2-word features (bigrams)\") # logistic regression, top K + 2-word sequences\n",
      "plot.plot(fpr3, tpr3, label=\"1-word\") # logistic regression, top K\n",
      "plot.legend()\n",
      "plot.xlabel(\"FPR\")\n",
      "plot.ylabel(\"TPR\")\n",
      "18/452: fprd, tprd, _ = roc_curve(y_test, dtc.predict_proba(x_test)[:,1])\n",
      "18/453:\n",
      "import sklearn\n",
      "from sklearn.naive_bayes import MultinomialNB, ComplementNB, BernoulliNB\n",
      "import pandas as pd\n",
      "import numpy as np\n",
      "from sklearn.metrics import roc_auc_score, confusion_matrix\n",
      "import pylab\n",
      "18/454:\n",
      "plot.plot(fpr, tpr)\n",
      "pylab.plot(np.linspace(0, 1, 100), np.linspace(0, 1, 100), ls='dashed', c='k')\n",
      "18/455:\n",
      "plot.figure()\n",
      "plot.plot(fprt, tprt, label=\"3-word features (trigrams)\")\n",
      "plot.plot(fpr, tpr, label=\"2-word features (bigrams)\") # logistic regression, top K + 2-word sequences\n",
      "plot.plot(fpr3, tpr3, label=\"1-word\") # logistic regression, top K\n",
      "plot.legend()\n",
      "plot.xlabel(\"FPR\")\n",
      "plot.ylabel(\"TPR\")\n",
      "pylab.plot(np.linspace(0, 1, 100), np.linspace(0, 1, 100), ls='dashed', c='k')\n",
      "18/456:\n",
      "plot.figure()\n",
      "plot.plot(fpr, tpr, label=\"logistic regression\") # logistic regression, top K + 2-word sequences\n",
      "plot.plot(fpr_mnb, tpr_mnb, label=\"multinomial naive bayes\") # MNB\n",
      "plot.plot(fpr_svm, tpr_svm, label=\"support vector machine\") # SVM\n",
      "pylab.plot(np.linspace(0, 1, 100), np.linspace(0, 1, 100), ls='dashed', c='k')\n",
      "plot.legend()\n",
      "plot.xlabel(\"FPR\")\n",
      "plot.ylabel(\"TPR\")\n",
      "18/457:\n",
      "plot.figure()\n",
      "plot.plot(fpr, tpr, label=\"top K features\") # logistic regression, top K + 2-word sequences\n",
      "plot.plot(fpr_all, tpr_all, label=\"all features\") # logistic regression, all + 2-word sequences\n",
      "pylab.plot(np.linspace(0, 1, 100), np.linspace(0, 1, 100), ls='dashed', c='k')\n",
      "plot.legend()\n",
      "plot.xlabel(\"FPR\")\n",
      "plot.ylabel(\"TPR\")\n",
      "18/458:\n",
      "plot.figure()\n",
      "plot.plot(fpr, tpr, label=\"limited stop words\") # logistic regression, top K + 2-word sequences\n",
      "plot.plot(fpr_alls, tpr_alls, label=\"all stop words\") # logistic regression, top K + all stop words + 2-word sequences\n",
      "pylab.plot(np.linspace(0, 1, 100), np.linspace(0, 1, 100), ls='dashed', c='k')\n",
      "plot.legend()\n",
      "plot.xlabel(\"FPR\")\n",
      "plot.ylabel(\"TPR\")\n",
      "18/459:\n",
      "x2 = \"test__bag_of_words_0.csv\"\n",
      "x_test = pd.read_csv(x2, header=None)\n",
      "x_test.shape\n",
      "18/460: plot.plot(fpr_dtc, tpr_dtc, label=\"decision tree classifier\") # DTC\n",
      "18/461: fpr_dtc, tpr_dtc, _ = roc_curve(y_test, dtc.predict_proba(x_test)[:,1])\n",
      "18/462:\n",
      "plot.figure()\n",
      "plot.plot(fpr, tpr, label=\"logistic regression\") # logistic regression, top K + 2-word sequences\n",
      "plot.plot(fpr_mnb, tpr_mnb, label=\"multinomial naive bayes\") # MNB\n",
      "plot.plot(fpr_svm, tpr_svm, label=\"support vector machine\") # SVM\n",
      "plot.plot(fpr_dtc, tpr_dtc, label=\"decision tree classifier\") # DTC\n",
      "\n",
      "pylab.plot(np.linspace(0, 1, 100), np.linspace(0, 1, 100), ls='dashed', c='k')\n",
      "plot.legend()\n",
      "plot.xlabel(\"FPR\")\n",
      "plot.ylabel(\"TPR\")\n",
      "18/463: fpr_etc, tpr_etc, _ = roc_curve(y_test, etc.predict_proba(x_test)[:,1])\n",
      "18/464:\n",
      "plot.figure()\n",
      "plot.plot(fpr, tpr, label=\"logistic regression\") # logistic regression, top K + 2-word sequences\n",
      "plot.plot(fpr_mnb, tpr_mnb, label=\"multinomial naive bayes\") # MNB\n",
      "plot.plot(fpr_svm, tpr_svm, label=\"support vector machine\") # SVM\n",
      "plot.plot(fpr_dtc, tpr_dtc, label=\"decision tree classifier\") # DTC\n",
      "plot.plot(fpr_etc, tpr_etc, label=\"extra trees classifier\") # ETC\n",
      "pylab.plot(np.linspace(0, 1, 100), np.linspace(0, 1, 100), ls='dashed', c='k')\n",
      "plot.legend()\n",
      "plot.xlabel(\"FPR\")\n",
      "plot.ylabel(\"TPR\")\n",
      "18/465: fpr_twit, tpr_twit, _ = roc_curve(twitter_ys, lr_top.predict_proba(twitter_test[selected_top])[:,1])\n",
      "18/466: lr_final.score(twitter_test[selected_top], twitter_ys)\n",
      "18/467: fpr_twit, tpr_twit, _ = roc_curve(twitter_ys, lr_final.predict_proba(twitter_test[selected_top])[:,1])\n",
      "18/468: fpr_twit2, tpr_twit2, _ = roc_curve(twitter_ys, lr_twitter.predict_proba(twitter_test[selected_top])[:,1])\n",
      "18/469: fpr_twit2, tpr_twit2, _ = roc_curve(twitter_ys, lr_twitter.predict_proba(twitter_test)[:,1])\n",
      "18/470: fpr_twit, tpr_twit, _ = roc_curve(twitter_train_ys[4500:], lr_final.predict_proba(twitter_train[4500:])[:,1])\n",
      "18/471:\n",
      "trainWordsT = open(\"twitter5000_vocab_5.txt\", \"r\")\n",
      "trainWordsT = trainWordsT.readlines()\n",
      "coefsT = model_twitter.coef_[0]\n",
      "tuplesT = sorted(zip(trainWordsT, coefsT), key=(lambda x: x[1]))\n",
      "print [(i[0], i[1]) for i in tuplesT[:10]]\n",
      "print [(i[0], i[1]) for i in tuplesT[-10:]]\n",
      "18/472:\n",
      "trainWords = open(\"temp2_vocab_5.txt\", \"r\")\n",
      "trainWords = trainWords.readlines()\n",
      "coefs = model_top.coef_[0]\n",
      "tuples = sorted(zip(trainWords, coefs), key=(lambda x: x[1]))\n",
      "print [(i[0], i[1]) for i in tuples[:10]]\n",
      "print [(i[0], i[1]) for i in tuples[-10:]]\n",
      "18/473:\n",
      "x = \"temp2_bag_of_words_5.csv\"\n",
      "x_train = pd.read_csv(x, header=None)\n",
      "x_train.shape\n",
      "18/474:\n",
      "x2 = \"temp_bag_of_words_0.csv\"\n",
      "x_test = pd.read_csv(x2, header=None)\n",
      "x_test.shape\n",
      "18/475:\n",
      "topk = []\n",
      "topktest = []\n",
      "ks = [1000,5000,7500,10000,12500]\n",
      "for i in ks:\n",
      "    selector = SelectKBest(chi2, k=i).fit(x_train, y_train)\n",
      "    x_top = selector.transform(x_train)\n",
      "    lr_top = LogisticRegression(solver='lbfgs', C=1)\n",
      "    model_top = lr_top.fit(x_top, y_train)\n",
      "    topk.append(lr_top.score(x_top, y_train))\n",
      "    selected = selector.get_support(indices=True)\n",
      "    topktest.append(lr_top.score(x_test[selected], y_test))\n",
      "18/476:\n",
      "plot.figure()\n",
      "plot.plot(ks, topk)\n",
      "plot.plot(ks, topktest)\n",
      "print ks[np.argmax(topktest)], np.max(topktest)\n",
      "18/477: plot.plot(ks, topktest)\n",
      "18/478:\n",
      "trainWords = open(\"temp2_vocab_5.txt\", \"r\")\n",
      "trainWords = trainWords.readlines()\n",
      "coefs = model_top.coef_[0]\n",
      "tuples = sorted(zip(trainWords, coefs), key=(lambda x: x[1]))\n",
      "print [(i[0], i[1]) for i in tuples[:10]]\n",
      "print [(i[0], i[1]) for i in tuples[-10:]]\n",
      "18/479: # fewer stopwords\n",
      "18/480: # Logistic Regression, bigrams, few stopwords, selectKbest\n",
      "18/481: # trigrams\n",
      "18/482: # ROC curves for evaluation\n",
      "18/483:\n",
      "def precisionrecall(y_test, y_pred):\n",
      "    tn, fp, fn, tp = confusion_matrix(y_test, y_pred).ravel()\n",
      "    precision = (float(tp)/(tp+fp))\n",
      "    recall = (float(tp)/(tp+fn))\n",
      "    f = 2*precision*recall/(precision + recall)\n",
      "    return precision, recall, f\n",
      "18/484: precisionrecall(y_test, mnb.predict(x_test))\n",
      "18/485:\n",
      "# multinomial naive bayes\n",
      "mnb = MultinomialNB()\n",
      "18/486:\n",
      "# original preprocessor\n",
      "x = \"train_bag_of_words_5.csv\"\n",
      "x_train_orig = pd.read_csv(x, header=None)\n",
      "x_train_orig.shape\n",
      "18/487:\n",
      "y = open(\"train_classes_5.txt\", \"r\")\n",
      "y = y.readlines()\n",
      "y_train = [int(x) for x in y]\n",
      "len(y_train)\n",
      "18/488: mnb.fit(x_train_orig, y_train)\n",
      "18/489:\n",
      "# training accuracy\n",
      "mnb.score(x_train_orig, y_train)\n",
      "18/490:\n",
      "x2 = \"test__bag_of_words_0.csv\"\n",
      "x_test = pd.read_csv(x2, header=None)\n",
      "x_test.shape\n",
      "18/491:\n",
      "y2 = open(\"test_classes_5.txt\", \"r\")\n",
      "y2 = y2.readlines()\n",
      "y_test = [int(x) for x in y2]\n",
      "len(y_test)\n",
      "18/492:\n",
      "# testing accuracy\n",
      "mnb.score(x_test, y_test)\n",
      "18/493: roc_auc_score(y_test, mnb.predict_proba(x_test)[:,1])\n",
      "18/494:\n",
      "tn, fp, fn, tp = confusion_matrix(y_test, mnb.predict(x_test)).ravel()\n",
      "tn, fp, fn, tp\n",
      "18/495:\n",
      "def precisionrecall(y_test, y_pred):\n",
      "    tn, fp, fn, tp = confusion_matrix(y_test, y_pred).ravel()\n",
      "    precision = (float(tp)/(tp+fp))\n",
      "    recall = (float(tp)/(tp+fn))\n",
      "    f = 2*precision*recall/(precision + recall)\n",
      "    return precision, recall, f\n",
      "18/496: precisionrecall(y_test, mnb.predict(x_test))\n",
      "18/497:\n",
      "topk = []\n",
      "topktest = []\n",
      "ks = [20,50,100,150,200,250,300,330,335,340,350,400,450,500,540]\n",
      "for i in ks:\n",
      "    selector = SelectKBest(chi2, k=i).fit(x_train_orig, y_train)\n",
      "    x_top = selector.transform(x_train_orig)\n",
      "    mnb = MultinomialNB()\n",
      "    model_top = mnb.fit(x_top, y_train)\n",
      "    topk.append(mnb.score(x_top, y_train))\n",
      "    selected = selector.get_support(indices=True)\n",
      "    topktest.append(mnb.score(x_test[selected], y_test))\n",
      "\n",
      "plot.figure()\n",
      "plot.plot(ks, topk)\n",
      "plot.plot(ks, topktest)\n",
      "plot.show()\n",
      "\n",
      "print(ks[np.argmax(topktest)], np.max(topktest))\n",
      "18/498: cnb = ComplementNB()\n",
      "18/499: cnb.fit(x_train_orig, y_train)\n",
      "18/500: cnb.score(x_train_orig, y_train)\n",
      "18/501: cnb.score(x_test, y_test)\n",
      "18/502: roc_auc_score(y_test, cnb.predict_proba(x_test)[:,1])\n",
      "18/503: precisionrecall(y_test, cnb.predict(x_test))\n",
      "18/504:\n",
      "bnb = BernoulliNB()\n",
      "bnb.fit(x_train_orig, y_train)\n",
      "bnb.score(x_train_orig, y_train)\n",
      "18/505: bnb.score(x_test, y_test)\n",
      "18/506: roc_auc_score(y_test, bnb.predict_proba(x_test)[:,1])\n",
      "18/507: precisionrecall(y_test, bnb.predict(x_test))\n",
      "18/508:\n",
      "from sklearn.tree import DecisionTreeClassifier\n",
      "dtc = DecisionTreeClassifier(random_state=0)\n",
      "dtc.fit(x_train_orig, y_train)\n",
      "dtc.score(x_train_orig, y_train)\n",
      "18/509: dtc.score(x_test, y_test)\n",
      "18/510: roc_auc_score(y_test, dtc.predict_proba(x_test)[:,1])\n",
      "18/511: fpr_dtc, tpr_dtc, _ = roc_curve(y_test, dtc.predict_proba(x_test)[:,1])\n",
      "18/512: precisionrecall(y_test, dtc.predict(x_test))\n",
      "18/513:\n",
      "from sklearn.ensemble import ExtraTreesClassifier\n",
      "etc = ExtraTreesClassifier()\n",
      "etc.fit(x_train_orig, y_train)\n",
      "etc.score(x_train_orig, y_train)\n",
      "18/514: etc.score(x_test, y_test)\n",
      "18/515: roc_auc_score(y_test, etc.predict_proba(x_test)[:,1])\n",
      "18/516: fpr_etc, tpr_etc, _ = roc_curve(y_test, etc.predict_proba(x_test)[:,1])\n",
      "18/517: precisionrecall(y_test, etc.predict(x_test))\n",
      "18/518:\n",
      "from sklearn.svm import LinearSVC\n",
      "svm = LinearSVC(C=.25)\n",
      "svm.fit(x_train_orig, y_train)\n",
      "svm.score(x_train_orig, y_train)\n",
      "18/519: svm.score(x_test, y_test)\n",
      "18/520: roc_auc_score(y_test, svm.decision_function(x_test))\n",
      "18/521: precisionrecall(y_test, svm.predict(x_test))\n",
      "18/522: from sklearn.linear_model import LogisticRegression\n",
      "18/523: from sklearn.linear_model import LogisticRegression\n",
      "18/524: lr = LogisticRegression(solver='lbfgs', C=0.25)\n",
      "18/525: model = lr.fit(x_train_orig, y_train)\n",
      "18/526: lr.score(x_train_orig, y_train)\n",
      "18/527: lr.score(x_test, y_test)\n",
      "18/528: roc_auc_score(y_test, lr.predict_proba(x_test)[:,1])\n",
      "18/529: precisionrecall(y_test, lr.predict(x_test))\n",
      "18/530: precisionrecall(y_test, lr.predict(x_test))\n",
      "18/531:\n",
      "# finding the best regularization penalty\n",
      "lr_train = []\n",
      "lr_test = []\n",
      "ws = [0.01, 0.05, 0.1, 0.25, 0.5, 0.75, 0.9, 1]\n",
      "for w in ws:\n",
      "    lr = LogisticRegression(solver='lbfgs', C=w)\n",
      "    lr.fit(x_train_orig, y_train)\n",
      "    lr_train.append(lr.score(x_train_orig, y_train))\n",
      "    lr_test.append(lr.score(x_test, y_test))\n",
      "18/532: from matplotlib import pyplot as plot\n",
      "18/533:\n",
      "plot.figure()\n",
      "plot.plot(ws, lr_train)\n",
      "plot.plot(ws, lr_test)\n",
      "18/534:\n",
      "print lr_train[3]\n",
      "print lr_test[3]\n",
      "18/535:\n",
      "trainWords = open(\"train_vocab_5.txt\", \"r\")\n",
      "trainWords = trainWords.readlines()\n",
      "18/536:\n",
      "# seeing the \"best\" features - most predictive words\n",
      "coefs = model.coef_[0]\n",
      "tuples = sorted(zip(trainWords, coefs), key=(lambda x: x[1]))\n",
      "print [i[0] for i in tuples[:10]]\n",
      "print [i[0] for i in tuples[-10:]]\n",
      "18/537: from sklearn.feature_extraction.text import TfidfTransformer\n",
      "18/538: # Logistic regression, top K\n",
      "18/539: from sklearn.feature_selection import SelectKBest, chi2\n",
      "18/540:\n",
      "selector = SelectKBest(chi2, k=335).fit(x_train_orig, y_train)\n",
      "x_topk = selector.transform(x_train_orig)\n",
      "x_topk.shape\n",
      "18/541: lr_select = LogisticRegression(solver='lbfgs', C=1)\n",
      "18/542: model_select = lr_select.fit(x_topk, y_train)\n",
      "18/543: lr_select.score(x_topk, y_train)\n",
      "18/544: lr_select.predict(x_topk)[65]\n",
      "18/545:\n",
      "selected_indices = selector.get_support(indices=True)\n",
      "# for i in selected_indices:\n",
      "#     print trainWords[i]\n",
      "18/546: x_testk = x_test[selected_indices]\n",
      "18/547: lr_select.score(x_testk, y_test)\n",
      "18/548: roc_auc_score(y_test, lr_select.predict_proba(x_testk)[:,1])\n",
      "18/549: precisionrecall(y_test, lr_select.predict(x_testk))\n",
      "18/550: precisionrecall(y_test, lr_select.predict(x_testk))\n",
      "18/551:\n",
      "topk = []\n",
      "topktest = []\n",
      "ks = [20,50,100,150,200,250,300,330,335,340,350,400,450,500,540]\n",
      "for i in ks:\n",
      "    selector = SelectKBest(chi2, k=i).fit(x_train_orig, y_train)\n",
      "    x_top = selector.transform(x_train_orig)\n",
      "    lr_top = LogisticRegression(solver='lbfgs', C=1)\n",
      "    model_top = lr_top.fit(x_top, y_train)\n",
      "    topk.append(lr_top.score(x_top, y_train))\n",
      "    selected = selector.get_support(indices=True)\n",
      "    topktest.append(lr_top.score(x_test[selected], y_test))\n",
      "18/552:\n",
      "plot.figure()\n",
      "plot.plot(ks, topk)\n",
      "plot.plot(ks, topktest)\n",
      "print ks[np.argmax(topktest)], np.max(topktest)\n",
      "18/553: # fewer stopwords\n",
      "18/554:\n",
      "x = \"stop_bag_of_words_0.csv\"\n",
      "x_tests = pd.read_csv(x, header=None)\n",
      "x_tests.shape\n",
      "18/555:\n",
      "x = \"stopt_bag_of_words_5.csv\"\n",
      "x_trains = pd.read_csv(x, header=None)\n",
      "x_trains.shape\n",
      "18/556:\n",
      "topk = []\n",
      "topktest = []\n",
      "ks = [20,50,100,150,200,250,300,350,400,450,500,540]\n",
      "for i in ks:\n",
      "    selector = SelectKBest(chi2, k=i).fit(x_trains, y_train)\n",
      "    x_top = selector.transform(x_trains)\n",
      "    lr_top = LogisticRegression(solver='lbfgs', C=1)\n",
      "    model_top = lr_top.fit(x_top, y_train)\n",
      "    topk.append(lr_top.score(x_top, y_train))\n",
      "    selected = selector.get_support(indices=True)\n",
      "    topktest.append(lr_top.score(x_tests[selected], y_test))\n",
      "18/557:\n",
      "plot.figure()\n",
      "plot.plot(ks, topk)\n",
      "plot.plot(ks, topktest)\n",
      "print ks[np.argmax(topktest)], np.max(topktest)\n",
      "18/558:\n",
      "selector = SelectKBest(chi2, k=400).fit(x_trains, y_train)\n",
      "x_top = selector.transform(x_trains)\n",
      "lr_top = LogisticRegression(solver='lbfgs', C=1)\n",
      "model_top = lr_top.fit(x_top, y_train)\n",
      "topk.append(lr_top.score(x_top, y_train))\n",
      "selected = selector.get_support(indices=True)\n",
      "lr_top.score(x_top, y_train)\n",
      "18/559: lr_top.score(x_tests[selected], y_test)\n",
      "18/560: roc_auc_score(y_test, lr_top.predict_proba(x_tests[selected])[:,1])\n",
      "18/561: precisionrecall(y_test, lr_top.predict(x_tests[selected]))\n",
      "18/562:\n",
      "from sklearn.svm import LinearSVC\n",
      "svm = LinearSVC(C=0.7)\n",
      "svm.fit(x_train, y_train)\n",
      "svm.score(x_train, y_train)\n",
      "18/563: svm.score(x_test, y_test)\n",
      "18/564:\n",
      "from sklearn.svm import LinearSVC\n",
      "svm = LinearSVC(C=0.7)\n",
      "svm.fit(x_train_orig, y_train)\n",
      "svm.score(x_train_orig, y_train)\n",
      "18/565: svm.score(x_test, y_test)\n",
      "18/566: # Logistic regression, CountVectorizer, 2-word sequences\n",
      "18/567:\n",
      "x = \"temp2_bag_of_words_5.csv\"\n",
      "x_train = pd.read_csv(x, header=None)\n",
      "x_train.shape\n",
      "18/568:\n",
      "x2 = \"temp_bag_of_words_0.csv\"\n",
      "x_test = pd.read_csv(x2, header=None)\n",
      "x_test.shape\n",
      "18/569:\n",
      "topk = []\n",
      "topktest = []\n",
      "ks = [1000,5000,7500,10000,12500]\n",
      "for i in ks:\n",
      "    selector = SelectKBest(chi2, k=i).fit(x_train, y_train)\n",
      "    x_top = selector.transform(x_train)\n",
      "    lr_top = LogisticRegression(solver='lbfgs', C=1)\n",
      "    model_top = lr_top.fit(x_top, y_train)\n",
      "    topk.append(lr_top.score(x_top, y_train))\n",
      "    selected = selector.get_support(indices=True)\n",
      "    topktest.append(lr_top.score(x_test[selected], y_test))\n",
      "18/570:\n",
      "plot.figure()\n",
      "plot.plot(ks, topk)\n",
      "plot.plot(ks, topktest)\n",
      "print ks[np.argmax(topktest)], np.max(topktest)\n",
      "18/571: plot.plot(ks, topktest)\n",
      "18/572:\n",
      "trainWords = open(\"temp2_vocab_5.txt\", \"r\")\n",
      "trainWords = trainWords.readlines()\n",
      "coefs = model_top.coef_[0]\n",
      "tuples = sorted(zip(trainWords, coefs), key=(lambda x: x[1]))\n",
      "print [(i[0], i[1]) for i in tuples[:10]]\n",
      "print [(i[0], i[1]) for i in tuples[-10:]]\n",
      "18/573: # Logistic Regression, CountVectorizer, 2-word sequences, min_df=3\n",
      "18/574:\n",
      "x3 = \"temp3_bag_of_words_5.csv\"\n",
      "x_train = pd.read_csv(x3, header=None)\n",
      "x_train.shape\n",
      "18/575:\n",
      "x4 = \"temp4_bag_of_words_0.csv\"\n",
      "x_test = pd.read_csv(x4, header=None)\n",
      "x_test.shape\n",
      "18/576:\n",
      "topk = []\n",
      "topktest = []\n",
      "ks = [100,500,1000,1500,2000,2500,3000,3500,4000]\n",
      "for i in ks:\n",
      "    selector = SelectKBest(chi2, k=i).fit(x_train, y_train)\n",
      "    x_top = selector.transform(x_train)\n",
      "    lr_top = LogisticRegression(solver='lbfgs', C=1)\n",
      "    model_top = lr_top.fit(x_top, y_train)\n",
      "    topk.append(lr_top.score(x_top, y_train))\n",
      "    selected = selector.get_support(indices=True)\n",
      "    topktest.append(lr_top.score(x_test[selected], y_test))\n",
      "18/577:\n",
      "plot.figure()\n",
      "plot.plot(ks, topk)\n",
      "plot.plot(ks, topktest)\n",
      "print ks[np.argmax(topktest)], np.max(topktest)\n",
      "18/578: # Logistic Regression, bigrams, few stopwords, selectKbest\n",
      "18/579:\n",
      "x5 = \"temp5_bag_of_words_5.csv\"\n",
      "x_train = pd.read_csv(x5, header=None)\n",
      "x_train.shape\n",
      "18/580:\n",
      "x6 = \"temp6_bag_of_words_0.csv\"\n",
      "x_test = pd.read_csv(x6, header=None)\n",
      "x_test.shape\n",
      "18/581:\n",
      "topk = []\n",
      "topktest = []\n",
      "ks = [1000,5000,7500,8500,9000,9500,10000,12500]\n",
      "for i in ks:\n",
      "    selector = SelectKBest(chi2, k=i).fit(x_train, y_train)\n",
      "    x_top = selector.transform(x_train)\n",
      "    lr_top = LogisticRegression(solver='lbfgs', C=1)\n",
      "    model_top = lr_top.fit(x_top, y_train)\n",
      "    topk.append(lr_top.score(x_top, y_train))\n",
      "    selected = selector.get_support(indices=True)\n",
      "    topktest.append(lr_top.score(x_test[selected], y_test))\n",
      "18/582:\n",
      "plot.figure()\n",
      "plot.plot(ks, topk)\n",
      "plot.plot(ks, topktest)\n",
      "print ks[np.argmax(topktest)], np.max(topktest)\n",
      "18/583: # Logistic Regression, CountVectorizer, 2-word sequence, top K=9000, stopwords\n",
      "18/584:\n",
      "x = \"temp2_bag_of_words_5.csv\"\n",
      "x_train = pd.read_csv(x, header=None)\n",
      "x_train.shape\n",
      "18/585:\n",
      "x2 = \"temp_bag_of_words_0.csv\"\n",
      "x_test = pd.read_csv(x2, header=None)\n",
      "x_test.shape\n",
      "18/586:\n",
      "selector = SelectKBest(chi2, k=9000).fit(x_train, y_train)\n",
      "x_top = selector.transform(x_train)\n",
      "lr_final = LogisticRegression(solver='lbfgs', C=1)\n",
      "model_final = lr_final.fit(x_top, y_train)\n",
      "selected_top = selector.get_support(indices=True)\n",
      "lr_final.score(x_test[selected_top], y_test)\n",
      "18/587: lr_final.score(x_top, y_train)\n",
      "18/588: precisionrecall(y_test, lr_final.predict(x_test[selected_top]))\n",
      "18/589: roc_auc_score(y_test, lr_final.predict_proba(x_test[selected_top])[:,1])\n",
      "18/590: precisionrecall(y_test, lr_final.predict(x_test[selected_top]))\n",
      "18/591: roc_auc_score(y_test, lr_final.predict_proba(x_test[selected_top])[:,1])\n",
      "18/592: # trigrams\n",
      "18/593:\n",
      "xt = \"trigrams_bag_of_words_5.csv\"\n",
      "xt_train = pd.read_csv(xt, header=None)\n",
      "xt_train.shape\n",
      "18/594:\n",
      "xt2 = \"trigrams_bag_of_words_0.csv\"\n",
      "xt_test = pd.read_csv(xt2, header=None)\n",
      "xt_test.shape\n",
      "18/595:\n",
      "topk = []\n",
      "topktest = []\n",
      "ks = [1000,5000,9000,10000,15000,20000,25000]\n",
      "for i in ks:\n",
      "    selector = SelectKBest(chi2, k=i).fit(xt_train, y_train)\n",
      "    x_top = selector.transform(xt_train)\n",
      "    lr_top = LogisticRegression(solver='lbfgs', C=1)\n",
      "    model_top = lr_top.fit(x_top, y_train)\n",
      "    topk.append(lr_top.score(x_top, y_train))\n",
      "    selected = selector.get_support(indices=True)\n",
      "    topktest.append(lr_top.score(xt_test[selected], y_test))\n",
      "18/596:\n",
      "plot.figure()\n",
      "plot.plot(ks, topk)\n",
      "plot.plot(ks, topktest)\n",
      "print ks[np.argmax(topktest)], np.max(topktest)\n",
      "18/597: roc_auc_score(y_test, lr_top.predict_proba(xt_test[selected])[:,1])\n",
      "18/598: lr_top.score(x_top, y_train)\n",
      "18/599: precisionrecall(y_test, lr_top.predict(xt_test[selected]))\n",
      "18/600: fprt, tprt, _ = roc_curve(y_test, lr_top.predict_proba(xt_test[selected])[:,1])\n",
      "18/601: from sklearn.metrics import roc_curve\n",
      "18/602: # ROC curves for evaluation\n",
      "18/603: fpr, tpr, _ = roc_curve(y_test, lr_final.predict_proba(x_test[selected_top])[:,1])\n",
      "18/604:\n",
      "plot.plot(fpr, tpr)\n",
      "pylab.plot(np.linspace(0, 1, 100), np.linspace(0, 1, 100), ls='dashed', c='k')\n",
      "18/605: from sklearn.metrics import roc_auc_score\n",
      "18/606: roc_auc_score(y_test, lr_final.predict_proba(x_test[selected_top])[:,1])\n",
      "18/607: # ROC curve: logistic regression with (1,2) vs 1 word sequences\n",
      "18/608: fpr3, tpr3, _ = roc_curve(y_test, lr_select.predict_proba(x_testk)[:,1])\n",
      "18/609:\n",
      "plot.figure()\n",
      "plot.plot(fprt, tprt, label=\"3-word features (trigrams)\")\n",
      "plot.plot(fpr, tpr, label=\"2-word features (bigrams)\") # logistic regression, top K + 2-word sequences\n",
      "plot.plot(fpr3, tpr3, label=\"1-word\") # logistic regression, top K\n",
      "plot.legend()\n",
      "plot.xlabel(\"FPR\")\n",
      "plot.ylabel(\"TPR\")\n",
      "pylab.plot(np.linspace(0, 1, 100), np.linspace(0, 1, 100), ls='dashed', c='k')\n",
      "18/610: # ROC curve: logistic regression vs MNB vs SVM\n",
      "18/611:\n",
      "mnb.fit(x_train, y_train)\n",
      "fpr_mnb, tpr_mnb, _ = roc_curve(y_test, mnb.predict_proba(x_test)[:,1])\n",
      "18/612:\n",
      "svm = LinearSVC(C=.5)\n",
      "svm.fit(x_train, y_train)\n",
      "fpr_svm, tpr_svm, _ = roc_curve(y_test, svm.decision_function(x_test))\n",
      "18/613: svm.score(x_test, y_test)\n",
      "18/614:\n",
      "plot.figure()\n",
      "plot.plot(fpr, tpr, label=\"logistic regression\") # logistic regression, top K + 2-word sequences\n",
      "plot.plot(fpr_mnb, tpr_mnb, label=\"multinomial naive bayes\") # MNB\n",
      "plot.plot(fpr_svm, tpr_svm, label=\"support vector machine\") # SVM\n",
      "plot.plot(fpr_dtc, tpr_dtc, label=\"decision tree classifier\") # DTC\n",
      "plot.plot(fpr_etc, tpr_etc, label=\"extra trees classifier\") # ETC\n",
      "pylab.plot(np.linspace(0, 1, 100), np.linspace(0, 1, 100), ls='dashed', c='k')\n",
      "plot.legend()\n",
      "plot.xlabel(\"FPR\")\n",
      "plot.ylabel(\"TPR\")\n",
      "18/615: # ROC curve: logistic regression: top K vs all\n",
      "18/616:\n",
      "lr_all = LogisticRegression(solver='lbfgs', C=1)\n",
      "model_all = lr_all.fit(x_train, y_train)\n",
      "lr_all.score(x_test, y_test)\n",
      "fpr_all, tpr_all, _ = roc_curve(y_test, lr_all.predict_proba(x_test)[:,1])\n",
      "18/617:\n",
      "plot.figure()\n",
      "plot.plot(fpr, tpr, label=\"top K features\") # logistic regression, top K + 2-word sequences\n",
      "plot.plot(fpr_all, tpr_all, label=\"all features\") # logistic regression, all + 2-word sequences\n",
      "pylab.plot(np.linspace(0, 1, 100), np.linspace(0, 1, 100), ls='dashed', c='k')\n",
      "plot.legend()\n",
      "plot.xlabel(\"FPR\")\n",
      "plot.ylabel(\"TPR\")\n",
      "18/618: # ROC curve: logistic regression: few stopwords vs all\n",
      "18/619:\n",
      "xs = \"tempstop_bag_of_words_5.csv\"\n",
      "xs_train = pd.read_csv(xs, header=None)\n",
      "xs_train.shape\n",
      "18/620:\n",
      "selector = SelectKBest(chi2, k=9000).fit(xs_train, y_train)\n",
      "xs_top = selector.transform(xs_train)\n",
      "lrs_top = LogisticRegression(solver='lbfgs', C=1)\n",
      "models_top = lrs_top.fit(xs_top, y_train)\n",
      "selected = selector.get_support(indices=True)\n",
      "lrs_top.score(xs_test[selected], y_test)\n",
      "18/621: fpr_alls, tpr_alls, _ = roc_curve(y_test, lrs_top.predict_proba(xs_test[selected])[:,1])\n",
      "18/622:\n",
      "plot.figure()\n",
      "plot.plot(fpr, tpr, label=\"limited stop words\") # logistic regression, top K + 2-word sequences\n",
      "plot.plot(fpr_alls, tpr_alls, label=\"all stop words\") # logistic regression, top K + all stop words + 2-word sequences\n",
      "pylab.plot(np.linspace(0, 1, 100), np.linspace(0, 1, 100), ls='dashed', c='k')\n",
      "plot.legend()\n",
      "plot.xlabel(\"FPR\")\n",
      "plot.ylabel(\"TPR\")\n",
      "18/623: x_twitter = pd.read_csv(\"twitter.csv\", header=0, usecols=[1,3], low_memory=False)\n",
      "18/624: x_twitter.shape\n",
      "18/625: # x_twitter\n",
      "18/626:\n",
      "k = 0\n",
      "for row in x_twitter.itertuples():\n",
      "    k = k + 1\n",
      "    if k > 10:\n",
      "        break\n",
      "    print str(row[0]) + \"\\t\" + row[2] + \"\\t\" + str(row[1])\n",
      "18/627:\n",
      "twitter = \"twitter5000_bag_of_words_5.csv\"\n",
      "twitter_train = pd.read_csv(twitter, header=None)\n",
      "twitter_train.shape\n",
      "18/628:\n",
      "plot.figure()\n",
      "plot.plot(fpr, tpr, label=\"limited stop words\") # logistic regression, top K + 2-word sequences\n",
      "plot.plot(fpr_alls, tpr_alls, label=\"all stop words\") # logistic regression, top K + all stop words + 2-word sequences\n",
      "pylab.plot(np.linspace(0, 1, 100), np.linspace(0, 1, 100), ls='dashed', c='k')\n",
      "plot.legend()\n",
      "plot.xlabel(\"FPR\")\n",
      "plot.ylabel(\"TPR\")\n",
      "18/629: # ROC curves\n",
      "18/630: # ROC curves - stop words\n",
      "18/631:\n",
      "trainplusdata = \"train+_bag_of_words_5.csv\"\n",
      "trainplus = pd.read_csv(trainplusdata, header=None)\n",
      "trainplus.shape\n",
      "18/632:\n",
      "trainplus_y = open(\"train+_classes_0.txt\", \"r\")\n",
      "trainplus_y = trainplus_y.readlines()\n",
      "18/633:\n",
      "trainplus_y = open(\"train+_classes_5.txt\", \"r\")\n",
      "trainplus_y = trainplus_y.readlines()\n",
      "18/634:\n",
      "lr_trainplus = LogisticRegression(solver='lbfgs', C=1)\n",
      "model_trainplus = lr_trainplus.fit(trainplus, trainplus_y)\n",
      "lr_twitter.score(trainplus, trainplus)\n",
      "18/635:\n",
      "lr_trainplus = LogisticRegression(solver='lbfgs', C=1)\n",
      "model_trainplus = lr_trainplus.fit(trainplus, trainplus_y)\n",
      "lr_trainplus.score(trainplus, trainplus)\n",
      "18/636:\n",
      "lr_trainplus = LogisticRegression(solver='lbfgs', C=1)\n",
      "model_trainplus = lr_trainplus.fit(trainplus, trainplus_y)\n",
      "lr_trainplus.score(trainplus, trainplus_y)\n",
      "18/637:\n",
      "testplusdata = \"test+_bag_of_words_0.csv\"\n",
      "testplus = pd.read_csv(testplusdata, header=None)\n",
      "testplus.shape\n",
      "18/638:\n",
      "testplus_y = open(\"test+_classes_5.txt\", \"r\")\n",
      "testplus_y = testplus_y.readlines()\n",
      "18/639:\n",
      "testplus_y = open(\"test+_classes_0.txt\", \"r\")\n",
      "testplus_y = testplus_y.readlines()\n",
      "18/640: lr_trainplus.score(testplus, testplus_y)\n",
      "18/641: # original training data + twitter data\n",
      "18/642:\n",
      "selector = SelectKBest(chi2, k=9000).fit(trainplus, trainplus_y)\n",
      "xtrainplustop = selector.transform(trainplus)\n",
      "trainplustop = LogisticRegression(solver='lbfgs', C=1)\n",
      "trainplustopmodel = trainplustop.fit(ztrainplustop, trainplus_y)\n",
      "selectedtrainplus = selector.get_support(indices=True)\n",
      "trainplustop.score(trainplus[selectedtrainplus], trainplus_y)\n",
      "18/643:\n",
      "selector = SelectKBest(chi2, k=9000).fit(trainplus, trainplus_y)\n",
      "xtrainplustop = selector.transform(trainplus)\n",
      "trainplustop = LogisticRegression(solver='lbfgs', C=1)\n",
      "trainplustopmodel = trainplustop.fit(xtrainplustop, trainplus_y)\n",
      "selectedtrainplus = selector.get_support(indices=True)\n",
      "trainplustop.score(trainplus[selectedtrainplus], trainplus_y)\n",
      "18/644: trainplustop.score(testplus[selectedtrainplus], trainplus_y)\n",
      "18/645: trainplustop.score(testplus[selectedtrainplus], testplus_y)\n",
      "18/646:\n",
      "selector = SelectKBest(chi2, k=20000).fit(trainplus, trainplus_y)\n",
      "xtrainplustop = selector.transform(trainplus)\n",
      "trainplustop = LogisticRegression(solver='lbfgs', C=1)\n",
      "trainplustopmodel = trainplustop.fit(xtrainplustop, trainplus_y)\n",
      "selectedtrainplus = selector.get_support(indices=True)\n",
      "trainplustop.score(trainplus[selectedtrainplus], trainplus_y)\n",
      "18/647: trainplustop.score(testplus[selectedtrainplus], testplus_y)\n",
      "18/648:\n",
      "trainWordsP = open(\"train+_vocab_5.txt\", \"r\")\n",
      "trainWordsP = trainWordsP.readlines()\n",
      "coefsP = model_trainplus.coef_[0]\n",
      "tuplesP = sorted(zip(trainWordsP, coefsP), key=(lambda x: x[1]))\n",
      "print [(i[0], i[1]) for i in tuplesP[:10]]\n",
      "print [(i[0], i[1]) for i in tuplesP[-10:]]\n",
      "18/649: lr_trainplus.score()\n",
      "18/650: lr_trainplus.score(twitter_train[4500:], twitter_train_ys[4500:])\n",
      "18/651:\n",
      "x5 = \"temp5_bag_of_words_5.csv\"\n",
      "x_train = pd.read_csv(x5, header=None)\n",
      "x_train.shape\n",
      "18/652:\n",
      "x6 = \"temp6_bag_of_words_0.csv\"\n",
      "x_test = pd.read_csv(x6, header=None)\n",
      "x_test.shape\n",
      "18/653:\n",
      "topk = []\n",
      "topktest = []\n",
      "ks = [1000,5000,7500,8500,9000,9500,10000,12500]\n",
      "for i in ks:\n",
      "    selector = SelectKBest(chi2, k=i).fit(x_train, y_train)\n",
      "    x_top = selector.transform(x_train)\n",
      "    lr_top = LogisticRegression(solver='lbfgs', C=1)\n",
      "    model_top = lr_top.fit(x_top, y_train)\n",
      "    topk.append(lr_top.score(x_top, y_train))\n",
      "    selected = selector.get_support(indices=True)\n",
      "    topktest.append(lr_top.score(x_test[selected], y_test))\n",
      "18/654:\n",
      "plot.figure()\n",
      "plot.plot(ks, topk)\n",
      "plot.plot(ks, topktest)\n",
      "print ks[np.argmax(topktest)], np.max(topktest)\n",
      "18/655:\n",
      "x = \"temp2_bag_of_words_5.csv\"\n",
      "x_train = pd.read_csv(x, header=None)\n",
      "x_train.shape\n",
      "18/656:\n",
      "x2 = \"temp_bag_of_words_0.csv\"\n",
      "x_test = pd.read_csv(x2, header=None)\n",
      "x_test.shape\n",
      "18/657:\n",
      "topk = []\n",
      "topktest = []\n",
      "ks = [1000,5000,7500,10000,12500]\n",
      "for i in ks:\n",
      "    selector = SelectKBest(chi2, k=i).fit(x_train, y_train)\n",
      "    x_top = selector.transform(x_train)\n",
      "    lr_top = LogisticRegression(solver='lbfgs', C=1)\n",
      "    model_top = lr_top.fit(x_top, y_train)\n",
      "    topk.append(lr_top.score(x_top, y_train))\n",
      "    selected = selector.get_support(indices=True)\n",
      "    topktest.append(lr_top.score(x_test[selected], y_test))\n",
      "18/658:\n",
      "plot.figure()\n",
      "plot.plot(ks, topk)\n",
      "plot.plot(ks, topktest)\n",
      "print ks[np.argmax(topktest)], np.max(topktest)\n",
      "18/659:\n",
      "x = \"temp2_bag_of_words_5.csv\"\n",
      "x_train = pd.read_csv(x, header=None)\n",
      "x_train.shape\n",
      "18/660:\n",
      "x2 = \"temp_bag_of_words_0.csv\"\n",
      "x_test = pd.read_csv(x2, header=None)\n",
      "x_test.shape\n",
      "18/661:\n",
      "topk = []\n",
      "topktest = []\n",
      "ks = [1000,5000,7500,10000,12500]\n",
      "for i in ks:\n",
      "    selector = SelectKBest(chi2, k=i).fit(x_train, y_train)\n",
      "    x_top = selector.transform(x_train)\n",
      "    lr_top = LogisticRegression(solver='lbfgs', C=1)\n",
      "    model_top = lr_top.fit(x_top, y_train)\n",
      "    topk.append(lr_top.score(x_top, y_train))\n",
      "    selected = selector.get_support(indices=True)\n",
      "    topktest.append(lr_top.score(x_test[selected], y_test))\n",
      "18/662:\n",
      "topk = []\n",
      "topktest = []\n",
      "ks = [1000,5000,7500,9000,10000,12500]\n",
      "for i in ks:\n",
      "    selector = SelectKBest(chi2, k=i).fit(x_train, y_train)\n",
      "    x_top = selector.transform(x_train)\n",
      "    lr_top = LogisticRegression(solver='lbfgs', C=1)\n",
      "    model_top = lr_top.fit(x_top, y_train)\n",
      "    topk.append(lr_top.score(x_top, y_train))\n",
      "    selected = selector.get_support(indices=True)\n",
      "    topktest.append(lr_top.score(x_test[selected], y_test))\n",
      "18/663:\n",
      "plot.figure()\n",
      "plot.plot(ks, topk)\n",
      "plot.plot(ks, topktest)\n",
      "print ks[np.argmax(topktest)], np.max(topktest)\n",
      "18/664: plot.plot(ks, topktest)\n",
      "18/665:\n",
      "topk = []\n",
      "topktest = []\n",
      "ks = [1000,5000,7500,9000,10000,12500,15000,18000,19263]\n",
      "for i in ks:\n",
      "    selector = SelectKBest(chi2, k=i).fit(x_train, y_train)\n",
      "    x_top = selector.transform(x_train)\n",
      "    lr_top = LogisticRegression(solver='lbfgs', C=1)\n",
      "    model_top = lr_top.fit(x_top, y_train)\n",
      "    topk.append(lr_top.score(x_top, y_train))\n",
      "    selected = selector.get_support(indices=True)\n",
      "    topktest.append(lr_top.score(x_test[selected], y_test))\n",
      "18/666:\n",
      "plot.figure()\n",
      "plot.plot(ks, topk)\n",
      "plot.plot(ks, topktest)\n",
      "print ks[np.argmax(topktest)], np.max(topktest)\n",
      "18/667: plot.plot(ks, topktest)\n",
      "18/668:\n",
      "plot.figure()\n",
      "axes = plot.gca()\n",
      "axes.set_ylim([.5,1])\n",
      "plot.plot(ks, topk)\n",
      "plot.plot(ks, topktest)\n",
      "print ks[np.argmax(topktest)], np.max(topktest)\n",
      "18/669:\n",
      "plot.figure()\n",
      "axes = plot.gca()\n",
      "# axes.set_ylim([.5,1])\n",
      "plot.plot(ks, topk)\n",
      "plot.plot(ks, topktest)\n",
      "print ks[np.argmax(topktest)], np.max(topktest)\n",
      "18/670:\n",
      "topk = []\n",
      "topktest = []\n",
      "ks = [0,1000,5000,7500,9000,10000,12500,15000,18000,19263]\n",
      "for i in ks:\n",
      "    selector = SelectKBest(chi2, k=i).fit(x_train, y_train)\n",
      "    x_top = selector.transform(x_train)\n",
      "    lr_top = LogisticRegression(solver='lbfgs', C=1)\n",
      "    model_top = lr_top.fit(x_top, y_train)\n",
      "    topk.append(lr_top.score(x_top, y_train))\n",
      "    selected = selector.get_support(indices=True)\n",
      "    topktest.append(lr_top.score(x_test[selected], y_test))\n",
      "18/671:\n",
      "plot.figure()\n",
      "axes = plot.gca()\n",
      "# axes.set_ylim([.5,1])\n",
      "plot.plot(ks, topk)\n",
      "plot.plot(ks, topktest)\n",
      "print ks[np.argmax(topktest)], np.max(topktest)\n",
      "18/672:\n",
      "topk = []\n",
      "topktest = []\n",
      "ks = [100,1000,5000,7500,9000,10000,12500,15000,18000,19263]\n",
      "for i in ks:\n",
      "    selector = SelectKBest(chi2, k=i).fit(x_train, y_train)\n",
      "    x_top = selector.transform(x_train)\n",
      "    lr_top = LogisticRegression(solver='lbfgs', C=1)\n",
      "    model_top = lr_top.fit(x_top, y_train)\n",
      "    topk.append(lr_top.score(x_top, y_train))\n",
      "    selected = selector.get_support(indices=True)\n",
      "    topktest.append(lr_top.score(x_test[selected], y_test))\n",
      "18/673:\n",
      "plot.figure()\n",
      "axes = plot.gca()\n",
      "# axes.set_ylim([.5,1])\n",
      "plot.plot(ks, topk)\n",
      "plot.plot(ks, topktest)\n",
      "print ks[np.argmax(topktest)], np.max(topktest)\n",
      "18/674:\n",
      "topk = []\n",
      "topktest = []\n",
      "ks = [500,1000,5000,7500,9000,10000,12500,15000,18000,19263]\n",
      "for i in ks:\n",
      "    selector = SelectKBest(chi2, k=i).fit(x_train, y_train)\n",
      "    x_top = selector.transform(x_train)\n",
      "    lr_top = LogisticRegression(solver='lbfgs', C=1)\n",
      "    model_top = lr_top.fit(x_top, y_train)\n",
      "    topk.append(lr_top.score(x_top, y_train))\n",
      "    selected = selector.get_support(indices=True)\n",
      "    topktest.append(lr_top.score(x_test[selected], y_test))\n",
      "18/675:\n",
      "plot.figure()\n",
      "axes = plot.gca()\n",
      "# axes.set_ylim([.5,1])\n",
      "plot.plot(ks, topk)\n",
      "plot.plot(ks, topktest)\n",
      "print ks[np.argmax(topktest)], np.max(topktest)\n",
      "18/676:\n",
      "plot.figure()\n",
      "plot.plot(ks, topk, label=\"train accuracy\")\n",
      "plot.plot(ks, topktest, label=\"test accuracy\")\n",
      "plot.xlabel(\"number of features k\")\n",
      "plot.ylabel(\"accuracy rate\")\n",
      "print ks[np.argmax(topktest)], np.max(topktest)\n",
      "18/677:\n",
      "plot.figure()\n",
      "plot.plot(ks, topk, label=\"train accuracy\")\n",
      "plot.plot(ks, topktest, label=\"test accuracy\")\n",
      "plot.legend()\n",
      "plot.xlabel(\"number of features k\")\n",
      "plot.ylabel(\"accuracy rate\")\n",
      "print ks[np.argmax(topktest)], np.max(topktest)\n",
      "18/678:\n",
      "topkw = []\n",
      "topktestw = []\n",
      "ws = [0, 0.01, 0.1, 0.25, 0.5, 0.75, 0.9, 1]\n",
      "for i in ws:\n",
      "    selectorw = SelectKBest(chi2, k=9000).fit(x_train, y_train)\n",
      "    x_topw = selector.transform(x_train)\n",
      "    lr_topw = LogisticRegression(solver='lbfgs', C=w)\n",
      "    model_topw = lr_topw.fit(x_topw, y_train)\n",
      "    topkw.append(lr_top.score(x_topw, y_train))\n",
      "    selectedw = selectorw.get_support(indices=True)\n",
      "    topktestw.append(lr_topw.score(x_test[selectedw], y_test))\n",
      "18/679:\n",
      "topkw = []\n",
      "topktestw = []\n",
      "ws = [0, 0.01, 0.1, 0.25, 0.5, 0.75, 0.9, 1]\n",
      "for i in ws:\n",
      "    selectorw = SelectKBest(chi2, k=9000).fit(x_train, y_train)\n",
      "    x_topw = selectorw.transform(x_train)\n",
      "    lr_topw = LogisticRegression(solver='lbfgs', C=w)\n",
      "    model_topw = lr_topw.fit(x_topw, y_train)\n",
      "    topkw.append(lr_top.score(x_topw, y_train))\n",
      "    selectedw = selectorw.get_support(indices=True)\n",
      "    topktestw.append(lr_topw.score(x_test[selectedw], y_test))\n",
      "18/680:\n",
      "topkw = []\n",
      "topktestw = []\n",
      "ws = [0, 0.01, 0.1, 0.25, 0.5, 0.75, 0.9, 1]\n",
      "for i in ws:\n",
      "    selectorw = SelectKBest(chi2, k=9000).fit(x_train, y_train)\n",
      "    x_topw = selectorw.transform(x_train)\n",
      "    lr_topw = LogisticRegression(solver='lbfgs', C=w)\n",
      "    model_topw = lr_topw.fit(x_topw, y_train)\n",
      "    topkw.append(lr_topw.score(x_topw, y_train))\n",
      "    selectedw = selectorw.get_support(indices=True)\n",
      "    topktestw.append(lr_topw.score(x_test[selectedw], y_test))\n",
      "18/681:\n",
      "plot.figure()\n",
      "plot.plot(ws, topkw, label=\"train accuracy\")\n",
      "plot.plot(ws, topktestw, label=\"test accuracy\")\n",
      "# plot.legend()\n",
      "# plot.xlabel(\"number of features k\")\n",
      "# plot.ylabel(\"accuracy rate\")\n",
      "print ks[np.argmax(topktestw)], np.max(topktestw)\n",
      "18/682:\n",
      "topkw = []\n",
      "topktestw = []\n",
      "ws = [0, 0.01, 0.1, 0.25, 0.5, 0.75, 0.9, 1]\n",
      "for i in ws:\n",
      "    selectorw = SelectKBest(chi2, k=9000).fit(x_train, y_train)\n",
      "    x_topw = selectorw.transform(x_train)\n",
      "    lr_topw = LogisticRegression(solver='lbfgs', C=i)\n",
      "    model_topw = lr_topw.fit(x_topw, y_train)\n",
      "    topkw.append(lr_topw.score(x_topw, y_train))\n",
      "    selectedw = selectorw.get_support(indices=True)\n",
      "    topktestw.append(lr_topw.score(x_test[selectedw], y_test))\n",
      "18/683:\n",
      "topkw = []\n",
      "topktestw = []\n",
      "ws = [0.01, 0.1, 0.25, 0.5, 0.75, 0.9, 1]\n",
      "for i in ws:\n",
      "    selectorw = SelectKBest(chi2, k=9000).fit(x_train, y_train)\n",
      "    x_topw = selectorw.transform(x_train)\n",
      "    lr_topw = LogisticRegression(solver='lbfgs', C=i)\n",
      "    model_topw = lr_topw.fit(x_topw, y_train)\n",
      "    topkw.append(lr_topw.score(x_topw, y_train))\n",
      "    selectedw = selectorw.get_support(indices=True)\n",
      "    topktestw.append(lr_topw.score(x_test[selectedw], y_test))\n",
      "18/684:\n",
      "plot.figure()\n",
      "plot.plot(ws, topkw, label=\"train accuracy\")\n",
      "plot.plot(ws, topktestw, label=\"test accuracy\")\n",
      "# plot.legend()\n",
      "# plot.xlabel(\"number of features k\")\n",
      "# plot.ylabel(\"accuracy rate\")\n",
      "print ks[np.argmax(topktestw)], np.max(topktestw)\n",
      "18/685:\n",
      "topkw = []\n",
      "topktestw = []\n",
      "ws = [0.01, 0.1, 0.25, 0.5, 0.75, 0.9, 1,2]\n",
      "for i in ws:\n",
      "    selectorw = SelectKBest(chi2, k=9000).fit(x_train, y_train)\n",
      "    x_topw = selectorw.transform(x_train)\n",
      "    lr_topw = LogisticRegression(solver='lbfgs', C=i)\n",
      "    model_topw = lr_topw.fit(x_topw, y_train)\n",
      "    topkw.append(lr_topw.score(x_topw, y_train))\n",
      "    selectedw = selectorw.get_support(indices=True)\n",
      "    topktestw.append(lr_topw.score(x_test[selectedw], y_test))\n",
      "18/686:\n",
      "plot.figure()\n",
      "plot.plot(ws, topkw, label=\"train accuracy\")\n",
      "plot.plot(ws, topktestw, label=\"test accuracy\")\n",
      "# plot.legend()\n",
      "# plot.xlabel(\"number of features k\")\n",
      "# plot.ylabel(\"accuracy rate\")\n",
      "print ks[np.argmax(topktestw)], np.max(topktestw)\n",
      "18/687: len(topktestw)\n",
      "18/688: np.argmax(topktestw)\n",
      "18/689:\n",
      "plot.figure()\n",
      "plot.plot(ws, topkw, label=\"train accuracy\")\n",
      "plot.plot(ws, topktestw, label=\"test accuracy\")\n",
      "# plot.legend()\n",
      "# plot.xlabel(\"number of features k\")\n",
      "# plot.ylabel(\"accuracy rate\")\n",
      "print ws[np.argmax(topktestw)], np.max(topktestw)\n",
      "18/690:\n",
      "plot.figure()\n",
      "plot.plot(ws, topkw, label=\"train accuracy\")\n",
      "plot.plot(ws, topktestw, label=\"test accuracy\")\n",
      "plot.legend()\n",
      "plot.xlabel(\"regularization penalty inverse w\")\n",
      "plot.ylabel(\"accuracy rate\")\n",
      "print ws[np.argmax(topktestw)], np.max(topktestw)\n",
      "18/691:\n",
      "topkw = []\n",
      "topktestw = []\n",
      "ws = [0.01, 0.1, 0.25, 0.5, 0.75, 0.9, 1]\n",
      "for i in ws:\n",
      "    selectorw = SelectKBest(chi2, k=9000).fit(x_train, y_train)\n",
      "    x_topw = selectorw.transform(x_train)\n",
      "    lr_topw = LogisticRegression(solver='lbfgs', C=i)\n",
      "    model_topw = lr_topw.fit(x_topw, y_train)\n",
      "    topkw.append(lr_topw.score(x_topw, y_train))\n",
      "    selectedw = selectorw.get_support(indices=True)\n",
      "    topktestw.append(lr_topw.score(x_test[selectedw], y_test))\n",
      "18/692:\n",
      "plot.figure()\n",
      "plot.plot(ws, topkw, label=\"train accuracy\")\n",
      "plot.plot(ws, topktestw, label=\"test accuracy\")\n",
      "plot.legend()\n",
      "plot.xlabel(\"regularization penalty inverse w\")\n",
      "plot.ylabel(\"accuracy rate\")\n",
      "print ws[np.argmax(topktestw)], np.max(topktestw)\n",
      "18/693: pos = sum(y_train)\n",
      "18/694: sum(y_train)\n",
      "18/695: sum(y_test)\n",
      "18/696: sum(trainplus_y)\n",
      "18/697: sum(twitter_ys)\n",
      "18/698:\n",
      "bnb = BernoulliNB(binarize=1)\n",
      "bnb.fit(x_train_orig, y_train)\n",
      "bnb.score(x_train_orig, y_train)\n",
      "18/699:\n",
      "bnb = BernoulliNB(binarize)\n",
      "bnb.fit(x_train_orig, y_train)\n",
      "bnb.score(x_train_orig, y_train)\n",
      "18/700:\n",
      "bnb = BernoulliNB(binarize=0)\n",
      "bnb.fit(x_train_orig, y_train)\n",
      "bnb.score(x_train_orig, y_train)\n",
      "18/701:\n",
      "bnb = BernoulliNB()\n",
      "bnb.fit(x_train_orig, y_train)\n",
      "bnb.score(x_train_orig, y_train)\n",
      "18/702:\n",
      "plot.figure()\n",
      "plot.plot(fprt, tprt, label=\"3-word features (trigrams)\")\n",
      "plot.plot(fpr, tpr, label=\"2-word features (bigrams)\") # logistic regression, top K + 2-word sequences\n",
      "plot.plot(fpr3, tpr3, label=\"1-word\") # logistic regression, top K\n",
      "plot.legend()\n",
      "plot.xlabel(\"FPR\")\n",
      "plot.ylabel(\"TPR\")\n",
      "plot.title(\"Figure 2\")\n",
      "pylab.plot(np.linspace(0, 1, 100), np.linspace(0, 1, 100), ls='dashed', c='k')\n",
      "18/703:\n",
      "plot.figure()\n",
      "plot.plot(fprt, tprt, label=\"3-word features (trigrams)\")\n",
      "plot.plot(fpr, tpr, label=\"2-word features (bigrams)\") # logistic regression, top K + 2-word sequences\n",
      "plot.plot(fpr3, tpr3, label=\"1-word\") # logistic regression, top K\n",
      "plot.legend()\n",
      "plot.xlabel(\"FPR\")\n",
      "plot.ylabel(\"TPR\")\n",
      "plot.title(\"Figure 3\")\n",
      "pylab.plot(np.linspace(0, 1, 100), np.linspace(0, 1, 100), ls='dashed', c='k')\n",
      "18/704:\n",
      "plot.figure()\n",
      "plot.plot(fpr, tpr, label=\"logistic regression\") # logistic regression, top K + 2-word sequences\n",
      "plot.plot(fpr_mnb, tpr_mnb, label=\"multinomial naive bayes\") # MNB\n",
      "plot.plot(fpr_svm, tpr_svm, label=\"support vector machine\") # SVM\n",
      "plot.plot(fpr_dtc, tpr_dtc, label=\"decision tree classifier\") # DTC\n",
      "plot.plot(fpr_etc, tpr_etc, label=\"extra trees classifier\") # ETC\n",
      "pylab.plot(np.linspace(0, 1, 100), np.linspace(0, 1, 100), ls='dashed', c='k')\n",
      "plot.legend()\n",
      "plot.title(\"Figure 1\")\n",
      "plot.xlabel(\"FPR\")\n",
      "plot.ylabel(\"TPR\")\n",
      "18/705:\n",
      "plot.figure()\n",
      "plot.plot(fpr, tpr, label=\"top K features\") # logistic regression, top K + 2-word sequences\n",
      "plot.plot(fpr_all, tpr_all, label=\"all features\") # logistic regression, all + 2-word sequences\n",
      "pylab.plot(np.linspace(0, 1, 100), np.linspace(0, 1, 100), ls='dashed', c='k')\n",
      "plot.legend()\n",
      "plot.title(\"Figure 2\")\n",
      "plot.xlabel(\"FPR\")\n",
      "plot.ylabel(\"TPR\")\n",
      "18/706:\n",
      "plot.figure()\n",
      "plot.plot(fpr, tpr, label=\"limited stop words\") # logistic regression, top K + 2-word sequences\n",
      "plot.plot(fpr_alls, tpr_alls, label=\"all stop words\") # logistic regression, top K + all stop words + 2-word sequences\n",
      "pylab.plot(np.linspace(0, 1, 100), np.linspace(0, 1, 100), ls='dashed', c='k')\n",
      "plot.legend()\n",
      "plot.title(\"Figure 4\")\n",
      "plot.xlabel(\"FPR\")\n",
      "plot.ylabel(\"TPR\")\n",
      "18/707:\n",
      "plot.figure()\n",
      "plot.plot(ks, topk, label=\"train accuracy\")\n",
      "plot.plot(ks, topktest, label=\"test accuracy\")\n",
      "plot.legend()\n",
      "plot.xlabel(\"number of features k\")\n",
      "plot.ylabel(\"accuracy rate\")\n",
      "plot.title(\"Figure 5\")\n",
      "print ks[np.argmax(topktest)], np.max(topktest)\n",
      "18/708: # multinomial naive bayes, no smoothing\n",
      "18/709:\n",
      "# multinomial naive bayes, no smoothing\n",
      "mnb_nosmooth = MultinomialNB(alpha=0)\n",
      "18/710:\n",
      "# multinomial naive bayes, uniform prior\n",
      "mnb_prior = MultinomialNB(fit_prior=False)\n",
      "18/711:\n",
      "mnb_nosmooth.fit(x_train_orig, y_train)\n",
      "mnb_nosmooth.score(x_train_orig, y_train)\n",
      "18/712:\n",
      "# multinomial naive bayes, no smoothing\n",
      "mnb_nosmooth = MultinomialNB(alpha=1e-10)\n",
      "18/713:\n",
      "mnb_nosmooth.fit(x_train_orig, y_train)\n",
      "mnb_nosmooth.score(x_train_orig, y_train)\n",
      "18/714: mnb_nosmooth.score(x_test, y_test)\n",
      "18/715:\n",
      "x2 = \"test__bag_of_words_0.csv\"\n",
      "x_test = pd.read_csv(x2, header=None)\n",
      "x_test.shape\n",
      "18/716:\n",
      "y2 = open(\"test_classes_5.txt\", \"r\")\n",
      "y2 = y2.readlines()\n",
      "y_test = [int(x) for x in y2]\n",
      "len(y_test)\n",
      "18/717: mnb_nosmooth.score(x_test, y_test)\n",
      "18/718:\n",
      "mnb_prior.fit(x_train_orig, y_train)\n",
      "mnb_prior.score(x_train_orig, y_train)\n",
      "18/719: mnb_prior.score(x_test, y_test)\n",
      "18/720:\n",
      "plot.figure()\n",
      "plot.plot(ks, topk, label=\"train accuracy\")\n",
      "plot.plot(ks, topktest, label=\"test accuracy\")\n",
      "plot.legend()\n",
      "plot.xlabel(\"number of features k\")\n",
      "plot.ylabel(\"accuracy rate\")\n",
      "plot.title(\"Figure 5: Feature selection - top K features\")\n",
      "print ks[np.argmax(topktest)], np.max(topktest)\n",
      "18/721:\n",
      "plot.figure()\n",
      "plot.plot(fprt, tprt, label=\"3-word features (trigrams)\")\n",
      "plot.plot(fpr, tpr, label=\"2-word features (bigrams)\") # logistic regression, top K + 2-word sequences\n",
      "plot.plot(fpr3, tpr3, label=\"1-word\") # logistic regression, top K\n",
      "plot.legend()\n",
      "plot.xlabel(\"FPR\")\n",
      "plot.ylabel(\"TPR\")\n",
      "plot.title(\"Figure 3: ngrams for n=1,2,3\")\n",
      "pylab.plot(np.linspace(0, 1, 100), np.linspace(0, 1, 100), ls='dashed', c='k')\n",
      "18/722:\n",
      "plot.figure()\n",
      "plot.plot(fpr, tpr, label=\"logistic regression\") # logistic regression, top K + 2-word sequences\n",
      "plot.plot(fpr_mnb, tpr_mnb, label=\"multinomial naive bayes\") # MNB\n",
      "plot.plot(fpr_svm, tpr_svm, label=\"support vector machine\") # SVM\n",
      "plot.plot(fpr_dtc, tpr_dtc, label=\"decision tree classifier\") # DTC\n",
      "plot.plot(fpr_etc, tpr_etc, label=\"extra trees classifier\") # ETC\n",
      "pylab.plot(np.linspace(0, 1, 100), np.linspace(0, 1, 100), ls='dashed', c='k')\n",
      "plot.legend()\n",
      "plot.title(\"Figure 1: Comparison of Classifiers\")\n",
      "plot.xlabel(\"FPR\")\n",
      "plot.ylabel(\"TPR\")\n",
      "18/723:\n",
      "plot.figure()\n",
      "plot.plot(fpr, tpr, label=\"logistic regression\") # logistic regression, top K + 2-word sequences\n",
      "plot.plot(fpr_mnb, tpr_mnb, label=\"multinomial naive bayes\") # MNB\n",
      "plot.plot(fpr_svm, tpr_svm, label=\"support vector machine\") # SVM\n",
      "plot.plot(fpr_dtc, tpr_dtc, label=\"decision tree classifier\") # DTC\n",
      "plot.plot(fpr_etc, tpr_etc, label=\"extra trees classifier\") # ETC\n",
      "pylab.plot(np.linspace(0, 1, 100), np.linspace(0, 1, 100), ls='dashed', c='k')\n",
      "plot.legend()\n",
      "plot.title(\"Figure 1: Comparing Classifiers\")\n",
      "plot.xlabel(\"FPR\")\n",
      "plot.ylabel(\"TPR\")\n",
      "18/724:\n",
      "plot.figure()\n",
      "plot.plot(fpr, tpr, label=\"top K features\") # logistic regression, top K + 2-word sequences\n",
      "plot.plot(fpr_all, tpr_all, label=\"all features\") # logistic regression, all + 2-word sequences\n",
      "pylab.plot(np.linspace(0, 1, 100), np.linspace(0, 1, 100), ls='dashed', c='k')\n",
      "plot.legend()\n",
      "plot.title(\"Figure 2: Feature selection\")\n",
      "plot.xlabel(\"FPR\")\n",
      "plot.ylabel(\"TPR\")\n",
      "18/725:\n",
      "plot.figure()\n",
      "plot.plot(fpr, tpr, label=\"top K features\") # logistic regression, top K + 2-word sequences\n",
      "plot.plot(fpr_all, tpr_all, label=\"all features\") # logistic regression, all + 2-word sequences\n",
      "pylab.plot(np.linspace(0, 1, 100), np.linspace(0, 1, 100), ls='dashed', c='k')\n",
      "plot.legend()\n",
      "plot.title(\"Figure 2: Feature Selection\")\n",
      "plot.xlabel(\"FPR\")\n",
      "plot.ylabel(\"TPR\")\n",
      "18/726:\n",
      "plot.figure()\n",
      "plot.plot(fpr, tpr, label=\"limited stop words\") # logistic regression, top K + 2-word sequences\n",
      "plot.plot(fpr_alls, tpr_alls, label=\"all stop words\") # logistic regression, top K + all stop words + 2-word sequences\n",
      "pylab.plot(np.linspace(0, 1, 100), np.linspace(0, 1, 100), ls='dashed', c='k')\n",
      "plot.legend()\n",
      "plot.title(\"Figure 4: Reduced stopwords\")\n",
      "plot.xlabel(\"FPR\")\n",
      "plot.ylabel(\"TPR\")\n",
      "18/727:\n",
      "plot.figure()\n",
      "plot.plot(fpr, tpr, label=\"limited stop words\") # logistic regression, top K + 2-word sequences\n",
      "plot.plot(fpr_alls, tpr_alls, label=\"all stop words\") # logistic regression, top K + all stop words + 2-word sequences\n",
      "pylab.plot(np.linspace(0, 1, 100), np.linspace(0, 1, 100), ls='dashed', c='k')\n",
      "plot.legend()\n",
      "plot.title(\"Figure 4: Reduced Stopwords\")\n",
      "plot.xlabel(\"FPR\")\n",
      "plot.ylabel(\"TPR\")\n",
      "18/728:\n",
      "import sklearn\n",
      "from sklearn.naive_bayes import MultinomialNB, ComplementNB, BernoulliNB\n",
      "import pandas as pd\n",
      "import numpy as np\n",
      "from sklearn.metrics import roc_auc_score, confusion_matrix\n",
      "import pylab\n",
      "from sklearn.feature_selection import SelectKBest, chi2\n",
      "20/1:\n",
      "import sklearn\n",
      "from sklearn.naive_bayes import MultinomialNB, ComplementNB, BernoulliNB\n",
      "import pandas as pd\n",
      "import numpy as np\n",
      "from sklearn.metrics import roc_auc_score, confusion_matrix\n",
      "import pylab\n",
      "from sklearn.feature_selection import SelectKBest, chi2\n",
      "20/2:\n",
      "# multinomial naive bayes, no smoothing\n",
      "mnb_nosmooth = MultinomialNB(alpha=1e-10)\n",
      "20/3:\n",
      "# multinomial naive bayes, uniform prior\n",
      "mnb_prior = MultinomialNB(fit_prior=False)\n",
      "20/4:\n",
      "# multinomial naive bayes\n",
      "mnb = MultinomialNB()\n",
      "20/5:\n",
      "# original preprocessor\n",
      "x = \"train_bag_of_words_5.csv\"\n",
      "x_train_orig = pd.read_csv(x, header=None)\n",
      "x_train_orig.shape\n",
      "20/6:\n",
      "y = open(\"train_classes_5.txt\", \"r\")\n",
      "y = y.readlines()\n",
      "y_train = [int(x) for x in y]\n",
      "len(y_train)\n",
      "20/7: sum(y_train)\n",
      "20/8: mnb.fit(x_train_orig, y_train)\n",
      "20/9:\n",
      "# training accuracy\n",
      "mnb.score(x_train_orig, y_train)\n",
      "20/10:\n",
      "x2 = \"test__bag_of_words_0.csv\"\n",
      "x_test = pd.read_csv(x2, header=None)\n",
      "x_test.shape\n",
      "20/11:\n",
      "y2 = open(\"test_classes_5.txt\", \"r\")\n",
      "y2 = y2.readlines()\n",
      "y_test = [int(x) for x in y2]\n",
      "len(y_test)\n",
      "20/12: sum(y_test)\n",
      "20/13:\n",
      "# testing accuracy\n",
      "mnb.score(x_test, y_test)\n",
      "20/14: roc_auc_score(y_test, mnb.predict_proba(x_test)[:,1])\n",
      "20/15:\n",
      "tn, fp, fn, tp = confusion_matrix(y_test, mnb.predict(x_test)).ravel()\n",
      "tn, fp, fn, tp\n",
      "20/16:\n",
      "def precisionrecall(y_test, y_pred):\n",
      "    tn, fp, fn, tp = confusion_matrix(y_test, y_pred).ravel()\n",
      "    precision = (float(tp)/(tp+fp))\n",
      "    recall = (float(tp)/(tp+fn))\n",
      "    f = 2*precision*recall/(precision + recall)\n",
      "    return precision, recall, f\n",
      "20/17: precisionrecall(y_test, mnb.predict(x_test))\n",
      "20/18:\n",
      "mnb_nosmooth.fit(x_train_orig, y_train)\n",
      "mnb_nosmooth.score(x_train_orig, y_train)\n",
      "20/19: mnb_nosmooth.score(x_test, y_test)\n",
      "20/20:\n",
      "mnb_prior.fit(x_train_orig, y_train)\n",
      "mnb_prior.score(x_train_orig, y_train)\n",
      "20/21: mnb_prior.score(x_test, y_test)\n",
      "20/22:\n",
      "topk = []\n",
      "topktest = []\n",
      "ks = [20,50,100,150,200,250,300,330,335,340,350,400,450,500,540]\n",
      "for i in ks:\n",
      "    selector = SelectKBest(chi2, k=i).fit(x_train_orig, y_train)\n",
      "    x_top = selector.transform(x_train_orig)\n",
      "    mnb = MultinomialNB()\n",
      "    model_top = mnb.fit(x_top, y_train)\n",
      "    topk.append(mnb.score(x_top, y_train))\n",
      "    selected = selector.get_support(indices=True)\n",
      "    topktest.append(mnb.score(x_test[selected], y_test))\n",
      "\n",
      "plot.figure()\n",
      "plot.plot(ks, topk)\n",
      "plot.plot(ks, topktest)\n",
      "plot.show()\n",
      "\n",
      "print(ks[np.argmax(topktest)], np.max(topktest))\n",
      "20/23: cnb.fit(x_train_orig, y_train)\n",
      "20/24:\n",
      "import sklearn\n",
      "from sklearn.naive_bayes import MultinomialNB, ComplementNB, BernoulliNB\n",
      "import pandas as pd\n",
      "import numpy as np\n",
      "from sklearn.metrics import roc_auc_score, confusion_matrix\n",
      "import pylab\n",
      "from sklearn.feature_selection import SelectKBest, chi2\n",
      "from matplotlib import pyplot as plot\n",
      "20/25:\n",
      "# multinomial naive bayes, no smoothing\n",
      "mnb_nosmooth = MultinomialNB(alpha=1e-10)\n",
      "20/26:\n",
      "# multinomial naive bayes, uniform prior\n",
      "mnb_prior = MultinomialNB(fit_prior=False)\n",
      "20/27:\n",
      "# multinomial naive bayes\n",
      "mnb = MultinomialNB()\n",
      "20/28:\n",
      "# original preprocessor\n",
      "x = \"train_bag_of_words_5.csv\"\n",
      "x_train_orig = pd.read_csv(x, header=None)\n",
      "x_train_orig.shape\n",
      "20/29:\n",
      "y = open(\"train_classes_5.txt\", \"r\")\n",
      "y = y.readlines()\n",
      "y_train = [int(x) for x in y]\n",
      "len(y_train)\n",
      "20/30: sum(y_train)\n",
      "20/31: mnb.fit(x_train_orig, y_train)\n",
      "20/32:\n",
      "# training accuracy\n",
      "mnb.score(x_train_orig, y_train)\n",
      "20/33:\n",
      "x2 = \"test__bag_of_words_0.csv\"\n",
      "x_test = pd.read_csv(x2, header=None)\n",
      "x_test.shape\n",
      "20/34:\n",
      "y2 = open(\"test_classes_5.txt\", \"r\")\n",
      "y2 = y2.readlines()\n",
      "y_test = [int(x) for x in y2]\n",
      "len(y_test)\n",
      "20/35: sum(y_test)\n",
      "20/36:\n",
      "# testing accuracy\n",
      "mnb.score(x_test, y_test)\n",
      "20/37: roc_auc_score(y_test, mnb.predict_proba(x_test)[:,1])\n",
      "20/38:\n",
      "tn, fp, fn, tp = confusion_matrix(y_test, mnb.predict(x_test)).ravel()\n",
      "tn, fp, fn, tp\n",
      "20/39:\n",
      "def precisionrecall(y_test, y_pred):\n",
      "    tn, fp, fn, tp = confusion_matrix(y_test, y_pred).ravel()\n",
      "    precision = (float(tp)/(tp+fp))\n",
      "    recall = (float(tp)/(tp+fn))\n",
      "    f = 2*precision*recall/(precision + recall)\n",
      "    return precision, recall, f\n",
      "20/40: precisionrecall(y_test, mnb.predict(x_test))\n",
      "20/41:\n",
      "mnb_nosmooth.fit(x_train_orig, y_train)\n",
      "mnb_nosmooth.score(x_train_orig, y_train)\n",
      "20/42: mnb_nosmooth.score(x_test, y_test)\n",
      "20/43:\n",
      "mnb_prior.fit(x_train_orig, y_train)\n",
      "mnb_prior.score(x_train_orig, y_train)\n",
      "20/44: mnb_prior.score(x_test, y_test)\n",
      "20/45:\n",
      "topk = []\n",
      "topktest = []\n",
      "ks = [20,50,100,150,200,250,300,330,335,340,350,400,450,500,540]\n",
      "for i in ks:\n",
      "    selector = SelectKBest(chi2, k=i).fit(x_train_orig, y_train)\n",
      "    x_top = selector.transform(x_train_orig)\n",
      "    mnb = MultinomialNB()\n",
      "    model_top = mnb.fit(x_top, y_train)\n",
      "    topk.append(mnb.score(x_top, y_train))\n",
      "    selected = selector.get_support(indices=True)\n",
      "    topktest.append(mnb.score(x_test[selected], y_test))\n",
      "\n",
      "plot.figure()\n",
      "plot.plot(ks, topk)\n",
      "plot.plot(ks, topktest)\n",
      "plot.show()\n",
      "\n",
      "print(ks[np.argmax(topktest)], np.max(topktest))\n",
      "20/46: cnb = ComplementNB()\n",
      "20/47: cnb.fit(x_train_orig, y_train)\n",
      "20/48: cnb.score(x_train_orig, y_train)\n",
      "20/49: cnb.score(x_test, y_test)\n",
      "20/50: roc_auc_score(y_test, cnb.predict_proba(x_test)[:,1])\n",
      "20/51: precisionrecall(y_test, cnb.predict(x_test))\n",
      "20/52:\n",
      "bnb = BernoulliNB()\n",
      "bnb.fit(x_train_orig, y_train)\n",
      "bnb.score(x_train_orig, y_train)\n",
      "20/53: bnb.score(x_test, y_test)\n",
      "20/54: roc_auc_score(y_test, bnb.predict_proba(x_test)[:,1])\n",
      "20/55: precisionrecall(y_test, bnb.predict(x_test))\n",
      "20/56:\n",
      "from sklearn.tree import DecisionTreeClassifier\n",
      "dtc = DecisionTreeClassifier(random_state=0)\n",
      "dtc.fit(x_train_orig, y_train)\n",
      "dtc.score(x_train_orig, y_train)\n",
      "20/57: dtc.score(x_test, y_test)\n",
      "20/58: roc_auc_score(y_test, dtc.predict_proba(x_test)[:,1])\n",
      "20/59: fpr_dtc, tpr_dtc, _ = roc_curve(y_test, dtc.predict_proba(x_test)[:,1])\n",
      "20/60: precisionrecall(y_test, dtc.predict(x_test))\n",
      "20/61:\n",
      "from sklearn.ensemble import ExtraTreesClassifier\n",
      "etc = ExtraTreesClassifier()\n",
      "etc.fit(x_train_orig, y_train)\n",
      "etc.score(x_train_orig, y_train)\n",
      "20/62: etc.score(x_test, y_test)\n",
      "20/63:\n",
      "import sklearn\n",
      "from sklearn.naive_bayes import MultinomialNB, ComplementNB, BernoulliNB\n",
      "import pandas as pd\n",
      "import numpy as np\n",
      "from sklearn.metrics import roc_auc_score, confusion_matrix, roc_curve\n",
      "import pylab\n",
      "from sklearn.feature_selection import SelectKBest, chi2\n",
      "from matplotlib import pyplot as plot\n",
      "20/64: fpr_dtc, tpr_dtc, _ = roc_curve(y_test, dtc.predict_proba(x_test)[:,1])\n",
      "20/65: precisionrecall(y_test, dtc.predict(x_test))\n",
      "20/66:\n",
      "from sklearn.ensemble import ExtraTreesClassifier\n",
      "etc = ExtraTreesClassifier()\n",
      "etc.fit(x_train_orig, y_train)\n",
      "etc.score(x_train_orig, y_train)\n",
      "20/67: etc.score(x_test, y_test)\n",
      "20/68: roc_auc_score(y_test, etc.predict_proba(x_test)[:,1])\n",
      "20/69: fpr_etc, tpr_etc, _ = roc_curve(y_test, etc.predict_proba(x_test)[:,1])\n",
      "20/70: precisionrecall(y_test, etc.predict(x_test))\n",
      "20/71:\n",
      "from sklearn.svm import LinearSVC\n",
      "svm = LinearSVC(C=.25)\n",
      "svm.fit(x_train_orig, y_train)\n",
      "svm.score(x_train_orig, y_train)\n",
      "20/72: svm.score(x_test, y_test)\n",
      "20/73: roc_auc_score(y_test, svm.decision_function(x_test))\n",
      "20/74: precisionrecall(y_test, svm.predict(x_test))\n",
      "20/75: from sklearn.linear_model import LogisticRegression\n",
      "20/76: lr = LogisticRegression(solver='lbfgs', C=0.25)\n",
      "20/77: model = lr.fit(x_train_orig, y_train)\n",
      "20/78: lr.score(x_train_orig, y_train)\n",
      "20/79: lr.score(x_test, y_test)\n",
      "20/80: roc_auc_score(y_test, lr.predict_proba(x_test)[:,1])\n",
      "20/81: precisionrecall(y_test, lr.predict(x_test))\n",
      "20/82:\n",
      "# finding the best regularization penalty\n",
      "lr_train = []\n",
      "lr_test = []\n",
      "ws = [0.01, 0.05, 0.1, 0.25, 0.5, 0.75, 0.9, 1]\n",
      "for w in ws:\n",
      "    lr = LogisticRegression(solver='lbfgs', C=w)\n",
      "    lr.fit(x_train_orig, y_train)\n",
      "    lr_train.append(lr.score(x_train_orig, y_train))\n",
      "    lr_test.append(lr.score(x_test, y_test))\n",
      "20/83:\n",
      "plot.figure()\n",
      "plot.plot(ws, lr_train)\n",
      "plot.plot(ws, lr_test)\n",
      "20/84:\n",
      "print lr_train[3]\n",
      "print lr_test[3]\n",
      "20/85:\n",
      "trainWords = open(\"train_vocab_5.txt\", \"r\")\n",
      "trainWords = trainWords.readlines()\n",
      "20/86:\n",
      "# seeing the \"best\" features - most predictive words\n",
      "coefs = model.coef_[0]\n",
      "tuples = sorted(zip(trainWords, coefs), key=(lambda x: x[1]))\n",
      "print [i[0] for i in tuples[:10]]\n",
      "print [i[0] for i in tuples[-10:]]\n",
      "20/87: from sklearn.feature_extraction.text import TfidfTransformer\n",
      "20/88: # Logistic regression, top K\n",
      "20/89:\n",
      "selector = SelectKBest(chi2, k=335).fit(x_train_orig, y_train)\n",
      "x_topk = selector.transform(x_train_orig)\n",
      "x_topk.shape\n",
      "20/90: lr_select = LogisticRegression(solver='lbfgs', C=1)\n",
      "20/91: model_select = lr_select.fit(x_topk, y_train)\n",
      "20/92: lr_select.score(x_topk, y_train)\n",
      "20/93: lr_select.predict(x_topk)[65]\n",
      "20/94:\n",
      "selected_indices = selector.get_support(indices=True)\n",
      "# for i in selected_indices:\n",
      "#     print trainWords[i]\n",
      "20/95: x_testk = x_test[selected_indices]\n",
      "20/96: lr_select.score(x_testk, y_test)\n",
      "20/97: roc_auc_score(y_test, lr_select.predict_proba(x_testk)[:,1])\n",
      "20/98: precisionrecall(y_test, lr_select.predict(x_testk))\n",
      "20/99:\n",
      "topk = []\n",
      "topktest = []\n",
      "ks = [20,50,100,150,200,250,300,330,335,340,350,400,450,500,540]\n",
      "for i in ks:\n",
      "    selector = SelectKBest(chi2, k=i).fit(x_train_orig, y_train)\n",
      "    x_top = selector.transform(x_train_orig)\n",
      "    lr_top = LogisticRegression(solver='lbfgs', C=1)\n",
      "    model_top = lr_top.fit(x_top, y_train)\n",
      "    topk.append(lr_top.score(x_top, y_train))\n",
      "    selected = selector.get_support(indices=True)\n",
      "    topktest.append(lr_top.score(x_test[selected], y_test))\n",
      "20/100:\n",
      "plot.figure()\n",
      "plot.plot(ks, topk)\n",
      "plot.plot(ks, topktest)\n",
      "print ks[np.argmax(topktest)], np.max(topktest)\n",
      "20/101: # fewer stopwords\n",
      "20/102:\n",
      "x = \"stop_bag_of_words_0.csv\"\n",
      "x_tests = pd.read_csv(x, header=None)\n",
      "x_tests.shape\n",
      "20/103:\n",
      "x = \"stopt_bag_of_words_5.csv\"\n",
      "x_trains = pd.read_csv(x, header=None)\n",
      "x_trains.shape\n",
      "20/104:\n",
      "topk = []\n",
      "topktest = []\n",
      "ks = [20,50,100,150,200,250,300,350,400,450,500,540]\n",
      "for i in ks:\n",
      "    selector = SelectKBest(chi2, k=i).fit(x_trains, y_train)\n",
      "    x_top = selector.transform(x_trains)\n",
      "    lr_top = LogisticRegression(solver='lbfgs', C=1)\n",
      "    model_top = lr_top.fit(x_top, y_train)\n",
      "    topk.append(lr_top.score(x_top, y_train))\n",
      "    selected = selector.get_support(indices=True)\n",
      "    topktest.append(lr_top.score(x_tests[selected], y_test))\n",
      "20/105:\n",
      "plot.figure()\n",
      "plot.plot(ks, topk)\n",
      "plot.plot(ks, topktest)\n",
      "print ks[np.argmax(topktest)], np.max(topktest)\n",
      "20/106:\n",
      "selector = SelectKBest(chi2, k=400).fit(x_trains, y_train)\n",
      "x_top = selector.transform(x_trains)\n",
      "lr_top = LogisticRegression(solver='lbfgs', C=1)\n",
      "model_top = lr_top.fit(x_top, y_train)\n",
      "topk.append(lr_top.score(x_top, y_train))\n",
      "selected = selector.get_support(indices=True)\n",
      "lr_top.score(x_top, y_train)\n",
      "20/107: lr_top.score(x_tests[selected], y_test)\n",
      "20/108: roc_auc_score(y_test, lr_top.predict_proba(x_tests[selected])[:,1])\n",
      "20/109: precisionrecall(y_test, lr_top.predict(x_tests[selected]))\n",
      "20/110: # SVM, top K\n",
      "20/111:\n",
      "from sklearn.svm import LinearSVC\n",
      "svm = LinearSVC(C=0.7)\n",
      "svm.fit(x_train_orig, y_train)\n",
      "svm.score(x_train_orig, y_train)\n",
      "20/112: svm.score(x_test, y_test)\n",
      "20/113: # Logistic regression, CountVectorizer, 2-word sequences\n",
      "20/114:\n",
      "x = \"temp2_bag_of_words_5.csv\"\n",
      "x_train = pd.read_csv(x, header=None)\n",
      "x_train.shape\n",
      "20/115:\n",
      "x2 = \"temp_bag_of_words_0.csv\"\n",
      "x_test = pd.read_csv(x2, header=None)\n",
      "x_test.shape\n",
      "20/116:\n",
      "topkw = []\n",
      "topktestw = []\n",
      "ws = [0.01, 0.1, 0.25, 0.5, 0.75, 0.9, 1]\n",
      "for i in ws:\n",
      "    selectorw = SelectKBest(chi2, k=9000).fit(x_train, y_train)\n",
      "    x_topw = selectorw.transform(x_train)\n",
      "    lr_topw = LogisticRegression(solver='lbfgs', C=i)\n",
      "    model_topw = lr_topw.fit(x_topw, y_train)\n",
      "    topkw.append(lr_topw.score(x_topw, y_train))\n",
      "    selectedw = selectorw.get_support(indices=True)\n",
      "    topktestw.append(lr_topw.score(x_test[selectedw], y_test))\n",
      "20/117:\n",
      "plot.figure()\n",
      "plot.plot(ws, topkw, label=\"train accuracy\")\n",
      "plot.plot(ws, topktestw, label=\"test accuracy\")\n",
      "plot.legend()\n",
      "plot.xlabel(\"regularization penalty inverse w\")\n",
      "plot.ylabel(\"accuracy rate\")\n",
      "print ws[np.argmax(topktestw)], np.max(topktestw)\n",
      "20/118:\n",
      "topk = []\n",
      "topktest = []\n",
      "ks = [500,1000,5000,7500,9000,10000,12500,15000,18000,19263]\n",
      "for i in ks:\n",
      "    selector = SelectKBest(chi2, k=i).fit(x_train, y_train)\n",
      "    x_top = selector.transform(x_train)\n",
      "    lr_top = LogisticRegression(solver='lbfgs', C=1)\n",
      "    model_top = lr_top.fit(x_top, y_train)\n",
      "    topk.append(lr_top.score(x_top, y_train))\n",
      "    selected = selector.get_support(indices=True)\n",
      "    topktest.append(lr_top.score(x_test[selected], y_test))\n",
      "20/119:\n",
      "plot.figure()\n",
      "plot.plot(ks, topk, label=\"train accuracy\")\n",
      "plot.plot(ks, topktest, label=\"test accuracy\")\n",
      "plot.legend()\n",
      "plot.xlabel(\"number of features k\")\n",
      "plot.ylabel(\"accuracy rate\")\n",
      "plot.title(\"Figure 5: Feature selection - top K features\")\n",
      "print ks[np.argmax(topktest)], np.max(topktest)\n",
      "20/120: plot.plot(ks, topktest)\n",
      "20/121:\n",
      "trainWords = open(\"temp2_vocab_5.txt\", \"r\")\n",
      "trainWords = trainWords.readlines()\n",
      "coefs = model_top.coef_[0]\n",
      "tuples = sorted(zip(trainWords, coefs), key=(lambda x: x[1]))\n",
      "print [(i[0], i[1]) for i in tuples[:10]]\n",
      "print [(i[0], i[1]) for i in tuples[-10:]]\n",
      "20/122: # Logistic Regression, CountVectorizer, 2-word sequences, min_df=3\n",
      "20/123:\n",
      "x3 = \"temp3_bag_of_words_5.csv\"\n",
      "x_train = pd.read_csv(x3, header=None)\n",
      "x_train.shape\n",
      "20/124:\n",
      "x4 = \"temp4_bag_of_words_0.csv\"\n",
      "x_test = pd.read_csv(x4, header=None)\n",
      "x_test.shape\n",
      "20/125:\n",
      "topk = []\n",
      "topktest = []\n",
      "ks = [100,500,1000,1500,2000,2500,3000,3500,4000]\n",
      "for i in ks:\n",
      "    selector = SelectKBest(chi2, k=i).fit(x_train, y_train)\n",
      "    x_top = selector.transform(x_train)\n",
      "    lr_top = LogisticRegression(solver='lbfgs', C=1)\n",
      "    model_top = lr_top.fit(x_top, y_train)\n",
      "    topk.append(lr_top.score(x_top, y_train))\n",
      "    selected = selector.get_support(indices=True)\n",
      "    topktest.append(lr_top.score(x_test[selected], y_test))\n",
      "20/126:\n",
      "plot.figure()\n",
      "plot.plot(ks, topk)\n",
      "plot.plot(ks, topktest)\n",
      "print ks[np.argmax(topktest)], np.max(topktest)\n",
      "20/127: # Logistic Regression, bigrams, few stopwords, selectKbest\n",
      "20/128:\n",
      "x5 = \"temp5_bag_of_words_5.csv\"\n",
      "x_train = pd.read_csv(x5, header=None)\n",
      "x_train.shape\n",
      "20/129:\n",
      "x6 = \"temp6_bag_of_words_0.csv\"\n",
      "x_test = pd.read_csv(x6, header=None)\n",
      "x_test.shape\n",
      "20/130:\n",
      "topk = []\n",
      "topktest = []\n",
      "ks = [1000,5000,7500,8500,9000,9500,10000,12500]\n",
      "for i in ks:\n",
      "    selector = SelectKBest(chi2, k=i).fit(x_train, y_train)\n",
      "    x_top = selector.transform(x_train)\n",
      "    lr_top = LogisticRegression(solver='lbfgs', C=1)\n",
      "    model_top = lr_top.fit(x_top, y_train)\n",
      "    topk.append(lr_top.score(x_top, y_train))\n",
      "    selected = selector.get_support(indices=True)\n",
      "    topktest.append(lr_top.score(x_test[selected], y_test))\n",
      "20/131:\n",
      "plot.figure()\n",
      "plot.plot(ks, topk)\n",
      "plot.plot(ks, topktest)\n",
      "print ks[np.argmax(topktest)], np.max(topktest)\n",
      "20/132: # Logistic Regression, CountVectorizer, 2-word sequence, top K=9000, stopwords\n",
      "20/133:\n",
      "x = \"temp2_bag_of_words_5.csv\"\n",
      "x_train = pd.read_csv(x, header=None)\n",
      "x_train.shape\n",
      "20/134:\n",
      "x2 = \"temp_bag_of_words_0.csv\"\n",
      "x_test = pd.read_csv(x2, header=None)\n",
      "x_test.shape\n",
      "20/135:\n",
      "selector = SelectKBest(chi2, k=9000).fit(x_train, y_train)\n",
      "x_top = selector.transform(x_train)\n",
      "lr_final = LogisticRegression(solver='lbfgs', C=1)\n",
      "model_final = lr_final.fit(x_top, y_train)\n",
      "selected_top = selector.get_support(indices=True)\n",
      "lr_final.score(x_test[selected_top], y_test)\n",
      "20/136: lr_final.score(x_top, y_train)\n",
      "20/137: precisionrecall(y_test, lr_final.predict(x_test[selected_top]))\n",
      "20/138: roc_auc_score(y_test, lr_final.predict_proba(x_test[selected_top])[:,1])\n",
      "20/139: # trigrams\n",
      "20/140:\n",
      "xt = \"trigrams_bag_of_words_5.csv\"\n",
      "xt_train = pd.read_csv(xt, header=None)\n",
      "xt_train.shape\n",
      "20/141:\n",
      "xt2 = \"trigrams_bag_of_words_0.csv\"\n",
      "xt_test = pd.read_csv(xt2, header=None)\n",
      "xt_test.shape\n",
      "20/142:\n",
      "topk = []\n",
      "topktest = []\n",
      "ks = [1000,5000,9000,10000,15000,20000,25000]\n",
      "for i in ks:\n",
      "    selector = SelectKBest(chi2, k=i).fit(xt_train, y_train)\n",
      "    x_top = selector.transform(xt_train)\n",
      "    lr_top = LogisticRegression(solver='lbfgs', C=1)\n",
      "    model_top = lr_top.fit(x_top, y_train)\n",
      "    topk.append(lr_top.score(x_top, y_train))\n",
      "    selected = selector.get_support(indices=True)\n",
      "    topktest.append(lr_top.score(xt_test[selected], y_test))\n",
      "20/143:\n",
      "plot.figure()\n",
      "plot.plot(ks, topk)\n",
      "plot.plot(ks, topktest)\n",
      "print ks[np.argmax(topktest)], np.max(topktest)\n",
      "20/144: roc_auc_score(y_test, lr_top.predict_proba(xt_test[selected])[:,1])\n",
      "20/145: lr_top.score(x_top, y_train)\n",
      "20/146: precisionrecall(y_test, lr_top.predict(xt_test[selected]))\n",
      "20/147: fprt, tprt, _ = roc_curve(y_test, lr_top.predict_proba(xt_test[selected])[:,1])\n",
      "20/148: from sklearn.metrics import roc_curve\n",
      "20/149: # ROC curves for evaluation\n",
      "20/150: fpr, tpr, _ = roc_curve(y_test, lr_final.predict_proba(x_test[selected_top])[:,1])\n",
      "20/151:\n",
      "plot.plot(fpr, tpr)\n",
      "pylab.plot(np.linspace(0, 1, 100), np.linspace(0, 1, 100), ls='dashed', c='k')\n",
      "20/152: roc_auc_score(y_test, lr_final.predict_proba(x_test[selected_top])[:,1])\n",
      "20/153: # ROC curve: logistic regression with (1,2) vs 1 word sequences\n",
      "20/154: fpr3, tpr3, _ = roc_curve(y_test, lr_select.predict_proba(x_testk)[:,1])\n",
      "20/155: # ROC curves for evaluation\n",
      "20/156: fpr, tpr, _ = roc_curve(y_test, lr_final.predict_proba(x_test[selected_top])[:,1])\n",
      "20/157:\n",
      "plot.plot(fpr, tpr)\n",
      "pylab.plot(np.linspace(0, 1, 100), np.linspace(0, 1, 100), ls='dashed', c='k')\n",
      "20/158: roc_auc_score(y_test, lr_final.predict_proba(x_test[selected_top])[:,1])\n",
      "20/159: # ROC curve: logistic regression with (1,2) vs 1 word sequences\n",
      "20/160: fpr3, tpr3, _ = roc_curve(y_test, lr_select.predict_proba(x_testk)[:,1])\n",
      "20/161:\n",
      "plot.figure()\n",
      "plot.plot(fprt, tprt, label=\"3-word features (trigrams)\")\n",
      "plot.plot(fpr, tpr, label=\"2-word features (bigrams)\") # logistic regression, top K + 2-word sequences\n",
      "plot.plot(fpr3, tpr3, label=\"1-word\") # logistic regression, top K\n",
      "plot.legend()\n",
      "plot.xlabel(\"FPR\")\n",
      "plot.ylabel(\"TPR\")\n",
      "plot.title(\"Figure 3: ngrams for n=1,2,3\")\n",
      "pylab.plot(np.linspace(0, 1, 100), np.linspace(0, 1, 100), ls='dashed', c='k')\n",
      "20/162: # ROC curve: logistic regression vs MNB vs SVM\n",
      "20/163:\n",
      "mnb.fit(x_train, y_train)\n",
      "fpr_mnb, tpr_mnb, _ = roc_curve(y_test, mnb.predict_proba(x_test)[:,1])\n",
      "20/164:\n",
      "svm = LinearSVC(C=.5)\n",
      "svm.fit(x_train, y_train)\n",
      "fpr_svm, tpr_svm, _ = roc_curve(y_test, svm.decision_function(x_test))\n",
      "20/165: svm.score(x_test, y_test)\n",
      "20/166:\n",
      "plot.figure()\n",
      "plot.plot(fpr, tpr, label=\"logistic regression\") # logistic regression, top K + 2-word sequences\n",
      "plot.plot(fpr_mnb, tpr_mnb, label=\"multinomial naive bayes\") # MNB\n",
      "plot.plot(fpr_svm, tpr_svm, label=\"support vector machine\") # SVM\n",
      "plot.plot(fpr_dtc, tpr_dtc, label=\"decision tree classifier\") # DTC\n",
      "plot.plot(fpr_etc, tpr_etc, label=\"extra trees classifier\") # ETC\n",
      "pylab.plot(np.linspace(0, 1, 100), np.linspace(0, 1, 100), ls='dashed', c='k')\n",
      "plot.legend()\n",
      "plot.title(\"Figure 1: Comparing Classifiers\")\n",
      "plot.xlabel(\"FPR\")\n",
      "plot.ylabel(\"TPR\")\n",
      "20/167: # ROC curve: logistic regression: top K vs all\n",
      "20/168:\n",
      "lr_all = LogisticRegression(solver='lbfgs', C=1)\n",
      "model_all = lr_all.fit(x_train, y_train)\n",
      "lr_all.score(x_test, y_test)\n",
      "fpr_all, tpr_all, _ = roc_curve(y_test, lr_all.predict_proba(x_test)[:,1])\n",
      "20/169:\n",
      "plot.figure()\n",
      "plot.plot(fpr, tpr, label=\"top K features\") # logistic regression, top K + 2-word sequences\n",
      "plot.plot(fpr_all, tpr_all, label=\"all features\") # logistic regression, all + 2-word sequences\n",
      "pylab.plot(np.linspace(0, 1, 100), np.linspace(0, 1, 100), ls='dashed', c='k')\n",
      "plot.legend()\n",
      "plot.title(\"Figure 2: Feature Selection\")\n",
      "plot.xlabel(\"FPR\")\n",
      "plot.ylabel(\"TPR\")\n",
      "20/170: # ROC curve: logistic regression: few stopwords vs all\n",
      "20/171:\n",
      "xs = \"tempstop_bag_of_words_5.csv\"\n",
      "xs_train = pd.read_csv(xs, header=None)\n",
      "xs_train.shape\n",
      "20/172:\n",
      "xst = \"tempstop_bag_of_words_0.csv\"\n",
      "xs_test = pd.read_csv(xst, header=None)\n",
      "xs_test.shape\n",
      "20/173:\n",
      "selector = SelectKBest(chi2, k=9000).fit(xs_train, y_train)\n",
      "xs_top = selector.transform(xs_train)\n",
      "lrs_top = LogisticRegression(solver='lbfgs', C=1)\n",
      "models_top = lrs_top.fit(xs_top, y_train)\n",
      "selected = selector.get_support(indices=True)\n",
      "lrs_top.score(xs_test[selected], y_test)\n",
      "20/174: fpr_alls, tpr_alls, _ = roc_curve(y_test, lrs_top.predict_proba(xs_test[selected])[:,1])\n",
      "20/175:\n",
      "plot.figure()\n",
      "plot.plot(fpr, tpr, label=\"limited stop words\") # logistic regression, top K + 2-word sequences\n",
      "plot.plot(fpr_alls, tpr_alls, label=\"all stop words\") # logistic regression, top K + all stop words + 2-word sequences\n",
      "pylab.plot(np.linspace(0, 1, 100), np.linspace(0, 1, 100), ls='dashed', c='k')\n",
      "plot.legend()\n",
      "plot.title(\"Figure 4: Reduced Stopwords\")\n",
      "plot.xlabel(\"FPR\")\n",
      "plot.ylabel(\"TPR\")\n",
      "20/176: x_twitter = pd.read_csv(\"twitter.csv\", header=0, usecols=[1,3], low_memory=False)\n",
      "20/177: x_twitter.shape\n",
      "20/178: # x_twitter\n",
      "20/179:\n",
      "k = 0\n",
      "for row in x_twitter.itertuples():\n",
      "    k = k + 1\n",
      "    if k > 10:\n",
      "        break\n",
      "    print str(row[0]) + \"\\t\" + row[2] + \"\\t\" + str(row[1])\n",
      "20/180:\n",
      "twitter = \"twitter5000_bag_of_words_5.csv\"\n",
      "twitter_train = pd.read_csv(twitter, header=None)\n",
      "twitter_train.shape\n",
      "20/181:\n",
      "twitter_train_y = open(\"twitter5000_classes_5.txt\", \"r\")\n",
      "twitter_train_y = twitter_train_y.readlines()\n",
      "twitter_train_ys = []\n",
      "count = 0\n",
      "for x in twitter_train_y:\n",
      "    count += 1\n",
      "    # fixing the dataset errors\n",
      "    if count == 4290 or count == 5182:\n",
      "        x = 0\n",
      "    twitter_train_ys.append(int(x))\n",
      "20/182:\n",
      "twittertest = \"twittertest_bag_of_words_0.csv\"\n",
      "twitter_test = pd.read_csv(twittertest, header=None)\n",
      "twitter_test.shape\n",
      "20/183:\n",
      "twitter_y = open(\"twittertest_classes_0.txt\", \"r\")\n",
      "twitter_y = twitter_y.readlines()\n",
      "twitter_ys = []\n",
      "count = 0\n",
      "for x in twitter_y:\n",
      "    count += 1\n",
      "    # fixing the dataset errors\n",
      "    if count == 4290 or count == 5182:\n",
      "        x = 0\n",
      "    twitter_ys.append(int(x))\n",
      "20/184: sum(twitter_ys)\n",
      "20/185: lr_final.score(twitter_test[selected_top], twitter_ys)\n",
      "20/186:\n",
      "lr_twitter = LogisticRegression(solver='lbfgs', C=1)\n",
      "model_twitter = lr_twitter.fit(twitter_train[:4500], twitter_train_ys[:4500])\n",
      "lr_twitter.score(twitter_train[:4500], twitter_train_ys[:4500])\n",
      "20/187: lr_twitter.score(twitter_train[4500:], twitter_train_ys[4500:])\n",
      "20/188: # original training data + twitter data\n",
      "20/189:\n",
      "trainplusdata = \"train+_bag_of_words_5.csv\"\n",
      "trainplus = pd.read_csv(trainplusdata, header=None)\n",
      "trainplus.shape\n",
      "20/190:\n",
      "trainWordsP = open(\"train+_vocab_5.txt\", \"r\")\n",
      "trainWordsP = trainWordsP.readlines()\n",
      "coefsP = model_trainplus.coef_[0]\n",
      "tuplesP = sorted(zip(trainWordsP, coefsP), key=(lambda x: x[1]))\n",
      "print [(i[0], i[1]) for i in tuplesP[:10]]\n",
      "print [(i[0], i[1]) for i in tuplesP[-10:]]\n",
      "20/191:\n",
      "plot.figure()\n",
      "plot.plot(fpr, tpr, label=\"limited stop words\") # logistic regression, top K + 2-word sequences\n",
      "plot.plot(fpr_alls, tpr_alls, label=\"all stop words\") # logistic regression, top K + all stop words + 2-word sequences\n",
      "pylab.plot(np.linspace(0, 1, 100), np.linspace(0, 1, 100), ls='dashed', c='k')\n",
      "plot.legend()\n",
      "plot.xlabel(\"FPR\")\n",
      "plot.ylabel(\"TPR\")\n",
      "20/192:\n",
      "trainWordsT = open(\"twitter5000_vocab_5.txt\", \"r\")\n",
      "trainWordsT = trainWordsT.readlines()\n",
      "coefsT = model_twitter.coef_[0]\n",
      "tuplesT = sorted(zip(trainWordsT, coefsT), key=(lambda x: x[1]))\n",
      "print [(i[0], i[1]) for i in tuplesT[:10]]\n",
      "print [(i[0], i[1]) for i in tuplesT[-10:]]\n",
      "20/193: tn, fp, fn, tp = confusion_matrix(y_test, lr_top.predict(x_test[selected])).ravel()\n",
      "20/194: tn, fp, fn, tp\n",
      "20/195:\n",
      "#Finding the top elements in logistic regression\n",
      "lr = LogisticRegression(solver='lbfgs', C=0.25)\n",
      "model = lr.fit(x_train_orig, y_train)\n",
      "weights = model.coef_[0]\n",
      "topTwenty = np.argpartition(weights, -20)[-20:]\n",
      "topTwentyWords = [x_train.columns.values[a] for a in topTwenty]\n",
      "20/196:\n",
      "x_test_orig = pd.read_csv(\"test__bag_of_words_0.csv\", header=None)\n",
      "x_test_orig.shape\n",
      "20/197:\n",
      "testReviewsFile = open(\"test.txt\")\n",
      "testReviews = testRevi1ewsFile.readlines()\n",
      "testReviews = [a.strip() for a in testReviews]\n",
      "20/198:\n",
      "testReviewsFile = open(\"test.txt\")\n",
      "testReviews = testReviewsFile.readlines()\n",
      "testReviews = [a.strip() for a in testReviews]\n",
      "20/199:\n",
      "# Find what kinds of reviews are misclassified with logistic regression\n",
      "logisticModelAnswers = model.predict(x_test_orig)\n",
      "logisticModelProbabilities = model.predict_proba(x_test_orig)[:,1]\n",
      "# 0 if predicted positive and is positive or if predicted negative and is negative. \n",
      "correct = [1 if a == b else 0 for (a, b) in zip(logisticModelAnswers, y_train)]\n",
      "for index, element in enumerate(correct):\n",
      "    if element == 1:\n",
      "        logisticModelProbabilities[index] = 10\n",
      "# 1 if predicted negative and is positive, 1 if predicted positive and is negative, -1 if predicted negative and is positive\n",
      "misCalculated = [a - b for (a, b) in zip(logisticModelAnswers, y_train)]\n",
      "degreeWrong = [a - b for (a, b) in zip(logisticModelProbabilities, y_train)]\n",
      "reallyWrong = [(a,b) for (a,b) in enumerate(degreeWrong) if abs(b) >= .8 and b <= abs(1)]\n",
      "\n",
      "# Count the number of top words in the logistic regression ones that were really misclassified. \n",
      "predictedPositiveButNegativeIndicies = [int(a) for (a,b) in reallyWrong if b > 0]\n",
      "\n",
      "count = 0\n",
      "for index in predictedPositiveButNegativeIndicies:\n",
      "    for word in topTwentyWords:\n",
      "        if str(word) in testReviews[index]:\n",
      "            count += 1\n",
      "print(count)\n",
      "\n",
      "# print(misCalculated)\n",
      "20/200:\n",
      "from sklearn.ensemble import ExtraTreesClassifier\n",
      "import operator\n",
      "import pandas\n",
      "\n",
      "trainFile = open(\"train_vocab_5.txt\")\n",
      "trainWords = list(trainFile)\n",
      "for i, w in enumerate(trainWords):\n",
      "    trainWords[i] = w.replace(\"\\n\", \"\")\n",
      "print(trainWords)\n",
      "\n",
      "trainBagOfWords = pandas.read_csv(\"train_bag_of_words_5.csv\", names=trainWords)\n",
      "trainLabels = pandas.read_csv(\"train_classes_5.txt\", names=[\"class\"])\n",
      "testBagOfWords = pandas.read_csv(\"test__bag_of_words_0.csv\", names=trainWords)\n",
      "testLabels = pandas.read_csv(\"test__classes_0.txt\", names=[\"class\"])\n",
      "#print(bagOfWords)\n",
      "#print(labels)\n",
      "trainArrayBag = trainBagOfWords.values\n",
      "trainArrayLabels = trainLabels.values[:, 0]\n",
      "testArrayBag = testBagOfWords.values\n",
      "testArrayLabels = testLabels.values[:, 0]\n",
      "#print(arrayBag)\n",
      "#print(arrayLabels)\n",
      "model = ExtraTreesClassifier()\n",
      "fit = model.fit(trainArrayBag, trainArrayLabels)\n",
      "#dictData = dict(enumerate(model.feature_importances_))\n",
      "#sortedDict = sorted(dictData.items(), key=operator.itemgetter(1))\n",
      "#print(sortedDict)\n",
      "print(fit.score(trainArrayBag, trainArrayLabels))\n",
      "print(fit.score(testArrayBag, testArrayLabels))\n",
      "\n",
      "sortedDict = sorted(zip(trainWords, model.feature_importances_), key=operator.itemgetter(1))\n",
      "print(sortedDict)\n",
      "21/1: precisionrecall(twitter_ys, lr_final.predict(twitter_test[selected_top]))\n",
      "21/2: pip install sklearn\n",
      "21/3: sklearn\n",
      "21/4: print sklearn.__version__\n",
      "21/5:\n",
      "import sklearn\n",
      "print sklearn.__version__\n",
      "22/1:\n",
      "closeness_centralities = []\n",
      "for actor in actorlist:\n",
      "    closeness_centralities.append(closeness_centrality.get(actor))\n",
      "x_c = np.array(closeness_centralities)\n",
      "y = np.array(boxofficegross)\n",
      "slope, intercept, r_value, p_value, std_err = stats.linregress(x_c,y)\n",
      "slope, intercept, r_value, p_value, std_err\n",
      "22/2:\n",
      "degree_centralities = []\n",
      "actorlist = [a for a in actorw]\n",
      "for a in actorm:\n",
      "    actorlist.append(a)\n",
      "for actor in actorlist:\n",
      "    degree_centralities.append(degree_centrality.get(actor))\n",
      "22/3:\n",
      "import networkx as nx\n",
      "import csv\n",
      "import matplotlib.pyplot as plot\n",
      "22/4:\n",
      "from bs4 import BeautifulSoup\n",
      "import urllib2\n",
      "22/5:\n",
      "from sklearn.linear_model import LinearRegression, LogisticRegression\n",
      "import numpy as np\n",
      "from scipy import stats\n",
      "22/6:\n",
      "page0 = urllib2.urlopen(\"https://en.wikipedia.org/wiki/List_of_actors_with_Academy_Award_nominations\")\n",
      "soup0 = BeautifulSoup(page0, 'html.parser')\n",
      "22/7: actors0 = soup0.find_all('tr')[11:][:-11]\n",
      "22/8: actors0[0].find('a').text\n",
      "22/9:\n",
      "academylist = []\n",
      "for actor in actors0:\n",
      "    academylist.append(actor.find('a').text)\n",
      "22/10: academylist\n",
      "22/11:\n",
      "page1 = urllib2.urlopen(\"https://www.boxofficemojo.com/people/?view=Actor&p=.htm\")\n",
      "soup1 = BeautifulSoup(page1, 'html.parser')\n",
      "22/12: actors1 = soup1.find_all('tr')[2].find_all('tr')[1:]\n",
      "22/13: actors1\n",
      "22/14: boxofficedict = {}\n",
      "22/15:\n",
      "def addtodict(actors):\n",
      "    for actor in actors:\n",
      "        name = actor.find('b').find('b').text\n",
      "        boxoffice = str(actor.find('td', attrs={'align':'right'}).text[1:]).replace(\",\", \"\")\n",
      "        if \"k\" in boxoffice:\n",
      "            boxoffice = float(boxoffice.replace(\"k\", \"\"))/1000\n",
      "        boxofficedict[name] = float(boxoffice)\n",
      "22/16: boxofficedict\n",
      "22/17:\n",
      "page2 = urllib2.urlopen(\"https://www.boxofficemojo.com/people/?view=Actor&pagenum=2&sort=person&order=ASC&p=.htm\")\n",
      "soup2 = BeautifulSoup(page2, 'html.parser')\n",
      "actors2 = soup2.find_all('tr')[2].find_all('tr')[1:]\n",
      "22/18:\n",
      "page3 = urllib2.urlopen(\"https://www.boxofficemojo.com/people/?view=Actor&pagenum=3&sort=person&order=ASC&p=.htm\")\n",
      "soup3 = BeautifulSoup(page3, 'html.parser')\n",
      "actors3 = soup3.find_all('tr')[2].find_all('tr')[1:]\n",
      "22/19:\n",
      "addtodict(actors1)\n",
      "addtodict(actors2)\n",
      "addtodict(actors3)\n",
      "22/20: len(boxofficedict)\n",
      "22/21: boxofficedict\n",
      "22/22:\n",
      "with open('title.basics.tsv') as tsvfile:\n",
      "    reader = csv.reader(tsvfile, delimiter='\\t')\n",
      "    i = 0\n",
      "    for row in reader:\n",
      "        if i < 200:\n",
      "            print(row)\n",
      "            i = i + 1\n",
      "        else:\n",
      "            break\n",
      "22/23:\n",
      "dict = {}\n",
      "with open('name.basics.tsv') as tsvfile:\n",
      "    reader = csv.reader(tsvfile, delimiter='\\t')\n",
      "    i = 0\n",
      "    for row in reader:\n",
      "        if i < 200:\n",
      "            print(row)\n",
      "            i = i + 1\n",
      "        if i > 1:\n",
      "            dict[row[0]] = row[1]\n",
      "22/24:\n",
      "G = nx.Graph()\n",
      "Gw = nx.Graph()\n",
      "Gm = nx.Graph()\n",
      "moviecountdict = {}\n",
      "\n",
      "with open('title.principals.tsv') as tsvfile:\n",
      "    reader = csv.reader(tsvfile, delimiter='\\t')\n",
      "    prev = \"\"\n",
      "    actors = []\n",
      "    for row in reader:\n",
      "        if row[3] == 'actor' and row[2] in dict and dict[row[2]] in boxofficedict:\n",
      "            Gm.add_node(dict[row[2]])\n",
      "            G.add_node(dict[row[2]])\n",
      "            if dict[row[2]] in moviecountdict:\n",
      "                moviecountdict[dict[row[2]]] += 1\n",
      "            else:\n",
      "                moviecountdict[dict[row[2]]] = 1\n",
      "        elif row[3] == 'actress' and row[2] in dict and dict[row[2]] in boxofficedict:\n",
      "            Gw.add_node(dict[row[2]])\n",
      "            G.add_node(dict[row[2]])\n",
      "            if dict[row[2]] in moviecountdict:\n",
      "                moviecountdict[dict[row[2]]] += 1\n",
      "            else:\n",
      "                moviecountdict[dict[row[2]]] = 1\n",
      "        else:\n",
      "            continue\n",
      "        if row[0] != prev:\n",
      "            for a1 in actors:\n",
      "                for a2 in actors:\n",
      "                    if a1 == a2:\n",
      "                        continue\n",
      "                    G.add_edge(a1, a2)\n",
      "                    if a1 in Gw.nodes and a2 in Gw.nodes:\n",
      "                        Gw.add_edge(a1, a2)\n",
      "                    elif a1 in Gm.nodes and a2 in Gm.nodes:\n",
      "                        Gm.add_edge(a1, a2)\n",
      "            actors = [dict[row[2]]]\n",
      "        else:\n",
      "            actors.append(dict[row[2]])\n",
      "        prev = row[0]\n",
      "22/25: moviecountdict\n",
      "22/26: list(G.nodes)\n",
      "22/27: len(G.nodes)\n",
      "22/28: list(G.edges)\n",
      "22/29: len(G.edges)\n",
      "22/30: nx.draw(G, with_labels=False, font_weight='bold')\n",
      "22/31: nx.draw_networkx_edges(G)\n",
      "22/32: nx.draw_networkx_edges(G, nx.spring_layout(G))\n",
      "22/33: nx.draw_networkx_edges(G, nx.circular_layout(G))\n",
      "22/34: nx.draw_networkx_edges(G, nx.random_layout(G))\n",
      "22/35: nx.draw_networkx_edges(G, nx.shell_layout(G))\n",
      "22/36: nx.draw_networkx_edges(G, nx.spectral_layout(G))\n",
      "22/37: nx.draw_networkx_edges(G, nx.shell_layout(G), G.nodes[:100])\n",
      "22/38: nx.draw_networkx_edges(G, nx.shell_layout(G), list(G.nodes)[:100])\n",
      "22/39: nx.draw_networkx_edges(G, nx.shell_layout(G), nodelist=list(G.nodes)[:100])\n",
      "22/40: nx.draw_networkx_edges(G, nx.shell_layout(G), nodelist=list(G.nodes)[:10])\n",
      "22/41: nx.draw_networkx_edges(G, nx.random_layout(G), nodelist=list(G.nodes)[:10])\n",
      "22/42: nx.draw_networkx_edges(G, nx.random_layout(G), nodelist=list(G.nodes)[:1])\n",
      "22/43: nx.draw(G, with_labels=True)\n",
      "22/44: nx.draw(Gw)\n",
      "22/45: nx.draw(Gm)\n",
      "22/46:\n",
      "actorw = sorted(Gw.nodes)\n",
      "actorm = sorted(Gm.nodes)\n",
      "22/47: degree_centrality = nx.degree_centrality(G)\n",
      "22/48: sorted(degree_centrality, key=degree_centrality.get, reverse=True)\n",
      "22/49: eigenvector_centrality = nx.eigenvector_centrality(G)\n",
      "22/50: sorted(eigenvector_centrality, key=eigenvector_centrality.get, reverse=True)\n",
      "22/51: closeness_centrality = nx.closeness_centrality(G)\n",
      "22/52: sorted(closeness_centrality, key=closeness_centrality.get, reverse=True)\n",
      "22/53: betweenness_centrality = nx.betweenness_centrality(G)\n",
      "22/54: sorted(betweenness_centrality, key=betweenness_centrality.get, reverse=True)\n",
      "22/55:\n",
      "degree_centralities = []\n",
      "actorlist = [a for a in actorw]\n",
      "for a in actorm:\n",
      "    actorlist.append(a)\n",
      "for actor in actorlist:\n",
      "    degree_centralities.append(degree_centrality.get(actor))\n",
      "22/56: len(actorw)\n",
      "22/57: actorlist\n",
      "22/58: degree_centralities\n",
      "22/59:\n",
      "boxofficegross = []\n",
      "for actor in actorlist:\n",
      "    boxofficegross.append(boxofficedict[actor])\n",
      "22/60: boxofficegross\n",
      "22/61: plot.plot(degree_centralities, boxofficegross, \"ro\")\n",
      "22/62: plot.plot(degree_centralities[:313], boxofficegross[:313], \"ro\")\n",
      "22/63: plot.plot(degree_centralities[313:], boxofficegross[313:], \"ro\")\n",
      "22/64:\n",
      "x = np.array(degree_centralities)\n",
      "y = np.array(boxofficegross)\n",
      "slope, intercept, r_value, p_value, std_err = stats.linregress(x,y)\n",
      "slope, intercept, r_value, p_value, std_err\n",
      "22/65:\n",
      "plot.figure()\n",
      "plot.plot(x, y, \"o\")\n",
      "yfit = [intercept + slope * xi for xi in x]\n",
      "plot.plot(x, yfit)\n",
      "22/66:\n",
      "xw = np.array(degree_centralities[:313])\n",
      "yw = np.array(boxofficegross[:313])\n",
      "slope, intercept, r_value, p_value, std_err = stats.linregress(xw,yw)\n",
      "slope, intercept, r_value, p_value, std_err\n",
      "22/67:\n",
      "xm = np.array(degree_centralities[313:])\n",
      "ym = np.array(boxofficegross[313:])\n",
      "slope, intercept, r_value, p_value, std_err = stats.linregress(xm,ym)\n",
      "slope, intercept, r_value, p_value, std_err\n",
      "22/68:\n",
      "academynominated = []\n",
      "for a in actorlist:\n",
      "    if a in academylist:\n",
      "        academynominated.append(1)\n",
      "    else:\n",
      "        academynominated.append(0)\n",
      "22/69:\n",
      "x_train = np.concatenate((x[0:213], x[313:734]))\n",
      "x_test = np.concatenate((x[213:313], x[734:]))\n",
      "y_train = np.concatenate((academynominated[0:213], academynominated[313:734]))\n",
      "y_test = np.concatenate((academynominated[213:313], academynominated[734:]))\n",
      "22/70: log = LogisticRegression(solver='lbfgs').fit(x, academynominated)\n",
      "22/71: log = LogisticRegression(solver='lbfgs').fit(x_train, academynominated)\n",
      "22/72: log = LogisticRegression(solver='lbfgs').fit(x_train, y_train)\n",
      "22/73: log = LogisticRegression(solver='lbfgs').fit(x.reshape(-1, 1), academynominated)\n",
      "22/74: log.score(x, academynominated)\n",
      "22/75: log.score(x.reshape(-1, 1), academynominated)\n",
      "22/76:\n",
      "lr = LogisticRegression(solver='lbfgs').fit(x_train, y_train)\n",
      "lr.score(x_train, y_train)\n",
      "22/77:\n",
      "lr = LogisticRegression(solver='lbfgs').fit(x_train.reshape(-1, 1), y_train)\n",
      "lr.score(x_train.reshape(-1, 1), y_train)\n",
      "22/78: lr.score(x_test.reshape(-1, 1), y_test)\n",
      "22/79: nx.write_gexf(G, \"graph.gexf\")\n",
      "22/80:\n",
      "eigenvector_centralities = []\n",
      "for actor in actorlist:\n",
      "    eigenvector_centralities.append(eigenvector_centrality.get(actor))\n",
      "22/81:\n",
      "x_e = np.array(eigenvector_centralities)\n",
      "y = np.array(boxofficegross)\n",
      "slope, intercept, r_value, p_value, std_err = stats.linregress(x_e,y)\n",
      "slope, intercept, r_value, p_value, std_err\n",
      "22/82:\n",
      "betweenness_centralities = []\n",
      "for actor in actorlist:\n",
      "    betweenness_centralities.append(betweenness_centrality.get(actor))\n",
      "x_b = np.array(betweenness_centralities)\n",
      "y = np.array(boxofficegross)\n",
      "slope, intercept, r_value, p_value, std_err = stats.linregress(x_b,y)\n",
      "slope, intercept, r_value, p_value, std_err\n",
      "22/83:\n",
      "closeness_centralities = []\n",
      "for actor in actorlist:\n",
      "    closeness_centralities.append(closeness_centrality.get(actor))\n",
      "x_c = np.array(closeness_centralities)\n",
      "y = np.array(boxofficegross)\n",
      "slope, intercept, r_value, p_value, std_err = stats.linregress(x_c,y)\n",
      "slope, intercept, r_value, p_value, std_err\n",
      "22/84:\n",
      "log = LogisticRegression(solver='lbfgs').fit(x_e.reshape(-1,1), academynominated)\n",
      "log.score(x_e.reshape(-1,1), academynominated)\n",
      "22/85:\n",
      "log = LogisticRegression(solver='lbfgs').fit(x_b.reshape(-1,1), academynominated)\n",
      "log.score(x_b.reshape(-1,1), academynominated)\n",
      "22/86:\n",
      "log = LogisticRegression(solver='lbfgs').fit(x_c.reshape(-1,1), academynominated)\n",
      "log.score(x_c.reshape(-1,1), academynominated)\n",
      "22/87:\n",
      "moviecounts = []\n",
      "for actor in actorlist:\n",
      "    moviecounts.append(moviecountdict.get(actor))\n",
      "22/88:\n",
      "xc = np.array(moviecounts)\n",
      "yc = np.array(boxofficegross)\n",
      "slope, intercept, r_value, p_value, std_err = stats.linregress(xc,yc)\n",
      "slope, intercept, r_value, p_value, std_err\n",
      "22/89:\n",
      "plot.figure()\n",
      "plot.plot(moviecounts, boxofficegross, \"o\")\n",
      "plot.plot(moviecounts, [slope*x + intercept for x in moviecounts])\n",
      "22/90: constraint = nx.algorithms.structuralholes.constraint(G)\n",
      "22/91: sorted(constraint, key=constraint.get, reverse=True)\n",
      "22/92: effective_size = nx.algorithms.structuralholes.effective_size(G)\n",
      "22/93: sorted(constraint, key=constraint.get, reverse=False)\n",
      "22/94: effective_size = nx.algorithms.structuralholes.effective_size(G)\n",
      "22/95: sorted(effective_size, key=effective_sized.get, reverse=True)\n",
      "22/96: sorted(effective_size, key=effective_size.get, reverse=True)\n",
      "22/97: plot.plot(constraint, boxofficegross)\n",
      "22/98:\n",
      "constraints = []\n",
      "for actor in actorlist:\n",
      "    constraints.append(constraints.get(actor))\n",
      "x_co = np.array(constraints)\n",
      "y = np.array(boxofficegross)\n",
      "slope, intercept, r_value, p_value, std_err = stats.linregress(x_co,y)\n",
      "slope, intercept, r_value, p_value, std_err\n",
      "22/99:\n",
      "constraints = []\n",
      "for actor in actorlist:\n",
      "    constraints.append(constraint.get(actor))\n",
      "x_co = np.array(constraints)\n",
      "y = np.array(boxofficegross)\n",
      "slope, intercept, r_value, p_value, std_err = stats.linregress(x_co,y)\n",
      "slope, intercept, r_value, p_value, std_err\n",
      "22/100:\n",
      "constraints = []\n",
      "for actor in actorlist:\n",
      "    constraints.append(constraint.get(actor))\n",
      "x_co = np.array(constraints)\n",
      "y = np.array(boxofficegross)\n",
      "slope, intercept, r_value, p_value, std_err = stats.linregress(x_co,y)\n",
      "slope, intercept, r_value, p_value, std_err\n",
      "constraints\n",
      "22/101:\n",
      "constraints = []\n",
      "for actor in actorlist:\n",
      "    constraints.append(constraint.get(actor))\n",
      "x_co = np.array(constraints)\n",
      "y = np.array(boxofficegross)\n",
      "slope, intercept, r_value, p_value, std_err = stats.linregress(x_co,y)\n",
      "slope, intercept, r_value, p_value, std_err\n",
      "22/102:\n",
      "effective_sizes = []\n",
      "for actor in actorlist:\n",
      "    effective_sizes.append(effective_size.get(actor))\n",
      "x_ef = np.array(effective_sizes)\n",
      "y = np.array(boxofficegross)\n",
      "slope, intercept, r_value, p_value, std_err = stats.linregress(x_ef,y)\n",
      "slope, intercept, r_value, p_value, std_err\n",
      "22/103:\n",
      "closeness_centralities = []\n",
      "for actor in actorlist:\n",
      "    closeness_centralities.append(closeness_centrality.get(actor))\n",
      "x_c = np.array(closeness_centralities)\n",
      "y = np.array(boxofficegross)\n",
      "slope, intercept, r_value, p_value, std_err = stats.linregress(x_c,y)\n",
      "slope, intercept, r_value, p_value, std_err\n",
      "22/104:\n",
      "closeness_centralities = []\n",
      "for actor in actorlist:\n",
      "    closeness_centralities.append(closeness_centrality.get(actor))\n",
      "x_c = np.array(closeness_centralities)\n",
      "y = np.array(boxofficegross)\n",
      "slope, intercept, r_value, p_value, std_err = stats.linregress(x_c,y)\n",
      "slope, intercept, r_value, p_value, std_err \n",
      "closeness_centralities\n",
      "22/105:\n",
      "closeness_centralities = []\n",
      "for actor in actorlist:\n",
      "    closeness_centralities.append(closeness_centrality.get(actor))\n",
      "x_c = np.array(closeness_centralities)\n",
      "y = np.array(boxofficegross)\n",
      "slope, intercept, r_value, p_value, std_err = stats.linregress(x_c,y)\n",
      "slope, intercept, r_value, p_value, std_err\n",
      "22/106:\n",
      "constraints = []\n",
      "for actor in actorlist:\n",
      "    constraints.append(constraint.get(actor))\n",
      "x_co = ~np.isnan(np.array(constraints))\n",
      "y = np.array(boxofficegross)\n",
      "slope, intercept, r_value, p_value, std_err = stats.linregress(x_co,y)\n",
      "slope, intercept, r_value, p_value, std_err\n",
      "22/107:\n",
      "effective_sizes = []\n",
      "for actor in actorlist:\n",
      "    effective_sizes.append(effective_size.get(actor))\n",
      "x_ef = ~np.isnan(np.array(effective_sizes))\n",
      "y = np.array(boxofficegross)\n",
      "slope, intercept, r_value, p_value, std_err = stats.linregress(x_ef,y)\n",
      "slope, intercept, r_value, p_value, std_err\n",
      "22/108: X = df[['degree_centralities', 'constraints', 'moviecount']]\n",
      "22/109:\n",
      "X = sm.add_constant(X) # adding a constant\n",
      "Y = boxofficegross\n",
      "model = sm.OLS(Y, X).fit()\n",
      "predictions = model.predict(X) \n",
      " \n",
      "print_model = model.summary()\n",
      "print(print_model)\n",
      "22/110:\n",
      "df = pd.DataFrame(\n",
      "    {\n",
      "        'degree_centralities': degree_centralities,\n",
      "        'eigenvector_centralities': eigenvector_centralities,\n",
      "        'betweenness_centralities': eigenvector_centralities,\n",
      "        'closeness_centralities': eigenvector_centralities,\n",
      "        'constraints': constraints,\n",
      "        'effective_sizes': effective_sizes,\n",
      "        'moviecount': moviecounts,\n",
      "        'boxofficegross': boxofficegross,\n",
      "        'academynominated': academynominated\n",
      "    })\n",
      "22/111:\n",
      "import statsmodels.api as sm\n",
      "import pandas as pd\n",
      "22/112:\n",
      "df = pd.DataFrame(\n",
      "    {\n",
      "        'degree_centralities': degree_centralities,\n",
      "        'eigenvector_centralities': eigenvector_centralities,\n",
      "        'betweenness_centralities': eigenvector_centralities,\n",
      "        'closeness_centralities': eigenvector_centralities,\n",
      "        'constraints': constraints,\n",
      "        'effective_sizes': effective_sizes,\n",
      "        'moviecount': moviecounts,\n",
      "        'boxofficegross': boxofficegross,\n",
      "        'academynominated': academynominated\n",
      "    })\n",
      "22/113:\n",
      "df = pd.DataFrame(\n",
      "    {\n",
      "        'degree_centralities': degree_centralities,\n",
      "        'eigenvector_centralities': eigenvector_centralities,\n",
      "        'betweenness_centralities': eigenvector_centralities,\n",
      "        'closeness_centralities': eigenvector_centralities,\n",
      "        'constraints': constraints,\n",
      "        'effective_sizes': effective_sizes,\n",
      "        'moviecount': moviecounts,\n",
      "        'boxofficegross': boxofficegross,\n",
      "        'academynominated': academynominated\n",
      "    })\n",
      "22/114: X = df[['degree_centralities', 'eigenvector_centralities', 'betweenness_centralities', 'constraints', 'effective_sizes', 'moviecount']]\n",
      "22/115: X = df[['degree_centralities', 'eigenvector_centralities', 'betweenness_centralities', 'closeness_centralities', 'constraints', 'effective_sizes', 'moviecount']]\n",
      "22/116:\n",
      "X = sm.add_constant(X) # adding a constant\n",
      "Y = boxofficegross\n",
      "model = sm.OLS(Y, X).fit()\n",
      "predictions = model.predict(X) \n",
      " \n",
      "print_model = model.summary()\n",
      "print(print_model)\n",
      "22/117:\n",
      "constraints = []\n",
      "for actor in actorlist:\n",
      "    constraints.append(constraint.get(actor))\n",
      "constraints = ~np.isnan(np.array(constraints))\n",
      "x_co = constraints\n",
      "y = np.array(boxofficegross)\n",
      "slope, intercept, r_value, p_value, std_err = stats.linregress(x_co,y)\n",
      "slope, intercept, r_value, p_value, std_err\n",
      "22/118:\n",
      "effective_sizes = []\n",
      "for actor in actorlist:\n",
      "    effective_sizes.append(effective_size.get(actor))\n",
      "effective_sizes = ~np.isnan(np.array(effective_sizes))\n",
      "x_ef = effective_sizes\n",
      "y = np.array(boxofficegross)\n",
      "slope, intercept, r_value, p_value, std_err = stats.linregress(x_ef,y)\n",
      "slope, intercept, r_value, p_value, std_err\n",
      "22/119:\n",
      "df = pd.DataFrame(\n",
      "    {\n",
      "        'degree_centralities': degree_centralities,\n",
      "        'eigenvector_centralities': eigenvector_centralities,\n",
      "        'betweenness_centralities': eigenvector_centralities,\n",
      "        'closeness_centralities': eigenvector_centralities,\n",
      "        'constraints': constraints,\n",
      "        'effective_sizes': effective_sizes,\n",
      "        'moviecount': moviecounts,\n",
      "        'boxofficegross': boxofficegross,\n",
      "        'academynominated': academynominated\n",
      "    })\n",
      "22/120: X = df[['degree_centralities', 'eigenvector_centralities', 'betweenness_centralities', 'closeness_centralities', 'constraints', 'effective_sizes', 'moviecount']]\n",
      "22/121:\n",
      "X = sm.add_constant(X) # adding a constant\n",
      "Y = boxofficegross\n",
      "model = sm.OLS(Y, X).fit()\n",
      "predictions = model.predict(X) \n",
      " \n",
      "print_model = model.summary()\n",
      "print(print_model)\n",
      "22/122: plot.plot(constraints, boxofficegross)\n",
      "22/123: plot.plot(constraints, boxofficegross, \"o\")\n",
      "22/124:\n",
      "df = pd.DataFrame(\n",
      "    {\n",
      "        'degree_centralities': degree_centralities,\n",
      "        'eigenvector_centralities': eigenvector_centralities,\n",
      "        'betweenness_centralities': betweenness_centralities,\n",
      "        'closeness_centralities': closeness_centralities,\n",
      "        'moviecount': moviecounts,\n",
      "        'boxofficegross': boxofficegross,\n",
      "        'academynominated': academynominated\n",
      "    })\n",
      "22/125: X = df[['degree_centralities', 'eigenvector_centralities', 'betweenness_centralities', 'closeness_centralities', 'moviecount']]\n",
      "22/126:\n",
      "X = sm.add_constant(X) # adding a constant\n",
      "Y = boxofficegross\n",
      "model = sm.OLS(Y, X).fit()\n",
      "predictions = model.predict(X) \n",
      " \n",
      "print_model = model.summary()\n",
      "print(print_model)\n",
      "23/1:\n",
      "import networkx as nx\n",
      "import csv\n",
      "import matplotlib.pyplot as plot\n",
      "23/2:\n",
      "from bs4 import BeautifulSoup\n",
      "import urllib2\n",
      "23/3:\n",
      "from sklearn.linear_model import LinearRegression, LogisticRegression\n",
      "import numpy as np\n",
      "from scipy import stats\n",
      "23/4:\n",
      "page0 = urllib2.urlopen(\"https://en.wikipedia.org/wiki/List_of_actors_with_Academy_Award_nominations\")\n",
      "soup0 = BeautifulSoup(page0, 'html.parser')\n",
      "23/5: actors0 = soup0.find_all('tr')[11:][:-11]\n",
      "23/6: actors0[0].find('a').text\n",
      "23/7:\n",
      "academylist = []\n",
      "for actor in actors0:\n",
      "    academylist.append(actor.find('a').text)\n",
      "23/8: academylist\n",
      "23/9:\n",
      "page1 = urllib2.urlopen(\"https://www.boxofficemojo.com/people/?view=Actor&p=.htm\")\n",
      "soup1 = BeautifulSoup(page1, 'html.parser')\n",
      "23/10: actors1 = soup1.find_all('tr')[2].find_all('tr')[1:]\n",
      "23/11: actors1\n",
      "23/12: boxofficedict = {}\n",
      "23/13:\n",
      "def addtodict(actors):\n",
      "    for actor in actors:\n",
      "        name = actor.find('b').find('b').text\n",
      "        boxoffice = str(actor.find('td', attrs={'align':'right'}).text[1:]).replace(\",\", \"\")\n",
      "        if \"k\" in boxoffice:\n",
      "            boxoffice = float(boxoffice.replace(\"k\", \"\"))/1000\n",
      "        boxofficedict[name] = float(boxoffice)\n",
      "23/14: boxofficedict\n",
      "23/15:\n",
      "page2 = urllib2.urlopen(\"https://www.boxofficemojo.com/people/?view=Actor&pagenum=2&sort=person&order=ASC&p=.htm\")\n",
      "soup2 = BeautifulSoup(page2, 'html.parser')\n",
      "actors2 = soup2.find_all('tr')[2].find_all('tr')[1:]\n",
      "23/16:\n",
      "page3 = urllib2.urlopen(\"https://www.boxofficemojo.com/people/?view=Actor&pagenum=3&sort=person&order=ASC&p=.htm\")\n",
      "soup3 = BeautifulSoup(page3, 'html.parser')\n",
      "actors3 = soup3.find_all('tr')[2].find_all('tr')[1:]\n",
      "23/17:\n",
      "addtodict(actors1)\n",
      "addtodict(actors2)\n",
      "addtodict(actors3)\n",
      "23/18: len(boxofficedict)\n",
      "23/19: boxofficedict\n",
      "23/20:\n",
      "with open('title.basics.tsv') as tsvfile:\n",
      "    reader = csv.reader(tsvfile, delimiter='\\t')\n",
      "    i = 0\n",
      "    for row in reader:\n",
      "        if i < 200:\n",
      "            print(row)\n",
      "            i = i + 1\n",
      "        else:\n",
      "            break\n",
      "23/21:\n",
      "dict = {}\n",
      "with open('name.basics.tsv') as tsvfile:\n",
      "    reader = csv.reader(tsvfile, delimiter='\\t')\n",
      "    i = 0\n",
      "    for row in reader:\n",
      "        if i < 200:\n",
      "            print(row)\n",
      "            i = i + 1\n",
      "        if i > 1:\n",
      "            dict[row[0]] = row[1]\n",
      "23/22:\n",
      "G = nx.Graph()\n",
      "Gw = nx.Graph()\n",
      "Gm = nx.Graph()\n",
      "moviecountdict = {}\n",
      "\n",
      "with open('title.principals.tsv') as tsvfile:\n",
      "    reader = csv.reader(tsvfile, delimiter='\\t')\n",
      "    prev = \"\"\n",
      "    actors = []\n",
      "    for row in reader:\n",
      "        if row[3] == 'actor' and row[2] in dict and dict[row[2]] in boxofficedict:\n",
      "            Gm.add_node(dict[row[2]])\n",
      "            G.add_node(dict[row[2]])\n",
      "            if dict[row[2]] in moviecountdict:\n",
      "                moviecountdict[dict[row[2]]] += 1\n",
      "            else:\n",
      "                moviecountdict[dict[row[2]]] = 1\n",
      "        elif row[3] == 'actress' and row[2] in dict and dict[row[2]] in boxofficedict:\n",
      "            Gw.add_node(dict[row[2]])\n",
      "            G.add_node(dict[row[2]])\n",
      "            if dict[row[2]] in moviecountdict:\n",
      "                moviecountdict[dict[row[2]]] += 1\n",
      "            else:\n",
      "                moviecountdict[dict[row[2]]] = 1\n",
      "        else:\n",
      "            continue\n",
      "        if row[0] != prev:\n",
      "            for a1 in actors:\n",
      "                for a2 in actors:\n",
      "                    if a1 == a2:\n",
      "                        continue\n",
      "                    G.add_edge(a1, a2)\n",
      "                    if a1 in Gw.nodes and a2 in Gw.nodes:\n",
      "                        Gw.add_edge(a1, a2)\n",
      "                    elif a1 in Gm.nodes and a2 in Gm.nodes:\n",
      "                        Gm.add_edge(a1, a2)\n",
      "            actors = [dict[row[2]]]\n",
      "        else:\n",
      "            actors.append(dict[row[2]])\n",
      "        prev = row[0]\n",
      "23/23: moviecountdict\n",
      "23/24: list(G.nodes)\n",
      "23/25: len(G.nodes)\n",
      "23/26: list(G.edges)\n",
      "23/27: len(G.edges)\n",
      "23/28: nx.draw(G, with_labels=False, font_weight='bold')\n",
      "23/29: nx.draw(G, with_labels=True)\n",
      "23/30:\n",
      "actorw = sorted(Gw.nodes)\n",
      "actorm = sorted(Gm.nodes)\n",
      "23/31: degree_centrality = nx.degree_centrality(G)\n",
      "23/32: sorted(degree_centrality, key=degree_centrality.get, reverse=True)\n",
      "23/33: eigenvector_centrality = nx.eigenvector_centrality(G)\n",
      "23/34: sorted(eigenvector_centrality, key=eigenvector_centrality.get, reverse=True)\n",
      "23/35: closeness_centrality = nx.closeness_centrality(G)\n",
      "23/36: sorted(closeness_centrality, key=closeness_centrality.get, reverse=True)\n",
      "23/37: betweenness_centrality = nx.betweenness_centrality(G)\n",
      "23/38: sorted(betweenness_centrality, key=betweenness_centrality.get, reverse=True)\n",
      "23/39:\n",
      "degree_centralities = []\n",
      "actorlist = [a for a in actorw]\n",
      "for a in actorm:\n",
      "    actorlist.append(a)\n",
      "for actor in actorlist:\n",
      "    degree_centralities.append(degree_centrality.get(actor))\n",
      "23/40: len(actorw)\n",
      "23/41: actorlist\n",
      "23/42: degree_centralities\n",
      "23/43:\n",
      "boxofficegross = []\n",
      "for actor in actorlist:\n",
      "    boxofficegross.append(boxofficedict[actor])\n",
      "23/44: boxofficegross\n",
      "23/45: plot.plot(degree_centralities, boxofficegross, \"ro\")\n",
      "23/46: plot.plot(degree_centralities[:313], boxofficegross[:313], \"ro\")\n",
      "23/47: plot.plot(degree_centralities[313:], boxofficegross[313:], \"ro\")\n",
      "23/48:\n",
      "x = np.array(degree_centralities)\n",
      "y = np.array(boxofficegross)\n",
      "slope, intercept, r_value, p_value, std_err = stats.linregress(x,y)\n",
      "slope, intercept, r_value, p_value, std_err\n",
      "23/49:\n",
      "plot.figure()\n",
      "plot.plot(x, y, \"o\")\n",
      "yfit = [intercept + slope * xi for xi in x]\n",
      "plot.plot(x, yfit)\n",
      "23/50:\n",
      "xw = np.array(degree_centralities[:313])\n",
      "yw = np.array(boxofficegross[:313])\n",
      "slope, intercept, r_value, p_value, std_err = stats.linregress(xw,yw)\n",
      "slope, intercept, r_value, p_value, std_err\n",
      "23/51:\n",
      "xm = np.array(degree_centralities[313:])\n",
      "ym = np.array(boxofficegross[313:])\n",
      "slope, intercept, r_value, p_value, std_err = stats.linregress(xm,ym)\n",
      "slope, intercept, r_value, p_value, std_err\n",
      "23/52:\n",
      "academynominated = []\n",
      "for a in actorlist:\n",
      "    if a in academylist:\n",
      "        academynominated.append(1)\n",
      "    else:\n",
      "        academynominated.append(0)\n",
      "23/53:\n",
      "x_train = np.concatenate((x[0:213], x[313:734]))\n",
      "x_test = np.concatenate((x[213:313], x[734:]))\n",
      "y_train = np.concatenate((academynominated[0:213], academynominated[313:734]))\n",
      "y_test = np.concatenate((academynominated[213:313], academynominated[734:]))\n",
      "23/54: log = LogisticRegression(solver='lbfgs').fit(x.reshape(-1, 1), academynominated)\n",
      "23/55: log.score(x.reshape(-1, 1), academynominated)\n",
      "23/56:\n",
      "lr = LogisticRegression(solver='lbfgs').fit(x_train.reshape(-1, 1), y_train)\n",
      "lr.score(x_train.reshape(-1, 1), y_train)\n",
      "23/57: lr.score(x_test.reshape(-1, 1), y_test)\n",
      "23/58:\n",
      "eigenvector_centralities = []\n",
      "for actor in actorlist:\n",
      "    eigenvector_centralities.append(eigenvector_centrality.get(actor))\n",
      "23/59:\n",
      "x_e = np.array(eigenvector_centralities)\n",
      "y = np.array(boxofficegross)\n",
      "slope, intercept, r_value, p_value, std_err = stats.linregress(x_e,y)\n",
      "slope, intercept, r_value, p_value, std_err\n",
      "23/60:\n",
      "betweenness_centralities = []\n",
      "for actor in actorlist:\n",
      "    betweenness_centralities.append(betweenness_centrality.get(actor))\n",
      "x_b = np.array(betweenness_centralities)\n",
      "y = np.array(boxofficegross)\n",
      "slope, intercept, r_value, p_value, std_err = stats.linregress(x_b,y)\n",
      "slope, intercept, r_value, p_value, std_err\n",
      "23/61:\n",
      "closeness_centralities = []\n",
      "for actor in actorlist:\n",
      "    closeness_centralities.append(closeness_centrality.get(actor))\n",
      "x_c = np.array(closeness_centralities)\n",
      "y = np.array(boxofficegross)\n",
      "slope, intercept, r_value, p_value, std_err = stats.linregress(x_c,y)\n",
      "slope, intercept, r_value, p_value, std_err\n",
      "23/62:\n",
      "log_e = LogisticRegression(solver='lbfgs').fit(x_e.reshape(-1,1), academynominated)\n",
      "log_e.score(x_e.reshape(-1,1), academynominated)\n",
      "23/63:\n",
      "log_e = LogisticRegression(solver='lbfgs').fit(x_e.reshape(-1,1), academynominated)\n",
      "log_e.score(x_e.reshape(-1,1), academynominated)\n",
      "x_e\n",
      "23/64:\n",
      "log = LogisticRegression(solver='lbfgs').fit(x_b.reshape(-1,1), academynominated)\n",
      "log.score(x_b.reshape(-1,1), academynominated)\n",
      "x_b\n",
      "23/65:\n",
      "log_e = LogisticRegression(solver='lbfgs').fit(x_e.reshape(-1,1), academynominated)\n",
      "log_e.score(x_e.reshape(-1,1), academynominated)\n",
      "23/66:\n",
      "log = LogisticRegression(solver='lbfgs').fit(x_b.reshape(-1,1), academynominated)\n",
      "log.score(x_b.reshape(-1,1), academynominated)\n",
      "23/67:\n",
      "log = LogisticRegression(solver='lbfgs').fit(x_c.reshape(-1,1), academynominated)\n",
      "log.score(x_c.reshape(-1,1), academynominated)\n",
      "23/68:\n",
      "log_c = LogisticRegression(solver='lbfgs').fit(x_c.reshape(-1,1), academynominated)\n",
      "log_C.score(x_c.reshape(-1,1), academynominated)\n",
      "23/69:\n",
      "log_c = LogisticRegression(solver='lbfgs').fit(x_c.reshape(-1,1), academynominated)\n",
      "log_c.score(x_c.reshape(-1,1), academynominated)\n",
      "23/70:\n",
      "log_e = LogisticRegression(solver='lbfgs').fit(x_e.reshape(-1,1), academynominated)\n",
      "log_e.score(x.reshape(-1,1), academynominated)\n",
      "23/71:\n",
      "log_e = LogisticRegression(solver='lbfgs').fit(x_e.reshape(-1,1), academynominated)\n",
      "log_e.score(x_e.reshape(-1,1), academynominated)\n",
      "23/72: np.correlate(x_e, x_b)\n",
      "23/73: np.correlate(x_e, x)\n",
      "23/74: np.corrcoef(x_e, x)\n",
      "23/75: print np.corrcoef(x, x_e), np.corrcoef(x, x_b), np.corrcoef(x, x_c)\n",
      "23/76:\n",
      "print np.corrcoef(x, x_e)\n",
      "print np.corrcoef(x, x_b)\n",
      "print np.corrcoef(x, x_c)\n",
      "23/77:\n",
      "print np.corrcoef(x, x_e)[0,1]\n",
      "print np.corrcoef(x, x_b)\n",
      "print np.corrcoef(x, x_c)\n",
      "23/78:\n",
      "print np.corrcoef(x, x_e)[0,1]\n",
      "print np.corrcoef(x, x_b)[0,1]\n",
      "print np.corrcoef(x, x_c)[0,1]\n",
      "23/79: pred_x = log.predict(x.reshape(-1, 1))\n",
      "23/80:\n",
      "pred_x = log.predict(x.reshape(-1, 1))\n",
      "pred_e = log_e.predict(x.reshape(-1, 1))\n",
      "23/81: pred_x\n",
      "23/82: pred_e\n",
      "23/83: pred_x\n",
      "23/84: count(academynominated)\n",
      "23/85: sum(academynominated)\n",
      "23/86: len(academynominated)\n",
      "23/87: sum(academynominated)\n",
      "23/88: slope, intercept, r_value, p_value, std_err = stats.linregress(x, academynominated)\n",
      "23/89:\n",
      "slope, intercept, r_value, p_value, std_err = stats.linregress(x, academynominated)\n",
      "slope, intercept, r_value, p_value, std_err\n",
      "23/90: plot.plot(x, academynominated)\n",
      "23/91: plot.plot(x, academynominated, \"o\")\n",
      "23/92:\n",
      "X2 = df[['degree_centralities']]\n",
      "X2 = sm.add_constant(X2) # adding a constant\n",
      "Y2 = academynominated\n",
      "model2 = sm.OLS(Y2, X2).fit()\n",
      "predictions2 = model2.predict(X2) \n",
      "print_model2 = model2.summary()\n",
      "print(print_model)\n",
      "23/93:\n",
      "X2 = df[['degree_centralities']]\n",
      "X2 = sm.add_constant(X2) # adding a constant\n",
      "Y2 = academynominated\n",
      "model2 = sm.OLS(Y2, X2).fit()\n",
      "predictions2 = model2.predict(X2) \n",
      "print_model2 = model2.summary()\n",
      "print(print_model2)\n",
      "23/94:\n",
      "df = pd.DataFrame(\n",
      "    {\n",
      "        'degree_centralities': degree_centralities,\n",
      "        'eigenvector_centralities': eigenvector_centralities,\n",
      "        'betweenness_centralities': betweenness_centralities,\n",
      "        'closeness_centralities': closeness_centralities,\n",
      "        'moviecount': moviecounts,\n",
      "        'boxofficegross': boxofficegross,\n",
      "        'academynominated': academynominated\n",
      "    })\n",
      "23/95:\n",
      "import statsmodels.api as sm\n",
      "import pandas as pd\n",
      "23/96:\n",
      "df = pd.DataFrame(\n",
      "    {\n",
      "        'degree_centralities': degree_centralities,\n",
      "        'eigenvector_centralities': eigenvector_centralities,\n",
      "        'betweenness_centralities': betweenness_centralities,\n",
      "        'closeness_centralities': closeness_centralities,\n",
      "        'moviecount': moviecounts,\n",
      "        'boxofficegross': boxofficegross,\n",
      "        'academynominated': academynominated\n",
      "    })\n",
      "23/97:\n",
      "moviecounts = []\n",
      "for actor in actorlist:\n",
      "    moviecounts.append(moviecountdict.get(actor))\n",
      "23/98:\n",
      "df = pd.DataFrame(\n",
      "    {\n",
      "        'degree_centralities': degree_centralities,\n",
      "        'eigenvector_centralities': eigenvector_centralities,\n",
      "        'betweenness_centralities': betweenness_centralities,\n",
      "        'closeness_centralities': closeness_centralities,\n",
      "        'moviecount': moviecounts,\n",
      "        'boxofficegross': boxofficegross,\n",
      "        'academynominated': academynominated\n",
      "    })\n",
      "23/99:\n",
      "X2 = df[['degree_centralities']]\n",
      "X2 = sm.add_constant(X2) # adding a constant\n",
      "Y2 = academynominated\n",
      "model2 = sm.OLS(Y2, X2).fit()\n",
      "predictions2 = model2.predict(X2) \n",
      "print_model2 = model2.summary()\n",
      "print(print_model2)\n",
      "23/100:\n",
      "X2 = df[['degree_centralities', 'moviecount']]\n",
      "X2 = sm.add_constant(X2) # adding a constant\n",
      "Y2 = academynominated\n",
      "model2 = sm.OLS(Y2, X2).fit()\n",
      "predictions2 = model2.predict(X2) \n",
      "print_model2 = model2.summary()\n",
      "print(print_model2)\n",
      "23/101:\n",
      "X2 = df[['degree_centralities', 'eigenvector_centralities', 'betweenness_centralities', 'closeness_centralities', 'moviecount']]\n",
      "X2 = sm.add_constant(X2) # adding a constant\n",
      "Y2 = academynominated\n",
      "model2 = sm.OLS(Y2, X2).fit()\n",
      "predictions2 = model2.predict(X2) \n",
      "print_model2 = model2.summary()\n",
      "print(print_model2)\n",
      "23/102: plot.plot(model.fit())\n",
      "23/103: plot.plot(model2.fit())\n",
      "23/104: plot.plot(x, predictions2)\n",
      "23/105: plot.plot(x, predictions2, \"0\")\n",
      "23/106: plot.plot(x, predictions2, \"o\")\n",
      "23/107: plot.plot(predictions2, academynominated, \"o\")\n",
      "23/108:\n",
      "predictions2bin = [1 if x >= 0.5 else 0 for x in predictions2]\n",
      "plot.plot(predictions2bin, academynominated, \"o\")\n",
      "23/109:\n",
      "predictions2bin = [1 if x >= 0.5 else 0 for x in predictions2]\n",
      "wrong = [1 if predictions2bin[i] == academynominated[i] else 0 for i in range(834)]\n",
      "23/110:\n",
      "predictions2bin = [1 if x >= 0.5 else 0 for x in predictions2]\n",
      "wrong = [1 if predictions2bin[i] == academynominated[i] else 0 for i in range(834)]\n",
      "sum(wrong)\n",
      "23/111: predictions2bin\n",
      "23/112: academynominated\n",
      "23/113:\n",
      "X2 = df[['degree_centralities']] #, 'eigenvector_centralities', 'betweenness_centralities', 'closeness_centralities', 'moviecount']]\n",
      "X2 = sm.add_constant(X2) # adding a constant\n",
      "Y2 = academynominated\n",
      "model2 = sm.OLS(Y2, X2).fit()\n",
      "predictions2 = model2.predict(X2) \n",
      "print_model2 = model2.summary()\n",
      "print(print_model2)\n",
      "23/114:\n",
      "predictions2bin = [1 if x >= 0.5 else 0 for x in predictions2]\n",
      "wrong = [1 if predictions2bin[i] == academynominated[i] else 0 for i in range(834)]\n",
      "sum(wrong)\n",
      "23/115:\n",
      "X2 = df[['degree_centralities', 'moviecount']] #, 'eigenvector_centralities', 'betweenness_centralities', 'closeness_centralities', 'moviecount']]\n",
      "X2 = sm.add_constant(X2) # adding a constant\n",
      "Y2 = academynominated\n",
      "model2 = sm.OLS(Y2, X2).fit()\n",
      "predictions2 = model2.predict(X2) \n",
      "print_model2 = model2.summary()\n",
      "print(print_model2)\n",
      "23/116:\n",
      "predictions2bin = [1 if x >= 0.5 else 0 for x in predictions2]\n",
      "wrong = [1 if predictions2bin[i] == academynominated[i] else 0 for i in range(834)]\n",
      "sum(wrong)\n",
      "23/117:\n",
      "X2 = df[['eigenvector_centralities', 'moviecount']] #, 'eigenvector_centralities', 'betweenness_centralities', 'closeness_centralities', 'moviecount']]\n",
      "X2 = sm.add_constant(X2) # adding a constant\n",
      "Y2 = academynominated\n",
      "model2 = sm.OLS(Y2, X2).fit()\n",
      "predictions2 = model2.predict(X2) \n",
      "print_model2 = model2.summary()\n",
      "print(print_model2)\n",
      "23/118:\n",
      "predictions2bin = [1 if x >= 0.5 else 0 for x in predictions2]\n",
      "wrong = [1 if predictions2bin[i] == academynominated[i] else 0 for i in range(834)]\n",
      "sum(wrong)\n",
      "23/119:\n",
      "predictions2bin = [1 if x >= 0.5 else 0 for x in predictions2]\n",
      "wrong = [0 if predictions2bin[i] == academynominated[i] else 1 for i in range(834)]\n",
      "sum(wrong)\n",
      "23/120:\n",
      "predictions2bin = [1 if x >= 0.5 else 0 for x in predictions2]\n",
      "wrong = [0 if predictions2bin[i] == academynominated[i] else 1 for i in range(834)]\n",
      "print sum(predictions2bin), sum(academynominated), sum(wrong), len(wrong), float(sum(wrong))/len(wrong)\n",
      "23/121:\n",
      "predictions2bin = [1 if x >= 0.5 else 0 for x in predictions2]\n",
      "wrong = [0 if predictions2bin[i] == academynominated[i] else 1 for i in range(834)]\n",
      "print sum(predictions2bin), sum(academynominated), sum(wrong), len(wrong), float(sum(wrong))/len(wrong), 1-float(sum(wrong))/len(wrong)\n",
      "23/122:\n",
      "predictions2bin = [1 if x >= 0.5 else 0 for x in predictions2]\n",
      "wrong = [0 if predictions2bin[i] == academynominated[i] else 1 for i in range(834)]\n",
      "print sum(predictions2bin), sum(academynominated)\n",
      "print sum(wrong), len(wrong)\n",
      "print float(sum(wrong))/len(wrong), 1-float(sum(wrong))/len(wrong)\n",
      "23/123:\n",
      "X2e = df[['eigenvector_centralities', 'moviecount']] #, 'eigenvector_centralities', 'betweenness_centralities', 'closeness_centralities', 'moviecount']]\n",
      "X2e = sm.add_constant(X2e) # adding a constant\n",
      "Y2e = academynominated\n",
      "model2e = sm.OLS(Y2, X2e).fit()\n",
      "predictions2e = model2e.predict(X2e) \n",
      "print_model2e = model2e.summary()\n",
      "print(print_model2e)\n",
      "23/124:\n",
      "predictions2ebin = [1 if x >= 0.5 else 0 for x in predictions2e]\n",
      "wrong = [0 if predictions2ebin[i] == academynominated[i] else 1 for i in range(834)]\n",
      "print sum(predictions2ebin), sum(academynominated)\n",
      "print sum(wrong), len(wrong)\n",
      "print float(sum(wrong))/len(wrong), 1-float(sum(wrong))/len(wrong)\n",
      "23/125:\n",
      "X2 = df[['degree_centralities', 'moviecount']] #, 'eigenvector_centralities', 'betweenness_centralities', 'closeness_centralities', 'moviecount']]\n",
      "X2 = sm.add_constant(X2) # adding a constant\n",
      "Y2 = academynominated\n",
      "model2 = sm.OLS(Y2, X2).fit()\n",
      "predictions2 = model2.predict(X2) \n",
      "print_model2 = model2.summary()\n",
      "print(print_model2)\n",
      "23/126:\n",
      "predictions2bin = [1 if x >= 0.5 else 0 for x in predictions2]\n",
      "wrong = [0 if predictions2bin[i] == academynominated[i] else 1 for i in range(834)]\n",
      "print sum(predictions2bin), sum(academynominated)\n",
      "print sum(wrong), len(wrong)\n",
      "print float(sum(wrong))/len(wrong), 1-float(sum(wrong))/len(wrong)\n",
      "23/127:\n",
      "X2 = df[['degree_centralities', 'eigenvector_centralities', 'betweenness_centralities', 'closeness_centralities', 'moviecount']]\n",
      "X2 = sm.add_constant(X2) # adding a constant\n",
      "Y2 = academynominated\n",
      "model2 = sm.OLS(Y2, X2).fit()\n",
      "predictions2 = model2.predict(X2) \n",
      "print_model2 = model2.summary()\n",
      "print(print_model2)\n",
      "24/1:\n",
      "import networkx as nx\n",
      "import csv\n",
      "import matplotlib.pyplot as plot\n",
      "24/2:\n",
      "from bs4 import BeautifulSoup\n",
      "import urllib2\n",
      "24/3:\n",
      "from sklearn.linear_model import LinearRegression, LogisticRegression\n",
      "import numpy as np\n",
      "from scipy import stats\n",
      "24/4:\n",
      "page0 = urllib2.urlopen(\"https://en.wikipedia.org/wiki/List_of_actors_with_Academy_Award_nominations\")\n",
      "soup0 = BeautifulSoup(page0, 'html.parser')\n",
      "24/5: actors0 = soup0.find_all('tr')[11:][:-11]\n",
      "24/6: actors0[0].find('a').text\n",
      "24/7:\n",
      "academylist = []\n",
      "for actor in actors0:\n",
      "    academylist.append(actor.find('a').text)\n",
      "24/8: academylist\n",
      "24/9:\n",
      "page1 = urllib2.urlopen(\"https://www.boxofficemojo.com/people/?view=Actor&p=.htm\")\n",
      "soup1 = BeautifulSoup(page1, 'html.parser')\n",
      "24/10: actors1 = soup1.find_all('tr')[2].find_all('tr')[1:]\n",
      "24/11: actors1\n",
      "24/12: boxofficedict = {}\n",
      "24/13:\n",
      "def addtodict(actors):\n",
      "    for actor in actors:\n",
      "        name = actor.find('b').find('b').text\n",
      "        boxoffice = str(actor.find('td', attrs={'align':'right'}).text[1:]).replace(\",\", \"\")\n",
      "        if \"k\" in boxoffice:\n",
      "            boxoffice = float(boxoffice.replace(\"k\", \"\"))/1000\n",
      "        boxofficedict[name] = float(boxoffice)\n",
      "24/14: boxofficedict\n",
      "24/15:\n",
      "page2 = urllib2.urlopen(\"https://www.boxofficemojo.com/people/?view=Actor&pagenum=2&sort=person&order=ASC&p=.htm\")\n",
      "soup2 = BeautifulSoup(page2, 'html.parser')\n",
      "actors2 = soup2.find_all('tr')[2].find_all('tr')[1:]\n",
      "24/16:\n",
      "page3 = urllib2.urlopen(\"https://www.boxofficemojo.com/people/?view=Actor&pagenum=3&sort=person&order=ASC&p=.htm\")\n",
      "soup3 = BeautifulSoup(page3, 'html.parser')\n",
      "actors3 = soup3.find_all('tr')[2].find_all('tr')[1:]\n",
      "24/17:\n",
      "addtodict(actors1)\n",
      "addtodict(actors2)\n",
      "addtodict(actors3)\n",
      "24/18: len(boxofficedict)\n",
      "24/19: boxofficedict\n",
      "24/20:\n",
      "with open('title.basics.tsv') as tsvfile:\n",
      "    reader = csv.reader(tsvfile, delimiter='\\t')\n",
      "    i = 0\n",
      "    for row in reader:\n",
      "        if i < 200:\n",
      "            print(row)\n",
      "            i = i + 1\n",
      "        else:\n",
      "            break\n",
      "24/21:\n",
      "dict = {}\n",
      "with open('name.basics.tsv') as tsvfile:\n",
      "    reader = csv.reader(tsvfile, delimiter='\\t')\n",
      "    i = 0\n",
      "    for row in reader:\n",
      "        if i < 200:\n",
      "            print(row)\n",
      "            i = i + 1\n",
      "        if i > 1:\n",
      "            dict[row[0]] = row[1]\n",
      "24/22:\n",
      "G = nx.Graph()\n",
      "Gw = nx.Graph()\n",
      "Gm = nx.Graph()\n",
      "moviecountdict = {}\n",
      "\n",
      "with open('title.principals.tsv') as tsvfile:\n",
      "    reader = csv.reader(tsvfile, delimiter='\\t')\n",
      "    prev = \"\"\n",
      "    actors = []\n",
      "    for row in reader:\n",
      "        if row[3] == 'actor' and row[2] in dict and dict[row[2]] in boxofficedict:\n",
      "            Gm.add_node(dict[row[2]])\n",
      "            G.add_node(dict[row[2]])\n",
      "            if dict[row[2]] in moviecountdict:\n",
      "                moviecountdict[dict[row[2]]] += 1\n",
      "            else:\n",
      "                moviecountdict[dict[row[2]]] = 1\n",
      "        elif row[3] == 'actress' and row[2] in dict and dict[row[2]] in boxofficedict:\n",
      "            Gw.add_node(dict[row[2]])\n",
      "            G.add_node(dict[row[2]])\n",
      "            if dict[row[2]] in moviecountdict:\n",
      "                moviecountdict[dict[row[2]]] += 1\n",
      "            else:\n",
      "                moviecountdict[dict[row[2]]] = 1\n",
      "        else:\n",
      "            continue\n",
      "        if row[0] != prev:\n",
      "            for a1 in actors:\n",
      "                for a2 in actors:\n",
      "                    if a1 == a2:\n",
      "                        continue\n",
      "                    G.add_edge(a1, a2)\n",
      "                    if a1 in Gw.nodes and a2 in Gw.nodes:\n",
      "                        Gw.add_edge(a1, a2)\n",
      "                    elif a1 in Gm.nodes and a2 in Gm.nodes:\n",
      "                        Gm.add_edge(a1, a2)\n",
      "            actors = [dict[row[2]]]\n",
      "        else:\n",
      "            actors.append(dict[row[2]])\n",
      "        prev = row[0]\n",
      "24/23: moviecountdict\n",
      "24/24: list(G.nodes)\n",
      "24/25: len(G.nodes)\n",
      "24/26: nx.draw(G, list(G.nodes)[:10])\n",
      "24/27: nx.draw(G, G.nodes[:10])\n",
      "24/28: list(G.nodes)[:10]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "24/29: nx.draw(G.subgraph([0:10]))\n",
      "24/30: nx.draw(G.subgraph(range(10)))\n",
      "24/31: nx.draw(G.subgraph(range(100)))\n",
      "24/32: nx.draw(G.subgraph([0,1,2]))\n",
      "24/33: nx.draw(G.subgraph([i for i in range(200)]))\n",
      "24/34: H = G.subgraph([i for i in range(200)])\n",
      "24/35: nx.draw(H)\n",
      "24/36: H = G.subgraph([i for i in range(800)])\n",
      "24/37: nx.draw(H)\n",
      "24/38:\n",
      "nx.draw(H)\n",
      "nx.show()\n",
      "24/39:\n",
      "nx.draw(H)\n",
      "plot.show()\n",
      "24/40:\n",
      "nx.draw(H)\n",
      "plot.draw()\n",
      "24/41: H.edges\n",
      "24/42: H.nodes\n",
      "24/43: len(G.nodes)\n",
      "24/44: H = G.subgraph(list(G.nodes)[:10])\n",
      "24/45: H.nodes\n",
      "24/46: nx.draw(G, with_labels=False, font_weight='bold')\n",
      "24/47: nx.draw(H)\n",
      "24/48: nx.draw(H, with_labels=True)\n",
      "24/49: H = G.subgraph(list(G.nodes)[:100])\n",
      "24/50: nx.draw(H, with_labels=True)\n",
      "24/51: H = G.subgraph(list(G.nodes)[:50])\n",
      "24/52: nx.draw(H, with_labels=True)\n",
      "24/53: H = G.subgraph(list(G.nodes)[:20])\n",
      "24/54: nx.draw(H, with_labels=True)\n",
      "24/55: H = G.subgraph(list(G.nodes)[:15])\n",
      "24/56: nx.draw(H, with_labels=True)\n",
      "24/57: H = G.subgraph(list(G.nodes)[:50])\n",
      "24/58: nx.draw(H, with_labels=True)\n",
      "24/59:\n",
      "node_and_degree = G.degree()\n",
      "(largest_hub, degree) = sorted(node_and_degree, key=itemgetter(1))[-1]\n",
      "# Create ego graph of main hub\n",
      "hub_ego = nx.ego_graph(G, largest_hub)\n",
      "# Draw graph\n",
      "pos = nx.spring_layout(hub_ego)\n",
      "nx.draw(hub_ego, pos, node_color='b', node_size=50, with_labels=False)\n",
      "# Draw ego as large and red\n",
      "nx.draw_networkx_nodes(hub_ego, pos, nodelist=[largest_hub], node_size=300, node_color='r')\n",
      "plt.show()\n",
      "24/60: from operator import itemgetter\n",
      "24/61:\n",
      "node_and_degree = G.degree()\n",
      "(largest_hub, degree) = sorted(node_and_degree, key=itemgetter(1))[-1]\n",
      "# Create ego graph of main hub\n",
      "hub_ego = nx.ego_graph(G, largest_hub)\n",
      "# Draw graph\n",
      "pos = nx.spring_layout(hub_ego)\n",
      "nx.draw(hub_ego, pos, node_color='b', node_size=50, with_labels=False)\n",
      "# Draw ego as large and red\n",
      "nx.draw_networkx_nodes(hub_ego, pos, nodelist=[largest_hub], node_size=300, node_color='r')\n",
      "plt.show()\n",
      "24/62:\n",
      "node_and_degree = G.degree()\n",
      "(largest_hub, degree) = sorted(node_and_degree, key=itemgetter(1))[-1]\n",
      "# Create ego graph of main hub\n",
      "hub_ego = nx.ego_graph(G, largest_hub)\n",
      "# Draw graph\n",
      "pos = nx.spring_layout(hub_ego)\n",
      "nx.draw(hub_ego, pos, node_color='c', node_size=50, with_labels=False)\n",
      "# Draw ego as large and red\n",
      "nx.draw_networkx_nodes(hub_ego, pos, nodelist=[largest_hub], node_size=300, node_color='b')\n",
      "plt.show()\n",
      "24/63:\n",
      "node_and_degree = G.degree()\n",
      "(largest_hub, degree) = sorted(node_and_degree, key=itemgetter(1))[-1]\n",
      "# Create ego graph of main hub\n",
      "hub_ego = nx.ego_graph(G, largest_hub)\n",
      "# Draw graph\n",
      "pos = nx.spring_layout(hub_ego)\n",
      "nx.draw(hub_ego, pos, node_color='c', node_size=50, with_labels=False)\n",
      "# Draw ego as large and red\n",
      "nx.draw_networkx_nodes(hub_ego, pos, nodelist=[largest_hub], node_size=300, node_color='b')\n",
      "plot.show()\n",
      "24/64:\n",
      "node_and_degree = G.degree()\n",
      "(largest_hub, degree) = sorted(node_and_degree, key=itemgetter(1))[-1]\n",
      "# Create ego graph of main hub\n",
      "hub_ego = nx.ego_graph(G, largest_hub)\n",
      "# Draw graph\n",
      "pos = nx.spring_layout(hub_ego)\n",
      "nx.draw(hub_ego, pos, node_color='c', node_size=50, with_labels=False)\n",
      "# Draw ego as large and red\n",
      "nx.draw_networkx_nodes(hub_ego, pos, nodelist=[largest_hub], node_size=300, node_color='b')\n",
      "24/65:\n",
      "node_and_degree = G.degree()\n",
      "(largest_hub, degree) = sorted(node_and_degree, key=itemgetter(1))[-1]\n",
      "# Create ego graph of main hub\n",
      "hub_ego = nx.ego_graph(G, largest_hub)\n",
      "# Draw graph\n",
      "pos = nx.spring_layout(hub_ego)\n",
      "nx.draw(hub_ego, pos, node_color='c', node_size=50, with_labels=True)\n",
      "# Draw ego as large and red\n",
      "nx.draw_networkx_nodes(hub_ego, pos, nodelist=[largest_hub], node_size=300, node_color='b')\n",
      "24/66:\n",
      "node_and_degree = G.degree()\n",
      "(largest_hub, degree) = sorted(node_and_degree, key=itemgetter(1))[-1]\n",
      "# Create ego graph of main hub\n",
      "hub_ego = nx.ego_graph(G, largest_hub)\n",
      "# Draw graph\n",
      "pos = nx.spring_layout(hub_ego)\n",
      "nx.draw(hub_ego, pos, node_color='c', node_size=10, with_labels=True)\n",
      "# Draw ego as large and red\n",
      "nx.draw_networkx_nodes(hub_ego, pos, nodelist=[largest_hub], node_size=300, node_color='b')\n",
      "24/67:\n",
      "node_and_degree = G.degree()\n",
      "(largest_hub, degree) = sorted(node_and_degree, key=itemgetter(1))[-1]\n",
      "# Create ego graph of main hub\n",
      "hub_ego = nx.ego_graph(G, largest_hub)\n",
      "# Draw graph\n",
      "pos = nx.spring_layout(hub_ego)\n",
      "nx.draw(hub_ego, pos, node_color='c', node_size=50, with_labels=False)\n",
      "# Draw ego as large and red\n",
      "nx.draw_networkx_nodes(hub_ego, pos, nodelist=[largest_hub], node_size=300, node_color='b')\n",
      "24/68:\n",
      "node_and_degree = G.degree()\n",
      "(largest_hub, degree) = sorted(node_and_degree, key=itemgetter(1))[-1]\n",
      "# Create ego graph of main hub\n",
      "hub_ego = nx.ego_graph(G, largest_hub)\n",
      "# Draw graph\n",
      "pos = nx.spring_layout(hub_ego)\n",
      "nx.draw(hub_ego, pos, node_color='c', node_size=50, with_labels=False)\n",
      "# Draw ego as large and red\n",
      "nx.draw_networkx_nodes(hub_ego, pos, nodelist=[largest_hub], node_size=200, node_color='b')\n",
      "24/69:\n",
      "node_and_degree = G.degree()\n",
      "(largest_hub, degree) = sorted(node_and_degree, key=itemgetter(1))[-1]\n",
      "# Create ego graph of main hub\n",
      "hub_ego = nx.ego_graph(G, largest_hub)\n",
      "# Draw graph\n",
      "pos = nx.spring_layout(hub_ego)\n",
      "nx.draw(hub_ego, pos, node_color='c', node_size=50, with_labels=False, linewidths=0)\n",
      "# Draw ego as large and red\n",
      "nx.draw_networkx_nodes(hub_ego, pos, nodelist=[largest_hub], node_size=200, node_color='b')\n",
      "24/70:\n",
      "node_and_degree = G.degree()\n",
      "(largest_hub, degree) = sorted(node_and_degree, key=itemgetter(1))[-1]\n",
      "# Create ego graph of main hub\n",
      "hub_ego = nx.ego_graph(G, largest_hub)\n",
      "# Draw graph\n",
      "pos = nx.spring_layout(hub_ego)\n",
      "nx.draw(hub_ego, pos, node_color='c', node_size=50, with_labels=False)\n",
      "# Draw ego as large and red\n",
      "nx.draw_networkx_nodes(hub_ego, pos, nodelist=[largest_hub], node_size=200, node_color='b', linewidths=0)\n",
      "24/71:\n",
      "node_and_degree = G.degree()\n",
      "(largest_hub, degree) = sorted(node_and_degree, key=itemgetter(1))[-1]\n",
      "# Create ego graph of main hub\n",
      "hub_ego = nx.ego_graph(G, largest_hub)\n",
      "# Draw graph\n",
      "pos = nx.spring_layout(hub_ego)\n",
      "nx.draw(hub_ego, pos, node_color='c', node_size=50, with_labels=False)\n",
      "# Draw ego as large and red\n",
      "nx.draw_networkx_nodes(hub_ego, pos, nodelist=[largest_hub], node_size=200, node_color='b', width=0)\n",
      "24/72:\n",
      "node_and_degree = G.degree()\n",
      "(largest_hub, degree) = sorted(node_and_degree, key=itemgetter(1))[-1]\n",
      "# Create ego graph of main hub\n",
      "hub_ego = nx.ego_graph(G, largest_hub)\n",
      "# Draw graph\n",
      "pos = nx.spring_layout(hub_ego)\n",
      "nx.draw(hub_ego, pos, node_color='c', node_size=50, with_labels=False)\n",
      "# Draw ego as large and red\n",
      "nx.draw_networkx_nodes(hub_ego, pos, nodelist=[largest_hub], node_size=200, node_color='b')\n",
      "24/73:\n",
      "node_and_degree = G.degree()\n",
      "(largest_hub, degree) = sorted(node_and_degree, key=itemgetter(1))[-1]\n",
      "# Create ego graph of main hub\n",
      "hub_ego = nx.ego_graph(G, largest_hub)\n",
      "# Draw graph\n",
      "pos = nx.spring_layout(hub_ego)\n",
      "options = {\n",
      "    'node_color': 'black',\n",
      "    'node_size': 50,\n",
      "    'line_color': 'grey',\n",
      "    'linewidths': 0,\n",
      "    'width': 0.1,\n",
      "}\n",
      "nx.draw(hub_ego, pos, node_color='c', node_size=50, with_labels=False, **options)\n",
      "# Draw ego as large and red\n",
      "nx.draw_networkx_nodes(hub_ego, pos, nodelist=[largest_hub], node_size=200, node_color='b')\n",
      "24/74:\n",
      "node_and_degree = G.degree()\n",
      "(largest_hub, degree) = sorted(node_and_degree, key=itemgetter(1))[-1]\n",
      "# Create ego graph of main hub\n",
      "hub_ego = nx.ego_graph(G, largest_hub)\n",
      "# Draw graph\n",
      "pos = nx.spring_layout(hub_ego)\n",
      "options = {\n",
      "    'node_color': 'black',\n",
      "    'node_size': 50,\n",
      "    'line_color': 'grey',\n",
      "    'linewidths': 0,\n",
      "    'width': 0.1,\n",
      "}\n",
      "nx.draw(hub_ego, pos, with_labels=False, **options)\n",
      "# Draw ego as large and red\n",
      "nx.draw_networkx_nodes(hub_ego, pos, nodelist=[largest_hub], node_size=200, node_color='b')\n",
      "24/75:\n",
      "node_and_degree = G.degree()\n",
      "(largest_hub, degree) = sorted(node_and_degree, key=itemgetter(1))[-1]\n",
      "# Create ego graph of main hub\n",
      "hub_ego = nx.ego_graph(G, largest_hub)\n",
      "# Draw graph\n",
      "pos = nx.spring_layout(hub_ego)\n",
      "options = {\n",
      "    'node_color': 'c',\n",
      "    'node_size': 20,\n",
      "    'line_color': 'grey',\n",
      "    'linewidths': 0,\n",
      "    'width': 0.1,\n",
      "}\n",
      "nx.draw(hub_ego, pos, with_labels=False, **options)\n",
      "# Draw ego as large and red\n",
      "nx.draw_networkx_nodes(hub_ego, pos, nodelist=[largest_hub], node_size=200, node_color='b')\n",
      "24/76:\n",
      "node_and_degree = G.degree()\n",
      "(largest_hub, degree) = sorted(node_and_degree, key=itemgetter(1))[-1]\n",
      "# Create ego graph of main hub\n",
      "hub_ego = nx.ego_graph(G, largest_hub)\n",
      "# Draw graph\n",
      "pos = nx.spring_layout(hub_ego)\n",
      "options = {\n",
      "    'node_color': 'c',\n",
      "    'node_size': 20,\n",
      "    'line_color': 'grey',\n",
      "    'linewidths': 0,\n",
      "    'width': 0.1,\n",
      "}\n",
      "nx.draw(hub_ego, pos, with_labels=False, **options)\n",
      "# Draw ego as large and red\n",
      "nx.draw_networkx_nodes(hub_ego, pos, nodelist=[largest_hub], node_size=100, node_color='b')\n",
      "24/77: largest_hub\n",
      "24/78:\n",
      "def egograph(name):\n",
      "    hub_ego = nx.ego_graph(G, name)\n",
      "    pos = nx.spring_layout(hub_ego)\n",
      "    options = {\n",
      "        'node_color': 'c',\n",
      "        'node_size': 20,\n",
      "        'line_color': 'grey',\n",
      "        'linewidths': 0,\n",
      "        'width': 0.1,\n",
      "    }\n",
      "    nx.draw(hub_ego, pos, with_labels=False, **options)\n",
      "    nx.draw_networkx_nodes(hub_ego, pos, nodelist=[name], node_size=100, node_color='b')\n",
      "24/79: egograph(\"Samuel L. Jackson\")\n",
      "24/80: egograph(\"Seth Rogen\")\n",
      "24/81: sorted(boxofficedict, key=boxofficedict.get)\n",
      "24/82: sorted(boxofficedict, key=boxofficedict.get, reverse=True)\n",
      "24/83: boxofficesorted = sorted(boxofficedict, key=boxofficedict.get, reverse=True)\n",
      "24/84:\n",
      "boxofficesorted = sorted(boxofficedict, key=boxofficedict.get, reverse=True)\n",
      "boxofficesorted\n",
      "24/85:\n",
      "box0 = boxofficesorted[0]\n",
      "print box0\n",
      "egograph(box0)\n",
      "24/86:\n",
      "box1 = boxofficesorted[0]\n",
      "print box1\n",
      "egograph(box1)\n",
      "24/87:\n",
      "box100 = boxofficesorted[99]\n",
      "print box100\n",
      "egograph(box100)\n",
      "24/88:\n",
      "box500 = boxofficesorted[499]\n",
      "print box500\n",
      "egograph(box500)\n",
      "24/89:\n",
      "box400 = boxofficesorted[399]\n",
      "print box400\n",
      "egograph(box400)\n",
      "24/90:\n",
      "box800 = boxofficesorted[799]\n",
      "print box800\n",
      "egograph(box800)\n",
      "24/91:\n",
      "box10 = boxofficesorted[9]\n",
      "print box10\n",
      "egograph(box10)\n",
      "24/92:\n",
      "box0 = boxofficesorted[19]\n",
      "print box0\n",
      "egograph(box0)\n",
      "24/93:\n",
      "box20 = boxofficesorted[19]\n",
      "print box20\n",
      "egograph(box20)\n",
      "26/1:\n",
      "import networkx as nx\n",
      "import csv\n",
      "import matplotlib.pyplot as plot\n",
      "26/2:\n",
      "from bs4 import BeautifulSoup\n",
      "import urllib2\n",
      "26/3:\n",
      "from sklearn.linear_model import LinearRegression, LogisticRegression\n",
      "import numpy as np\n",
      "from scipy import stats\n",
      "26/4:\n",
      "page0 = urllib2.urlopen(\"https://en.wikipedia.org/wiki/List_of_actors_with_Academy_Award_nominations\")\n",
      "soup0 = BeautifulSoup(page0, 'html.parser')\n",
      "26/5: actors0 = soup0.find_all('tr')[11:][:-11]\n",
      "26/6: actors0[0].find('a').text\n",
      "26/7:\n",
      "academylist = []\n",
      "for actor in actors0:\n",
      "    academylist.append(actor.find('a').text)\n",
      "26/8: academylist\n",
      "26/9:\n",
      "page1 = urllib2.urlopen(\"https://www.boxofficemojo.com/people/?view=Actor&p=.htm\")\n",
      "soup1 = BeautifulSoup(page1, 'html.parser')\n",
      "26/10: actors1 = soup1.find_all('tr')[2].find_all('tr')[1:]\n",
      "26/11: actors1\n",
      "26/12: boxofficedict = {}\n",
      "26/13:\n",
      "def addtodict(actors):\n",
      "    for actor in actors:\n",
      "        name = actor.find('b').find('b').text\n",
      "        boxoffice = str(actor.find('td', attrs={'align':'right'}).text[1:]).replace(\",\", \"\")\n",
      "        if \"k\" in boxoffice:\n",
      "            boxoffice = float(boxoffice.replace(\"k\", \"\"))/1000\n",
      "        boxofficedict[name] = float(boxoffice)\n",
      "26/14: boxofficedict\n",
      "26/15:\n",
      "page2 = urllib2.urlopen(\"https://www.boxofficemojo.com/people/?view=Actor&pagenum=2&sort=person&order=ASC&p=.htm\")\n",
      "soup2 = BeautifulSoup(page2, 'html.parser')\n",
      "actors2 = soup2.find_all('tr')[2].find_all('tr')[1:]\n",
      "26/16:\n",
      "page3 = urllib2.urlopen(\"https://www.boxofficemojo.com/people/?view=Actor&pagenum=3&sort=person&order=ASC&p=.htm\")\n",
      "soup3 = BeautifulSoup(page3, 'html.parser')\n",
      "actors3 = soup3.find_all('tr')[2].find_all('tr')[1:]\n",
      "26/17:\n",
      "addtodict(actors1)\n",
      "addtodict(actors2)\n",
      "addtodict(actors3)\n",
      "26/18: len(boxofficedict)\n",
      "26/19: boxofficedict\n",
      "26/20:\n",
      "boxofficesorted = sorted(boxofficedict, key=boxofficedict.get, reverse=True)\n",
      "boxofficesorted\n",
      "26/21:\n",
      "with open('title.basics.tsv') as tsvfile:\n",
      "    reader = csv.reader(tsvfile, delimiter='\\t')\n",
      "    i = 0\n",
      "    for row in reader:\n",
      "        if i < 200:\n",
      "            print(row)\n",
      "            i = i + 1\n",
      "        else:\n",
      "            break\n",
      "26/22:\n",
      "dict = {}\n",
      "with open('name.basics.tsv') as tsvfile:\n",
      "    reader = csv.reader(tsvfile, delimiter='\\t')\n",
      "    i = 0\n",
      "    for row in reader:\n",
      "        if i < 200:\n",
      "            print(row)\n",
      "            i = i + 1\n",
      "        if i > 1:\n",
      "            dict[row[0]] = row[1]\n",
      "26/23:\n",
      "G = nx.Graph()\n",
      "Gw = nx.Graph()\n",
      "Gm = nx.Graph()\n",
      "moviecountdict = {}\n",
      "\n",
      "with open('title.principals.tsv') as tsvfile:\n",
      "    reader = csv.reader(tsvfile, delimiter='\\t')\n",
      "    prev = \"\"\n",
      "    actors = []\n",
      "    for row in reader:\n",
      "        if row[3] == 'actor' and row[2] in dict and dict[row[2]] in boxofficedict:\n",
      "            Gm.add_node(dict[row[2]])\n",
      "            G.add_node(dict[row[2]])\n",
      "            if dict[row[2]] in moviecountdict:\n",
      "                moviecountdict[dict[row[2]]] += 1\n",
      "            else:\n",
      "                moviecountdict[dict[row[2]]] = 1\n",
      "        elif row[3] == 'actress' and row[2] in dict and dict[row[2]] in boxofficedict:\n",
      "            Gw.add_node(dict[row[2]])\n",
      "            G.add_node(dict[row[2]])\n",
      "            if dict[row[2]] in moviecountdict:\n",
      "                moviecountdict[dict[row[2]]] += 1\n",
      "            else:\n",
      "                moviecountdict[dict[row[2]]] = 1\n",
      "        else:\n",
      "            continue\n",
      "        if row[0] != prev:\n",
      "            for a1 in actors:\n",
      "                for a2 in actors:\n",
      "                    if a1 == a2:\n",
      "                        continue\n",
      "                    G.add_edge(a1, a2)\n",
      "                    if a1 in Gw.nodes and a2 in Gw.nodes:\n",
      "                        Gw.add_edge(a1, a2)\n",
      "                    elif a1 in Gm.nodes and a2 in Gm.nodes:\n",
      "                        Gm.add_edge(a1, a2)\n",
      "            actors = [dict[row[2]]]\n",
      "        else:\n",
      "            actors.append(dict[row[2]])\n",
      "        prev = row[0]\n",
      "26/24: moviecountdict\n",
      "26/25: list(G.nodes)\n",
      "26/26: len(G.nodes)\n",
      "26/27: list(G.edges)\n",
      "26/28: len(G.edges)\n",
      "26/29: H = G.subgraph(list(G.nodes)[:50])\n",
      "26/30: nx.draw(H, with_labels=True)\n",
      "26/31:\n",
      "def egograph(name):\n",
      "    hub_ego = nx.ego_graph(G, name)\n",
      "    pos = nx.spring_layout(hub_ego)\n",
      "    options = {\n",
      "        'node_color': 'c',\n",
      "        'node_size': 20,\n",
      "        'line_color': 'grey',\n",
      "        'linewidths': 0,\n",
      "        'width': 0.1,\n",
      "    }\n",
      "    nx.draw(hub_ego, pos, with_labels=False, **options)\n",
      "    nx.draw_networkx_nodes(hub_ego, pos, nodelist=[name], node_size=100, node_color='b')\n",
      "26/32:\n",
      "box1 = boxofficesorted[0]\n",
      "print box1\n",
      "egograph(box1)\n",
      "26/33:\n",
      "box100 = boxofficesorted[99]\n",
      "print box100\n",
      "egograph(box100)\n",
      "26/34:\n",
      "box400 = boxofficesorted[399]\n",
      "print box400\n",
      "egograph(box400)\n",
      "26/35:\n",
      "box800 = boxofficesorted[799]\n",
      "print box800\n",
      "egograph(box800)\n",
      "26/36: nx.draw(G, with_labels=False, font_weight='bold')\n",
      "26/37: nx.draw(G, with_labels=True)\n",
      "26/38: nx.draw(Gw)\n",
      "26/39: nx.draw(Gm)\n",
      "26/40: np.histogram(degree_centralities)\n",
      "26/41:\n",
      "actorw = sorted(Gw.nodes)\n",
      "actorm = sorted(Gm.nodes)\n",
      "26/42: degree_centrality = nx.degree_centrality(G)\n",
      "26/43: sorted(degree_centrality, key=degree_centrality.get, reverse=True)\n",
      "26/44: eigenvector_centrality = nx.eigenvector_centrality(G)\n",
      "26/45: sorted(eigenvector_centrality, key=eigenvector_centrality.get, reverse=True)\n",
      "26/46: closeness_centrality = nx.closeness_centrality(G)\n",
      "26/47: sorted(closeness_centrality, key=closeness_centrality.get, reverse=True)\n",
      "26/48: betweenness_centrality = nx.betweenness_centrality(G)\n",
      "26/49: sorted(betweenness_centrality, key=betweenness_centrality.get, reverse=True)\n",
      "26/50:\n",
      "degree_centralities = []\n",
      "actorlist = [a for a in actorw]\n",
      "for a in actorm:\n",
      "    actorlist.append(a)\n",
      "for actor in actorlist:\n",
      "    degree_centralities.append(degree_centrality.get(actor))\n",
      "26/51: len(actorw)\n",
      "26/52: actorlist\n",
      "26/53: degree_centralities\n",
      "26/54: np.histogram(degree_centralities)\n",
      "26/55: plot.hist(np.histogram(degree_centralities))\n",
      "26/56: np.histogram(degree_centralities)\n",
      "26/57: plot.histogram(degree_centralities)\n",
      "26/58: plot.hist(degree_centralities)\n",
      "26/59: plot.hist(boxofficegross)\n",
      "26/60:\n",
      "boxofficegross = []\n",
      "for actor in actorlist:\n",
      "    boxofficegross.append(boxofficedict[actor])\n",
      "26/61: boxofficegross\n",
      "26/62: plot.hist(boxofficegross)\n",
      "26/63: plot.hist(academynominated)\n",
      "26/64:\n",
      "academynominated = []\n",
      "for a in actorlist:\n",
      "    if a in academylist:\n",
      "        academynominated.append(1)\n",
      "    else:\n",
      "        academynominated.append(0)\n",
      "26/65: plot.hist(academynominated)\n",
      "26/66: plot.hist(academynominated, bins=2)\n",
      "26/67:\n",
      "eigenvector_centralities = []\n",
      "for actor in actorlist:\n",
      "    eigenvector_centralities.append(eigenvector_centrality.get(actor))\n",
      "26/68: plot.hist(eigenvector_centralities)\n",
      "26/69:\n",
      "betweenness_centralities = []\n",
      "for actor in actorlist:\n",
      "    betweenness_centralities.append(betweenness_centrality.get(actor))\n",
      "x_b = np.array(betweenness_centralities)\n",
      "y = np.array(boxofficegross)\n",
      "slope, intercept, r_value, p_value, std_err = stats.linregress(x_b,y)\n",
      "slope, intercept, r_value, p_value, std_err\n",
      "26/70: plot.hist(betweenness_centralities)\n",
      "26/71:\n",
      "closeness_centralities = []\n",
      "for actor in actorlist:\n",
      "    closeness_centralities.append(closeness_centrality.get(actor))\n",
      "x_c = np.array(closeness_centralities)\n",
      "y = np.array(boxofficegross)\n",
      "slope, intercept, r_value, p_value, std_err = stats.linregress(x_c,y)\n",
      "slope, intercept, r_value, p_value, std_err\n",
      "26/72: plot.hist(closeness_centralities)\n",
      "26/73:\n",
      "G = nx.Graph()\n",
      "Gw = nx.Graph()\n",
      "Gm = nx.Graph()\n",
      "moviecountdict = {}\n",
      "moviecountdict2 = {}\n",
      "\n",
      "with open('title.principals.tsv') as tsvfile:\n",
      "    reader = csv.reader(tsvfile, delimiter='\\t')\n",
      "    prev = \"\"\n",
      "    actors = []\n",
      "    for row in reader:\n",
      "        if row[3] == 'actor' and row[2] in dict and dict[row[2]] in boxofficedict:\n",
      "            Gm.add_node(dict[row[2]])\n",
      "            G.add_node(dict[row[2]])\n",
      "            if dict[row[2]] in moviecountdict:\n",
      "                moviecountdict[dict[row[2]]] += 1\n",
      "            else:\n",
      "                moviecountdict[dict[row[2]]] = 1\n",
      "            if dict[row[2]] in moviecountdict2 and \"tv\" not in row[1]:\n",
      "                moviecountdict2[dict[row[2]]] += 1\n",
      "            elif \"tv\" not in row[1]:\n",
      "                moviecountdict2[dict[row[2]]] = 1\n",
      "        elif row[3] == 'actress' and row[2] in dict and dict[row[2]] in boxofficedict:\n",
      "            Gw.add_node(dict[row[2]])\n",
      "            G.add_node(dict[row[2]])\n",
      "            if dict[row[2]] in moviecountdict:\n",
      "                moviecountdict[dict[row[2]]] += 1\n",
      "            else:\n",
      "                moviecountdict[dict[row[2]]] = 1\n",
      "            if dict[row[2]] in moviecountdict2 and \"tv\" not in row[1]:\n",
      "                moviecountdict2[dict[row[2]]] += 1\n",
      "            elif \"tv\" not in row[1]:\n",
      "                moviecountdict2[dict[row[2]]] = 1\n",
      "        else:\n",
      "            continue\n",
      "        if row[0] != prev:\n",
      "            for a1 in actors:\n",
      "                for a2 in actors:\n",
      "                    if a1 == a2:\n",
      "                        continue\n",
      "                    G.add_edge(a1, a2)\n",
      "                    if a1 in Gw.nodes and a2 in Gw.nodes:\n",
      "                        Gw.add_edge(a1, a2)\n",
      "                    elif a1 in Gm.nodes and a2 in Gm.nodes:\n",
      "                        Gm.add_edge(a1, a2)\n",
      "            actors = [dict[row[2]]]\n",
      "        else:\n",
      "            actors.append(dict[row[2]])\n",
      "        prev = row[0]\n",
      "26/74: \"tv\" not in \"tv show\"\n",
      "26/75: moviecountdict2\n",
      "26/76:\n",
      "movies = {}\n",
      "with open('title.basics.tsv') as tsvfile:\n",
      "    reader = csv.reader(tsvfile, delimiter='\\t')\n",
      "    i = 0\n",
      "    for row in reader:\n",
      "        movies[row[0]] = row[1]\n",
      "26/77:\n",
      "G = nx.Graph()\n",
      "Gw = nx.Graph()\n",
      "Gm = nx.Graph()\n",
      "moviecountdict = {}\n",
      "moviecountdict2 = {}\n",
      "\n",
      "with open('title.principals.tsv') as tsvfile:\n",
      "    reader = csv.reader(tsvfile, delimiter='\\t')\n",
      "    prev = \"\"\n",
      "    actors = []\n",
      "    for row in reader:\n",
      "        if row[3] == 'actor' and row[2] in dict and dict[row[2]] in boxofficedict:\n",
      "            Gm.add_node(dict[row[2]])\n",
      "            G.add_node(dict[row[2]])\n",
      "            if dict[row[2]] in moviecountdict:\n",
      "                moviecountdict[dict[row[2]]] += 1\n",
      "            else:\n",
      "                moviecountdict[dict[row[2]]] = 1\n",
      "            if dict[row[2]] in moviecountdict2 and \"tv\" not in movies[row[0]]:\n",
      "                moviecountdict2[dict[row[2]]] += 1\n",
      "            elif \"tv\" not in row[1]:\n",
      "                moviecountdict2[dict[row[2]]] = 1\n",
      "        elif row[3] == 'actress' and row[2] in dict and dict[row[2]] in boxofficedict:\n",
      "            Gw.add_node(dict[row[2]])\n",
      "            G.add_node(dict[row[2]])\n",
      "            if dict[row[2]] in moviecountdict:\n",
      "                moviecountdict[dict[row[2]]] += 1\n",
      "            else:\n",
      "                moviecountdict[dict[row[2]]] = 1\n",
      "            if dict[row[2]] in moviecountdict2 and \"tv\" not in movies[row[0]]:\n",
      "                moviecountdict2[dict[row[2]]] += 1\n",
      "            elif \"tv\" not in row[1]:\n",
      "                moviecountdict2[dict[row[2]]] = 1\n",
      "        else:\n",
      "            continue\n",
      "        if row[0] != prev:\n",
      "            for a1 in actors:\n",
      "                for a2 in actors:\n",
      "                    if a1 == a2:\n",
      "                        continue\n",
      "                    G.add_edge(a1, a2)\n",
      "                    if a1 in Gw.nodes and a2 in Gw.nodes:\n",
      "                        Gw.add_edge(a1, a2)\n",
      "                    elif a1 in Gm.nodes and a2 in Gm.nodes:\n",
      "                        Gm.add_edge(a1, a2)\n",
      "            actors = [dict[row[2]]]\n",
      "        else:\n",
      "            actors.append(dict[row[2]])\n",
      "        prev = row[0]\n",
      "26/78:\n",
      "G = nx.Graph()\n",
      "Gw = nx.Graph()\n",
      "Gm = nx.Graph()\n",
      "moviecountdict = {}\n",
      "moviecountdict2 = {}\n",
      "\n",
      "with open('title.principals.tsv') as tsvfile:\n",
      "    reader = csv.reader(tsvfile, delimiter='\\t')\n",
      "    prev = \"\"\n",
      "    actors = []\n",
      "    for row in reader:\n",
      "        if row[3] == 'actor' and row[2] in dict and dict[row[2]] in boxofficedict:\n",
      "            Gm.add_node(dict[row[2]])\n",
      "            G.add_node(dict[row[2]])\n",
      "            if dict[row[2]] in moviecountdict:\n",
      "                moviecountdict[dict[row[2]]] += 1\n",
      "            else:\n",
      "                moviecountdict[dict[row[2]]] = 1\n",
      "            if dict[row[2]] in moviecountdict2 and \"tv\" not in movies[row[0]]:\n",
      "                moviecountdict2[dict[row[2]]] += 1\n",
      "            elif \"tv\" not in movies[row[0]]:\n",
      "                moviecountdict2[dict[row[2]]] = 1\n",
      "        elif row[3] == 'actress' and row[2] in dict and dict[row[2]] in boxofficedict:\n",
      "            Gw.add_node(dict[row[2]])\n",
      "            G.add_node(dict[row[2]])\n",
      "            if dict[row[2]] in moviecountdict:\n",
      "                moviecountdict[dict[row[2]]] += 1\n",
      "            else:\n",
      "                moviecountdict[dict[row[2]]] = 1\n",
      "            if dict[row[2]] in moviecountdict2 and \"tv\" not in movies[row[0]]:\n",
      "                moviecountdict2[dict[row[2]]] += 1\n",
      "            elif \"tv\" not in movies[row[0]]:\n",
      "                moviecountdict2[dict[row[2]]] = 1\n",
      "        else:\n",
      "            continue\n",
      "        if row[0] != prev:\n",
      "            for a1 in actors:\n",
      "                for a2 in actors:\n",
      "                    if a1 == a2:\n",
      "                        continue\n",
      "                    G.add_edge(a1, a2)\n",
      "                    if a1 in Gw.nodes and a2 in Gw.nodes:\n",
      "                        Gw.add_edge(a1, a2)\n",
      "                    elif a1 in Gm.nodes and a2 in Gm.nodes:\n",
      "                        Gm.add_edge(a1, a2)\n",
      "            actors = [dict[row[2]]]\n",
      "        else:\n",
      "            actors.append(dict[row[2]])\n",
      "        prev = row[0]\n",
      "26/79:\n",
      "G = nx.Graph()\n",
      "Gw = nx.Graph()\n",
      "Gm = nx.Graph()\n",
      "moviecountdict = {}\n",
      "moviecountdict2 = {}\n",
      "\n",
      "with open('title.principals.tsv') as tsvfile:\n",
      "    reader = csv.reader(tsvfile, delimiter='\\t')\n",
      "    prev = \"\"\n",
      "    actors = []\n",
      "    for row in reader:\n",
      "        if row[3] == 'actor' and row[2] in dict and dict[row[2]] in boxofficedict:\n",
      "            Gm.add_node(dict[row[2]])\n",
      "            G.add_node(dict[row[2]])\n",
      "            if dict[row[2]] in moviecountdict:\n",
      "                moviecountdict[dict[row[2]]] += 1\n",
      "            else:\n",
      "                moviecountdict[dict[row[2]]] = 1\n",
      "            if dict[row[2]] in moviecountdict2 and row[0] in movies and \"tv\" not in movies[row[0]]:\n",
      "                moviecountdict2[dict[row[2]]] += 1\n",
      "            elif row[0] in movies and \"tv\" not in movies[row[0]]:\n",
      "                moviecountdict2[dict[row[2]]] = 1\n",
      "        elif row[3] == 'actress' and row[2] in dict and dict[row[2]] in boxofficedict:\n",
      "            Gw.add_node(dict[row[2]])\n",
      "            G.add_node(dict[row[2]])\n",
      "            if dict[row[2]] in moviecountdict:\n",
      "                moviecountdict[dict[row[2]]] += 1\n",
      "            else:\n",
      "                moviecountdict[dict[row[2]]] = 1\n",
      "            if dict[row[2]] in moviecountdict2 and row[0] in movies and \"tv\" not in movies[row[0]]:\n",
      "                moviecountdict2[dict[row[2]]] += 1\n",
      "            elif row[0] in movies and \"tv\" not in movies[row[0]]:\n",
      "                moviecountdict2[dict[row[2]]] = 1\n",
      "        else:\n",
      "            continue\n",
      "        if row[0] != prev:\n",
      "            for a1 in actors:\n",
      "                for a2 in actors:\n",
      "                    if a1 == a2:\n",
      "                        continue\n",
      "                    G.add_edge(a1, a2)\n",
      "                    if a1 in Gw.nodes and a2 in Gw.nodes:\n",
      "                        Gw.add_edge(a1, a2)\n",
      "                    elif a1 in Gm.nodes and a2 in Gm.nodes:\n",
      "                        Gm.add_edge(a1, a2)\n",
      "            actors = [dict[row[2]]]\n",
      "        else:\n",
      "            actors.append(dict[row[2]])\n",
      "        prev = row[0]\n",
      "26/80: moviecountdict2\n",
      "26/81:\n",
      "moviecounts2 = []\n",
      "for actor in actorlist:\n",
      "    moviecounts2.append(moviecountdict2.get(actor))\n",
      "26/82:\n",
      "xc2 = np.array(moviecounts2)\n",
      "yc2 = np.array(boxofficegross)\n",
      "slope, intercept, r_value, p_value, std_err = stats.linregress(xc2,yc2)\n",
      "slope, intercept, r_value, p_value, std_err\n",
      "26/83:\n",
      "moviecounts2 = []\n",
      "for actor in actorlist:\n",
      "    if moviecountdict2.get(actor):\n",
      "        moviecounts2.append(moviecountdict2.get(actor))\n",
      "    else:\n",
      "        moviecounts2.append(0)\n",
      "26/84:\n",
      "xc2 = np.array(moviecounts2)\n",
      "yc2 = np.array(boxofficegross)\n",
      "slope, intercept, r_value, p_value, std_err = stats.linregress(xc2,yc2)\n",
      "slope, intercept, r_value, p_value, std_err\n",
      "26/85:\n",
      "plot.figure()\n",
      "plot.plot(moviecounts2, boxofficegross, \"o\")\n",
      "plot.plot(moviecounts2, [slope*x + intercept for x in moviecounts])\n",
      "26/86:\n",
      "plot.figure()\n",
      "plot.plot(moviecounts2, boxofficegross, \"o\")\n",
      "plot.plot(moviecounts2, [slope*x + intercept for x in moviecounts2])\n",
      "26/87:\n",
      "df = pd.DataFrame(\n",
      "    {\n",
      "        'degree_centralities': degree_centralities,\n",
      "        'eigenvector_centralities': eigenvector_centralities,\n",
      "        'betweenness_centralities': betweenness_centralities,\n",
      "        'closeness_centralities': closeness_centralities,\n",
      "        'moviecount': moviecounts2,\n",
      "        'boxofficegross': boxofficegross,\n",
      "        'academynominated': academynominated\n",
      "    })\n",
      "26/88:\n",
      "import statsmodels.api as sm\n",
      "import pandas as pd\n",
      "26/89:\n",
      "df = pd.DataFrame(\n",
      "    {\n",
      "        'degree_centralities': degree_centralities,\n",
      "        'eigenvector_centralities': eigenvector_centralities,\n",
      "        'betweenness_centralities': betweenness_centralities,\n",
      "        'closeness_centralities': closeness_centralities,\n",
      "        'moviecount': moviecounts2,\n",
      "        'boxofficegross': boxofficegross,\n",
      "        'academynominated': academynominated\n",
      "    })\n",
      "26/90: X = df[['degree_centralities', 'eigenvector_centralities', 'betweenness_centralities', 'closeness_centralities', 'moviecount']]\n",
      "26/91:\n",
      "X = sm.add_constant(X) # adding a constant\n",
      "Y = boxofficegross\n",
      "model = sm.OLS(Y, X).fit()\n",
      "predictions = model.predict(X) \n",
      " \n",
      "print_model = model.summary()\n",
      "print(print_model)\n",
      "26/92:\n",
      "G = nx.Graph()\n",
      "Gw = nx.Graph()\n",
      "Gm = nx.Graph()\n",
      "moviecountdict = {}\n",
      "moviecountdict2 = {}\n",
      "\n",
      "with open('title.principals.tsv') as tsvfile:\n",
      "    reader = csv.reader(tsvfile, delimiter='\\t')\n",
      "    prev = \"\"\n",
      "    actors = []\n",
      "    for row in reader:\n",
      "        if row[3] == 'actor' and row[2] in dict and dict[row[2]] in boxofficedict:\n",
      "            Gm.add_node(dict[row[2]])\n",
      "            G.add_node(dict[row[2]])\n",
      "            if dict[row[2]] in moviecountdict:\n",
      "                moviecountdict[dict[row[2]]] += 1\n",
      "            else:\n",
      "                moviecountdict[dict[row[2]]] = 1\n",
      "            if dict[row[2]] in moviecountdict2 and row[0] in movies and \"tvEpisode\" not in movies[row[0]]:\n",
      "                moviecountdict2[dict[row[2]]] += 1\n",
      "            elif row[0] in movies and \"tvEpisode\" not in movies[row[0]]:\n",
      "                moviecountdict2[dict[row[2]]] = 1\n",
      "        elif row[3] == 'actress' and row[2] in dict and dict[row[2]] in boxofficedict:\n",
      "            Gw.add_node(dict[row[2]])\n",
      "            G.add_node(dict[row[2]])\n",
      "            if dict[row[2]] in moviecountdict:\n",
      "                moviecountdict[dict[row[2]]] += 1\n",
      "            else:\n",
      "                moviecountdict[dict[row[2]]] = 1\n",
      "            if dict[row[2]] in moviecountdict2 and row[0] in movies and \"tvEpisode\" not in movies[row[0]]:\n",
      "                moviecountdict2[dict[row[2]]] += 1\n",
      "            elif row[0] in movies and \"tvEpisode\" not in movies[row[0]]:\n",
      "                moviecountdict2[dict[row[2]]] = 1\n",
      "        else:\n",
      "            continue\n",
      "        if row[0] != prev:\n",
      "            for a1 in actors:\n",
      "                for a2 in actors:\n",
      "                    if a1 == a2:\n",
      "                        continue\n",
      "                    G.add_edge(a1, a2)\n",
      "                    if a1 in Gw.nodes and a2 in Gw.nodes:\n",
      "                        Gw.add_edge(a1, a2)\n",
      "                    elif a1 in Gm.nodes and a2 in Gm.nodes:\n",
      "                        Gm.add_edge(a1, a2)\n",
      "            actors = [dict[row[2]]]\n",
      "        else:\n",
      "            actors.append(dict[row[2]])\n",
      "        prev = row[0]\n",
      "26/93:\n",
      "df = pd.DataFrame(\n",
      "    {\n",
      "        'degree_centralities': degree_centralities,\n",
      "        'eigenvector_centralities': eigenvector_centralities,\n",
      "        'betweenness_centralities': betweenness_centralities,\n",
      "        'closeness_centralities': closeness_centralities,\n",
      "        'moviecount': moviecounts,\n",
      "        'boxofficegross': boxofficegross,\n",
      "        'academynominated': academynominated\n",
      "    })\n",
      "26/94: moviecountdict2\n",
      "26/95:\n",
      "moviecounts2 = []\n",
      "for actor in actorlist:\n",
      "    if moviecountdict2.get(actor):\n",
      "        moviecounts2.append(moviecountdict2.get(actor))\n",
      "    else:\n",
      "        moviecounts2.append(0)\n",
      "26/96:\n",
      "xc = np.array(moviecounts)\n",
      "yc = np.array(boxofficegross)\n",
      "slope, intercept, r_value, p_value, std_err = stats.linregress(xc,yc)\n",
      "slope, intercept, r_value, p_value, std_err\n",
      "26/97:\n",
      "plot.figure()\n",
      "plot.plot(moviecounts, boxofficegross, \"o\")\n",
      "plot.plot(moviecounts, [slope*x + intercept for x in moviecounts])\n",
      "26/98:\n",
      "moviecounts = []\n",
      "for actor in actorlist:\n",
      "    moviecounts.append(moviecountdict.get(actor))\n",
      "26/99:\n",
      "xc = np.array(moviecounts)\n",
      "yc = np.array(boxofficegross)\n",
      "slope, intercept, r_value, p_value, std_err = stats.linregress(xc,yc)\n",
      "slope, intercept, r_value, p_value, std_err\n",
      "26/100:\n",
      "plot.figure()\n",
      "plot.plot(moviecounts, boxofficegross, \"o\")\n",
      "plot.plot(moviecounts, [slope*x + intercept for x in moviecounts])\n",
      "26/101:\n",
      "xc2 = np.array(moviecounts2)\n",
      "yc2 = np.array(boxofficegross)\n",
      "slope, intercept, r_value, p_value, std_err = stats.linregress(xc2,yc2)\n",
      "slope, intercept, r_value, p_value, std_err\n",
      "26/102:\n",
      "plot.figure()\n",
      "plot.plot(moviecounts2, boxofficegross, \"o\")\n",
      "plot.plot(moviecounts2, [slope*x + intercept for x in moviecounts2])\n",
      "26/103:\n",
      "df = pd.DataFrame(\n",
      "    {\n",
      "        'degree_centralities': degree_centralities,\n",
      "        'eigenvector_centralities': eigenvector_centralities,\n",
      "        'betweenness_centralities': betweenness_centralities,\n",
      "        'closeness_centralities': closeness_centralities,\n",
      "        'moviecount': moviecounts2,\n",
      "        'boxofficegross': boxofficegross,\n",
      "        'academynominated': academynominated\n",
      "    })\n",
      "26/104: X = df[['degree_centralities', 'eigenvector_centralities', 'betweenness_centralities', 'closeness_centralities', 'moviecount']]\n",
      "26/105:\n",
      "X = sm.add_constant(X) # adding a constant\n",
      "Y = boxofficegross\n",
      "model = sm.OLS(Y, X).fit()\n",
      "predictions = model.predict(X) \n",
      " \n",
      "print_model = model.summary()\n",
      "print(print_model)\n",
      "26/106:\n",
      "G = nx.Graph()\n",
      "Gw = nx.Graph()\n",
      "Gm = nx.Graph()\n",
      "moviecountdict = {}\n",
      "moviecountdict2 = {}\n",
      "\n",
      "with open('title.principals.tsv') as tsvfile:\n",
      "    reader = csv.reader(tsvfile, delimiter='\\t')\n",
      "    prev = \"\"\n",
      "    actors = []\n",
      "    for row in reader:\n",
      "        if row[3] == 'actor' and row[2] in dict and dict[row[2]] in boxofficedict:\n",
      "            Gm.add_node(dict[row[2]])\n",
      "            G.add_node(dict[row[2]])\n",
      "            if dict[row[2]] in moviecountdict:\n",
      "                moviecountdict[dict[row[2]]] += 1\n",
      "            else:\n",
      "                moviecountdict[dict[row[2]]] = 1\n",
      "            if dict[row[2]] in moviecountdict2 and row[0] in movies and \"tv\" not in movies[row[0]]:\n",
      "                moviecountdict2[dict[row[2]]] += 1\n",
      "            elif row[0] in movies and \"tv\" not in movies[row[0]]:\n",
      "                moviecountdict2[dict[row[2]]] = 1\n",
      "        elif row[3] == 'actress' and row[2] in dict and dict[row[2]] in boxofficedict:\n",
      "            Gw.add_node(dict[row[2]])\n",
      "            G.add_node(dict[row[2]])\n",
      "            if dict[row[2]] in moviecountdict:\n",
      "                moviecountdict[dict[row[2]]] += 1\n",
      "            else:\n",
      "                moviecountdict[dict[row[2]]] = 1\n",
      "            if dict[row[2]] in moviecountdict2 and row[0] in movies and \"tv\" not in movies[row[0]]:\n",
      "                moviecountdict2[dict[row[2]]] += 1\n",
      "            elif row[0] in movies and \"tv\" not in movies[row[0]]:\n",
      "                moviecountdict2[dict[row[2]]] = 1\n",
      "        else:\n",
      "            continue\n",
      "        if row[0] != prev:\n",
      "            for a1 in actors:\n",
      "                for a2 in actors:\n",
      "                    if a1 == a2:\n",
      "                        continue\n",
      "                    G.add_edge(a1, a2)\n",
      "                    if a1 in Gw.nodes and a2 in Gw.nodes:\n",
      "                        Gw.add_edge(a1, a2)\n",
      "                    elif a1 in Gm.nodes and a2 in Gm.nodes:\n",
      "                        Gm.add_edge(a1, a2)\n",
      "            actors = [dict[row[2]]]\n",
      "        else:\n",
      "            actors.append(dict[row[2]])\n",
      "        prev = row[0]\n",
      "26/107: moviecountdict2\n",
      "26/108:\n",
      "moviecounts2 = []\n",
      "for actor in actorlist:\n",
      "    if moviecountdict2.get(actor):\n",
      "        moviecounts2.append(moviecountdict2.get(actor))\n",
      "    else:\n",
      "        moviecounts2.append(0)\n",
      "26/109:\n",
      "xc2 = np.array(moviecounts2)\n",
      "yc2 = np.array(boxofficegross)\n",
      "slope, intercept, r_value, p_value, std_err = stats.linregress(xc2,yc2)\n",
      "slope, intercept, r_value, p_value, std_err\n",
      "26/110:\n",
      "plot.figure()\n",
      "plot.plot(moviecounts2, boxofficegross, \"o\")\n",
      "plot.plot(moviecounts2, [slope*x + intercept for x in moviecounts2])\n",
      "26/111:\n",
      "df = pd.DataFrame(\n",
      "    {\n",
      "        'degree_centralities': degree_centralities,\n",
      "        'eigenvector_centralities': eigenvector_centralities,\n",
      "        'betweenness_centralities': betweenness_centralities,\n",
      "        'closeness_centralities': closeness_centralities,\n",
      "        'moviecount': moviecounts2,\n",
      "        'boxofficegross': boxofficegross,\n",
      "        'academynominated': academynominated\n",
      "    })\n",
      "26/112: X = df[['degree_centralities', 'eigenvector_centralities', 'betweenness_centralities', 'closeness_centralities', 'moviecount']]\n",
      "26/113:\n",
      "X = sm.add_constant(X) # adding a constant\n",
      "Y = boxofficegross\n",
      "model = sm.OLS(Y, X).fit()\n",
      "predictions = model.predict(X) \n",
      " \n",
      "print_model = model.summary()\n",
      "print(print_model)\n",
      "26/114:\n",
      "df = pd.DataFrame(\n",
      "    {\n",
      "        'degree_centralities': degree_centralities,\n",
      "        'eigenvector_centralities': eigenvector_centralities,\n",
      "        'betweenness_centralities': betweenness_centralities,\n",
      "        'closeness_centralities': closeness_centralities,\n",
      "        'moviecount': moviecounts,\n",
      "        'boxofficegross': boxofficegross,\n",
      "        'academynominated': academynominated\n",
      "    })\n",
      "26/115: X = df[['degree_centralities', 'eigenvector_centralities', 'betweenness_centralities', 'closeness_centralities', 'moviecount']]\n",
      "26/116:\n",
      "X = sm.add_constant(X) # adding a constant\n",
      "Y = boxofficegross\n",
      "model = sm.OLS(Y, X).fit()\n",
      "predictions = model.predict(X) \n",
      " \n",
      "print_model = model.summary()\n",
      "print(print_model)\n",
      "26/117:\n",
      "X2 = df[['degree_centralities', 'eigenvector_centralities', 'betweenness_centralities', 'closeness_centralities', 'moviecount']]\n",
      "X2 = sm.add_constant(X2) # adding a constant\n",
      "Y2 = academynominated\n",
      "model2 = sm.OLS(Y2, X2).fit()\n",
      "predictions2 = model2.predict(X2) \n",
      "print_model2 = model2.summary()\n",
      "print(print_model2)\n",
      "26/118:\n",
      "G = nx.Graph()\n",
      "Gw = nx.Graph()\n",
      "Gm = nx.Graph()\n",
      "moviecountdict = {}\n",
      "moviecountdict2 = {}\n",
      "\n",
      "with open('title.principals.tsv') as tsvfile:\n",
      "    reader = csv.reader(tsvfile, delimiter='\\t')\n",
      "    prev = \"\"\n",
      "    actors = []\n",
      "    for row in reader:\n",
      "        if row[3] == 'actor' and row[2] in dict and dict[row[2]] in boxofficedict:\n",
      "            Gm.add_node(dict[row[2]])\n",
      "            G.add_node(dict[row[2]])\n",
      "            if dict[row[2]] in moviecountdict:\n",
      "                moviecountdict[dict[row[2]]] += 1\n",
      "            else:\n",
      "                moviecountdict[dict[row[2]]] = 1\n",
      "            if dict[row[2]] in moviecountdict2 and row[0] in movies and movies[row[0]] == \"movie\":\n",
      "                moviecountdict2[dict[row[2]]] += 1\n",
      "            elif row[0] in movies and movies[row[0]] == \"movie\":\n",
      "                moviecountdict2[dict[row[2]]] = 1\n",
      "        elif row[3] == 'actress' and row[2] in dict and dict[row[2]] in boxofficedict:\n",
      "            Gw.add_node(dict[row[2]])\n",
      "            G.add_node(dict[row[2]])\n",
      "            if dict[row[2]] in moviecountdict:\n",
      "                moviecountdict[dict[row[2]]] += 1\n",
      "            else:\n",
      "                moviecountdict[dict[row[2]]] = 1\n",
      "            if dict[row[2]] in moviecountdict2 and row[0] in movies and movies[row[0]] == \"movie\":\n",
      "                moviecountdict2[dict[row[2]]] += 1\n",
      "            elif row[0] in movies and movies[row[0]] == \"movie\":\n",
      "                moviecountdict2[dict[row[2]]] = 1\n",
      "        else:\n",
      "            continue\n",
      "        if row[0] != prev:\n",
      "            for a1 in actors:\n",
      "                for a2 in actors:\n",
      "                    if a1 == a2:\n",
      "                        continue\n",
      "                    G.add_edge(a1, a2)\n",
      "                    if a1 in Gw.nodes and a2 in Gw.nodes:\n",
      "                        Gw.add_edge(a1, a2)\n",
      "                    elif a1 in Gm.nodes and a2 in Gm.nodes:\n",
      "                        Gm.add_edge(a1, a2)\n",
      "            actors = [dict[row[2]]]\n",
      "        else:\n",
      "            actors.append(dict[row[2]])\n",
      "        prev = row[0]\n",
      "26/119: moviecountdict2\n",
      "26/120:\n",
      "moviecounts2 = []\n",
      "for actor in actorlist:\n",
      "    if moviecountdict2.get(actor):\n",
      "        moviecounts2.append(moviecountdict2.get(actor))\n",
      "    else:\n",
      "        moviecounts2.append(0)\n",
      "26/121:\n",
      "xc2 = np.array(moviecounts2)\n",
      "yc2 = np.array(boxofficegross)\n",
      "slope, intercept, r_value, p_value, std_err = stats.linregress(xc2,yc2)\n",
      "slope, intercept, r_value, p_value, std_err\n",
      "26/122:\n",
      "plot.figure()\n",
      "plot.plot(moviecounts2, boxofficegross, \"o\")\n",
      "plot.plot(moviecounts2, [slope*x + intercept for x in moviecounts2])\n",
      "26/123:\n",
      "df = pd.DataFrame(\n",
      "    {\n",
      "        'degree_centralities': degree_centralities,\n",
      "        'eigenvector_centralities': eigenvector_centralities,\n",
      "        'betweenness_centralities': betweenness_centralities,\n",
      "        'closeness_centralities': closeness_centralities,\n",
      "        'moviecount': moviecounts2,\n",
      "        'boxofficegross': boxofficegross,\n",
      "        'academynominated': academynominated\n",
      "    })\n",
      "26/124: X = df[['degree_centralities', 'eigenvector_centralities', 'betweenness_centralities', 'closeness_centralities', 'moviecount']]\n",
      "26/125:\n",
      "X = sm.add_constant(X) # adding a constant\n",
      "Y = boxofficegross\n",
      "model = sm.OLS(Y, X).fit()\n",
      "predictions = model.predict(X) \n",
      " \n",
      "print_model = model.summary()\n",
      "print(print_model)\n",
      "26/126:\n",
      "df = pd.DataFrame(\n",
      "    {\n",
      "        'degree_centralities': degree_centralities,\n",
      "        'eigenvector_centralities': eigenvector_centralities,\n",
      "        'betweenness_centralities': betweenness_centralities,\n",
      "        'closeness_centralities': closeness_centralities,\n",
      "        'moviecount': moviecounts,\n",
      "        'boxofficegross': boxofficegross,\n",
      "        'academynominated': academynominated\n",
      "    })\n",
      "26/127: X = df[['degree_centralities', 'eigenvector_centralities', 'betweenness_centralities', 'closeness_centralities', 'moviecount']]\n",
      "26/128:\n",
      "X = sm.add_constant(X) # adding a constant\n",
      "Y = boxofficegross\n",
      "model = sm.OLS(Y, X).fit()\n",
      "predictions = model.predict(X) \n",
      " \n",
      "print_model = model.summary()\n",
      "print(print_model)\n",
      "26/129:\n",
      "df = pd.DataFrame(\n",
      "    {\n",
      "        'degree_centralities': degree_centralities,\n",
      "        'eigenvector_centralities': eigenvector_centralities,\n",
      "        'betweenness_centralities': betweenness_centralities,\n",
      "        'closeness_centralities': closeness_centralities,\n",
      "        'moviecount': moviecounts,\n",
      "        'boxofficegross': boxofficegross,\n",
      "        'academynominated': academynominated\n",
      "    })\n",
      "26/130: X = df[['degree_centralities', 'eigenvector_centralities', 'betweenness_centralities', 'closeness_centralities', 'moviecount']]\n",
      "26/131:\n",
      "X = sm.add_constant(X) # adding a constant\n",
      "Y = boxofficegross\n",
      "model = sm.OLS(Y, X).fit()\n",
      "predictions = model.predict(X) \n",
      " \n",
      "print_model = model.summary()\n",
      "print(print_model)\n",
      "26/132:\n",
      "G = nx.Graph()\n",
      "Gw = nx.Graph()\n",
      "Gm = nx.Graph()\n",
      "moviecountdict = {}\n",
      "\n",
      "with open('title.principals.tsv') as tsvfile:\n",
      "    reader = csv.reader(tsvfile, delimiter='\\t')\n",
      "    prev = \"\"\n",
      "    actors = []\n",
      "    for row in reader:\n",
      "        if row[3] == 'actor' and row[2] in dict and dict[row[2]] in boxofficedict:\n",
      "            Gm.add_node(dict[row[2]])\n",
      "            G.add_node(dict[row[2]])\n",
      "            if dict[row[2]] in moviecountdict:\n",
      "                moviecountdict[dict[row[2]]] += 1\n",
      "            else:\n",
      "                moviecountdict[dict[row[2]]] = 1\n",
      "        elif row[3] == 'actress' and row[2] in dict and dict[row[2]] in boxofficedict:\n",
      "            Gw.add_node(dict[row[2]])\n",
      "            G.add_node(dict[row[2]])\n",
      "            if dict[row[2]] in moviecountdict:\n",
      "                moviecountdict[dict[row[2]]] += 1\n",
      "            else:\n",
      "                moviecountdict[dict[row[2]]] = 1\n",
      "        else:\n",
      "            continue\n",
      "        if row[0] != prev:\n",
      "            for a1 in actors:\n",
      "                for a2 in actors:\n",
      "                    if a1 == a2:\n",
      "                        continue\n",
      "                    G.add_edge(a1, a2)\n",
      "                    if a1 in Gw.nodes and a2 in Gw.nodes:\n",
      "                        Gw.add_edge(a1, a2)\n",
      "                    elif a1 in Gm.nodes and a2 in Gm.nodes:\n",
      "                        Gm.add_edge(a1, a2)\n",
      "            actors = [dict[row[2]]]\n",
      "        else:\n",
      "            actors.append(dict[row[2]])\n",
      "        prev = row[0]\n",
      "26/133: moviecountdict\n",
      "26/134: list(G.nodes)\n",
      "26/135: len(G.nodes)\n",
      "26/136: list(G.edges)\n",
      "26/137: len(G.edges)\n",
      "26/138: H = G.subgraph(list(G.nodes)[:50])\n",
      "26/139: nx.draw(H, with_labels=True)\n",
      "26/140:\n",
      "def egograph(name):\n",
      "    hub_ego = nx.ego_graph(G, name)\n",
      "    pos = nx.spring_layout(hub_ego)\n",
      "    options = {\n",
      "        'node_color': 'c',\n",
      "        'node_size': 20,\n",
      "        'line_color': 'grey',\n",
      "        'linewidths': 0,\n",
      "        'width': 0.1,\n",
      "    }\n",
      "    nx.draw(hub_ego, pos, with_labels=False, **options)\n",
      "    nx.draw_networkx_nodes(hub_ego, pos, nodelist=[name], node_size=100, node_color='b')\n",
      "26/141:\n",
      "box1 = boxofficesorted[0]\n",
      "print box1\n",
      "egograph(box1)\n",
      "26/142:\n",
      "box100 = boxofficesorted[99]\n",
      "print box100\n",
      "egograph(box100)\n",
      "26/143:\n",
      "box400 = boxofficesorted[399]\n",
      "print box400\n",
      "egograph(box400)\n",
      "26/144:\n",
      "box800 = boxofficesorted[799]\n",
      "print box800\n",
      "egograph(box800)\n",
      "26/145: nx.draw(G, with_labels=False, font_weight='bold')\n",
      "26/146: nx.draw(G, with_labels=True)\n",
      "26/147: nx.draw(Gw)\n",
      "26/148: nx.draw(Gm)\n",
      "26/149:\n",
      "actorw = sorted(Gw.nodes)\n",
      "actorm = sorted(Gm.nodes)\n",
      "26/150: degree_centrality = nx.degree_centrality(G)\n",
      "26/151: sorted(degree_centrality, key=degree_centrality.get, reverse=True)\n",
      "26/152: eigenvector_centrality = nx.eigenvector_centrality(G)\n",
      "26/153: sorted(eigenvector_centrality, key=eigenvector_centrality.get, reverse=True)\n",
      "26/154: closeness_centrality = nx.closeness_centrality(G)\n",
      "26/155: sorted(closeness_centrality, key=closeness_centrality.get, reverse=True)\n",
      "26/156: betweenness_centrality = nx.betweenness_centrality(G)\n",
      "26/157: sorted(betweenness_centrality, key=betweenness_centrality.get, reverse=True)\n",
      "26/158:\n",
      "degree_centralities = []\n",
      "actorlist = [a for a in actorw]\n",
      "for a in actorm:\n",
      "    actorlist.append(a)\n",
      "for actor in actorlist:\n",
      "    degree_centralities.append(degree_centrality.get(actor))\n",
      "26/159: len(actorw)\n",
      "26/160: actorlist\n",
      "26/161: degree_centralities\n",
      "26/162: plot.hist(degree_centralities)\n",
      "26/163:\n",
      "boxofficegross = []\n",
      "for actor in actorlist:\n",
      "    boxofficegross.append(boxofficedict[actor])\n",
      "26/164: boxofficegross\n",
      "26/165: plot.hist(boxofficegross)\n",
      "26/166: plot.plot(degree_centralities, boxofficegross, \"ro\")\n",
      "26/167: plot.plot(degree_centralities[:313], boxofficegross[:313], \"ro\")\n",
      "26/168: plot.plot(degree_centralities[313:], boxofficegross[313:], \"ro\")\n",
      "26/169:\n",
      "x = np.array(degree_centralities)\n",
      "y = np.array(boxofficegross)\n",
      "slope, intercept, r_value, p_value, std_err = stats.linregress(x,y)\n",
      "slope, intercept, r_value, p_value, std_err\n",
      "26/170:\n",
      "plot.figure()\n",
      "plot.plot(x, y, \"o\")\n",
      "yfit = [intercept + slope * xi for xi in x]\n",
      "plot.plot(x, yfit)\n",
      "26/171:\n",
      "xw = np.array(degree_centralities[:313])\n",
      "yw = np.array(boxofficegross[:313])\n",
      "slope, intercept, r_value, p_value, std_err = stats.linregress(xw,yw)\n",
      "slope, intercept, r_value, p_value, std_err\n",
      "26/172:\n",
      "xm = np.array(degree_centralities[313:])\n",
      "ym = np.array(boxofficegross[313:])\n",
      "slope, intercept, r_value, p_value, std_err = stats.linregress(xm,ym)\n",
      "slope, intercept, r_value, p_value, std_err\n",
      "26/173:\n",
      "academynominated = []\n",
      "for a in actorlist:\n",
      "    if a in academylist:\n",
      "        academynominated.append(1)\n",
      "    else:\n",
      "        academynominated.append(0)\n",
      "26/174: plot.hist(academynominated, bins=2)\n",
      "26/175:\n",
      "x_train = np.concatenate((x[0:213], x[313:734]))\n",
      "x_test = np.concatenate((x[213:313], x[734:]))\n",
      "y_train = np.concatenate((academynominated[0:213], academynominated[313:734]))\n",
      "y_test = np.concatenate((academynominated[213:313], academynominated[734:]))\n",
      "26/176: log = LogisticRegression(solver='lbfgs').fit(x.reshape(-1, 1), academynominated)\n",
      "26/177: log.score(x.reshape(-1, 1), academynominated)\n",
      "26/178:\n",
      "lr = LogisticRegression(solver='lbfgs').fit(x_train.reshape(-1, 1), y_train)\n",
      "lr.score(x_train.reshape(-1, 1), y_train)\n",
      "26/179: lr.score(x_test.reshape(-1, 1), y_test)\n",
      "26/180: nx.write_gexf(G, \"graph.gexf\")\n",
      "26/181:\n",
      "eigenvector_centralities = []\n",
      "for actor in actorlist:\n",
      "    eigenvector_centralities.append(eigenvector_centrality.get(actor))\n",
      "26/182: plot.hist(eigenvector_centralities)\n",
      "26/183:\n",
      "x_e = np.array(eigenvector_centralities)\n",
      "y = np.array(boxofficegross)\n",
      "slope, intercept, r_value, p_value, std_err = stats.linregress(x_e,y)\n",
      "slope, intercept, r_value, p_value, std_err\n",
      "26/184:\n",
      "betweenness_centralities = []\n",
      "for actor in actorlist:\n",
      "    betweenness_centralities.append(betweenness_centrality.get(actor))\n",
      "x_b = np.array(betweenness_centralities)\n",
      "y = np.array(boxofficegross)\n",
      "slope, intercept, r_value, p_value, std_err = stats.linregress(x_b,y)\n",
      "slope, intercept, r_value, p_value, std_err\n",
      "26/185: plot.hist(betweenness_centralities)\n",
      "26/186:\n",
      "closeness_centralities = []\n",
      "for actor in actorlist:\n",
      "    closeness_centralities.append(closeness_centrality.get(actor))\n",
      "x_c = np.array(closeness_centralities)\n",
      "y = np.array(boxofficegross)\n",
      "slope, intercept, r_value, p_value, std_err = stats.linregress(x_c,y)\n",
      "slope, intercept, r_value, p_value, std_err\n",
      "26/187: plot.hist(closeness_centralities)\n",
      "26/188:\n",
      "log_e = LogisticRegression(solver='lbfgs').fit(x_e.reshape(-1,1), academynominated)\n",
      "log_e.score(x_e.reshape(-1,1), academynominated)\n",
      "26/189:\n",
      "log_b = LogisticRegression(solver='lbfgs').fit(x_b.reshape(-1,1), academynominated)\n",
      "log_b.score(x_b.reshape(-1,1), academynominated)\n",
      "26/190:\n",
      "log_c = LogisticRegression(solver='lbfgs').fit(x_c.reshape(-1,1), academynominated)\n",
      "log_c.score(x_c.reshape(-1,1), academynominated)\n",
      "26/191:\n",
      "pred_x = log.predict(x.reshape(-1, 1))\n",
      "pred_e = log_e.predict(x.reshape(-1, 1))\n",
      "26/192:\n",
      "slope, intercept, r_value, p_value, std_err = stats.linregress(x, academynominated)\n",
      "slope, intercept, r_value, p_value, std_err\n",
      "26/193: plot.plot(x, academynominated, \"o\")\n",
      "26/194:\n",
      "print np.corrcoef(x, x_e)[0,1]\n",
      "print np.corrcoef(x, x_b)[0,1]\n",
      "print np.corrcoef(x, x_c)[0,1]\n",
      "26/195:\n",
      "moviecounts = []\n",
      "for actor in actorlist:\n",
      "    moviecounts.append(moviecountdict.get(actor))\n",
      "26/196:\n",
      "xc = np.array(moviecounts)\n",
      "yc = np.array(boxofficegross)\n",
      "slope, intercept, r_value, p_value, std_err = stats.linregress(xc,yc)\n",
      "slope, intercept, r_value, p_value, std_err\n",
      "26/197:\n",
      "plot.figure()\n",
      "plot.plot(moviecounts, boxofficegross, \"o\")\n",
      "plot.plot(moviecounts, [slope*x + intercept for x in moviecounts])\n",
      "26/198:\n",
      "import statsmodels.api as sm\n",
      "import pandas as pd\n",
      "26/199:\n",
      "df = pd.DataFrame(\n",
      "    {\n",
      "        'degree_centralities': degree_centralities,\n",
      "        'eigenvector_centralities': eigenvector_centralities,\n",
      "        'betweenness_centralities': betweenness_centralities,\n",
      "        'closeness_centralities': closeness_centralities,\n",
      "        'moviecount': moviecounts,\n",
      "        'boxofficegross': boxofficegross,\n",
      "        'academynominated': academynominated\n",
      "    })\n",
      "26/200: X = df[['degree_centralities', 'eigenvector_centralities', 'betweenness_centralities', 'closeness_centralities', 'moviecount']]\n",
      "26/201:\n",
      "X = sm.add_constant(X) # adding a constant\n",
      "Y = boxofficegross\n",
      "model = sm.OLS(Y, X).fit()\n",
      "predictions = model.predict(X) \n",
      " \n",
      "print_model = model.summary()\n",
      "print(print_model)\n",
      "26/202:\n",
      "X2 = df[['degree_centralities', 'eigenvector_centralities', 'betweenness_centralities', 'closeness_centralities', 'moviecount']]\n",
      "X2 = sm.add_constant(X2) # adding a constant\n",
      "Y2 = academynominated\n",
      "model2 = sm.OLS(Y2, X2).fit()\n",
      "predictions2 = model2.predict(X2) \n",
      "print_model2 = model2.summary()\n",
      "print(print_model2)\n",
      "26/203:\n",
      "predictions2bin = [1 if x >= 0.5 else 0 for x in predictions2]\n",
      "wrong = [0 if predictions2bin[i] == academynominated[i] else 1 for i in range(834)]\n",
      "print sum(predictions2bin), sum(academynominated)\n",
      "print sum(wrong), len(wrong)\n",
      "print float(sum(wrong))/len(wrong), 1-float(sum(wrong))/len(wrong)\n",
      "26/204:\n",
      "X2e = df[['eigenvector_centralities', 'moviecount']] #, 'eigenvector_centralities', 'betweenness_centralities', 'closeness_centralities', 'moviecount']]\n",
      "X2e = sm.add_constant(X2e) # adding a constant\n",
      "Y2e = academynominated\n",
      "model2e = sm.OLS(Y2, X2e).fit()\n",
      "predictions2e = model2e.predict(X2e) \n",
      "print_model2e = model2e.summary()\n",
      "print(print_model2e)\n",
      "26/205:\n",
      "predictions2ebin = [1 if x >= 0.5 else 0 for x in predictions2e]\n",
      "wrong = [0 if predictions2ebin[i] == academynominated[i] else 1 for i in range(834)]\n",
      "print sum(predictions2ebin), sum(academynominated)\n",
      "print sum(wrong), len(wrong)\n",
      "print float(sum(wrong))/len(wrong), 1-float(sum(wrong))/len(wrong)\n",
      "26/206:\n",
      "G = nx.Graph()\n",
      "Gw = nx.Graph()\n",
      "Gm = nx.Graph()\n",
      "Gweighted = nx.Graph()\n",
      "moviecountdict = {}\n",
      "\n",
      "with open('title.principals.tsv') as tsvfile:\n",
      "    reader = csv.reader(tsvfile, delimiter='\\t')\n",
      "    prev = \"\"\n",
      "    actors = []\n",
      "    for row in reader:\n",
      "        if row[3] == 'actor' and row[2] in dict and dict[row[2]] in boxofficedict:\n",
      "            Gm.add_node(dict[row[2]])\n",
      "            G.add_node(dict[row[2]])\n",
      "            Gweighted.add_node(dict[row[2]])\n",
      "            if dict[row[2]] in moviecountdict:\n",
      "                moviecountdict[dict[row[2]]] += 1\n",
      "            else:\n",
      "                moviecountdict[dict[row[2]]] = 1\n",
      "        elif row[3] == 'actress' and row[2] in dict and dict[row[2]] in boxofficedict:\n",
      "            Gw.add_node(dict[row[2]])\n",
      "            G.add_node(dict[row[2]])\n",
      "            Gweighted.add_node(dict[row[2]])\n",
      "            if dict[row[2]] in moviecountdict:\n",
      "                moviecountdict[dict[row[2]]] += 1\n",
      "            else:\n",
      "                moviecountdict[dict[row[2]]] = 1\n",
      "        else:\n",
      "            continue\n",
      "        if row[0] != prev:\n",
      "            for a1 in actors:\n",
      "                for a2 in actors:\n",
      "                    if a1 == a2:\n",
      "                        continue\n",
      "                    G.add_edge(a1, a2)\n",
      "                    if Gweighted.has_edge(a1, a2):\n",
      "                        Gweighted.add_edge(a1, a2, weight=(Gweighted.get_edge_data(a1, a2)['weight'] + 1))\n",
      "                    else:\n",
      "                        Gweighted.add_edge(a1, a2, weight=1)\n",
      "                    if a1 in Gw.nodes and a2 in Gw.nodes:\n",
      "                        Gw.add_edge(a1, a2)\n",
      "                    elif a1 in Gm.nodes and a2 in Gm.nodes:\n",
      "                        Gm.add_edge(a1, a2)\n",
      "            actors = [dict[row[2]]]\n",
      "        else:\n",
      "            actors.append(dict[row[2]])\n",
      "        prev = row[0]\n",
      "26/207: len(Gweighted.edges())\n",
      "26/208: Gweighted.nodes()\n",
      "26/209:\n",
      "def kweighted(G, k):\n",
      "    Gnew = nx.Graph()\n",
      "    for (u, v, d) in G.edges(data=True):\n",
      "        if d >= k:\n",
      "            Gnew.add_edge(u, v)\n",
      "    return Gnew\n",
      "26/210: G2 = kweighted(Gweighted, 2)\n",
      "26/211:\n",
      "G2 = kweighted(Gweighted, 2)\n",
      "len(G2.nodes()), len(G2.edges())\n",
      "26/212:\n",
      "G3 = kweighted(Gweighted, 2)\n",
      "len(G3.nodes()), len(G3.edges())\n",
      "26/213: len(G.nodes())\n",
      "26/214: len(Gweighted.nodes())\n",
      "26/215:\n",
      "Gnew = nx.Graph()\n",
      "for (u, v, d) in G.edges(data=True):\n",
      "    if d >= k:\n",
      "        Gnew.add_edge(u, v)\n",
      "len(Gnew.edges())\n",
      "26/216:\n",
      "Gnew = nx.Graph()\n",
      "for (u, v, d) in G.edges(data=True):\n",
      "    if d >= 2:\n",
      "        Gnew.add_edge(u, v)\n",
      "len(Gnew.edges())\n",
      "26/217:\n",
      "Gnew = nx.Graph()\n",
      "for (u, v, d) in G.edges(data=True):\n",
      "    if d >= 5:\n",
      "        Gnew.add_edge(u, v)\n",
      "len(Gnew.edges())\n",
      "26/218:\n",
      "Gnew = nx.Graph()\n",
      "for (u, v, d) in G.edges(data=True):\n",
      "    if d >= 5:\n",
      "        print u, v, d\n",
      "        Gnew.add_edge(u, v)\n",
      "len(Gnew.edges())\n",
      "26/219:\n",
      "def kweighted(G, k):\n",
      "    Gnew = nx.Graph()\n",
      "    for (u, v, d) in G.edges(data=True):\n",
      "        if d['weight'] >= k:\n",
      "            Gnew.add_edge(u, v)\n",
      "    return Gnew\n",
      "26/220:\n",
      "G2 = kweighted(Gweighted, 2)\n",
      "len(G2.nodes()), len(G2.edges())\n",
      "26/221:\n",
      "G3 = kweighted(Gweighted, 2)\n",
      "len(G3.nodes()), len(G3.edges())\n",
      "26/222:\n",
      "def kweighted(G_old, k):\n",
      "    G_new = nx.Graph()\n",
      "    for (u, v, d) in G_old.edges(data=True):\n",
      "        if d['weight'] >= k:\n",
      "            G_new.add_edge(u, v)\n",
      "    return G_new\n",
      "26/223:\n",
      "G2 = kweighted(Gweighted, 2)\n",
      "len(G2.nodes()), len(G2.edges())\n",
      "26/224:\n",
      "G3 = kweighted(Gweighted, 2)\n",
      "len(G3.nodes()), len(G3.edges())\n",
      "26/225:\n",
      "def kweighted(G_old, k):\n",
      "    G_new = nx.Graph()\n",
      "    for (u, v, d) in G_old.edges(data=True):\n",
      "        print u, v, d\n",
      "        if d['weight'] >= k:\n",
      "            G_new.add_edge(u, v)\n",
      "    return G_new\n",
      "26/226:\n",
      "G2 = kweighted(Gweighted, 2)\n",
      "len(G2.nodes()), len(G2.edges())\n",
      "26/227:\n",
      "G3 = kweighted(Gweighted, 3)\n",
      "len(G3.nodes()), len(G3.edges())\n",
      "26/228:\n",
      "def kweighted(G_old, k):\n",
      "    G_new = nx.Graph()\n",
      "    for (u, v, d) in G_old.edges(data=True):\n",
      "        if d['weight'] >= k:\n",
      "            G_new.add_edge(u, v)\n",
      "    return G_new\n",
      "26/229:\n",
      "G3 = kweighted(Gweighted, 3)\n",
      "len(G3.nodes()), len(G3.edges())\n",
      "26/230:\n",
      "G = nx.Graph()\n",
      "Gw = nx.Graph()\n",
      "Gm = nx.Graph()\n",
      "Gweighted = nx.Graph()\n",
      "moviecountdict = {}\n",
      "\n",
      "with open('title.principals.tsv') as tsvfile:\n",
      "    reader = csv.reader(tsvfile, delimiter='\\t')\n",
      "    prev = \"\"\n",
      "    actors = []\n",
      "    for row in reader:\n",
      "        if row[3] == 'actor' and row[2] in dict and dict[row[2]] in boxofficedict:\n",
      "            Gm.add_node(dict[row[2]])\n",
      "            G.add_node(dict[row[2]])\n",
      "            Gweighted.add_node(dict[row[2]])\n",
      "            if dict[row[2]] in moviecountdict:\n",
      "                moviecountdict[dict[row[2]]] += 1\n",
      "            else:\n",
      "                moviecountdict[dict[row[2]]] = 1\n",
      "        elif row[3] == 'actress' and row[2] in dict and dict[row[2]] in boxofficedict:\n",
      "            Gw.add_node(dict[row[2]])\n",
      "            G.add_node(dict[row[2]])\n",
      "            Gweighted.add_node(dict[row[2]])\n",
      "            if dict[row[2]] in moviecountdict:\n",
      "                moviecountdict[dict[row[2]]] += 1\n",
      "            else:\n",
      "                moviecountdict[dict[row[2]]] = 1\n",
      "        else:\n",
      "            continue\n",
      "        if row[0] != prev:\n",
      "            for i in range(len(actors)):\n",
      "                for j in range(i, len(actors)):\n",
      "                    a1 = actors[i]\n",
      "                    a2 = actors[j]\n",
      "                    if a1 == a2:\n",
      "                        continue\n",
      "                    G.add_edge(a1, a2)\n",
      "                    if Gweighted.has_edge(a1, a2):\n",
      "                        Gweighted.add_edge(a1, a2, weight=(Gweighted.get_edge_data(a1, a2)['weight'] + 1))\n",
      "                    else:\n",
      "                        Gweighted.add_edge(a1, a2, weight=1)\n",
      "                    if a1 in Gw.nodes and a2 in Gw.nodes:\n",
      "                        Gw.add_edge(a1, a2)\n",
      "                    elif a1 in Gm.nodes and a2 in Gm.nodes:\n",
      "                        Gm.add_edge(a1, a2)\n",
      "            actors = [dict[row[2]]]\n",
      "        else:\n",
      "            actors.append(dict[row[2]])\n",
      "        prev = row[0]\n",
      "26/231:\n",
      "def kweighted(G_old, k):\n",
      "    G_new = nx.Graph()\n",
      "    for (u, v, d) in G_old.edges(data=True):\n",
      "        if d['weight'] >= k:\n",
      "            G_new.add_edge(u, v)\n",
      "    return G_new\n",
      "26/232:\n",
      "G2 = kweighted(Gweighted, 2)\n",
      "len(G2.nodes()), len(G2.edges())\n",
      "26/233:\n",
      "G3 = kweighted(Gweighted, 3)\n",
      "len(G3.nodes()), len(G3.edges())\n",
      "26/234:\n",
      "def kweighted(G_old, k):\n",
      "    G_new = nx.Graph()\n",
      "    for (u, v, d) in G_old.edges(data=True):\n",
      "        print u,v,d\n",
      "        if d['weight'] >= k:\n",
      "            G_new.add_edge(u, v)\n",
      "    return G_new\n",
      "26/235:\n",
      "G2 = kweighted(Gweighted, 2)\n",
      "len(G2.nodes()), len(G2.edges())\n",
      "26/236: print len(G.nodes(), Gweighted.nodes())\n",
      "26/237: print len(G.nodes()), len(Gweighted.nodes())\n",
      "26/238: print len(G.edges()), len(Gweighted.edges())\n",
      "26/239:\n",
      "def kweighted(G_old, k, show):\n",
      "    G_new = nx.Graph()\n",
      "    for (u, v, d) in G_old.edges(data=True):\n",
      "        if show:\n",
      "            print u,v,d\n",
      "        if d['weight'] >= k:\n",
      "            G_new.add_edge(u, v)\n",
      "    return G_new\n",
      "26/240: kweighted(Gweighted, 0, show)\n",
      "26/241: kweighted(Gweighted, 0, True)\n",
      "26/242:\n",
      "G2 = kweighted(Gweighted, 2)\n",
      "len(G2.nodes()), len(G2.edges())\n",
      "26/243:\n",
      "G2 = kweighted(Gweighted, 2, False)\n",
      "len(G2.nodes()), len(G2.edges())\n",
      "26/244:\n",
      "G3 = kweighted(Gweighted, 3, False)\n",
      "len(G3.nodes()), len(G3.edges())\n",
      "26/245:\n",
      "G4 = kweighted(Gweighted, 4, False)\n",
      "len(G4.nodes()), len(G4.edges())\n",
      "26/246: nx.draw(G4)\n",
      "26/247:\n",
      "G5 = kweighted(Gweighted, 5, False)\n",
      "len(G5.nodes()), len(G5.edges())\n",
      "26/248:\n",
      "G10 = kweighted(Gweighted, 10, False)\n",
      "len(G10.nodes()), len(G10.edges())\n",
      "26/249: nx.draw(G10)\n",
      "26/250:\n",
      "G4 = kweighted(Gweighted, 4, False)\n",
      "len(G4.nodes()), len(G4.edges())\n",
      "26/251:\n",
      "for i in range(len(boxofficedict)):\n",
      "    boxofficedict[i] = boxofficedict[i].replace(\",\", \"\")\n",
      "26/252:\n",
      "def addtodict(actors):\n",
      "    for actor in actors:\n",
      "        name = actor.find('b').find('b').text.replace(\",\", \"\")\n",
      "        boxoffice = str(actor.find('td', attrs={'align':'right'}).text[1:]).replace(\",\", \"\")\n",
      "        if \"k\" in boxoffice:\n",
      "            boxoffice = float(boxoffice.replace(\"k\", \"\"))/1000\n",
      "        boxofficedict[name] = float(boxoffice)\n",
      "26/253: boxofficedict\n",
      "26/254: boxofficedict = {}\n",
      "26/255:\n",
      "def addtodict(actors):\n",
      "    for actor in actors:\n",
      "        name = actor.find('b').find('b').text.replace(\",\", \"\")\n",
      "        boxoffice = str(actor.find('td', attrs={'align':'right'}).text[1:]).replace(\",\", \"\")\n",
      "        if \"k\" in boxoffice:\n",
      "            boxoffice = float(boxoffice.replace(\"k\", \"\"))/1000\n",
      "        boxofficedict[name] = float(boxoffice)\n",
      "26/256: boxofficedict\n",
      "26/257:\n",
      "page2 = urllib2.urlopen(\"https://www.boxofficemojo.com/people/?view=Actor&pagenum=2&sort=person&order=ASC&p=.htm\")\n",
      "soup2 = BeautifulSoup(page2, 'html.parser')\n",
      "actors2 = soup2.find_all('tr')[2].find_all('tr')[1:]\n",
      "26/258:\n",
      "page3 = urllib2.urlopen(\"https://www.boxofficemojo.com/people/?view=Actor&pagenum=3&sort=person&order=ASC&p=.htm\")\n",
      "soup3 = BeautifulSoup(page3, 'html.parser')\n",
      "actors3 = soup3.find_all('tr')[2].find_all('tr')[1:]\n",
      "26/259:\n",
      "addtodict(actors1)\n",
      "addtodict(actors2)\n",
      "addtodict(actors3)\n",
      "26/260: len(boxofficedict)\n",
      "26/261: boxofficedict\n",
      "26/262:\n",
      "boxofficesorted = sorted(boxofficedict, key=boxofficedict.get, reverse=True)\n",
      "boxofficesorted\n",
      "26/263:\n",
      "G = nx.Graph()\n",
      "Gw = nx.Graph()\n",
      "Gm = nx.Graph()\n",
      "Gweighted = nx.Graph()\n",
      "moviecountdict = {}\n",
      "\n",
      "with open('title.principals.tsv') as tsvfile:\n",
      "    reader = csv.reader(tsvfile, delimiter='\\t')\n",
      "    prev = \"\"\n",
      "    actors = []\n",
      "    for row in reader:\n",
      "        if row[3] == 'actor' and row[2] in dict and dict[row[2]] in boxofficedict:\n",
      "            Gm.add_node(dict[row[2]])\n",
      "            G.add_node(dict[row[2]])\n",
      "            Gweighted.add_node(dict[row[2]])\n",
      "            if dict[row[2]] in moviecountdict:\n",
      "                moviecountdict[dict[row[2]]] += 1\n",
      "            else:\n",
      "                moviecountdict[dict[row[2]]] = 1\n",
      "        elif row[3] == 'actress' and row[2] in dict and dict[row[2]] in boxofficedict:\n",
      "            Gw.add_node(dict[row[2]])\n",
      "            G.add_node(dict[row[2]])\n",
      "            Gweighted.add_node(dict[row[2]])\n",
      "            if dict[row[2]] in moviecountdict:\n",
      "                moviecountdict[dict[row[2]]] += 1\n",
      "            else:\n",
      "                moviecountdict[dict[row[2]]] = 1\n",
      "        else:\n",
      "            continue\n",
      "        if row[0] != prev:\n",
      "            for i in range(len(actors)):\n",
      "                for j in range(i, len(actors)):\n",
      "                    a1 = actors[i]\n",
      "                    a2 = actors[j]\n",
      "                    if a1 == a2:\n",
      "                        continue\n",
      "                    G.add_edge(a1, a2)\n",
      "                    if Gweighted.has_edge(a1, a2):\n",
      "                        Gweighted.add_edge(a1, a2, weight=(Gweighted.get_edge_data(a1, a2)['weight'] + 1))\n",
      "                    else:\n",
      "                        Gweighted.add_edge(a1, a2, weight=1)\n",
      "                    if a1 in Gw.nodes and a2 in Gw.nodes:\n",
      "                        Gw.add_edge(a1, a2)\n",
      "                    elif a1 in Gm.nodes and a2 in Gm.nodes:\n",
      "                        Gm.add_edge(a1, a2)\n",
      "            actors = [dict[row[2]]]\n",
      "        else:\n",
      "            actors.append(dict[row[2]])\n",
      "        prev = row[0]\n",
      "26/264:\n",
      "G2 = kweighted(Gweighted, 2, False)\n",
      "len(G2.nodes()), len(G2.edges())\n",
      "26/265:\n",
      "G3 = kweighted(Gweighted, 3, False)\n",
      "len(G3.nodes()), len(G3.edges())\n",
      "26/266:\n",
      "G4 = kweighted(Gweighted, 4, False)\n",
      "len(G4.nodes()), len(G4.edges())\n",
      "26/267:\n",
      "G5 = kweighted(Gweighted, 5, False)\n",
      "len(G5.nodes()), len(G5.edges())\n",
      "26/268: list(G.nodes)\n",
      "26/269: len(G.nodes)\n",
      "26/270: list(G.edges)\n",
      "26/271: len(G.edges)\n",
      "26/272:\n",
      "actorw = sorted(Gw.nodes)\n",
      "actorm = sorted(Gm.nodes)\n",
      "26/273: degree_centrality = nx.degree_centrality(G)\n",
      "26/274: sorted(degree_centrality, key=degree_centrality.get, reverse=True)\n",
      "26/275: eigenvector_centrality = nx.eigenvector_centrality(G)\n",
      "26/276: sorted(eigenvector_centrality, key=eigenvector_centrality.get, reverse=True)\n",
      "26/277: closeness_centrality = nx.closeness_centrality(G)\n",
      "26/278: sorted(closeness_centrality, key=closeness_centrality.get, reverse=True)\n",
      "26/279: betweenness_centrality = nx.betweenness_centrality(G)\n",
      "26/280: sorted(betweenness_centrality, key=betweenness_centrality.get, reverse=True)\n",
      "26/281:\n",
      "degree_centralities = []\n",
      "actorlist = [a for a in actorw]\n",
      "for a in actorm:\n",
      "    actorlist.append(a)\n",
      "for actor in actorlist:\n",
      "    degree_centralities.append(degree_centrality.get(actor))\n",
      "26/282: len(actorw)\n",
      "26/283: actorlist\n",
      "26/284: degree_centralities\n",
      "26/285: plot.hist(degree_centralities)\n",
      "26/286:\n",
      "boxofficegross = []\n",
      "for actor in actorlist:\n",
      "    boxofficegross.append(boxofficedict[actor])\n",
      "26/287: boxofficegross\n",
      "26/288: plot.hist(boxofficegross)\n",
      "26/289: plot.plot(degree_centralities, boxofficegross, \"ro\")\n",
      "26/290: plot.plot(degree_centralities[:313], boxofficegross[:313], \"ro\")\n",
      "26/291: plot.plot(degree_centralities[313:], boxofficegross[313:], \"ro\")\n",
      "26/292:\n",
      "x = np.array(degree_centralities)\n",
      "y = np.array(boxofficegross)\n",
      "slope, intercept, r_value, p_value, std_err = stats.linregress(x,y)\n",
      "slope, intercept, r_value, p_value, std_err\n",
      "26/293:\n",
      "plot.figure()\n",
      "plot.plot(x, y, \"o\")\n",
      "yfit = [intercept + slope * xi for xi in x]\n",
      "plot.plot(x, yfit)\n",
      "26/294:\n",
      "xw = np.array(degree_centralities[:313])\n",
      "yw = np.array(boxofficegross[:313])\n",
      "slope, intercept, r_value, p_value, std_err = stats.linregress(xw,yw)\n",
      "slope, intercept, r_value, p_value, std_err\n",
      "26/295:\n",
      "xm = np.array(degree_centralities[313:])\n",
      "ym = np.array(boxofficegross[313:])\n",
      "slope, intercept, r_value, p_value, std_err = stats.linregress(xm,ym)\n",
      "slope, intercept, r_value, p_value, std_err\n",
      "26/296:\n",
      "academynominated = []\n",
      "for a in actorlist:\n",
      "    if a in academylist:\n",
      "        academynominated.append(1)\n",
      "    else:\n",
      "        academynominated.append(0)\n",
      "26/297: plot.hist(academynominated, bins=2)\n",
      "26/298:\n",
      "x_train = np.concatenate((x[0:213], x[313:734]))\n",
      "x_test = np.concatenate((x[213:313], x[734:]))\n",
      "y_train = np.concatenate((academynominated[0:213], academynominated[313:734]))\n",
      "y_test = np.concatenate((academynominated[213:313], academynominated[734:]))\n",
      "26/299: log = LogisticRegression(solver='lbfgs').fit(x.reshape(-1, 1), academynominated)\n",
      "26/300: log.score(x.reshape(-1, 1), academynominated)\n",
      "26/301:\n",
      "lr = LogisticRegression(solver='lbfgs').fit(x_train.reshape(-1, 1), y_train)\n",
      "lr.score(x_train.reshape(-1, 1), y_train)\n",
      "26/302: lr.score(x_test.reshape(-1, 1), y_test)\n",
      "26/303: log = LogisticRegression(solver='lbfgs').fit(x.reshape(-1, 1), academynominated)\n",
      "26/304: log.score(x.reshape(-1, 1), academynominated)\n",
      "26/305:\n",
      "lr = LogisticRegression(solver='lbfgs').fit(x_train.reshape(-1, 1), y_train)\n",
      "lr.score(x_train.reshape(-1, 1), y_train)\n",
      "26/306:\n",
      "eigenvector_centralities = []\n",
      "for actor in actorlist:\n",
      "    eigenvector_centralities.append(eigenvector_centrality.get(actor))\n",
      "26/307: plot.hist(eigenvector_centralities)\n",
      "26/308:\n",
      "x_e = np.array(eigenvector_centralities)\n",
      "y = np.array(boxofficegross)\n",
      "slope, intercept, r_value, p_value, std_err = stats.linregress(x_e,y)\n",
      "slope, intercept, r_value, p_value, std_err\n",
      "26/309:\n",
      "betweenness_centralities = []\n",
      "for actor in actorlist:\n",
      "    betweenness_centralities.append(betweenness_centrality.get(actor))\n",
      "x_b = np.array(betweenness_centralities)\n",
      "y = np.array(boxofficegross)\n",
      "slope, intercept, r_value, p_value, std_err = stats.linregress(x_b,y)\n",
      "slope, intercept, r_value, p_value, std_err\n",
      "26/310: plot.hist(betweenness_centralities)\n",
      "26/311:\n",
      "closeness_centralities = []\n",
      "for actor in actorlist:\n",
      "    closeness_centralities.append(closeness_centrality.get(actor))\n",
      "x_c = np.array(closeness_centralities)\n",
      "y = np.array(boxofficegross)\n",
      "slope, intercept, r_value, p_value, std_err = stats.linregress(x_c,y)\n",
      "slope, intercept, r_value, p_value, std_err\n",
      "26/312: plot.hist(closeness_centralities)\n",
      "26/313:\n",
      "log_e = LogisticRegression(solver='lbfgs').fit(x_e.reshape(-1,1), academynominated)\n",
      "log_e.score(x_e.reshape(-1,1), academynominated)\n",
      "26/314:\n",
      "log_b = LogisticRegression(solver='lbfgs').fit(x_b.reshape(-1,1), academynominated)\n",
      "log_b.score(x_b.reshape(-1,1), academynominated)\n",
      "26/315:\n",
      "log_c = LogisticRegression(solver='lbfgs').fit(x_c.reshape(-1,1), academynominated)\n",
      "log_c.score(x_c.reshape(-1,1), academynominated)\n",
      "26/316:\n",
      "slope, intercept, r_value, p_value, std_err = stats.linregress(x, academynominated)\n",
      "slope, intercept, r_value, p_value, std_err\n",
      "26/317: plot.plot(x, academynominated, \"o\")\n",
      "26/318:\n",
      "print np.corrcoef(x, x_e)[0,1]\n",
      "print np.corrcoef(x, x_b)[0,1]\n",
      "print np.corrcoef(x, x_c)[0,1]\n",
      "26/319:\n",
      "moviecounts = []\n",
      "for actor in actorlist:\n",
      "    moviecounts.append(moviecountdict.get(actor))\n",
      "26/320:\n",
      "xc = np.array(moviecounts)\n",
      "yc = np.array(boxofficegross)\n",
      "slope, intercept, r_value, p_value, std_err = stats.linregress(xc,yc)\n",
      "slope, intercept, r_value, p_value, std_err\n",
      "26/321:\n",
      "plot.figure()\n",
      "plot.plot(moviecounts, boxofficegross, \"o\")\n",
      "plot.plot(moviecounts, [slope*x + intercept for x in moviecounts])\n",
      "26/322:\n",
      "import statsmodels.api as sm\n",
      "import pandas as pd\n",
      "26/323:\n",
      "df = pd.DataFrame(\n",
      "    {\n",
      "        'degree_centralities': degree_centralities,\n",
      "        'eigenvector_centralities': eigenvector_centralities,\n",
      "        'betweenness_centralities': betweenness_centralities,\n",
      "        'closeness_centralities': closeness_centralities,\n",
      "        'moviecount': moviecounts,\n",
      "        'boxofficegross': boxofficegross,\n",
      "        'academynominated': academynominated\n",
      "    })\n",
      "26/324: X = df[['degree_centralities', 'eigenvector_centralities', 'betweenness_centralities', 'closeness_centralities', 'moviecount']]\n",
      "26/325:\n",
      "X = sm.add_constant(X) # adding a constant\n",
      "Y = boxofficegross\n",
      "model = sm.OLS(Y, X).fit()\n",
      "predictions = model.predict(X) \n",
      " \n",
      "print_model = model.summary()\n",
      "print(print_model)\n",
      "26/326:\n",
      "X2 = df[['degree_centralities', 'eigenvector_centralities', 'betweenness_centralities', 'closeness_centralities', 'moviecount']]\n",
      "X2 = sm.add_constant(X2) # adding a constant\n",
      "Y2 = academynominated\n",
      "model2 = sm.OLS(Y2, X2).fit()\n",
      "predictions2 = model2.predict(X2) \n",
      "print_model2 = model2.summary()\n",
      "print(print_model2)\n",
      "26/327:\n",
      "predictions2bin = [1 if x >= 0.5 else 0 for x in predictions2]\n",
      "wrong = [0 if predictions2bin[i] == academynominated[i] else 1 for i in range(834)]\n",
      "print sum(predictions2bin), sum(academynominated)\n",
      "print sum(wrong), len(wrong)\n",
      "print float(sum(wrong))/len(wrong), 1-float(sum(wrong))/len(wrong)\n",
      "26/328:\n",
      "predictions2bin = [1 if x >= 0.5 else 0 for x in predictions2]\n",
      "wrong = [0 if predictions2bin[i] == academynominated[i] else 1 for i in range(834)]\n",
      "print sum(predictions2bin), sum(academynominated)\n",
      "print sum(wrong), len(wrong)\n",
      "print float(sum(wrong))/len(wrong), 1-float(sum(wrong))/len(wrong)\n",
      "print sum(academynominated)/len(academynominated)\n",
      "26/329:\n",
      "predictions2bin = [1 if x >= 0.5 else 0 for x in predictions2]\n",
      "wrong = [0 if predictions2bin[i] == academynominated[i] else 1 for i in range(834)]\n",
      "print sum(predictions2bin), sum(academynominated)\n",
      "print sum(wrong), len(wrong)\n",
      "print float(sum(wrong))/len(wrong), 1-float(sum(wrong))/len(wrong)\n",
      "print float(sum(academynominated))/len(academynominated)\n",
      "26/330: from sklearn.metrics import confusion_matrix\n",
      "26/331: tn, fp, fn, tp = confusion_matrix(academynominated, predictions2bin).ravel()\n",
      "26/332:\n",
      "tn, fp, fn, tp = confusion_matrix(academynominated, predictions2bin).ravel()\n",
      "tn, fp, fn, tp\n",
      "26/333:\n",
      "X2e = df[['eigenvector_centralities', 'moviecount']] #, 'eigenvector_centralities', 'betweenness_centralities', 'closeness_centralities', 'moviecount']]\n",
      "X2e = sm.add_constant(X2e) # adding a constant\n",
      "Y2e = academynominated\n",
      "model2e = sm.OLS(Y2, X2e).fit()\n",
      "predictions2e = model2e.predict(X2e) \n",
      "print_model2e = model2e.summary()\n",
      "print(print_model2e)\n",
      "26/334:\n",
      "predictions2ebin = [1 if x >= 0.5 else 0 for x in predictions2e]\n",
      "wrong = [0 if predictions2ebin[i] == academynominated[i] else 1 for i in range(834)]\n",
      "print sum(predictions2ebin), sum(academynominated)\n",
      "print sum(wrong), len(wrong)\n",
      "print float(sum(wrong))/len(wrong), 1-float(sum(wrong))/len(wrong)\n",
      "26/335:\n",
      "tn2, fp2, fn2, tp2 = confusion_matrix(academynominated, predictions2ebin).ravel()\n",
      "tn2, fp2, fn2, tp2\n",
      "26/336: float(tp2)/(tp2 + fp2)\n",
      "26/337: float(tp2)/(tp2 + tn2)\n",
      "26/338: float(tp2)/(tp2 + fn2)\n",
      "26/339: float(tn2)/(tn2 + fp2)\n",
      "26/340:\n",
      "def recall_specificity_precision(tn, fp, fn, tp):\n",
      "    recall = float(tp)/(tp + fn)\n",
      "    specificity = float(tn)/(tn + fp)\n",
      "    precision = float(tp)/(tp + fp)\n",
      "    return recall, specificity, precision\n",
      "26/341:\n",
      "def measures(tn, fp, fn, tp):\n",
      "    accuracy = float(tn + fn)/(tn+fp+fn+tp)\n",
      "    recall = float(tp)/(tp + fn)\n",
      "    specificity = float(tn)/(tn + fp)\n",
      "    precision = float(tp)/(tp + fp)\n",
      "    return \"accuracy: %f\\nrecall: %f\\nspecificity: %f\\nprecision:%f\\n\" % (accuracy, recall, specificity, precision)\n",
      "26/342: recall_specificity_precision\n",
      "26/343: measures(tn, fp, fn, tp)\n",
      "26/344:\n",
      "def measures(tn, fp, fn, tp):\n",
      "    accuracy = float(tn + fn)/(tn+fp+fn+tp)\n",
      "    recall = float(tp)/(tp + fn)\n",
      "    specificity = float(tn)/(tn + fp)\n",
      "    precision = float(tp)/(tp + fp)\n",
      "    return \"accuracy: %f\\n recall: %f\\nspecificity: %f\\nprecision:%f\\n\" % (accuracy, recall, specificity, precision)\n",
      "26/345: measures(tn, fp, fn, tp)\n",
      "26/346:\n",
      "def measures(tn, fp, fn, tp):\n",
      "    accuracy = float(tn + fn)/(tn+fp+fn+tp)\n",
      "    recall = float(tp)/(tp + fn)\n",
      "    specificity = float(tn)/(tn + fp)\n",
      "    precision = float(tp)/(tp + fp)\n",
      "    return \"accuracy: %f \\n recall: %f\\nspecificity: %f\\nprecision:%f\\n\" % (accuracy, recall, specificity, precision)\n",
      "26/347: measures(tn, fp, fn, tp)\n",
      "26/348:\n",
      "def measures(tn, fp, fn, tp):\n",
      "    accuracy = float(tn + fn)/(tn+fp+fn+tp)\n",
      "    recall = float(tp)/(tp + fn)\n",
      "    specificity = float(tn)/(tn + fp)\n",
      "    precision = float(tp)/(tp + fp)\n",
      "    return \"accuracy: %f, recall: %f, specificity: %f, precision:%f\" % (accuracy, recall, specificity, precision)\n",
      "26/349: measures(tn, fp, fn, tp)\n",
      "26/350: print measures(tn, fp, fn, tp)\n",
      "26/351:\n",
      "def measures(tn, fp, fn, tp):\n",
      "    accuracy = float(tn + fn)/(tn+fp+fn+tp)\n",
      "    recall = float(tp)/(tp + fn)\n",
      "    specificity = float(tn)/(tn + fp)\n",
      "    precision = float(tp)/(tp + fp)\n",
      "    return \"accuracy: %f, recall: %f, specificity: %f, precision: %f\" % (accuracy, recall, specificity, precision)\n",
      "26/352: print measures(tn, fp, fn, tp)\n",
      "26/353: print measures(tn2, fp2, fn2, tp2)\n",
      "26/354: len(actorw)\n",
      "26/355:\n",
      "actorw = sorted(Gw.nodes)\n",
      "actorm = sorted(Gm.nodes)\n",
      "26/356: len(actorw)\n",
      "26/357: len(actorw), len(actorm)\n",
      "26/358: len(G.nodes)\n",
      "26/359:\n",
      "for a in actorw:\n",
      "    if a not in G.nodes():\n",
      "        print a\n",
      "26/360:\n",
      "for a in actorm:\n",
      "    if a not in G.nodes():\n",
      "        print a\n",
      "26/361:\n",
      "for a in actorm:\n",
      "    if a in Gw.nodes():\n",
      "        print a\n",
      "26/362:\n",
      "for a in actorw:\n",
      "    if a in Gm.nodes():\n",
      "        print a\n",
      "26/363: len(actorw)\n",
      "26/364: degree_centrality.get(\"Abigail Breslin\")\n",
      "26/365: plot.hist(degree_centralities[:313])\n",
      "26/366: plot.hist(degree_centralities[313:])\n",
      "26/367: np.mean(degree_centralities[:313]), np.median(degree_centralities[:313])\n",
      "26/368: np.mean(degree_centralities[:313]), np.sd(degree_centralities[:313])\n",
      "26/369: np.mean(degree_centralities[:313]), np.std(degree_centralities[:313])\n",
      "26/370: np.mean(degree_centralities[313:]), np.std(degree_centralities[313:])\n",
      "26/371: np.median(degree_centralities[313:]), np.std(degree_centralities[313:])\n",
      "26/372: np.median(degree_centralities[:313]), np.std(degree_centralities[:313])\n",
      "26/373: np.median(degree_centralities[313:]), np.mean(degree_centralities[313:]), np.std(degree_centralities[313:])\n",
      "26/374: np.median(degree_centralities[:313]), np.mean(degree_centralities[313:]), np.std(degree_centralities[:313])\n",
      "26/375: np.median(degree_centralities[:313]), np.mean(degree_centralities[:313]), np.std(degree_centralities[:313])\n",
      "26/376: plot.hist(boxofficegross[:313])\n",
      "26/377: plot.hist(boxofficegross[313:])\n",
      "26/378:\n",
      "plot.hist(boxofficegross[313:])\n",
      "np.median(boxofficegross[313:])\n",
      "26/379:\n",
      "plot.hist(boxofficegross[313:])\n",
      "np.median(boxofficegross[313:]), np.mean(boxofficegross[313:]), np.std(boxofficegross[313:])\n",
      "26/380:\n",
      "plot.hist(boxofficegross[:313])\n",
      "np.median(boxofficegross[:313]), np.mean(boxofficegross[:313]), np.std(boxofficegross[:313])\n",
      "26/381: np.median(degree_centralities[:313]), np.mean(degree_centralities[:313]), np.std(degree_centralities[:313])\n",
      "26/382:\n",
      "plot.hist(degree_centralities[:313])\n",
      "np.median(degree_centralities[:313]), np.mean(degree_centralities[:313]), np.std(degree_centralities[:313])\n",
      "26/383:\n",
      "plot.hist(degree_centralities)\n",
      "np.median(degree_centralities), np.mean(degree_centralities), np.std(degree_centralities)\n",
      "26/384:\n",
      "plot.hist(degree_centralities[313:])\n",
      "np.median(degree_centralities[313:]), np.mean(degree_centralities[313:]), np.std(degree_centralities[313:])\n",
      "26/385:\n",
      "plot.hist(boxofficegross)\n",
      "np.median(boxofficegross), np.mean(boxofficegross), np.std(boxofficegross)\n",
      "28/1:\n",
      "import pandas as pd\n",
      "import matplotlib.pyplot as plt\n",
      "import seaborn as sns\n",
      "import re\n",
      "import numpy as np\n",
      "import math\n",
      "26/386: from sklearn import preprocessing\n",
      "26/387:\n",
      "Xnorm = preprocessing.normalize(sm.add_constant(X)) # adding a constant\n",
      "Y = boxofficegross\n",
      "model = sm.OLS(Y, Xnorm).fit()\n",
      "predictions = model.predict(Xnorm) \n",
      " \n",
      "print_model = model.summary()\n",
      "print(print_model)\n",
      "26/388:\n",
      "Xnorm = preprocessing.normalize(X) # adding a constant\n",
      "Y = boxofficegross\n",
      "model = sm.OLS(Y, Xnorm).fit()\n",
      "predictions = model.predict(Xnorm) \n",
      " \n",
      "print_model = model.summary()\n",
      "print(print_model)\n",
      "26/389:\n",
      "Xnorm = (X-X.mean())/X.std() # adding a constant\n",
      "Y = boxofficegross\n",
      "model = sm.OLS(Y, Xnorm).fit()\n",
      "predictions = model.predict(Xnorm) \n",
      " \n",
      "print_model = model.summary()\n",
      "print(print_model)\n",
      "26/390:\n",
      "df1 = pd.DataFrame(\n",
      "    {\n",
      "        'degree_centralities': degree_centralities,\n",
      "        'eigenvector_centralities': eigenvector_centralities,\n",
      "        'betweenness_centralities': betweenness_centralities,\n",
      "        'closeness_centralities': closeness_centralities,\n",
      "        'moviecount': moviecounts,\n",
      "    })\n",
      "26/391:\n",
      "Xnorm = (df1-df1.mean())/df1.std() # adding a constant\n",
      "Y = boxofficegross\n",
      "model = sm.OLS(Y, Xnorm).fit()\n",
      "predictions = model.predict(Xnorm) \n",
      " \n",
      "print_model = model.summary()\n",
      "print(print_model)\n",
      "26/392:\n",
      "Xnorm = (df1-df1.min())/(df1.max()-df1.min()) # adding a constant\n",
      "Y = boxofficegross\n",
      "model = sm.OLS(Y, Xnorm).fit()\n",
      "predictions = model.predict(Xnorm) \n",
      " \n",
      "print_model = model.summary()\n",
      "print(print_model)\n",
      "26/393:\n",
      "Y2 = academynominated\n",
      "model2 = sm.OLS(Y2, Xnorm).fit()\n",
      "predictions2 = model2.predict(Xnorm) \n",
      "print_model2 = model2.summary()\n",
      "print(print_model2)\n",
      "26/394:\n",
      "predictions2bin = [1 if x >= 0.5 else 0 for x in predictions2]\n",
      "wrong = [0 if predictions2bin[i] == academynominated[i] else 1 for i in range(834)]\n",
      "print sum(predictions2bin), sum(academynominated)\n",
      "print sum(wrong), len(wrong)\n",
      "print float(sum(wrong))/len(wrong), 1-float(sum(wrong))/len(wrong)\n",
      "print float(sum(academynominated))/len(academynominated)\n",
      "26/395:\n",
      "X2 = df[['degree_centralities', 'eigenvector_centralities', 'betweenness_centralities', 'closeness_centralities', 'moviecount']]\n",
      "X2 = sm.add_constant(X2) # adding a constant\n",
      "Y2 = academynominated\n",
      "model2 = sm.OLS(Y2, X2).fit()\n",
      "predictions2 = model2.predict(X2) \n",
      "print_model2 = model2.summary()\n",
      "print(print_model2)\n",
      "26/396:\n",
      "predictions2bin = [1 if x >= 0.5 else 0 for x in predictions2]\n",
      "wrong = [0 if predictions2bin[i] == academynominated[i] else 1 for i in range(834)]\n",
      "print sum(predictions2bin), sum(academynominated)\n",
      "print sum(wrong), len(wrong)\n",
      "print float(sum(wrong))/len(wrong), 1-float(sum(wrong))/len(wrong)\n",
      "print float(sum(academynominated))/len(academynominated)\n",
      "26/397: corr(df)\n",
      "26/398: df.corr()\n",
      "26/399: cluster_coefficient = nx.algorithms.cluster.clustering(G)\n",
      "26/400: sorted(cluster_coefficient, key=cluster_coefficient.get, reverse=True)\n",
      "26/401: sorted(cluster_coefficient, key=cluster_coefficient.get)\n",
      "26/402: sorted(cluster_coefficient, key=cluster_coefficient.get, reverse=True)\n",
      "26/403: cluster_coefficients = [cluster_coefficient.get(x) for x in actorlist]\n",
      "26/404:\n",
      "cluster_coefficients = [cluster_coefficient.get(x) for x in actorlist]\n",
      "cluster_coefficients\n",
      "26/405: import shap\n",
      "26/406:\n",
      "df = pd.DataFrame(\n",
      "    {\n",
      "        'degree_centralities': degree_centralities,\n",
      "        'eigenvector_centralities': eigenvector_centralities,\n",
      "        'betweenness_centralities': betweenness_centralities,\n",
      "        'closeness_centralities': closeness_centralities,\n",
      "        'cluster_coefficients': cluster_coefficients\n",
      "        'moviecount': moviecounts,\n",
      "        'boxofficegross': boxofficegross,\n",
      "        'academynominated': academynominated\n",
      "    })\n",
      "26/407:\n",
      "df = pd.DataFrame(\n",
      "    {\n",
      "        'degree_centralities': degree_centralities,\n",
      "        'eigenvector_centralities': eigenvector_centralities,\n",
      "        'betweenness_centralities': betweenness_centralities,\n",
      "        'closeness_centralities': closeness_centralities,\n",
      "        'cluster_coefficients': cluster_coefficients,\n",
      "        'moviecount': moviecounts,\n",
      "        'boxofficegross': boxofficegross,\n",
      "        'academynominated': academynominated\n",
      "    })\n",
      "26/408: df.corr()\n",
      "26/409: effective_size = nx.algorithms.structuralholes.effective_size(G)\n",
      "26/410: sorted(effective_size, key=cluster_coefficient.get, reverse=True)\n",
      "26/411: sorted(effective_size, key=effective_size.get, reverse=True)\n",
      "26/412:\n",
      "df = pd.DataFrame(\n",
      "    {\n",
      "        'degree_centralities': degree_centralities,\n",
      "        'eigenvector_centralities': eigenvector_centralities,\n",
      "        'betweenness_centralities': betweenness_centralities,\n",
      "        'closeness_centralities': closeness_centralities,\n",
      "        'cluster_coefficients': cluster_coefficients,\n",
      "        'effective_size': effective_sizes,\n",
      "        'moviecount': moviecounts,\n",
      "        'boxofficegross': boxofficegross,\n",
      "        'academynominated': academynominated\n",
      "    })\n",
      "26/413:\n",
      "effective_sizes = [effective_size.get(x) for x in actorlist]\n",
      "effective_sizes\n",
      "26/414:\n",
      "effective_sizes = [0 if np.isnan(effective_size.get(x)) else effective_size.get(x) for x in actorlist]\n",
      "effective_sizes\n",
      "26/415:\n",
      "plot.hist(effective_sizes)\n",
      "np.median(effective_sizes), np.mean(effective_sizes), np.std(effective_sizes)\n",
      "26/416:\n",
      "plot.hist(cluster_coefficients)\n",
      "np.median(cluster_coefficients), np.mean(cluster_coefficients), np.std(cluster_coefficients)\n",
      "26/417:\n",
      "df = pd.DataFrame(\n",
      "    {\n",
      "        'degree_centralities': degree_centralities,\n",
      "        'eigenvector_centralities': eigenvector_centralities,\n",
      "        'betweenness_centralities': betweenness_centralities,\n",
      "        'closeness_centralities': closeness_centralities,\n",
      "        'cluster_coefficients': cluster_coefficients,\n",
      "        'effective_size': effective_sizes,\n",
      "        'moviecount': moviecounts,\n",
      "        'boxofficegross': boxofficegross,\n",
      "        'academynominated': academynominated\n",
      "    })\n",
      "26/418: df.corr()\n",
      "26/419: df[:313].corr()\n",
      "26/420: df[313:].corr()\n",
      "26/421: nx.algorithms.cluster.average_clustering(G)\n",
      "26/422: nx.algorithms.cluster.average_clustering(Gm), nx.algorithms.cluster.clustering(Gw)\n",
      "26/423: nx.algorithms.cluster.average_clustering(Gm), nx.algorithms.cluster.average_clustering(Gw)\n",
      "26/424:\n",
      "nx.algorithms.cluster.average_clustering(Gm), nx.algorithms.cluster.average_clustering(Gw),\n",
      "nx.algorithms.cluster.average_clustering(G2)\n",
      "26/425:\n",
      "nx.algorithms.cluster.average_clustering(Gm), nx.algorithms.cluster.average_clustering(Gw), \\\n",
      "nx.algorithms.cluster.average_clustering(Gw)\n",
      "26/426:\n",
      "nx.algorithms.cluster.average_clustering(Gm), nx.algorithms.cluster.average_clustering(Gw), \\\n",
      "nx.algorithms.cluster.average_clustering(G2), nx.algorithms.cluster.average_clustering(G3), \\\n",
      "nx.algorithms.cluster.average_clustering(G4)\n",
      "29/1:\n",
      "data = pd.read_csv('output.csv')\n",
      "data.info()\n",
      "29/2:\n",
      "import pandas as pd\n",
      "import matplotlib.pyplot as plt\n",
      "import seaborn as sns\n",
      "import re\n",
      "import numpy as np\n",
      "import math\n",
      "29/3:\n",
      "import pandas as pd\n",
      "import matplotlib.pyplot as plt\n",
      "import seaborn as sns\n",
      "import re\n",
      "import numpy as np\n",
      "import math\n",
      "import ff\n",
      "29/4:\n",
      "data = pd.read_csv('output.csv')\n",
      "data.info()\n",
      "29/5: ff.select('ce3datey')\n",
      "29/6:\n",
      "data = pd.read_csv('output.csv', low_memory=False)\n",
      "data.info()\n",
      "29/7: data.head()\n",
      "29/8:\n",
      "background = pd.read_csv('background.csv', low_memory=False)\n",
      "background.info()\n",
      "29/9: background.head()\n",
      "29/10: ff.search({'name': 'data_source', 'op': 'eq', 'val': 'constructed'})\n",
      "29/11: ff.search([{'name': 'data_source', 'op': 'eq', 'val': 'constructed'}, {'name': 'name', 'op': 'like', 'val': '%e'}])\n",
      "29/12: import urllib\n",
      "29/13: ff.search([{'name': 'data_source', 'op': 'eq', 'val': 'constructed'}, {'name': 'name', 'op': 'like', 'val': '%e'}])\n",
      "29/14: ff.search({'name': 'name', 'op': 'eq', 'val': 'ce3datey'})\n",
      "29/15: import urllib.parse\n",
      "29/16: urllib.parse = urllib.quote_plus(searchString)\n",
      "29/17: urllib.parse = urllib.quote_plus\n",
      "29/18: ff.search({'name': 'data_source', 'op': 'eq', 'val': 'constructed'})\n",
      "29/19: urllib.parse = urllib.parse\n",
      "29/20: ff.search({'name': 'data_source', 'op': 'eq', 'val': 'constructed'})\n",
      "29/21: urllib.parse.quote = urllib.quote_plus\n",
      "29/22: ff.search({'name': 'data_source', 'op': 'eq', 'val': 'constructed'})\n",
      "29/23: urllib.parse.quote = urllib.quote_plus\n",
      "29/24:\n",
      "constructed = ff.search({'name': 'data_source', 'op': 'eq', 'val': 'constructed'})\n",
      "len(constructed), constricted.head()\n",
      "29/25:\n",
      "constructed = ff.search({'name': 'data_source', 'op': 'eq', 'val': 'constructed'})\n",
      "len(constructed), constructed.head()\n",
      "29/26:\n",
      "constructed = ff.search({'name': 'data_source', 'op': 'eq', 'val': 'constructed'})\n",
      "len(constructed)\n",
      "29/27:\n",
      "constructed = ff.search({'name': 'data_source', 'op': 'eq', 'val': 'constructed'})\n",
      "print len(constructed)\n",
      "constructed\n",
      "26/427: df[600:].corr()\n",
      "26/428: df[500:].corr()\n",
      "26/429: df[400:].corr()\n",
      "26/430: df[:200].corr()\n",
      "26/431: df[:100].corr()\n",
      "26/432: df[700:].corr()\n",
      "26/433: df[600:].corr()\n",
      "26/434: df[350:].corr()\n",
      "29/28:\n",
      "train = pd.read_csv('train.csv')\n",
      "train.info()\n",
      "29/29:\n",
      "test = pd.read_csv('train.csv')\n",
      "test.info()\n",
      "26/435:\n",
      "plot.hist(effective_sizes)\n",
      "np.median(effective_sizes), np.mean(effective_sizes), np.std(effective_sizes)\n",
      "plot.title(\"effective sizes\")\n",
      "26/436:\n",
      "plot.hist(effective_sizes)\n",
      "# np.median(effective_sizes), np.mean(effective_sizes), np.std(effective_sizes)\n",
      "plot.title(\"effective sizes\")\n",
      "26/437:\n",
      "plot.hist(effective_sizes)\n",
      "np.median(effective_sizes), np.mean(effective_sizes), np.std(effective_sizes)\n",
      "plot.title(\"effective sizes\")\n",
      "plot.show()\n",
      "26/438:\n",
      "plot.hist(effective_sizes)\n",
      "plot.title(\"effective sizes\")\n",
      "plot.show()\n",
      "np.median(effective_sizes), np.mean(effective_sizes), np.std(effective_sizes)\n",
      "26/439:\n",
      "plot.hist(effective_sizes)\n",
      "plot.title(\"effective sizes\")\n",
      "plot.show()\n",
      "np.median(effective_sizes), np.mean(effective_sizes), np.std(effective_sizes)\n",
      "26/440:\n",
      "plot.hist(effective_sizes)\n",
      "plot.title(\"Effective Size\")\n",
      "plot.show()\n",
      "np.median(effective_sizes), np.mean(effective_sizes), np.std(effective_sizes)\n",
      "26/441:\n",
      "plot.figure()\n",
      "plot.plot(x, y, \"o\")\n",
      "yfit = [intercept + slope * xi for xi in x]\n",
      "plot.plot(x, yfit, \"b\")\n",
      "26/442:\n",
      "x = np.array(degree_centralities)\n",
      "y = np.array(boxofficegross)\n",
      "slope, intercept, r_value, p_value, std_err = stats.linregress(x,y)\n",
      "slope, intercept, r_value, p_value, std_err\n",
      "26/443:\n",
      "plot.figure()\n",
      "plot.plot(x, y, \"o\")\n",
      "yfit = [intercept + slope * xi for xi in x]\n",
      "plot.plot(x, yfit, \"b\")\n",
      "26/444:\n",
      "plot.figure()\n",
      "plot.plot(x, y, \"o\")\n",
      "yfit = [intercept + slope * xi for xi in x]\n",
      "plot.plot(x, yfit, \"black\")\n",
      "29/30:\n",
      "from sklearn.linear_model import LinearRegression, Ridge, Lasso, ElasticNet\n",
      "from sklearn.ensemble import RandomForestRegressor, AdaBoostRegressor\n",
      "from sklearn.gaussian_process import GaussianProcessRegressor\n",
      "from sklearn.svm import SVC, LinearSVC\n",
      "30/1: train.head()\n",
      "30/2:\n",
      "data = pd.read_csv('output.csv', low_memory=False)\n",
      "data.info()\n",
      "30/3:\n",
      "import pandas as pd\n",
      "import matplotlib.pyplot as plt\n",
      "import seaborn as sns\n",
      "import re\n",
      "import numpy as np\n",
      "import math\n",
      "import ff\n",
      "30/4:\n",
      "from sklearn.linear_model import LinearRegression, Ridge, Lasso, ElasticNet\n",
      "from sklearn.ensemble import RandomForestRegressor, AdaBoostRegressor\n",
      "from sklearn.gaussian_process import GaussianProcessRegressor\n",
      "from sklearn.svm import SVC, LinearSVC\n",
      "30/5:\n",
      "data = pd.read_csv('output.csv', low_memory=False)\n",
      "data.info()\n",
      "30/6: %store data\n",
      "31/1: data.head()\n",
      "31/2:\n",
      "data = pd.read_csv('output.csv', low_memory=False)\n",
      "data.info()\n",
      "31/3:\n",
      "import pandas as pd\n",
      "import matplotlib.pyplot as plt\n",
      "import seaborn as sns\n",
      "import re\n",
      "import numpy as np\n",
      "import math\n",
      "import ff\n",
      "31/4:\n",
      "data = pd.read_csv('output.csv', low_memory=False)\n",
      "data.info()\n",
      "31/5: c = get_config()\n",
      "31/6: data.head()\n",
      "31/7:\n",
      "background = pd.read_csv('background.csv', low_memory=False)\n",
      "background.info()\n",
      "31/8: background.head()\n",
      "31/9: urllib.parse.quote = urllib.quote_plus\n",
      "31/10: urllib.parse.quote = urllib.quote_plus\n",
      "31/11:\n",
      "import urllib\n",
      "urllib.parse.quote = urllib.quote_plus\n",
      "31/12:\n",
      "constructed = ff.search({'name': 'data_source', 'op': 'eq', 'val': 'constructed'})\n",
      "print len(constructed)\n",
      "constructed\n",
      "31/13:\n",
      "import urllib\n",
      "urllib.parse.quote = urllib.quote_plus\n",
      "31/14:\n",
      "urllib.parse = urllib.parse\n",
      "urllib.parse.quote = urllib.quote_plus\n",
      "31/15:\n",
      "constructed = ff.search({'name': 'data_source', 'op': 'eq', 'val': 'constructed'})\n",
      "print len(constructed)\n",
      "constructed\n",
      "31/16:\n",
      "urllib.parse = urllib.quote_plus\n",
      "urllib.parse.quote = urllib.quote_plus\n",
      "31/17:\n",
      "constructed = ff.search({'name': 'data_source', 'op': 'eq', 'val': 'constructed'})\n",
      "print len(constructed)\n",
      "constructed\n",
      "31/18:\n",
      "import urllib\n",
      "urllib.parse = urllib.quote_plus\n",
      "urllib.parse.quote = urllib.quote_plus\n",
      "31/19:\n",
      "constructed = ff.search({'name': 'data_source', 'op': 'eq', 'val': 'constructed'})\n",
      "print len(constructed)\n",
      "constructed\n",
      "31/20:\n",
      "constructed = ff.search({'name': 'data_source', 'op': 'eq', 'val': 'constructed'})\n",
      "print len(constructed)\n",
      "constructed.head()\n",
      "31/21:\n",
      "constructed = ff.search({'name': 'data_source', 'op': 'eq', 'val': 'constructed'})\n",
      "print len(constructed)\n",
      "constructed\n",
      "31/22:\n",
      "train = pd.read_csv('train.csv')\n",
      "train.info()\n",
      "31/23:\n",
      "test = pd.read_csv('train.csv')\n",
      "test.info()\n",
      "31/24: train.head()\n",
      "31/25: data.head()\n",
      "31/26: lr = LinearRegression()\n",
      "31/27:\n",
      "from sklearn.linear_model import LinearRegression, Ridge, Lasso, ElasticNet\n",
      "from sklearn.ensemble import RandomForestRegressor, AdaBoostRegressor\n",
      "from sklearn.gaussian_process import GaussianProcessRegressor\n",
      "from sklearn.svm import SVC, LinearSVC\n",
      "31/28: lr = LinearRegression()\n",
      "31/29:\n",
      "from sklearn.linear_model import LinearRegression, RidgeCV, LassoCV, MultiTaskLasso, ElasticNetCV, MultiTaskElasticNet\n",
      "from sklearn.ensemble import RandomForestRegressor, AdaBoostRegressor\n",
      "from sklearn.gaussian_process import GaussianProcessRegressor\n",
      "from sklearn.svm import SVC, LinearSVC\n",
      "26/445: nx.classes.function.density(G)\n",
      "26/446: nx.classes.function.density(G2)\n",
      "26/447: nx.classes.function.density(G), nx.algorithms.distance_measures.diameter(G), nx.algorithms.cluster.clustering(G)\n",
      "26/448: nx.classes.function.density(G), nx.algorithms.cluster.clustering(G)\n",
      "26/449: nx.classes.function.density(G), nx.algorithms.cluster.average_clustering(G)\n",
      "26/450: nx.classes.function.density(Gm), nx.algorithms.cluster.average_clustering(Gm)\n",
      "26/451: nx.classes.function.density(Gw), nx.algorithms.cluster.average_clustering(Gw)\n",
      "26/452:\n",
      "print nx.classes.function.density(G), nx.algorithms.cluster.average_clustering(G)\n",
      "print nx.classes.function.density(G), nx.algorithms.cluster.average_clustering(Gm)\n",
      "print nx.classes.function.density(G), nx.algorithms.cluster.average_clustering(Gw)\n",
      "print nx.classes.function.density(G), nx.algorithms.cluster.average_clustering(G2)\n",
      "print nx.classes.function.density(G), nx.algorithms.cluster.average_clustering(G3)\n",
      "print nx.classes.function.density(G), nx.algorithms.cluster.average_clustering(G4)\n",
      "26/453:\n",
      "print nx.classes.function.density(G), nx.algorithms.cluster.average_clustering(G)\n",
      "print nx.classes.function.density(Gm), nx.algorithms.cluster.average_clustering(Gm)\n",
      "print nx.classes.function.density(Gw), nx.algorithms.cluster.average_clustering(Gw)\n",
      "print nx.classes.function.density(G2), nx.algorithms.cluster.average_clustering(G2)\n",
      "print nx.classes.function.density(G3), nx.algorithms.cluster.average_clustering(G3)\n",
      "print nx.classes.function.density(G4), nx.algorithms.cluster.average_clustering(G4)\n",
      "26/454:\n",
      "def fill(g):\n",
      "    for a in actorlist:\n",
      "        if a not in g:\n",
      "            g.add_node(a)\n",
      "26/455: G2f = fill(G2)\n",
      "26/456: len(G2.nodes())\n",
      "26/457: len(G2.edges())\n",
      "26/458: len(G2f.edges())\n",
      "26/459:\n",
      "def fill(g):\n",
      "    for a in actorlist:\n",
      "        if a not in g:\n",
      "            g.add_node(a)\n",
      "    return g\n",
      "26/460: G2f = fill(G2)\n",
      "26/461: len(G2f.nodes())\n",
      "26/462: len(G2f.edges())\n",
      "26/463:\n",
      "G2 = kweighted(Gweighted, 2, False)\n",
      "len(G2.nodes()), len(G2.edges())\n",
      "26/464:\n",
      "fill(G2)\n",
      "fill(G3)\n",
      "fill(G4)\n",
      "31/30:\n",
      "plt.figure(figsize = (7,5), dpi=100)\n",
      "plt.matshow(data.corr(), fignum=1, cmap=plt.cm.bwr)\n",
      "cols = list(data.columns)\n",
      "plt.xticks(list(range(len(cols))), cols, rotation=45)\n",
      "plt.yticks(list(range(len(cols))), cols)\n",
      "plt.colorbar()\n",
      "34/1:\n",
      "import pandas as pd\n",
      "import matplotlib.pyplot as plt\n",
      "import seaborn as sns\n",
      "import re\n",
      "import numpy as np\n",
      "import math\n",
      "import ff\n",
      "34/2:\n",
      "from sklearn.linear_model import LinearRegression, RidgeCV, LassoCV, MultiTaskLasso, ElasticNetCV, MultiTaskElasticNet\n",
      "from sklearn.ensemble import RandomForestRegressor, AdaBoostRegressor\n",
      "from sklearn.gaussian_process import GaussianProcessRegressor\n",
      "from sklearn.svm import SVC, LinearSVC\n",
      "34/3:\n",
      "data = pd.read_csv('output.csv', low_memory=False)\n",
      "data.info()\n",
      "34/4: data.head()\n",
      "34/5:\n",
      "background = pd.read_csv('background.csv', low_memory=False)\n",
      "background.info()\n",
      "34/6: background.head()\n",
      "34/7:\n",
      "urllib.parse = urllib.quote_plus\n",
      "urllib.parse.quote = urllib.quote_plus\n",
      "34/8:\n",
      "import urllib\n",
      "urllib.parse = urllib.quote_plus\n",
      "urllib.parse.quote = urllib.quote_plus\n",
      "34/9:\n",
      "constructed = ff.search({'name': 'data_source', 'op': 'eq', 'val': 'constructed'})\n",
      "print len(constructed)\n",
      "constructed\n",
      "34/10:\n",
      "train = pd.read_csv('train.csv')\n",
      "train.info()\n",
      "34/11:\n",
      "test = pd.read_csv('train.csv')\n",
      "test.info()\n",
      "34/12: train.head()\n",
      "34/13: X = data[constructed]\n",
      "34/14:\n",
      "constructed = ff.search({'name': 'data_source', 'op': 'eq', 'val': 'constructed'})\n",
      "print len(constructed)\n",
      "constructed = [str(x) for x in constructed]\n",
      "constructed\n",
      "34/15: X = data[constructed]\n",
      "34/16: X = data[[constructed]]\n",
      "34/17: X = data[np.arr(constructed)]\n",
      "34/18: X = data[np.array(constructed)]\n",
      "34/19: X = data[['cf1intmon', 'cf1intyr']]\n",
      "34/20: X = data[['cm1fint', 'cm1tdiff']]\n",
      "34/21:\n",
      "constructed = ff.search({'name': 'data_source', 'op': 'eq', 'val': 'constructed'})\n",
      "print len(constructed)\n",
      "constructed = [str(x) for x in constructed]\n",
      "constructed = [x if x in background.columns for x in constructed]\n",
      "34/22:\n",
      "constructedvars = ff.search({'name': 'data_source', 'op': 'eq', 'val': 'constructed'})\n",
      "print len(constructedvars)\n",
      "constructedvars = [str(x) for x in constructedvars]\n",
      "constructed = []\n",
      "for x in constructedvars:\n",
      "    if x in data.columns:\n",
      "        constructed.append(x)\n",
      "34/23:\n",
      "constructedvars = ff.search({'name': 'data_source', 'op': 'eq', 'val': 'constructed'})\n",
      "print len(constructedvars)\n",
      "constructedvars = [str(x) for x in constructedvars]\n",
      "constructed = []\n",
      "for x in constructedvars:\n",
      "    if x in data.columns:\n",
      "        constructed.append(x)\n",
      "print len(constructed)\n",
      "constructed\n",
      "34/24: X = data[constructed]\n",
      "34/25: X.head()\n",
      "34/26: X.info()\n",
      "34/27: X[:2121]\n",
      "34/28:\n",
      "lr = LinearRegression(normalize=True)\n",
      "lr.fit(X[:2121], train['gpa'])\n",
      "34/29: train = train.fillna(train.mean().iloc[0])\n",
      "34/30:\n",
      "lr = LinearRegression(normalize=True)\n",
      "lr.fit(X[:2121], train['gpa'])\n",
      "34/31:\n",
      "lr = LinearRegression(normalize=True)\n",
      "lr.fit(X[:2121], train['gpa'])\n",
      "lr.score(X[:2121], train['gpa'])\n",
      "34/32:\n",
      "from sklearn.linear_model import LinearRegression, RidgeCV, LassoCV, MultiTaskLasso, ElasticNetCV, MultiTaskElasticNet\n",
      "from sklearn.ensemble import RandomForestRegressor, AdaBoostRegressor\n",
      "from sklearn.gaussian_process import GaussianProcessRegressor\n",
      "from sklearn.svm import SVC, LinearSVC\n",
      "from sklearn.metrics import mean_squared_error\n",
      "34/33: train[:25]\n",
      "34/34:\n",
      "train = pd.read_csv('train.csv')\n",
      "train.info()\n",
      "34/35: train[:25]\n",
      "34/36: test[:25]\n",
      "34/37:\n",
      "test = pd.read_csv('test.csv')\n",
      "test.info()\n",
      "34/38: test[:25]\n",
      "34/39: train[:25]['challengeID']\n",
      "34/40: train[:25]\n",
      "34/41: train[:10]\n",
      "34/42: test[:10]\n",
      "34/43:\n",
      "for i in range(4242):\n",
      "    if background['challengeID'][i] != i:\n",
      "        print i\n",
      "34/44:\n",
      "for i in range(4242):\n",
      "    if background['challengeID'][i] != i:\n",
      "        print i, background['challengeID'][i]\n",
      "34/45:\n",
      "for i in range(4242):\n",
      "    if background['challengeID'][i] != i+1:\n",
      "        print i, background['challengeID'][i]\n",
      "34/46: X[1,2]\n",
      "34/47: X[[1,2]]\n",
      "34/48:\n",
      "trainIDs = [x-1 for x in train['challengeID']]\n",
      "trainIDs\n",
      "34/49: trainIDs = [x-1 for x in train['challengeID']]\n",
      "34/50: X = data[constructed].loc[trainIDs]\n",
      "34/51:\n",
      "X = data[constructed].loc[trainIDs]\n",
      "X.shape\n",
      "34/52:\n",
      "train[:10]\n",
      "len(train)\n",
      "34/53: train[:10]\n",
      "34/54:\n",
      "gpaIDs = []\n",
      "for i in range(len(train)):\n",
      "    if not np.isnan(train['gpa']):\n",
      "        gpaIDs.append(train['challengeID'][i]-1)\n",
      "gpaIDs\n",
      "34/55:\n",
      "gpaIDs = []\n",
      "for i in range(len(train)):\n",
      "    if not np.isnan(train['gpa'][i]):\n",
      "        gpaIDs.append(train['challengeID'][i]-1)\n",
      "gpaIDs\n",
      "34/56:\n",
      "gpaIDs = []\n",
      "for i in range(len(train)):\n",
      "    if not np.isnan(train['gpa'][i]):\n",
      "        gpaIDs.append(train['challengeID'][i]-1)\n",
      "34/57:\n",
      "X = data[constructed].loc[gpaIDs]\n",
      "X.shape\n",
      "34/58:\n",
      "gpaIDs_test = []\n",
      "for i in range(len(test)):\n",
      "    if not np.isnan(test['gpa'][i]):\n",
      "        gpaIDs_test.append(test['challengeID'][i]-1)\n",
      "34/59: X = data[constructed].loc[gpaIDs_test]\n",
      "34/60:\n",
      "X = data[constructed].loc[gpaIDs]\n",
      "X.shape\n",
      "34/61: X_test = data[constructed].loc[gpaIDs_test]\n",
      "34/62:\n",
      "lr = LinearRegression(normalize=True)\n",
      "lr.fit(X, train['gpa'].loc[gpaIDs])\n",
      "lr_pred = lr.predict(X_test)\n",
      "mean_squared_error(lr_pred, test['gpa'].loc[gpaIDs_test])\n",
      "34/63: train['gpa'].loc[gpaIDs]\n",
      "34/64: train[:15]\n",
      "34/65: train['gpa'].loc[17]\n",
      "34/66: train['gpa'].loc[18]\n",
      "34/67: train['gpa'].loc[1]\n",
      "34/68: train['gpa'].loc[1:10]\n",
      "34/69:\n",
      "gpaIDs = []\n",
      "gpas = []\n",
      "for i in range(len(train)):\n",
      "    if not np.isnan(train['gpa'][i]):\n",
      "        gpaIDs.append(train['challengeID'][i]-1)\n",
      "        gpas.append(train['gpa'][i])\n",
      "34/70:\n",
      "lr = LinearRegression(normalize=True)\n",
      "lr.fit(X, gpas)\n",
      "lr_pred = lr.predict(X_test)\n",
      "mean_squared_error(lr_pred, gpas_test)\n",
      "34/71:\n",
      "gpaIDs_test = []\n",
      "gpas_test\n",
      "for i in range(len(test)):\n",
      "    if not np.isnan(test['gpa'][i]):\n",
      "        gpaIDs_test.append(test['challengeID'][i]-1)\n",
      "        gpas_test.append(test['gpa'][i])\n",
      "34/72:\n",
      "gpaIDs_test = []\n",
      "gpas_test = []\n",
      "for i in range(len(test)):\n",
      "    if not np.isnan(test['gpa'][i]):\n",
      "        gpaIDs_test.append(test['challengeID'][i]-1)\n",
      "        gpas_test.append(test['gpa'][i])\n",
      "34/73:\n",
      "lr = LinearRegression(normalize=True)\n",
      "lr.fit(X, gpas)\n",
      "lr_pred = lr.predict(X_test)\n",
      "mean_squared_error(lr_pred, gpas_test)\n",
      "34/74:\n",
      "# X = data[constructed].loc[gpaIDs]\n",
      "X = data.loc[gpaIDs]\n",
      "X.shape\n",
      "34/75:\n",
      "# X_test = data[constructed].loc[gpaIDs_test]\n",
      "X_test = data=.loc[gpaIDs_test]\n",
      "34/76:\n",
      "# X_test = data[constructed].loc[gpaIDs_test]\n",
      "X_test = data.loc[gpaIDs_test]\n",
      "34/77:\n",
      "lr = LinearRegression(normalize=True)\n",
      "lr.fit(X, gpas)\n",
      "lr_pred = lr.predict(X_test)\n",
      "mean_squared_error(lr_pred, gpas_test)\n",
      "34/78: X\n",
      "34/79: X.dtypes\n",
      "34/80: X.dtypes[X.dtypes != 'int64'][X.dtypes != 'float64']\n",
      "34/81: X['hv5_ppvtae']\n",
      "34/82: len(X.dtypes[X.dtypes != 'int64'][X.dtypes != 'float64'])\n",
      "34/83: X.drop(X.dtypes[X.dtypes != 'int64'][X.dtypes != 'float64'])\n",
      "34/84: len(X[X.dtypes != 'int64'][X.dtypes != 'float64'])\n",
      "34/85: len(X.dtypes[X.dtypes != 'int64'][X.dtypes != 'float64'])\n",
      "34/86: X.dtypes[X.dtypes != 'int64'][X.dtypes != 'float64'].shape\n",
      "34/87: X.dtypes[X.dtypes != 'int64'][X.dtypes != 'float64']\n",
      "34/88: X = X.select_dtypes(include=['float64', 'int64'])\n",
      "34/89:\n",
      "# X = data[constructed].loc[gpaIDs]\n",
      "X = data.loc[gpaIDs].select_dtypes(include=['float64', 'int64'])\n",
      "X.shape\n",
      "34/90:\n",
      "# X_test = data[constructed].loc[gpaIDs_test]\n",
      "X_test = data.loc[gpaIDs_test].select_dtypes(include=['float64', 'int64'])\n",
      "X_test.shape\n",
      "34/91:\n",
      "lr = LinearRegression(normalize=True)\n",
      "lr.fit(X, gpas)\n",
      "lr_pred = lr.predict(X_test)\n",
      "mean_squared_error(lr_pred, gpas_test)\n",
      "34/92:\n",
      "lr = LinearRegression()\n",
      "lr.fit(X, gpas)\n",
      "lr_pred = lr.predict(X_test)\n",
      "mean_squared_error(lr_pred, gpas_test)\n",
      "34/93:\n",
      "lr = LinearRegression(normalize=True)\n",
      "lr.fit(X, gpas)\n",
      "lr_pred = lr.predict(X_test)\n",
      "mean_squared_error(lr_pred, gpas_test)\n",
      "34/94:\n",
      "lr2 = LassoCV(normalize=True)\n",
      "lr2.fit(X, gpas)\n",
      "lr2_pred = lr2.predict(X_test)\n",
      "mean_squared_error(lr2_pred, gpas_test)\n",
      "34/95:\n",
      "lr2 = LassoCV(alphas=[0.1, 0.001], max_iter=10000, n_jobs=-1))\n",
      "lr2.fit(X, gpas)\n",
      "lr2_pred = lr2.predict(X_test)\n",
      "mean_squared_error(lr2_pred, gpas_test)\n",
      "34/96:\n",
      "lr2 = LassoCV(alphas=[0.1, 0.001], max_iter=10000, n_jobs=-1)\n",
      "lr2.fit(X, gpas)\n",
      "lr2_pred = lr2.predict(X_test)\n",
      "mean_squared_error(lr2_pred, gpas_test)\n",
      "35/1:\n",
      "import pandas as pd\n",
      "import matplotlib.pyplot as plt\n",
      "import seaborn as sns\n",
      "import re\n",
      "import numpy as np\n",
      "import math\n",
      "import ff\n",
      "35/2:\n",
      "from sklearn.linear_model import LinearRegression, RidgeCV, LassoCV, MultiTaskLasso, ElasticNetCV, MultiTaskElasticNet\n",
      "from sklearn.ensemble import RandomForestRegressor, AdaBoostRegressor\n",
      "from sklearn.gaussian_process import GaussianProcessRegressor\n",
      "from sklearn.svm import SVC, LinearSVC\n",
      "from sklearn.metrics import mean_squared_error\n",
      "35/3:\n",
      "data = pd.read_csv('output.csv', low_memory=False)\n",
      "data.info()\n",
      "35/4: data.head()\n",
      "35/5:\n",
      "background = pd.read_csv('background.csv', low_memory=False)\n",
      "background.info()\n",
      "35/6: background.head()\n",
      "35/7:\n",
      "import urllib\n",
      "urllib.parse = urllib.quote_plus\n",
      "urllib.parse.quote = urllib.quote_plus\n",
      "35/8:\n",
      "constructedvars = ff.search({'name': 'data_source', 'op': 'eq', 'val': 'constructed'})\n",
      "print len(constructedvars)\n",
      "constructedvars = [str(x) for x in constructedvars]\n",
      "constructed = []\n",
      "for x in constructedvars:\n",
      "    if x in data.columns:\n",
      "        constructed.append(x)\n",
      "print len(constructed)\n",
      "constructed\n",
      "35/9:\n",
      "train = pd.read_csv('train.csv')\n",
      "train.info()\n",
      "35/10:\n",
      "test = pd.read_csv('test.csv')\n",
      "test.info()\n",
      "35/11: train[:15]\n",
      "35/12:\n",
      "gpaIDs = []\n",
      "gpas = []\n",
      "for i in range(len(train)):\n",
      "    if not np.isnan(train['gpa'][i]):\n",
      "        gpaIDs.append(train['challengeID'][i]-1)\n",
      "        gpas.append(train['gpa'][i])\n",
      "35/13: test[:10]\n",
      "35/14:\n",
      "gpaIDs_test = []\n",
      "gpas_test = []\n",
      "for i in range(len(test)):\n",
      "    if not np.isnan(test['gpa'][i]):\n",
      "        gpaIDs_test.append(test['challengeID'][i]-1)\n",
      "        gpas_test.append(test['gpa'][i])\n",
      "35/15:\n",
      "# X = data[constructed].loc[gpaIDs]\n",
      "X = data.loc[gpaIDs].select_dtypes(include=['float64', 'int64'])\n",
      "X.shape\n",
      "35/16:\n",
      "# X_test = data[constructed].loc[gpaIDs_test]\n",
      "X_test = data.loc[gpaIDs_test].select_dtypes(include=['float64', 'int64'])\n",
      "X_test.shape\n",
      "35/17:\n",
      "lr = LinearRegression(normalize=True)\n",
      "lr.fit(X, gpas)\n",
      "lr_pred = lr.predict(X_test)\n",
      "mean_squared_error(lr_pred, gpas_test)\n",
      "35/18:\n",
      "lr2 = LassoCV(max_iter=1000)\n",
      "lr2.fit(X, gpas)\n",
      "lr2_pred = lr2.predict(X_test)\n",
      "mean_squared_error(lr2_pred, gpas_test)\n",
      "35/19:\n",
      "lr2 = LassoCV(max_iter=1000, cv=3)\n",
      "lr2.fit(X, gpas)\n",
      "lr2_pred = lr2.predict(X_test)\n",
      "mean_squared_error(lr2_pred, gpas_test)\n",
      "35/20:\n",
      "lr3 = RidgeCV(max_iter=1000, cv=3)\n",
      "lr3.fit(X, gpas)\n",
      "lr3_pred = lr3.predict(X_test)\n",
      "mean_squared_error(lr3_pred, gpas_test)\n",
      "35/21:\n",
      "lr3 = RidgeCV(cv=3)\n",
      "lr3.fit(X, gpas)\n",
      "lr3_pred = lr3.predict(X_test)\n",
      "mean_squared_error(lr3_pred, gpas_test)\n",
      "35/22:\n",
      "lr3 = RidgeCV(cv=10)\n",
      "lr3.fit(X, gpas)\n",
      "lr3_pred = lr3.predict(X_test)\n",
      "mean_squared_error(lr3_pred, gpas_test)\n",
      "35/23:\n",
      "lr3 = RidgeCV(cv=3)\n",
      "lr3.fit(X, gpas)\n",
      "lr3_pred = lr3.predict(X_test)\n",
      "mean_squared_error(lr3_pred, gpas_test)\n",
      "35/24:\n",
      "lr4 = ElasticNetCV(max_iter=1000, cv=3)\n",
      "lr4.fit(X, gpas)\n",
      "lr4_pred = lr4.predict(X_test)\n",
      "mean_squared_error(lr4_pred, gpas_test)\n",
      "35/25:\n",
      "rf = RandomForestRegressor()\n",
      "rf.fit(X, gpas)\n",
      "rf_pred = rf.predict(X_test)\n",
      "mean_squared_error(rf_pred, gpas_test)\n",
      "35/26:\n",
      "rf = RandomForestRegressor(n_estimators=100)\n",
      "rf.fit(X, gpas)\n",
      "rf_pred = rf.predict(X_test)\n",
      "mean_squared_error(rf_pred, gpas_test)\n",
      "35/27:\n",
      "Xc = data[constructed].loc[gpaIDs]\n",
      "Xc_test = data[constructed].loc[gpaIDs_test]\n",
      "lrc = LinearRegression(normalize=True)\n",
      "lrc.fit(Xc, gpas)\n",
      "lrc_pred = lrc.predict(Xc_test)\n",
      "mean_squared_error(lrc_pred, gpas_test)\n",
      "35/28:\n",
      "ab = AdaBoostRegressor()\n",
      "ab.fit(X, gpas)\n",
      "ab_pred = ab.predict(X_test)\n",
      "mean_squared_error(ab_pred, gpas_test)\n",
      "35/29:\n",
      "def makeXY(field):\n",
      "    IDs = []\n",
      "    Y_train = []\n",
      "    for i in range(len(train)):\n",
      "        if not np.isnan(train[field][i]):\n",
      "            IDs.append(train['challengeID'][i]-1)\n",
      "            Y.append(train[field][i])\n",
      "    IDs_test = []\n",
      "    Y_test = []\n",
      "    for i in range(len(test)):\n",
      "        if not np.isnan(test[field][i]):\n",
      "            IDs_test.append(test['challengeID'][i]-1)\n",
      "            Y_test.append(test[field][i])\n",
      "    X_train = data.loc[IDs].select_dtypes(include=['float64', 'int64'])\n",
      "    X_test = data.loc[IDs_test].select_dtypes(include=['float64', 'int64'])\n",
      "    return X_train, X_test, Y_train, Y_test\n",
      "35/30: Xgpa_train, Xgpa_test, Ygpa_train, Ygpa_test = makeXY('gpa')\n",
      "35/31:\n",
      "def makeXY(field):\n",
      "    IDs = []\n",
      "    Y_train = []\n",
      "    for i in range(len(train)):\n",
      "        if not np.isnan(train[field][i]):\n",
      "            IDs.append(train['challengeID'][i]-1)\n",
      "            Y_train.append(train[field][i])\n",
      "    IDs_test = []\n",
      "    Y_test = []\n",
      "    for i in range(len(test)):\n",
      "        if not np.isnan(test[field][i]):\n",
      "            IDs_test.append(test['challengeID'][i]-1)\n",
      "            Y_test.append(test[field][i])\n",
      "    X_train = data.loc[IDs].select_dtypes(include=['float64', 'int64'])\n",
      "    X_test = data.loc[IDs_test].select_dtypes(include=['float64', 'int64'])\n",
      "    return X_train, X_test, Y_train, Y_test\n",
      "35/32: Xgpa_train, Xgpa_test, Ygpa_train, Ygpa_test = makeXY('gpa')\n",
      "35/33: Xgpa_train.shape, Xgpa_test.shape, Ygpa_train.shape, Ygpa_test.shape\n",
      "35/34: Xgpa_train.shape, Xgpa_test.shape, len(Ygpa_train), len(Ygpa_test)\n",
      "35/35:\n",
      "constructedvars = ff.search({'name': 'data_source', 'op': 'eq', 'val': 'constructed'})\n",
      "print len(constructedvars)\n",
      "constructedvars = [str(x) for x in constructedvars]\n",
      "constructed = []\n",
      "for x in constructedvars:\n",
      "    if x in data.columns:\n",
      "        constructed.append(x)\n",
      "print len(constructed)\n",
      "constructed[:10]\n",
      "35/36: train[:10]\n",
      "35/37: train[:5]\n",
      "35/38: test[:5]\n",
      "35/39:\n",
      "lr = LinearRegression(normalize=True)\n",
      "lr.fit(Xgpa_train, Ygpa_train)\n",
      "lr_pred = lr.predict(Xgpa_test)\n",
      "mean_squared_error(lr_pred, Ygpa_test)\n",
      "35/40:\n",
      "def makeXY(field):\n",
      "    IDs_train = []\n",
      "    Y_train = []\n",
      "    for i in range(len(train)):\n",
      "        if not np.isnan(train[field][i]):\n",
      "            IDs_train.append(train['challengeID'][i]-1)\n",
      "            Y_train.append(train[field][i])\n",
      "    IDs_test = []\n",
      "    Y_test = []\n",
      "    for i in range(len(test)):\n",
      "        if not np.isnan(test[field][i]):\n",
      "            IDs_test.append(test['challengeID'][i]-1)\n",
      "            Y_test.append(test[field][i])\n",
      "    X_train = data.loc[IDs].select_dtypes(include=['float64', 'int64'])\n",
      "    X_test = data.loc[IDs_test].select_dtypes(include=['float64', 'int64'])\n",
      "    return X_train, X_test, Y_train, Y_test, IDs, IDs_test\n",
      "35/41: Xgpa_train, Xgpa_test, Ygpa_train, Ygpa_test = makeXY('gpa')\n",
      "35/42:\n",
      "def makeXY(field):\n",
      "    IDs_train = []\n",
      "    Y_train = []\n",
      "    for i in range(len(train)):\n",
      "        if not np.isnan(train[field][i]):\n",
      "            IDs_train.append(train['challengeID'][i]-1)\n",
      "            Y_train.append(train[field][i])\n",
      "    IDs_test = []\n",
      "    Y_test = []\n",
      "    for i in range(len(test)):\n",
      "        if not np.isnan(test[field][i]):\n",
      "            IDs_test.append(test['challengeID'][i]-1)\n",
      "            Y_test.append(test[field][i])\n",
      "    X_train = data.loc[IDs].select_dtypes(include=['float64', 'int64'])\n",
      "    X_test = data.loc[IDs_test].select_dtypes(include=['float64', 'int64'])\n",
      "    return X_train, X_test, Y_train, Y_test, IDs_train, IDs_test\n",
      "35/43: Xgpa_train, Xgpa_test, Ygpa_train, Ygpa_test = makeXY('gpa')\n",
      "35/44:\n",
      "def makeXY(field):\n",
      "    IDs_train = []\n",
      "    Y_train = []\n",
      "    for i in range(len(train)):\n",
      "        if not np.isnan(train[field][i]):\n",
      "            IDs_train.append(train['challengeID'][i]-1)\n",
      "            Y_train.append(train[field][i])\n",
      "    IDs_test = []\n",
      "    Y_test = []\n",
      "    for i in range(len(test)):\n",
      "        if not np.isnan(test[field][i]):\n",
      "            IDs_test.append(test['challengeID'][i]-1)\n",
      "            Y_test.append(test[field][i])\n",
      "    X_train = data.loc[IDs_train].select_dtypes(include=['float64', 'int64'])\n",
      "    X_test = data.loc[IDs_test].select_dtypes(include=['float64', 'int64'])\n",
      "    return X_train, X_test, Y_train, Y_test, IDs_train, IDs_test\n",
      "35/45: Xgpa_train, Xgpa_test, Ygpa_train, Ygpa_test = makeXY('gpa')\n",
      "35/46: Xgpa_train, Xgpa_test, Ygpa_train, Ygpa_test, IDgpa_train, IDgpa_test = makeXY('gpa')\n",
      "35/47: Xgpa_train.shape, Xgpa_test.shape, len(Ygpa_train), len(Ygpa_test)\n",
      "35/48:\n",
      "Xc = data[constructed].loc[IDgpa_train]\n",
      "Xc_test = data[constructed].loc[IDgpa_test]\n",
      "lrc = LinearRegression(normalize=True)\n",
      "lrc.fit(Xc, Ygpa_train)\n",
      "lrc_pred = lrc.predict(Xc_test)\n",
      "mean_squared_error(lrc_pred, Ygpa_test)\n",
      "35/49:\n",
      "lr = LinearRegression(normalize=True)\n",
      "lr.fit(Xgpa_train, Ygpa_train)\n",
      "lr_pred = lr.predict(Xgpa_test)\n",
      "mean_squared_error(lr_pred, Ygpa_test)\n",
      "35/50:\n",
      "lr2 = LassoCV(max_iter=10000, cv=3)\n",
      "lr2.fit(Xgpa_train, Ygpa_train)\n",
      "lr2_pred = lr2.predict(Xgpa_test)\n",
      "mean_squared_error(lr2_pred, Ygpa_test)\n",
      "35/51:\n",
      "lrR = RidgeCV(cv=3)\n",
      "lrR.fit(Xgpa_train, Ygpa_train)\n",
      "lrR_pred = lrR.predict(Xgpa_test)\n",
      "mean_squared_error(lrR_pred, Ygpa_test)\n",
      "35/52:\n",
      "lrR = RidgeCV(cv=5)\n",
      "lrR.fit(Xgpa_train, Ygpa_train)\n",
      "lrR_pred = lrR.predict(Xgpa_test)\n",
      "mean_squared_error(lrR_pred, Ygpa_test)\n",
      "35/53:\n",
      "lrR = RidgeCV(cv=10)\n",
      "lrR.fit(Xgpa_train, Ygpa_train)\n",
      "lrR_pred = lrR.predict(Xgpa_test)\n",
      "mean_squared_error(lrR_pred, Ygpa_test)\n",
      "35/54:\n",
      "lrL = LassoCV(max_iter=1000, cv=10)\n",
      "lrL.fit(Xgpa_train, Ygpa_train)\n",
      "lrL_pred = lrL.predict(Xgpa_test)\n",
      "mean_squared_error(lrL_pred, Ygpa_test)\n",
      "35/55: Xgrit_train, Xgrit_test, Ygrit_train, Ygrit_test, IDgrit_train, IDgrit_test = makeXY('grit')\n",
      "35/56: Xgrit_train.shape, Xgrit_test.shape, len(Ygrit_train), len(Ygrit_test)\n",
      "35/57:\n",
      "rf = RandomForestRegressor(n_estimators=100)\n",
      "rf.fit(Xgpa_train, Ygpa_train)\n",
      "rf_pred = rf.predict(Xgpa_test)\n",
      "mean_squared_error(rf_pred, Ygpa_test)\n",
      "35/58:\n",
      "def makeXY(field, data):\n",
      "    IDs_train = []\n",
      "    Y_train = []\n",
      "    for i in range(len(train)):\n",
      "        if not np.isnan(train[field][i]):\n",
      "            IDs_train.append(train['challengeID'][i]-1)\n",
      "            Y_train.append(train[field][i])\n",
      "    IDs_test = []\n",
      "    Y_test = []\n",
      "    for i in range(len(test)):\n",
      "        if not np.isnan(test[field][i]):\n",
      "            IDs_test.append(test['challengeID'][i]-1)\n",
      "            Y_test.append(test[field][i])\n",
      "    X_train = data.loc[IDs_train].select_dtypes(include=['float64', 'int64'])\n",
      "    X_test = data.loc[IDs_test].select_dtypes(include=['float64', 'int64'])\n",
      "    return X_train, X_test, Y_train, Y_test, IDs_train, IDs_test\n",
      "35/59:\n",
      "def makeXY(field, inputdata):\n",
      "    IDs_train = []\n",
      "    Y_train = []\n",
      "    for i in range(len(train)):\n",
      "        if not np.isnan(train[field][i]):\n",
      "            IDs_train.append(train['challengeID'][i]-1)\n",
      "            Y_train.append(train[field][i])\n",
      "    IDs_test = []\n",
      "    Y_test = []\n",
      "    for i in range(len(test)):\n",
      "        if not np.isnan(test[field][i]):\n",
      "            IDs_test.append(test['challengeID'][i]-1)\n",
      "            Y_test.append(test[field][i])\n",
      "    X_train = inputdata.loc[IDs_train].select_dtypes(include=['float64', 'int64'])\n",
      "    X_test = inputdata.loc[IDs_test].select_dtypes(include=['float64', 'int64'])\n",
      "    return X_train, X_test, Y_train, Y_test, IDs_train, IDs_test\n",
      "35/60: Xgpa_train, Xgpa_test, Ygpa_train, Ygpa_test, IDgpa_train, IDgpa_test = makeXY('gpa')\n",
      "35/61: Xgpa_train, Xgpa_test, Ygpa_train, Ygpa_test, IDgpa_train, IDgpa_test = makeXY('gpa', data)\n",
      "35/62: Xgpa_train.shape, Xgpa_test.shape, len(Ygpa_train), len(Ygpa_test)\n",
      "35/63:\n",
      "lrL = LassoCV(max_iter=10000, cv=5)\n",
      "lrL.fit(Xgpa_train, Ygpa_train)\n",
      "lrL_pred = lrL.predict(Xgpa_test)\n",
      "mean_squared_error(lrL_pred, Ygpa_test)\n",
      "35/64:\n",
      "lrEN = ElasticNetCV(max_iter=10000, cv=5)\n",
      "lrEN.fit(Xgpa_train, Ygpa_train)\n",
      "lrEN_pred = lrEN.predict(Xgpa_test)\n",
      "mean_squared_error(lrEN_pred, Ygpa_test)\n",
      "35/65:\n",
      "rf = RandomForestRegressor(n_estimators=100)\n",
      "rf.fit(Xgpa_train, Ygpa_train)\n",
      "rf_pred = rf.predict(Xgpa_test)\n",
      "mean_squared_error(rf_pred, Ygpa_test)\n",
      "35/66:\n",
      "ab = AdaBoostRegressor(n_estimators=100)\n",
      "ab.fit(Xgpa_train, Ygpa_train)\n",
      "ab_pred = ab.predict(Xgpa_test)\n",
      "mean_squared_error(ab_pred, Ygpa_test)\n",
      "35/67:\n",
      "from sklearn.linear_model import LinearRegression, RidgeCV, LassoCV, MultiTaskLasso, ElasticNetCV, MultiTaskElasticNet\n",
      "from sklearn.ensemble import RandomForestRegressor, AdaBoostRegressor\n",
      "from sklearn.gaussian_process import GaussianProcessRegressor\n",
      "from sklearn.svm import SVC, LinearSVC, SVR\n",
      "from sklearn.metrics import mean_squared_error\n",
      "35/68:\n",
      "gp = GaussianProcessRegressor()\n",
      "gp.fit(Xgpa_train, Ygpa_train)\n",
      "gp_pred = gp.predict(Xgpa_test)\n",
      "mean_squared_error(gp_pred, Ygpa_test)\n",
      "35/69:\n",
      "from sklearn.linear_model import LinearRegression, RidgeCV, LassoCV, MultiTaskLasso, ElasticNetCV, MultiTaskElasticNet\n",
      "from sklearn.ensemble import RandomForestRegressor, AdaBoostRegressor\n",
      "from sklearn.gaussian_process import GaussianProcessRegressor\n",
      "from sklearn.svm import SVC, LinearSVC, SVR\n",
      "from sklearn.metrics import mean_squared_error, brier_score_loss\n",
      "35/70:\n",
      "def fitmodels(X_train, X_test, Y_train, Y_test, models):\n",
      "    ret = []\n",
      "    for m in models:\n",
      "        m.fit(X_train, Y_train)\n",
      "        m_pred = m.predict(X_test)\n",
      "        ret.append((m, mean_squared_error(m_pred, Y_test)))\n",
      "    return ret\n",
      "35/71:\n",
      "lr2 = LinearRegression(normalize=True)\n",
      "lr2.fit(Xgrit_train, Ygrit_train)\n",
      "lr2_pred = lr2.predict(Xgrit_test)\n",
      "mean_squared_error(lr2_pred, Ygrit_test)\n",
      "35/72:\n",
      "lrR2 = RidgeCV(cv=5)\n",
      "lrR2.fit(Xgrit_train, Ygrit_train)\n",
      "lrR2_pred = lrR2.predict(Xgrit_test)\n",
      "mean_squared_error(lrR2_pred, Ygrit_test)\n",
      "35/73:\n",
      "rf2 = RandomForestRegressor(n_estimators=100)\n",
      "rf2.fit(Xgrit_train, Ygrit_train)\n",
      "rf2_pred = rf2.predict(Xgrit_test)\n",
      "mean_squared_error(rf2_pred, Ygrit_test)\n",
      "35/74:\n",
      "svr = SVR(kernel='rbf')\n",
      "svr.fit(Xgpa_train, Ygpa_train)\n",
      "svr_pred = svr.predict(Xgpa_test)\n",
      "mean_squared_error(svr_pred, Ygpa_test)\n",
      "35/75:\n",
      "svr = SVR(kernel='rbf', gamma='auto')\n",
      "svr.fit(Xgpa_train, Ygpa_train)\n",
      "svr_pred = svr.predict(Xgpa_test)\n",
      "mean_squared_error(svr_pred, Ygpa_test)\n",
      "35/76:\n",
      "sv = SVR(kernel='linear', gamma='auto')\n",
      "sv.fit(Xgpa_train, Ygpa_train)\n",
      "sv_pred = sv.predict(Xgpa_test)\n",
      "mean_squared_error(sv_pred, Ygpa_test)\n",
      "36/1:\n",
      "import pandas as pd\n",
      "import matplotlib.pyplot as plt\n",
      "import seaborn as sns\n",
      "import re\n",
      "import numpy as np\n",
      "import math\n",
      "import ff\n",
      "36/2:\n",
      "from sklearn.linear_model import LinearRegression, RidgeCV, LassoCV, MultiTaskLasso, ElasticNetCV, MultiTaskElasticNet\n",
      "from sklearn.ensemble import RandomForestRegressor, AdaBoostRegressor\n",
      "from sklearn.gaussian_process import GaussianProcessRegressor\n",
      "from sklearn.svm import SVC, LinearSVC, SVR\n",
      "from sklearn.metrics import mean_squared_error, brier_score_loss\n",
      "36/3:\n",
      "data = pd.read_csv('output.csv', low_memory=False)\n",
      "data.info()\n",
      "36/4: data.head()\n",
      "36/5:\n",
      "background = pd.read_csv('background.csv', low_memory=False)\n",
      "background.info()\n",
      "36/6: background.head()\n",
      "36/7:\n",
      "import urllib\n",
      "urllib.parse = urllib.quote_plus\n",
      "urllib.parse.quote = urllib.quote_plus\n",
      "36/8:\n",
      "constructedvars = ff.search({'name': 'data_source', 'op': 'eq', 'val': 'constructed'})\n",
      "print len(constructedvars)\n",
      "constructedvars = [str(x) for x in constructedvars]\n",
      "constructed = []\n",
      "for x in constructedvars:\n",
      "    if x in data.columns:\n",
      "        constructed.append(x)\n",
      "print len(constructed)\n",
      "constructed[:10]\n",
      "36/9:\n",
      "train = pd.read_csv('train.csv')\n",
      "train.info()\n",
      "36/10:\n",
      "test = pd.read_csv('test.csv')\n",
      "test.info()\n",
      "36/11: train[:5]\n",
      "36/12: test[:5]\n",
      "36/13:\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "def makeXY(field, inputdata):\n",
      "    IDs_train = []\n",
      "    Y_train = []\n",
      "    for i in range(len(train)):\n",
      "        if not np.isnan(train[field][i]):\n",
      "            IDs_train.append(train['challengeID'][i]-1)\n",
      "            Y_train.append(train[field][i])\n",
      "    IDs_test = []\n",
      "    Y_test = []\n",
      "    for i in range(len(test)):\n",
      "        if not np.isnan(test[field][i]):\n",
      "            IDs_test.append(test['challengeID'][i]-1)\n",
      "            Y_test.append(test[field][i])\n",
      "    X_train = inputdata.loc[IDs_train].select_dtypes(include=['float64', 'int64'])\n",
      "    X_test = inputdata.loc[IDs_test].select_dtypes(include=['float64', 'int64'])\n",
      "    return X_train, X_test, Y_train, Y_test, IDs_train, IDs_test\n",
      "36/14: Xgpa_train, Xgpa_test, Ygpa_train, Ygpa_test, IDgpa_train, IDgpa_test = makeXY('gpa', data)\n",
      "36/15: Xgpa_train.shape, Xgpa_test.shape, len(Ygpa_train), len(Ygpa_test)\n",
      "36/16:\n",
      "Xc = data[constructed].loc[IDgpa_train]\n",
      "Xc_test = data[constructed].loc[IDgpa_test]\n",
      "lrc = LinearRegression(normalize=True)\n",
      "lrc.fit(Xc, Ygpa_train)\n",
      "lrc_pred = lrc.predict(Xc_test)\n",
      "mean_squared_error(lrc_pred, Ygpa_test)\n",
      "36/17:\n",
      "def fitmodels(X_train, X_test, Y_train, Y_test, models):\n",
      "    ret = []\n",
      "    for m in models:\n",
      "        m.fit(X_train, Y_train)\n",
      "        m_pred = m.predict(X_test)\n",
      "        ret.append((m, mean_squared_error(m_pred, Y_test)))\n",
      "    return ret\n",
      "36/18:\n",
      "lr = LinearRegression(normalize=True)\n",
      "lr.fit(Xgpa_train, Ygpa_train)\n",
      "lr_pred = lr.predict(Xgpa_test)\n",
      "mean_squared_error(lr_pred, Ygpa_test)\n",
      "36/19:\n",
      "lrR = RidgeCV(cv=5)\n",
      "lrR.fit(Xgpa_train, Ygpa_train)\n",
      "lrR_pred = lrR.predict(Xgpa_test)\n",
      "mean_squared_error(lrR_pred, Ygpa_test)\n",
      "36/20:\n",
      "lrL = LassoCV(max_iter=10000, cv=5)\n",
      "lrL.fit(Xgpa_train, Ygpa_train)\n",
      "lrL_pred = lrL.predict(Xgpa_test)\n",
      "mean_squared_error(lrL_pred, Ygpa_test)\n",
      "36/21:\n",
      "lrEN = ElasticNetCV(max_iter=10000, cv=5)\n",
      "lrEN.fit(Xgpa_train, Ygpa_train)\n",
      "lrEN_pred = lrEN.predict(Xgpa_test)\n",
      "mean_squared_error(lrEN_pred, Ygpa_test)\n",
      "36/22:\n",
      "rf = RandomForestRegressor(n_estimators=100)\n",
      "rf.fit(Xgpa_train, Ygpa_train)\n",
      "rf_pred = rf.predict(Xgpa_test)\n",
      "mean_squared_error(rf_pred, Ygpa_test)\n",
      "36/23:\n",
      "ab = AdaBoostRegressor(n_estimators=100)\n",
      "ab.fit(Xgpa_train, Ygpa_train)\n",
      "ab_pred = ab.predict(Xgpa_test)\n",
      "mean_squared_error(ab_pred, Ygpa_test)\n",
      "36/24:\n",
      "rf = RandomForestRegressor(n_estimators=100)\n",
      "rf.fit(Xgpa_train, Ygpa_train)\n",
      "rf_pred = rf.predict(Xgpa_test)\n",
      "mean_squared_error(rf_pred, Ygpa_test)\n",
      "36/25:\n",
      "rf = RandomForestRegressor(n_estimators=100)\n",
      "rf.fit(Xgpa_train, Ygpa_train)\n",
      "rf_pred = rf.predict(Xgpa_test)\n",
      "mean_squared_error(rf_pred, Ygpa_test)\n",
      "36/26:\n",
      "rf = RandomForestRegressor(n_estimators=100)\n",
      "rf.fit(Xgpa_train, Ygpa_train)\n",
      "rf_pred = rf.predict(Xgpa_test)\n",
      "mean_squared_error(rf_pred, Ygpa_test)\n",
      "36/27:\n",
      "ab = AdaBoostRegressor(n_estimators=100)\n",
      "ab.fit(Xgpa_train, Ygpa_train)\n",
      "ab_pred = ab.predict(Xgpa_test)\n",
      "mean_squared_error(ab_pred, Ygpa_test)\n",
      "36/28:\n",
      "gp = GaussianProcessRegressor()\n",
      "gp.fit(Xgpa_train, Ygpa_train)\n",
      "gp_pred = gp.predict(Xgpa_test)\n",
      "mean_squared_error(gp_pred, Ygpa_test)\n",
      "36/29:\n",
      "svr = SVR(kernel='rbf', gamma='auto')\n",
      "svr.fit(Xgpa_train, Ygpa_train)\n",
      "svr_pred = svr.predict(Xgpa_test)\n",
      "mean_squared_error(svr_pred, Ygpa_test)\n",
      "36/30: Xgrit_train, Xgrit_test, Ygrit_train, Ygrit_test, IDgrit_train, IDgrit_test = makeXY('grit', data)\n",
      "36/31: Xgrit_train.shape, Xgrit_test.shape, len(Ygrit_train), len(Ygrit_test)\n",
      "36/32:\n",
      "lr2 = LinearRegression(normalize=True)\n",
      "lr2.fit(Xgrit_train, Ygrit_train)\n",
      "lr2_pred = lr2.predict(Xgrit_test)\n",
      "mean_squared_error(lr2_pred, Ygrit_test)\n",
      "36/33:\n",
      "lrR2 = RidgeCV(cv=5)\n",
      "lrR2.fit(Xgrit_train, Ygrit_train)\n",
      "lrR2_pred = lrR2.predict(Xgrit_test)\n",
      "mean_squared_error(lrR2_pred, Ygrit_test)\n",
      "36/34:\n",
      "rf2 = RandomForestRegressor(n_estimators=100)\n",
      "rf2.fit(Xgrit_train, Ygrit_train)\n",
      "rf2_pred = rf2.predict(Xgrit_test)\n",
      "mean_squared_error(rf2_pred, Ygrit_test)\n",
      "36/35:\n",
      "ab = AdaBoostRegressor(n_estimators=100)\n",
      "ab.fit(Xgpa_train, Ygpa_train)\n",
      "ab_pred = ab.predict(Xgpa_test)\n",
      "mean_squared_error(ab_pred, Ygpa_test)\n",
      "36/36:\n",
      "rf = RandomForestRegressor(n_estimators=100)\n",
      "rf.fit(Xgpa_train, Ygpa_train)\n",
      "rf_pred = rf.predict(Xgpa_test)\n",
      "mean_squared_error(rf_pred, Ygpa_test)\n",
      "36/37:\n",
      "rf = RandomForestRegressor(n_estimators=100)\n",
      "rf.fit(Xgpa_train, Ygpa_train)\n",
      "rf_pred = rf.predict(Xgpa_test)\n",
      "mean_squared_error(rf_pred, Ygpa_test)\n",
      "36/38:\n",
      "nunique = data.apply(pd.Series.nunique)\n",
      "cols_to_drop = nunique[nunique == 1].index\n",
      "data.drop(cols_to_drop, axis=1)\n",
      "36/39:\n",
      "nunique = data.apply(pd.Series.nunique)\n",
      "cols_to_drop = nunique[nunique == 1].index\n",
      "data.drop(cols_to_drop, axis=1)\n",
      "data.head()\n",
      "36/40:\n",
      "nunique = data.apply(pd.Series.nunique)\n",
      "cols_to_drop = nunique[nunique == 1].index\n",
      "data = data.drop(cols_to_drop, axis=1)\n",
      "data.head()\n",
      "36/41: Xgrit_train, Xgrit_test, Ygrit_train, Ygrit_test, IDgrit_train, IDgrit_test = makeXY('grit', data)\n",
      "36/42: Xgrit_train.shape, Xgrit_test.shape, len(Ygrit_train), len(Ygrit_test)\n",
      "36/43:\n",
      "rf2 = RandomForestRegressor(n_estimators=100)\n",
      "rf2.fit(Xgrit_train, Ygrit_train)\n",
      "rf2_pred = rf2.predict(Xgrit_test)\n",
      "mean_squared_error(rf2_pred, Ygrit_test)\n",
      "26/465:\n",
      "plot.hist(cluster_coefficients)\n",
      "plot.title(\"Cluster Coefficients\")\n",
      "plot.show()\n",
      "np.median(cluster_coefficients), np.mean(cluster_coefficients), np.std(cluster_coefficients)\n",
      "26/466:\n",
      "plot.hist(degree_centralities)\n",
      "plot.title(\"Degree Centrality\")\n",
      "plot.show()\n",
      "np.median(degree_centralities), np.mean(degree_centralities), np.std(degree_centralities)\n",
      "26/467:\n",
      "plot.hist(degree_centralities[:313])\n",
      "plot.title(\"Degree Centrality - Female Actors\")\n",
      "plot.show()\n",
      "np.median(degree_centralities[:313]), np.mean(degree_centralities[:313]), np.std(degree_centralities[:313])\n",
      "26/468:\n",
      "plot.hist(degree_centralities[313:])\n",
      "plot.title(\"Degree Centrality - Male Actors\")\n",
      "plot.show()\n",
      "np.median(degree_centralities[313:]), np.mean(degree_centralities[313:]), np.std(degree_centralities[313:])\n",
      "26/469:\n",
      "plot.hist(boxofficegross)\n",
      "plot.title(\"Box Office Gross Totals\")\n",
      "plot.show()\n",
      "np.median(boxofficegross), np.mean(boxofficegross), np.std(boxofficegross)\n",
      "36/44:\n",
      "lr2 = LinearRegression(normalize=True)\n",
      "lr2.fit(Xgrit_train, Ygrit_train)\n",
      "lr2_pred = lr2.predict(Xgrit_test)\n",
      "mean_squared_error(lr2_pred, Ygrit_test)\n",
      "36/45:\n",
      "lrR2 = RidgeCV(cv=5)\n",
      "lrR2.fit(Xgrit_train, Ygrit_train)\n",
      "lrR2_pred = lrR2.predict(Xgrit_test)\n",
      "mean_squared_error(lrR2_pred, Ygrit_test)\n",
      "36/46:\n",
      "def makeXY(field, inputdata):\n",
      "    IDs_train = []\n",
      "    Y_train = []\n",
      "    for i in range(len(train)):\n",
      "        if not np.isnan(train[field][i]):\n",
      "            IDs_train.append(train['challengeID'][i]-1)\n",
      "            Y_train.append(train[field][i])\n",
      "    IDs_test = []\n",
      "    Y_test = []\n",
      "    for i in range(len(test)):\n",
      "        if not np.isnan(test[field][i]):\n",
      "            IDs_test.append(test['challengeID'][i]-1)\n",
      "            Y_test.append(test[field][i])\n",
      "    X_train = inputdata.drop('challengeID', axis=1).loc[IDs_train].select_dtypes(include=['float64', 'int64'])\n",
      "    X_test = inputdata.drop('challengeID', axis=1).loc[IDs_test].select_dtypes(include=['float64', 'int64'])\n",
      "    return X_train, X_test, Y_train, Y_test, IDs_train, IDs_test\n",
      "36/47: Xgpa_train, Xgpa_test, Ygpa_train, Ygpa_test, IDgpa_train, IDgpa_test = makeXY('gpa', data)\n",
      "36/48: Xgpa_train.shape, Xgpa_test.shape, len(Ygpa_train), len(Ygpa_test)\n",
      "36/49:\n",
      "lr = LinearRegression(normalize=True)\n",
      "lr.fit(Xgpa_train, Ygpa_train)\n",
      "lr_pred = lr.predict(Xgpa_test)\n",
      "mean_squared_error(lr_pred, Ygpa_test)\n",
      "36/50:\n",
      "lrR = RidgeCV(cv=5)\n",
      "lrR.fit(Xgpa_train, Ygpa_train)\n",
      "lrR_pred = lrR.predict(Xgpa_test)\n",
      "mean_squared_error(lrR_pred, Ygpa_test)\n",
      "36/51:\n",
      "lrL = LassoCV(max_iter=10000, cv=5)\n",
      "lrL.fit(Xgpa_train, Ygpa_train)\n",
      "lrL_pred = lrL.predict(Xgpa_test)\n",
      "mean_squared_error(lrL_pred, Ygpa_test)\n",
      "26/470:\n",
      "plot.plot(degree_centralities, boxofficegross, \"ro\")\n",
      "plot.title(\"Box Office Gross Totals vs Degree Centrality\")\n",
      "plot.xlabel(\"Degree Centrality\")\n",
      "plot.ylabel(\"Box Office Gross Totals\")\n",
      "plot.show()\n",
      "36/52:\n",
      "# drop highly correlated features\n",
      "corr_matrix = data.corr().abs()\n",
      "upper = corr_matrix.where(np.triu(np.ones(corr_matrix.shape), k=1).astype(np.bool))\n",
      "to_drop = [column for column in upper.columns if any(upper[column] > 0.98)]\n",
      "data2 = data.drop(data.columns[to_drop], axis=1)\n",
      "data2.head()\n",
      "26/471:\n",
      "plot.figure()\n",
      "plot.plot(x, y, \"o\")\n",
      "yfit = [intercept + slope * xi for xi in x]\n",
      "plot.plot(x, yfit, \"black\")\n",
      "plot.title(\"Box Office Gross Totals vs Degree Centrality\")\n",
      "plot.xlabel(\"Degree Centrality\")\n",
      "plot.ylabel(\"Box Office Gross Totals\")\n",
      "plot.show()\n",
      "26/472:\n",
      "plot.hist(academynominated, bins=[0,1])\n",
      "plot.title(\"Academy Award Nominated Actors\")\n",
      "plot.show()\n",
      "26/473:\n",
      "plot.hist(academynominated, bins=[0,1,2])\n",
      "plot.title(\"Academy Award Nominated Actors\")\n",
      "plot.show()\n",
      "26/474:\n",
      "plot.hist(academynominated, bins=[0,1,2])\n",
      "plot.title(\"Academy Award Nominated Actors (1 if nominated)\")\n",
      "plot.show()\n",
      "26/475:\n",
      "plot.hist(eigenvector_centralities)\n",
      "plot.title(\"Eigenvector Centrality\")\n",
      "plot.show()\n",
      "26/476:\n",
      "plot.hist(betweenness_centralities)\n",
      "plot.title(\"Betweenness Centrality\")\n",
      "plot.show()\n",
      "26/477:\n",
      "plot.hist(closeness_centralities)\n",
      "plot.title(\"Closeness Centrality\")\n",
      "plot.show()\n",
      "26/478:\n",
      "plot.figure()\n",
      "plot.plot(moviecounts, boxofficegross, \"o\")\n",
      "plot.plot(moviecounts, [slope*x + intercept for x in moviecounts])\n",
      "plot.title(\"Box Office Gross Totals vs Number of Titles Actor Appeared In\")\n",
      "plot.show()\n",
      "26/479:\n",
      "xc = np.array(moviecounts)\n",
      "yc = np.array(boxofficegross)\n",
      "slope, intercept, r_value, p_value, std_err = stats.linregress(xc,yc)\n",
      "slope, intercept, r_value, p_value, std_err\n",
      "26/480:\n",
      "plot.figure()\n",
      "plot.plot(moviecounts, boxofficegross, \"o\")\n",
      "plot.plot(moviecounts, [slope*x + intercept for x in moviecounts])\n",
      "plot.title(\"Box Office Gross Totals vs Number of Titles Actor Appeared In\")\n",
      "plot.show()\n",
      "26/481:\n",
      "plot.figure()\n",
      "plot.plot(moviecounts, boxofficegross, \"o\")\n",
      "plot.plot(moviecounts, [slope*x + intercept for x in moviecounts], \"black\")\n",
      "plot.title(\"Box Office Gross Totals vs Number of Titles Actor Appeared In\")\n",
      "plot.show()\n",
      "26/482:\n",
      "Gtop = nx.Graph()\n",
      "Glow = nx.Graph()\n",
      "for a in G.nodes():\n",
      "    if a in academylist:\n",
      "        Gtop.add_node(a)\n",
      "    else:\n",
      "        Glow.add_node(a)\n",
      "for (u, v) in G.edges():\n",
      "    if u in academylist and v in academylist:\n",
      "        Gtop.add_edge(u, v)\n",
      "    elif u not in academylist and v not in academylist:\n",
      "        Glow.add_edge(u, v)\n",
      "26/483: len(Gtop.nodes()), len(Gtop.edges())\n",
      "26/484: len(Glow.nodes()), len(Glow.edges())\n",
      "26/485: G.degree()\n",
      "26/486: G.degree_centrality()\n",
      "26/487: sum(G.degree())\n",
      "26/488: sum(G.degree().values())\n",
      "26/489: G.degree().values()\n",
      "26/490: G.degree()\n",
      "26/491: dict(G.degree()).values()\n",
      "26/492:\n",
      "degrees = [val for (node, val) in G.degree()]\n",
      "sum(degrees)\n",
      "26/493:\n",
      "degrees = [val for (node, val) in G.degree()]\n",
      "sum(degrees)/len(G.nodes())\n",
      "26/494:\n",
      "degrees = [val for (node, val) in G.degree()]\n",
      "float(sum(degrees))/len(G.nodes())\n",
      "26/495:\n",
      "degrees = [val for (node, val) in G.degree()]\n",
      "np.mean(degrees), np.median(degrees), np.std(degrees)\n",
      "26/496:\n",
      "degrees = [val for (node, val) in G.degree()]\n",
      "np.median(degrees), np.mean(degrees), np.std(degrees)\n",
      "26/497:\n",
      "def degree_info(graph):\n",
      "    degrees = [val for (node, val) in graph.degree()]\n",
      "    return np.median(degrees), np.mean(degrees), np.std(degrees)\n",
      "26/498: degree_info(G)\n",
      "26/499:\n",
      "print nx.classes.function.density(G), nx.algorithms.cluster.average_clustering(G), degree_info(G)\n",
      "print nx.classes.function.density(Gm), nx.algorithms.cluster.average_clustering(Gm), degree_info(Gm)\n",
      "print nx.classes.function.density(Gw), nx.algorithms.cluster.average_clustering(Gw), degree_info(Gw)\n",
      "print nx.classes.function.density(G2), nx.algorithms.cluster.average_clustering(G2), degree_info(G2)\n",
      "print nx.classes.function.density(G3), nx.algorithms.cluster.average_clustering(G3), degree_info(G3)\n",
      "print nx.classes.function.density(G4), nx.algorithms.cluster.average_clustering(G4), degree_info(G4)\n",
      "print nx.classes.function.density(Gtop), nx.algorithms.cluster.average_clustering(Gtop), degree_info(Gtop)\n",
      "print nx.classes.function.density(Glow), nx.algorithms.cluster.average_clustering(Glow), degree_info(Glow)\n",
      "26/500:\n",
      "boxofficesorted = [str(a) for a in sorted(boxofficedict, key=boxofficedict.get, reverse=True)]\n",
      "boxofficesorted\n",
      "26/501:\n",
      "boxofficesorted = sorted(boxofficedict, key=boxofficedict.get, reverse=True)\n",
      "boxofficesorted\n",
      "26/502:\n",
      "for i in range(10):\n",
      "    print boxofficesorted[i] + \",\"\n",
      "26/503:\n",
      "for i in range(10):\n",
      "    print sorted(degree_centrality, key=degree_centrality.get, reverse=True)[i] + \",\"\n",
      "26/504:\n",
      "for i in range(10):\n",
      "    print sorted(eigenvector_centrality, key=eigenvector_centrality.get, reverse=True)[i] + \",\"\n",
      "26/505:\n",
      "for i in range(10):\n",
      "    print sorted(closeness_centrality, key=closeness_centrality.get, reverse=True)[i] + \",\"\n",
      "26/506:\n",
      "for i in range(10):\n",
      "    print sorted(betweenness_centrality, key=betweenness_centrality.get, reverse=True)[i] + \",\"\n",
      "26/507:\n",
      "for i in range(10):\n",
      "    print sorted(cluster_coefficients, key=cluster_coefficients.get, reverse=True)[i] + \",\"\n",
      "26/508:\n",
      "for i in range(10):\n",
      "    print sorted(cluster_coefficient, key=cluster_coefficient.get, reverse=True)[i] + \",\"\n",
      "26/509:\n",
      "for i in range(10):\n",
      "    print sorted(effective_size, key=effective_size.get, reverse=True)[i] + \",\"\n",
      "26/510: print stats.linregress(10*xw,yw)\n",
      "26/511: print stats.linregress(100*xw,yw)\n",
      "26/512:\n",
      "plot.hist(eigenvector_centralities)\n",
      "plot.title(\"Eigenvector Centrality\")\n",
      "plot.show()\n",
      "np.median(eigenvector_centralities), np.mean(eigenvector_centralities), np.std(eigenvector_centralities)\n",
      "26/513:\n",
      "plot.hist(betweenness_centralities)\n",
      "plot.title(\"Betweenness Centrality\")\n",
      "plot.show()\n",
      "np.median(betweenness_centralities), np.mean(betweenness_centralities), np.std(betweenness_centralities)\n",
      "26/514:\n",
      "plot.hist(closeness_centralities)\n",
      "plot.title(\"Closeness Centrality\")\n",
      "plot.show()\n",
      "np.median(closeness_centralities), np.mean(closeness_centralities), np.std(closeness_centralities)\n",
      "26/515:\n",
      "plot.figure()\n",
      "plot.plot(moviecounts, boxofficegross, \"o\")\n",
      "plot.plot(moviecounts, [slope*x + intercept for x in moviecounts], \"black\")\n",
      "plot.title(\"Box Office Gross vs Number of Titles Actor Appeared In\")\n",
      "plot.show()\n",
      "26/516:\n",
      "plot.hist(boxofficegross)\n",
      "plot.title(\"Box Office Gross\")\n",
      "plot.show()\n",
      "np.median(boxofficegross), np.mean(boxofficegross), np.std(boxofficegross)\n",
      "26/517:\n",
      "plot.plot(degree_centralities, boxofficegross, \"ro\")\n",
      "plot.title(\"Box Office Gross vs Degree Centrality\")\n",
      "plot.xlabel(\"Degree Centrality\")\n",
      "plot.ylabel(\"Box Office Gross Totals\")\n",
      "plot.show()\n",
      "26/518:\n",
      "plot.figure()\n",
      "plot.plot(x, y, \"o\")\n",
      "yfit = [intercept + slope * xi for xi in x]\n",
      "plot.plot(x, yfit, \"black\")\n",
      "plot.title(\"Box Office Gross vs Degree Centrality\")\n",
      "plot.xlabel(\"Degree Centrality\")\n",
      "plot.ylabel(\"Box Office Gross Totals\")\n",
      "plot.show()\n",
      "26/519:\n",
      "x = np.array(degree_centralities)\n",
      "y = np.array(boxofficegross)\n",
      "slope, intercept, r_value, p_value, std_err = stats.linregress(x,y)\n",
      "slope, intercept, r_value, p_value, std_err\n",
      "26/520:\n",
      "plot.figure()\n",
      "plot.plot(x, y, \"o\")\n",
      "yfit = [intercept + slope * xi for xi in x]\n",
      "plot.plot(x, yfit, \"black\")\n",
      "plot.title(\"Box Office Gross vs Degree Centrality\")\n",
      "plot.xlabel(\"Degree Centrality\")\n",
      "plot.ylabel(\"Box Office Gross Totals\")\n",
      "plot.show()\n",
      "26/521:\n",
      "plot.hist(cluster_coefficients)\n",
      "plot.title(\"Cluster Coefficients\")\n",
      "plot.show()\n",
      "np.min(cluster_coefficients), np.mean(cluster_coefficients), np.std(cluster_coefficients)\n",
      "26/522:\n",
      "plot.hist(cluster_coefficients)\n",
      "plot.title(\"Cluster Coefficients\")\n",
      "plot.show()\n",
      "np.max(cluster_coefficients), np.mean(cluster_coefficients), np.std(cluster_coefficients)\n",
      "26/523:\n",
      "plot.hist(effective_sizes)\n",
      "plot.title(\"Effective Size\")\n",
      "plot.show()\n",
      "np.min(effective_sizes), np.mean(effective_sizes), np.std(effective_sizes)\n",
      "26/524:\n",
      "plot.hist(effective_sizes)\n",
      "plot.title(\"Effective Size\")\n",
      "plot.show()\n",
      "np.max(effective_sizes), np.mean(effective_sizes), np.std(effective_sizes)\n",
      "26/525:\n",
      "plot.hist(degree_centralities)\n",
      "plot.title(\"Degree Centrality\")\n",
      "plot.show()\n",
      "np.min(degree_centralities), np.mean(degree_centralities), np.std(degree_centralities)\n",
      "26/526:\n",
      "plot.hist(degree_centralities)\n",
      "plot.title(\"Degree Centrality\")\n",
      "plot.show()\n",
      "np.max(degree_centralities), np.mean(degree_centralities), np.std(degree_centralities)\n",
      "26/527:\n",
      "plot.hist(eigenvector_centralities)\n",
      "plot.title(\"Eigenvector Centrality\")\n",
      "plot.show()\n",
      "np.min(eigenvector_centralities), np.mean(eigenvector_centralities), np.std(eigenvector_centralities)\n",
      "26/528:\n",
      "plot.hist(eigenvector_centralities)\n",
      "plot.title(\"Eigenvector Centrality\")\n",
      "plot.show()\n",
      "np.max(eigenvector_centralities), np.mean(eigenvector_centralities), np.std(eigenvector_centralities)\n",
      "26/529:\n",
      "plot.hist(closeness_centralities)\n",
      "plot.title(\"Closeness Centrality\")\n",
      "plot.show()\n",
      "np.min(closeness_centralities), np.mean(closeness_centralities), np.std(closeness_centralities)\n",
      "26/530:\n",
      "plot.hist(closeness_centralities)\n",
      "plot.title(\"Closeness Centrality\")\n",
      "plot.show()\n",
      "np.max(closeness_centralities), np.mean(closeness_centralities), np.std(closeness_centralities)\n",
      "26/531:\n",
      "plot.hist(betweenness_centralities)\n",
      "plot.title(\"Betweenness Centrality\")\n",
      "plot.show()\n",
      "np.min(betweenness_centralities), np.mean(betweenness_centralities), np.std(betweenness_centralities)\n",
      "26/532:\n",
      "plot.hist(betweenness_centralities)\n",
      "plot.title(\"Betweenness Centrality\")\n",
      "plot.show()\n",
      "np.max(betweenness_centralities), np.mean(betweenness_centralities), np.std(betweenness_centralities)\n",
      "26/533: sorted(moviecountdict, key=moviecountdict.get, reverse=True)\n",
      "26/534:\n",
      "from sklearn.linear_model import LogisticRegression\n",
      "logit = LogisticRegression()\n",
      "logit.fit(df[['degree_centralities']], academynominated)\n",
      "logit.score()\n",
      "26/535:\n",
      "from sklearn.linear_model import LogisticRegression\n",
      "logit = LogisticRegression()\n",
      "logit.fit(df[['degree_centralities']], academynominated)\n",
      "logit.score(df[['degree_centralities']], academynominated)\n",
      "26/536:\n",
      "from sklearn.linear_model import LogisticRegression\n",
      "logit = LogisticRegression()\n",
      "logit.fit(df[['degree_centralities']], academynominated)\n",
      "logit.score(df[['degree_centralities']], academynominated)\n",
      "pred = logit.predict(df[['degree_centralities']])\n",
      "sum(pred)\n",
      "26/537:\n",
      "from sklearn.linear_model import LogisticRegression\n",
      "logit = LogisticRegression()\n",
      "logit.fit(10*degree_centralities, academynominated)\n",
      "logit.score(10*degree_centralities, academynominated)\n",
      "pred = logit.predict(10*degree_centralities)\n",
      "sum(pred)\n",
      "26/538:\n",
      "from sklearn.linear_model import LogisticRegression\n",
      "logit = LogisticRegression()\n",
      "logit.fit(10*degree_centralities.reshape(-1,1), academynominated)\n",
      "logit.score(10*degree_centralities, academynominated)\n",
      "pred = logit.predict(10*degree_centralities)\n",
      "sum(pred)\n",
      "26/539:\n",
      "from sklearn.linear_model import LogisticRegression\n",
      "logit = LogisticRegression()\n",
      "logit.fit(np.array(10*degree_centralities).reshape(-1,1), academynominated)\n",
      "logit.score(10*degree_centralities, academynominated)\n",
      "pred = logit.predict(10*degree_centralities)\n",
      "sum(pred)\n",
      "26/540:\n",
      "from sklearn.linear_model import LogisticRegression\n",
      "logit = LogisticRegression()\n",
      "logit.fit(10*degree_centralities.reshape(-1,1), academynominated)\n",
      "logit.score(10*degree_centralities, academynominated)\n",
      "pred = logit.predict(10*degree_centralities)\n",
      "sum(pred)\n",
      "26/541:\n",
      "from sklearn.linear_model import LogisticRegression\n",
      "logit = LogisticRegression()\n",
      "logit.fit(10*degree_centralities, academynominated)\n",
      "logit.score(10*degree_centralities, academynominated)\n",
      "pred = logit.predict(10*degree_centralities)\n",
      "sum(pred)\n",
      "26/542:\n",
      "closeness_centralities = []\n",
      "for actor in actorlist:\n",
      "    closeness_centralities.append(closeness_centrality.get(actor))\n",
      "x_c = np.array(closeness_centralities)\n",
      "y = np.array(boxofficegross)\n",
      "slope, intercept, r_value, p_value, std_err = stats.linregress(x_c,y)\n",
      "slope, intercept, r_value, p_value, std_err\n",
      "26/543:\n",
      "slope, intercept, r_value, p_value, std_err = stats.linregress(x, academynominated)\n",
      "slope, intercept, r_value, p_value, std_err\n",
      "26/544:\n",
      "xc = np.array(moviecounts)\n",
      "yc = np.array(boxofficegross)\n",
      "slope, intercept, r_value, p_value, std_err = stats.linregress(xc,yc)\n",
      "slope, intercept, r_value, p_value, std_err\n",
      "26/545:\n",
      "betweenness_centralities = []\n",
      "for actor in actorlist:\n",
      "    betweenness_centralities.append(betweenness_centrality.get(actor))\n",
      "x_b = np.array(betweenness_centralities)\n",
      "y = np.array(boxofficegross)\n",
      "slope, intercept, r_value, p_value, std_err = stats.linregress(x_b,y)\n",
      "slope, intercept, r_value, p_value, std_err\n",
      "26/546:\n",
      "x = np.array(effective_sizes)\n",
      "y = np.array(boxofficegross)\n",
      "slope, intercept, r_value, p_value, std_err = stats.linregress(x,y)\n",
      "slope, intercept, r_value, p_value, std_err\n",
      "26/547:\n",
      "x = np.array(cluster_coefficients)\n",
      "y = np.array(boxofficegross)\n",
      "slope, intercept, r_value, p_value, std_err = stats.linregress(x,y)\n",
      "slope, intercept, r_value, p_value, std_err\n",
      "26/548:\n",
      "def genderstats(x):\n",
      "    xm = np.array(x[313:])\n",
      "    ym = np.array(boxofficegross[313:])\n",
      "    slopem, interceptm, r_valuem, p_valuem, std_errm = stats.linregress(xm,ym)\n",
      "    xw = np.array(x[:313])\n",
      "    yw = np.array(boxofficegross[:313])\n",
      "    slopew, interceptw, r_valuew, p_valuew, std_errw = stats.linregress(xw,yw)\n",
      "    print p_valuew, r_valuew, p_valuem, r_valuew\n",
      "26/549: genderstats(degree_centralities)\n",
      "26/550: genderstats(eigenvector_centralities)\n",
      "26/551:\n",
      "genderstats(eigenvector_centralities)\n",
      "genderstats(closeness_centralities)\n",
      "genderstats(betweenness_centralities)\n",
      "genderstats(effective_sizes)\n",
      "genderstats(cluster_coefficients)\n",
      "37/1:\n",
      "rfs = SelectFromModel(RandomForestRegressor(n_estimators=100))\n",
      "rfs.fit(Xgpa_train, Ygpa_train)\n",
      "rfs_pred = rfs.predict(Xgpa_test)\n",
      "print mean_squared_error(rfs_pred, Ygpa_test)\n",
      "print sum(rfs.get_support())\n",
      "print rfs.get_support()[:10]\n",
      "37/2:\n",
      "import pandas as pd\n",
      "import matplotlib.pyplot as plt\n",
      "import seaborn as sns\n",
      "import re\n",
      "import numpy as np\n",
      "import math\n",
      "import ff\n",
      "37/3:\n",
      "from sklearn.linear_model import LinearRegression, RidgeCV, LassoCV, MultiTaskLasso, ElasticNetCV, MultiTaskElasticNet\n",
      "from sklearn.ensemble import RandomForestRegressor, AdaBoostRegressor\n",
      "from sklearn.gaussian_process import GaussianProcessRegressor\n",
      "from sklearn.svm import SVC, LinearSVC, SVR\n",
      "from sklearn.metrics import mean_squared_error, brier_score_loss\n",
      "37/4:\n",
      "data = pd.read_csv('output.csv', low_memory=False)\n",
      "data.info()\n",
      "37/5: data.head()\n",
      "37/6:\n",
      "# drop columns where every value is the same\n",
      "nunique = data.apply(pd.Series.nunique)\n",
      "cols_to_drop = nunique[nunique == 1].index\n",
      "data = data.drop(cols_to_drop, axis=1)\n",
      "data.head()\n",
      "37/7:\n",
      "# drop highly correlated features\n",
      "# corr_matrix = data.corr().abs()\n",
      "# upper = corr_matrix.where(np.triu(np.ones(corr_matrix.shape), k=1).astype(np.bool))\n",
      "# to_drop = [column for column in upper.columns if any(upper[column] > 0.98)]\n",
      "# data2 = data.drop(data.columns[to_drop], axis=1)\n",
      "# data2.head()\n",
      "37/8:\n",
      "background = pd.read_csv('background.csv', low_memory=False)\n",
      "background.info()\n",
      "37/9: background.head()\n",
      "37/10:\n",
      "import urllib\n",
      "urllib.parse = urllib.quote_plus\n",
      "urllib.parse.quote = urllib.quote_plus\n",
      "37/11:\n",
      "# list of constructed variables\n",
      "constructedvars = ff.search({'name': 'data_source', 'op': 'eq', 'val': 'constructed'})\n",
      "print len(constructedvars)\n",
      "constructedvars = [str(x) for x in constructedvars]\n",
      "constructed = []\n",
      "for x in constructedvars:\n",
      "    if x in data.columns:\n",
      "        constructed.append(x)\n",
      "print len(constructed)\n",
      "constructed[:10]\n",
      "37/12:\n",
      "train = pd.read_csv('train.csv')\n",
      "train.info()\n",
      "37/13:\n",
      "test = pd.read_csv('test.csv')\n",
      "test.info()\n",
      "37/14: train[:5]\n",
      "37/15: test[:5]\n",
      "37/16:\n",
      "# make X and Y dataframes for test and train from inputdata \n",
      "def makeXY(field, inputdata):\n",
      "    IDs_train = []\n",
      "    Y_train = []\n",
      "    for i in range(len(train)):\n",
      "        if not np.isnan(train[field][i]):\n",
      "            IDs_train.append(train['challengeID'][i]-1)\n",
      "            Y_train.append(train[field][i])\n",
      "    IDs_test = []\n",
      "    Y_test = []\n",
      "    for i in range(len(test)):\n",
      "        if not np.isnan(test[field][i]):\n",
      "            IDs_test.append(test['challengeID'][i]-1)\n",
      "            Y_test.append(test[field][i])\n",
      "    X_train = inputdata.drop('challengeID', axis=1).loc[IDs_train].select_dtypes(include=['float64', 'int64'])\n",
      "    X_test = inputdata.drop('challengeID', axis=1).loc[IDs_test].select_dtypes(include=['float64', 'int64'])\n",
      "    return X_train, X_test, Y_train, Y_test, IDs_train, IDs_test\n",
      "37/17: Xgpa_train, Xgpa_test, Ygpa_train, Ygpa_test, IDgpa_train, IDgpa_test = makeXY('gpa', data)\n",
      "37/18: Xgpa_train.shape, Xgpa_test.shape, len(Ygpa_train), len(Ygpa_test)\n",
      "37/19:\n",
      "Xc = data[constructed].loc[IDgpa_train]\n",
      "Xc_test = data[constructed].loc[IDgpa_test]\n",
      "lrc = LinearRegression(normalize=True)\n",
      "lrc.fit(Xc, Ygpa_train)\n",
      "lrc_pred = lrc.predict(Xc_test)\n",
      "mean_squared_error(lrc_pred, Ygpa_test)\n",
      "37/20:\n",
      "def fitmodels(X_train, X_test, Y_train, Y_test, models):\n",
      "    ret = []\n",
      "    for m in models:\n",
      "        m.fit(X_train, Y_train)\n",
      "        m_pred = m.predict(X_test)\n",
      "        ret.append((m, mean_squared_error(m_pred, Y_test)))\n",
      "    return ret\n",
      "37/21:\n",
      "lr = LinearRegression(normalize=True)\n",
      "lr.fit(Xgpa_train, Ygpa_train)\n",
      "lr_pred = lr.predict(Xgpa_test)\n",
      "mean_squared_error(lr_pred, Ygpa_test)\n",
      "37/22:\n",
      "lrR = RidgeCV(cv=5)\n",
      "lrR.fit(Xgpa_train, Ygpa_train)\n",
      "lrR_pred = lrR.predict(Xgpa_test)\n",
      "mean_squared_error(lrR_pred, Ygpa_test)\n",
      "37/23:\n",
      "lrL = LassoCV(max_iter=10000, cv=5)\n",
      "lrL.fit(Xgpa_train, Ygpa_train)\n",
      "lrL_pred = lrL.predict(Xgpa_test)\n",
      "mean_squared_error(lrL_pred, Ygpa_test)\n",
      "26/552:\n",
      "print np.mean(boxofficegross[:313]), np.std(boxofficegross[:313])\n",
      "print np.mean(boxofficegross[313:]), np.std(boxofficegross[313:])\n",
      "37/24:\n",
      "lrEN = ElasticNetCV(max_iter=10000, cv=5)\n",
      "lrEN.fit(Xgpa_train, Ygpa_train)\n",
      "lrEN_pred = lrEN.predict(Xgpa_test)\n",
      "mean_squared_error(lrEN_pred, Ygpa_test)\n",
      "37/25:\n",
      "rf = RandomForestRegressor(n_estimators=100)\n",
      "rf.fit(Xgpa_train, Ygpa_train)\n",
      "rf_pred = rf.predict(Xgpa_test)\n",
      "mean_squared_error(rf_pred, Ygpa_test)\n",
      "37/26:\n",
      "rfs = SelectFromModel(RandomForestRegressor(n_estimators=100))\n",
      "rfs.fit(Xgpa_train, Ygpa_train)\n",
      "rfs_pred = rfs.predict(Xgpa_test)\n",
      "print mean_squared_error(rfs_pred, Ygpa_test)\n",
      "print sum(rfs.get_support())\n",
      "print rfs.get_support()[:10]\n",
      "37/27:\n",
      "from sklearn.linear_model import LinearRegression, RidgeCV, LassoCV, MultiTaskLasso, ElasticNetCV, MultiTaskElasticNet\n",
      "from sklearn.ensemble import RandomForestRegressor, AdaBoostRegressor\n",
      "from sklearn.gaussian_process import GaussianProcessRegressor\n",
      "from sklearn.svm import SVC, LinearSVC, SVR\n",
      "from sklearn.metrics import mean_squared_error, brier_score_loss\n",
      "from sklearn.feature_selection import SelectFromModel\n",
      "37/28:\n",
      "rfs = SelectFromModel(RandomForestRegressor(n_estimators=100))\n",
      "rfs.fit(Xgpa_train, Ygpa_train)\n",
      "rfs_pred = rfs.predict(Xgpa_test)\n",
      "print mean_squared_error(rfs_pred, Ygpa_test)\n",
      "print sum(rfs.get_support())\n",
      "print rfs.get_support()[:10]\n",
      "37/29:\n",
      "rf = RandomForestRegressor(n_estimators=100)\n",
      "rf.fit(Xgpa_train, Ygpa_train)\n",
      "rf_pred = rf.predict(Xgpa_test)\n",
      "mean_squared_error(rf_pred, Ygpa_test)\n",
      "37/30:\n",
      "rfselect = SelectFromModel(RandomForestRegressor(n_estimators=100))\n",
      "Xgpa_train_rf = rfselect.fit_transform(Xgpa_train, Ygpa_train)\n",
      "rfs = RandomForestRegressor(n_estimators=100).fit(Xgpa_train_rf, Ygpa_train)\n",
      "Xgpa_test_rf = rfselect.transform(Xgpa_test)\n",
      "rfs_pred = rfs.predict(Xgpa_test_rf)\n",
      "print mean_squared_error(rfs_pred, Ygpa_test)\n",
      "print sum(rfs.get_support())\n",
      "print rfs.get_support()[:10]\n",
      "37/31:\n",
      "rfselect = SelectFromModel(RandomForestRegressor(n_estimators=100))\n",
      "Xgpa_train_rf = rfselect.fit_transform(Xgpa_train, Ygpa_train)\n",
      "rfs = RandomForestRegressor(n_estimators=100).fit(Xgpa_train_rf, Ygpa_train)\n",
      "Xgpa_test_rf = rfselect.transform(Xgpa_test)\n",
      "rfs_pred = rfs.predict(Xgpa_test_rf)\n",
      "print mean_squared_error(rfs_pred, Ygpa_test)\n",
      "print sum(rfselect.get_support())\n",
      "print rfselect.get_support()[:10]\n",
      "37/32: rfs.feature_importances_()\n",
      "37/33: rfs.feature_importances_\n",
      "37/34: sorted(Xgpa_train.columns, key=rf.feature_importances_, reverse=True)\n",
      "37/35: sorted(Xgpa_train.columns, key=lambda i: rf.feature_importances_[i], reverse=True)\n",
      "37/36: sorted(Xgpa_train.columns, reverse=True)\n",
      "37/37:\n",
      "features = zip(Xgpa_train.columns, rf.feature_importances_[i])\n",
      "sorted(features, key=lambda x: x[1], reverse=True)\n",
      "37/38:\n",
      "features = zip(Xgpa_train.columns, rf.feature_importances_)\n",
      "sorted(features, key=lambda x: x[1], reverse=True)\n",
      "37/39:\n",
      "# features sorted by importance\n",
      "features = zip(Xgpa_train.columns, rf.feature_importances_)\n",
      "sorted(features, key=lambda x: x[1], reverse=True)\n",
      "37/40:\n",
      "# features sorted by importance\n",
      "featuresRF = zip(Xgpa_train.columns, rf.feature_importances_)\n",
      "sorted(featuresRF, key=lambda x: x[1], reverse=True)\n",
      "37/41:\n",
      "# features sorted by importance\n",
      "featuresEN = zip(Xgpa_train.columns, lrEN.feature_importances_)\n",
      "sorted(featuresEN, key=lambda x: x[1], reverse=True)\n",
      "37/42:\n",
      "# features sorted by importance\n",
      "features = zip(Xgpa_train.columns, rf.feature_importances_)\n",
      "sorted(features, key=lambda x: x[1], reverse=True)\n",
      "37/43:\n",
      "ab = AdaBoostRegressor(n_estimators=100)\n",
      "ab.fit(Xgpa_train, Ygpa_train)\n",
      "ab_pred = ab.predict(Xgpa_test)\n",
      "mean_squared_error(ab_pred, Ygpa_test)\n",
      "37/44:\n",
      "gp = GaussianProcessRegressor()\n",
      "gp.fit(Xgpa_train, Ygpa_train)\n",
      "gp_pred = gp.predict(Xgpa_test)\n",
      "mean_squared_error(gp_pred, Ygpa_test)\n",
      "37/45:\n",
      "svr = SVR(kernel='rbf', gamma='auto')\n",
      "svr.fit(Xgpa_train, Ygpa_train)\n",
      "svr_pred = svr.predict(Xgpa_test)\n",
      "mean_squared_error(svr_pred, Ygpa_test)\n",
      "26/553: genderstats(degree_centralities)\n",
      "26/554:\n",
      "def genderstats(x):\n",
      "    xm = np.array(x[313:])\n",
      "    ym = np.array(boxofficegross[313:])\n",
      "    slopem, interceptm, r_valuem, p_valuem, std_errm = stats.linregress(xm,ym)\n",
      "    xw = np.array(x[:313])\n",
      "    yw = np.array(boxofficegross[:313])\n",
      "    slopew, interceptw, r_valuew, p_valuew, std_errw = stats.linregress(xw,yw)\n",
      "    print p_valuew, r_valuew, p_valuem, r_valuew\n",
      "    print np.mean(x[:313]), np.std(x[:313])\n",
      "    print np.mean(x[313:]), np.std(x[313:])\n",
      "26/555: genderstats(degree_centralities)\n",
      "26/556:\n",
      "def genderstats(x):\n",
      "    xm = np.array(x[313:])\n",
      "    ym = np.array(boxofficegross[313:])\n",
      "    slopem, interceptm, r_valuem, p_valuem, std_errm = stats.linregress(xm,ym)\n",
      "    xw = np.array(x[:313])\n",
      "    yw = np.array(boxofficegross[:313])\n",
      "    slopew, interceptw, r_valuew, p_valuew, std_errw = stats.linregress(xw,yw)\n",
      "    print p_valuew, r_valuew, p_valuem, r_valuew\n",
      "    print np.mean(x[:313]), np.std(x[:313])\n",
      "    print np.mean(x[313:]), np.std(x[313:])\n",
      "    print \"\"\n",
      "26/557:\n",
      "print np.mean(boxofficegross[:313]), np.std(boxofficegross[:313])\n",
      "print np.mean(boxofficegross[313:]), np.std(boxofficegross[313:])\n",
      "26/558: genderstats(degree_centralities)\n",
      "26/559:\n",
      "genderstats(eigenvector_centralities)\n",
      "genderstats(closeness_centralities)\n",
      "genderstats(betweenness_centralities)\n",
      "genderstats(effective_sizes)\n",
      "genderstats(cluster_coefficients)\n",
      "26/560:\n",
      "def genderstats(x):\n",
      "    xm = np.array(x[313:])\n",
      "    ym = np.array(boxofficegross[313:])\n",
      "    slopem, interceptm, r_valuem, p_valuem, std_errm = stats.linregress(xm,ym)\n",
      "    xw = np.array(x[:313])\n",
      "    yw = np.array(boxofficegross[:313])\n",
      "    slopew, interceptw, r_valuew, p_valuew, std_errw = stats.linregress(xw,yw)\n",
      "    print p_valuew, r_valuew, p_valuem, r_valuem\n",
      "    print np.mean(x[:313]), np.std(x[:313])\n",
      "    print np.mean(x[313:]), np.std(x[313:])\n",
      "    print \"\"\n",
      "26/561:\n",
      "print np.mean(boxofficegross[:313]), np.std(boxofficegross[:313])\n",
      "print np.mean(boxofficegross[313:]), np.std(boxofficegross[313:])\n",
      "26/562: genderstats(degree_centralities)\n",
      "26/563:\n",
      "genderstats(eigenvector_centralities)\n",
      "genderstats(closeness_centralities)\n",
      "genderstats(betweenness_centralities)\n",
      "genderstats(effective_sizes)\n",
      "genderstats(cluster_coefficients)\n",
      "26/564:\n",
      "def genderstats(x):\n",
      "    xm = np.array(x[313:])\n",
      "    ym = np.array(boxofficegross[313:])\n",
      "    slopem, interceptm, r_valuem, p_valuem, std_errm = stats.linregress(xm,ym)\n",
      "    xw = np.array(x[:313])\n",
      "    yw = np.array(boxofficegross[:313])\n",
      "    slopew, interceptw, r_valuew, p_valuew, std_errw = stats.linregress(xw,yw)\n",
      "    print p_valuew, p_valuem\n",
      "    print r_valuew, r_valuem\n",
      "    print np.mean(x[:313]), np.std(x[:313])\n",
      "    print np.mean(x[313:]), np.std(x[313:])\n",
      "    print \"\"\n",
      "26/565:\n",
      "print np.mean(boxofficegross[:313]), np.std(boxofficegross[:313])\n",
      "print np.mean(boxofficegross[313:]), np.std(boxofficegross[313:])\n",
      "26/566: genderstats(degree_centralities)\n",
      "26/567:\n",
      "genderstats(eigenvector_centralities)\n",
      "genderstats(closeness_centralities)\n",
      "genderstats(betweenness_centralities)\n",
      "genderstats(effective_sizes)\n",
      "genderstats(cluster_coefficients)\n",
      "26/568:\n",
      "X2e = df[['eigenvector_centralities', 'moviecount']] #, 'eigenvector_centralities', 'betweenness_centralities', 'closeness_centralities', 'moviecount']]\n",
      "X2e = sm.add_constant(X2e) # adding a constant\n",
      "Y2e = academynominated\n",
      "model2e = sm.OLS(Y2, X2e).fit()\n",
      "predictions2e = model2e.predict(X2e) \n",
      "print_model2e = model2e.summary()\n",
      "# print(print_model2e)\n",
      "26/569:\n",
      "def awardstats(x):\n",
      "    X = sm.add_constant(df[[x]])\n",
      "    model = sm.OLS(academynominated, X).fit()\n",
      "    pred = model.predict(X)\n",
      "    tn, fp, fn, tp = confusion_matrix(academynominated, pred).ravel()\n",
      "    print measures(tn, fp, fn, tp)\n",
      "26/570: awardstats('degree_centrality')\n",
      "26/571: awardstats('degree_centralities')\n",
      "37/46: rf2.feature_importances_\n",
      "37/47: Xgrit_train, Xgrit_test, Ygrit_train, Ygrit_test, IDgrit_train, IDgrit_test = makeXY('grit', data)\n",
      "37/48: Xgrit_train.shape, Xgrit_test.shape, len(Ygrit_train), len(Ygrit_test)\n",
      "37/49:\n",
      "lr2 = LinearRegression(normalize=True)\n",
      "lr2.fit(Xgrit_train, Ygrit_train)\n",
      "lr2_pred = lr2.predict(Xgrit_test)\n",
      "mean_squared_error(lr2_pred, Ygrit_test)\n",
      "37/50:\n",
      "lrR2 = RidgeCV(cv=5)\n",
      "lrR2.fit(Xgrit_train, Ygrit_train)\n",
      "lrR2_pred = lrR2.predict(Xgrit_test)\n",
      "mean_squared_error(lrR2_pred, Ygrit_test)\n",
      "37/51:\n",
      "rf2 = RandomForestRegressor(n_estimators=100)\n",
      "rf2.fit(Xgrit_train, Ygrit_train)\n",
      "rf2_pred = rf2.predict(Xgrit_test)\n",
      "mean_squared_error(rf2_pred, Ygrit_test)\n",
      "37/52:\n",
      "# features sorted by importance\n",
      "featuresGRIT = zip(Xgrit_train.columns, rf2.feature_importances_)\n",
      "sorted(featuresGRIT, key=lambda x: x[1], reverse=True)\n",
      "37/53:\n",
      "def dropsame(d):\n",
      "    nunique = d.apply(pd.Series.nunique)\n",
      "    cols_to_drop = nunique[nunique == 1].index\n",
      "    return d.drop(cols_to_drop, axis=1)\n",
      "37/54: data2 = dropsame(background)\n",
      "37/55: data2.head()\n",
      "37/56: data2 = dropsame(background).select_dtypes(include=['float64', 'int64'])\n",
      "37/57: data2.head()\n",
      "37/58:\n",
      "for i in range(len(data2.columns)):\n",
      "    if data2[i].dtype == 'int64':\n",
      "        print \"y\"\n",
      "37/59:\n",
      "for i in range(len(data2.columns)):\n",
      "    if data2.loc[i].dtype == 'int64':\n",
      "        print \"y\"\n",
      "37/60: data2.m1intyr.dtype\n",
      "37/61: data2.m1intyr.dtype == 'float64'\n",
      "37/62: data2['m1intyr'].dtype == 'float64'\n",
      "37/63: data2['m1intyr'].mode()\n",
      "37/64: data2.m1intyr.mode()\n",
      "37/65: data2['challengeID'].mode()\n",
      "37/66: data2['challengeID'].median()\n",
      "37/67: data2['m1lenhr'].mode()\n",
      "37/68: data2['challengeID'].mode().iloc[0]\n",
      "37/69:\n",
      "for i in range(len(data2.columns)):\n",
      "    var = data2.columns[i]\n",
      "    if data2[var].dtype == 'int64':\n",
      "        data2[var] = data2[var].fillna(data2[var].mode().iloc[0])\n",
      "    else:\n",
      "        data2[var] = data2[var].fillna(data2[var].mean())\n",
      "37/70: data2.head()\n",
      "26/572:\n",
      "def awardstats(x):\n",
      "    X = sm.add_constant(df[x])\n",
      "    model = sm.OLS(academynominated, X).fit()\n",
      "    pred = model.predict(X)\n",
      "    tn, fp, fn, tp = confusion_matrix(academynominated, pred).ravel()\n",
      "    print measures(tn, fp, fn, tp)\n",
      "26/573: awardstats('degree_centralities')\n",
      "26/574: awardstats('eigenvector_centralities')\n",
      "26/575:\n",
      "def awardstats(x):\n",
      "    X = sm.add_constant(df[['eigenvector_centralities', 'moviecount']])\n",
      "    model = sm.OLS(academynominated, X).fit()\n",
      "    pred = model.predict(X)\n",
      "    tn, fp, fn, tp = confusion_matrix(academynominated, pred).ravel()\n",
      "    print measures(tn, fp, fn, tp)\n",
      "26/576: awardstats('eigenvector_centralities')\n",
      "26/577:\n",
      "X2e = df[['eigenvector_centralities', 'moviecount']] #, 'eigenvector_centralities', 'betweenness_centralities', 'closeness_centralities', 'moviecount']]\n",
      "X2e = sm.add_constant(X2e) # adding a constant\n",
      "Y2e = academynominated\n",
      "model2e = sm.OLS(Y2, X2e).fit()\n",
      "predictions2e = model2e.predict(X2e) \n",
      "print_model2e = model2e.summary()\n",
      "# print(print_model2e)\n",
      "26/578:\n",
      "def awardstats(x):\n",
      "    X = sm.add_constant(df[x])\n",
      "    model = sm.OLS(academynominated, X).fit()\n",
      "    pred = model.predict(X)\n",
      "    predbin = [1 if x >= 0.5 else 0 for x in pred]\n",
      "    tn, fp, fn, tp = confusion_matrix(academynominated, predbin).ravel()\n",
      "    print measures(tn, fp, fn, tp)\n",
      "26/579: awardstats('eigenvector_centralities')\n",
      "26/580:\n",
      "awardstats('degree_centralities')\n",
      "awardstats('eigenvector_centralities')\n",
      "awardstats('closeness_centralities')\n",
      "awardstats('betweenness_centralities')\n",
      "awardstats('effective_size')\n",
      "awardstats('cluster_coefficient')\n",
      "awardstats('movie_count')\n",
      "26/581:\n",
      "def measures(tn, fp, fn, tp):\n",
      "    accuracy = float(tn + fn)/(tn+fp+fn+tp)\n",
      "    recall = float(tp)/(tp + fn)\n",
      "    specificity = float(tn)/(tn + fp)\n",
      "    precision = np.nan if tp+fp==0 else float(tp)/(tp + fp)\n",
      "    return \"accuracy: %f, recall: %f, specificity: %f, precision: %f\" % (accuracy, recall, specificity, precision)\n",
      "26/582:\n",
      "awardstats('degree_centralities')\n",
      "awardstats('eigenvector_centralities')\n",
      "awardstats('closeness_centralities')\n",
      "awardstats('betweenness_centralities')\n",
      "awardstats('effective_size')\n",
      "awardstats('cluster_coefficient')\n",
      "awardstats('movie_count')\n",
      "26/583:\n",
      "awardstats('degree_centralities')\n",
      "awardstats('eigenvector_centralities')\n",
      "awardstats('closeness_centralities')\n",
      "awardstats('betweenness_centralities')\n",
      "awardstats('effective_size')\n",
      "awardstats('cluster_coefficients')\n",
      "awardstats('moviecount')\n",
      "26/584:\n",
      "def measures(tn, fp, fn, tp):\n",
      "    accuracy = float(tn + tp)/(tn+fp+fn+tp)\n",
      "    recall = float(tp)/(tp + fn)\n",
      "    specificity = float(tn)/(tn + fp)\n",
      "    precision = np.nan if tp+fp==0 else float(tp)/(tp + fp)\n",
      "    return \"accuracy: %f, recall: %f, specificity: %f, precision: %f\" % (accuracy, recall, specificity, precision)\n",
      "26/585: print measures(tn, fp, fn, tp)\n",
      "26/586: print measures(tn2, fp2, fn2, tp2)\n",
      "26/587:\n",
      "awardstats('degree_centralities')\n",
      "awardstats('eigenvector_centralities')\n",
      "awardstats('closeness_centralities')\n",
      "awardstats('betweenness_centralities')\n",
      "awardstats('effective_size')\n",
      "awardstats('cluster_coefficients')\n",
      "awardstats('moviecount')\n",
      "26/588:\n",
      "def getpr(graph):\n",
      "    degree = nx.degree_centrality(graph)\n",
      "    degrees = [degree.get(a) for a in actorlist]\n",
      "    eigenvector = nx.eigenvector_centrality(graph)\n",
      "    eigenvectors = [eigenvector.get(a) for a in actorlist]\n",
      "    close = nx.closeness_centrality(graph)\n",
      "    closes = [close.get(a) for a in actorlist]\n",
      "    between = nx.betweenness_centrality(graph)\n",
      "    betweens = [between.get(a) for a in actorlist]\n",
      "    effective = nx.algorithms.structuralholes.effective_size(graph)\n",
      "    effectives = [effective.get(a) for a in actorlist]\n",
      "    cluster = nx.algorithms.cluster.clustering(graph)\n",
      "    clusters = [cluster.get(a) for a in actorlist]\n",
      "    return degrees, eigenvectors, closes, betweens, effectives, clusters\n",
      "26/589: degrees, eigenvectors, closes, betweens, effectives, clusters = getpr(G2)\n",
      "26/590: degrees3, eigenvectors3, closes3, betweens3, effectives3, clusters3 = getpr(G3)\n",
      "26/591: degrees4, eigenvectors4, closes4, betweens4, effectives4, clusters4 = getpr(G4)\n",
      "26/592:\n",
      "def getarr(graph):\n",
      "    degree = nx.degree_centrality(graph)\n",
      "    degrees = [degree.get(a) for a in actorlist]\n",
      "    eigenvector = nx.eigenvector_centrality(graph)\n",
      "    eigenvectors = [eigenvector.get(a) for a in actorlist]\n",
      "    close = nx.closeness_centrality(graph)\n",
      "    closes = [close.get(a) for a in actorlist]\n",
      "    between = nx.betweenness_centrality(graph)\n",
      "    betweens = [between.get(a) for a in actorlist]\n",
      "    effective = nx.algorithms.structuralholes.effective_size(graph)\n",
      "    effectives = [effective.get(a) for a in actorlist]\n",
      "    cluster = nx.algorithms.cluster.clustering(graph)\n",
      "    clusters = [cluster.get(a) for a in actorlist]\n",
      "    return degrees, eigenvectors, closes, betweens, effectives, clusters\n",
      "26/593:\n",
      "def getarr(graph):\n",
      "    degree = nx.degree_centrality(graph)\n",
      "    degrees = [degree.get(a) for a in actorlist]\n",
      "    eigenvector = nx.eigenvector_centrality(graph)\n",
      "    eigenvectors = [eigenvector.get(a) for a in actorlist]\n",
      "    close = nx.closeness_centrality(graph)\n",
      "    closes = [close.get(a) for a in actorlist]\n",
      "    between = nx.betweenness_centrality(graph)\n",
      "    betweens = [between.get(a) for a in actorlist]\n",
      "    effective = nx.algorithms.structuralholes.effective_size(graph)\n",
      "    effectives = [effective.get(a) for a in actorlist]\n",
      "    cluster = nx.algorithms.cluster.clustering(graph)\n",
      "    clusters = [cluster.get(a) for a in actorlist]\n",
      "    return [degrees, eigenvectors, closes, betweens, effectives, clusters]\n",
      "26/594: arr2 = getarr(G2)\n",
      "26/595: arr3 = getarr(G3)\n",
      "26/596: arr4 = getarr(G4)\n",
      "26/597:\n",
      "def regs(arrs):\n",
      "    for arr in arrs:\n",
      "        slope, intercept, r_value, p_value, std_err = stats.linregress(arr,boxofficegross)\n",
      "        print r_value, p_value\n",
      "26/598: regs(arr2)\n",
      "26/599:\n",
      "arr2 = getarr(G2)\n",
      "arr2[4] = [0 if np.isnan(a) else a in arr2[4]]\n",
      "26/600: arr2 = getarr(G2)\n",
      "26/601: arr2[4]\n",
      "26/602: arr2[4] = [0 if np.isnan(a) else a in arr2[4]]\n",
      "26/603:\n",
      "# arr2[4] = [0 if np.isnan(a) else a in arr2[4]]\n",
      "arr2[4].dtype\n",
      "26/604:\n",
      "# arr2[4] = [0 if np.isnan(a) else a in arr2[4]]\n",
      "arr2[4]\n",
      "26/605:\n",
      "# arr2[4] = [0 if np.isnan(a) else a in arr2[4]]\n",
      "np.isnan(arr2[4][4])\n",
      "26/606:\n",
      "# arr2[4] = [0 if np.isnan(a) else a in arr2[4]]\n",
      "np.isnan(arr2[4][3])\n",
      "26/607: arr2[4] = [0 if np.isnan(a) else a for a in arr2[4]]\n",
      "26/608: arr2[4]\n",
      "26/609: arr3[4] = [0 if np.isnan(a) else a for a in arr3[4]]\n",
      "26/610: arr4[4] = [0 if np.isnan(a) else a for a in arr4[4]]\n",
      "26/611: regs(arr2)\n",
      "26/612: regs(arr3)\n",
      "26/613: regs(arr4)\n",
      "37/71: data2 = dropsame(background).select_dtypes(include=['float64', 'int64'])\n",
      "37/72: data2.head()\n",
      "37/73:\n",
      "# drop columns if more than 90% are NaN\n",
      "data2 = data2.dropna(thresh=424)\n",
      "data2.head()\n",
      "37/74:\n",
      "# drop columns if more than 80% are NaN\n",
      "data2 = data2.dropna(thresh=848)\n",
      "data2.head()\n",
      "37/75:\n",
      "# drop columns if more than 80% are NaN\n",
      "data2 = data2.dropna(thresh=424, axis=1)\n",
      "data2.head()\n",
      "37/76:\n",
      "# drop columns if more than 90% are NaN\n",
      "data2 = data2.dropna(thresh=424, axis=1)\n",
      "data2.head()\n",
      "37/77:\n",
      "for i in range(len(data2.columns)):\n",
      "    var = data2.columns[i]\n",
      "    if data2[var].dtype == 'int64':\n",
      "        data2[var] = data2[var].fillna(data2[var].mode().iloc[0])\n",
      "    else:\n",
      "        data2[var] = data2[var].fillna(data2[var].mean())\n",
      "37/78: data2.head()\n",
      "37/79: len(data2) - data2.count()\n",
      "37/80: data2.count()\n",
      "37/81: data2.count().sum()\n",
      "37/82: len(data2).sum() - data2.count().sum()\n",
      "37/83: len(data2) - data2.count().sum()\n",
      "37/84: data2.isnull().sum()\n",
      "37/85: sum(data2.isnull())\n",
      "37/86: data2.isnull()\n",
      "37/87: data2.isnull().sum()\n",
      "37/88: data2.isnull().sum().sum()\n",
      "37/89: data2.info()\n",
      "37/90: data2 = dropsame(background).select_dtypes(include=['float64', 'int64'])\n",
      "37/91: data2.head()\n",
      "37/92:\n",
      "# drop columns if more than 90% are NaN\n",
      "data2 = data2.dropna(thresh=424, axis=1)\n",
      "data2.head()\n",
      "37/93: data2.isnull().sum().sum()\n",
      "37/94:\n",
      "for i in range(len(data2.columns)):\n",
      "    var = data2.columns[i]\n",
      "    if data2[var].dtype == 'int64':\n",
      "        data2[var] = data2[var].fillna(data2[var].mode().iloc[0])\n",
      "    else:\n",
      "        data2[var] = data2[var].fillna(data2[var].mean())\n",
      "37/95: data2.isnull().sum().sum()\n",
      "37/96:\n",
      "for i in range(len(data2.columns)):\n",
      "    var = data2.columns[i]\n",
      "    if ff.select(var)['data_type'] != 'Continuous':\n",
      "        data2[var] = data2[var].fillna(data2[var].mode().iloc[0])\n",
      "    else:\n",
      "        print \"y\"\n",
      "        data2[var] = data2[var].fillna(data2[var].mean())\n",
      "37/97: data2 = dropsame(background).select_dtypes(include=['float64', 'int64'])\n",
      "37/98: data2.head()\n",
      "37/99:\n",
      "# drop columns if more than 90% are NaN\n",
      "data2 = data2.dropna(thresh=424, axis=1)\n",
      "data2.head()\n",
      "37/100: data2.isnull().sum().sum()\n",
      "37/101:\n",
      "for i in range(len(data2.columns)):\n",
      "    var = data2.columns[i]\n",
      "    if ff.select(var)['data_type'] != 'Continuous':\n",
      "        data2[var] = data2[var].fillna(data2[var].mode().iloc[0])\n",
      "    else:\n",
      "        print var\n",
      "        data2[var] = data2[var].fillna(data2[var].mean())\n",
      "37/102:\n",
      "for i in range(len(data2.columns)):\n",
      "    var = data2.columns[i]\n",
      "    print var\n",
      "    if ff.select(var)['data_type'] != 'Continuous':\n",
      "        data2[var] = data2[var].fillna(data2[var].mode().iloc[0])\n",
      "    else:\n",
      "        print var\n",
      "        data2[var] = data2[var].fillna(data2[var].mean())\n",
      "37/103:\n",
      "for i in range(len(data2.columns)-1):\n",
      "    var = data2.columns[i+1]\n",
      "    print var\n",
      "    if ff.select(var)['data_type'] != 'Continuous':\n",
      "        data2[var] = data2[var].fillna(data2[var].mode().iloc[0])\n",
      "    else:\n",
      "        print var\n",
      "        data2[var] = data2[var].fillna(data2[var].mean())\n",
      "37/104: ff.select(m1lenhr)\n",
      "37/105: ff.select('m1lenhr')\n",
      "37/106:\n",
      "for i in range(len(data2.columns)):\n",
      "    var = data2.columns[i]\n",
      "    if data2[var].dtype == 'int64':\n",
      "        data2[var] = data2[var].fillna(data2[var].mode().iloc[0])\n",
      "    else:\n",
      "        data2[var] = data2[var].fillna(data2[var].mean())\n",
      "37/107: data2.isnull().sum().sum()\n",
      "37/108: Xgpa_train2, Xgpa_test2, Ygpa_train2, Ygpa_test2, IDgpa_train2, IDgpa_test2 = makeXY('gpa', data2)\n",
      "37/109:\n",
      "rf2 = RandomForestRegressor(n_estimators=100)\n",
      "rf2.fit(Xgpa_train2, Ygpa_train2)\n",
      "rf2_pred = rf2.predict(Xgpa_test2)\n",
      "mean_squared_error(rf2_pred, Ygpa_test2)\n",
      "26/614:\n",
      "def regs(arrs):\n",
      "    for arr in arrs:\n",
      "        slope, intercept, r_value, p_value, std_err = stats.linregress(arr,boxofficegross)\n",
      "        print p_value, r_value\n",
      "26/615: regs(arr2)\n",
      "26/616: regs(arr3)\n",
      "26/617: regs(arr4)\n",
      "37/110:\n",
      "rf2 = RandomForestRegressor(n_estimators=200)\n",
      "rf2.fit(Xgpa_train2, Ygpa_train2)\n",
      "rf2_pred = rf2.predict(Xgpa_test2)\n",
      "mean_squared_error(rf2_pred, Ygpa_test2)\n",
      "37/111:\n",
      "rf2 = RandomForestRegressor(n_estimators=100)\n",
      "rf2.fit(Xgpa_train2, Ygpa_train2)\n",
      "rf2_pred = rf2.predict(Xgpa_test2)\n",
      "mean_squared_error(rf2_pred, Ygpa_test2)\n",
      "37/112:\n",
      "d0 = dropsame(background).select_dtypes(include=['float64', 'int64'])\n",
      "mindata = [0, 424, 424*2, 424*3, 424*4, 424*5] # 100%, 90%, 80%, 70%, 60%, 50%\n",
      "for i in range(len(mindata)):\n",
      "    d = d0.dropna(thresh=mindata[i], axis=1)\n",
      "    for i in range(len(d.columns)):\n",
      "        var = d.columns[i]\n",
      "        if d[var].dtype == 'int64':\n",
      "            d[var] = d[var].fillna(d[var].mode().iloc[0])\n",
      "        else:\n",
      "            d[var] = d[var].fillna(d[var].mean())\n",
      "    Xtrain, Xtest, Ytrain, Ytest, IDtrain, IDtest = makeXY('gpa', d)\n",
      "    r = RandomForestRegressor(n_estimators=100)\n",
      "    r.fit(Xtrain, Ytrain)\n",
      "    r_pred = r.predict(Xtest)\n",
      "    print i, mean_squared_error(r_pred, Ytest)\n",
      "37/113:\n",
      "d0 = dropsame(background).select_dtypes(include=['float64', 'int64'])\n",
      "mindata = [0, 424, 424*2, 424*3, 424*4, 424*5] # 100%, 90%, 80%, 70%, 60%, 50%\n",
      "for i in range(len(mindata)):\n",
      "    d = d0.dropna(thresh=mindata[i], axis=1)\n",
      "    for i in range(len(d.columns)):\n",
      "        var = d.columns[i]\n",
      "        if d[var].dtype == 'int64':\n",
      "            d[var] = d[var].fillna(d[var].mode().iloc[0])\n",
      "        else:\n",
      "            d[var] = d[var].fillna(d[var].mean())\n",
      "        print d.isnull().sum().sum()\n",
      "    Xtrain, Xtest, Ytrain, Ytest, IDtrain, IDtest = makeXY('gpa', d)\n",
      "    r = RandomForestRegressor(n_estimators=100)\n",
      "    r.fit(Xtrain, Ytrain)\n",
      "    r_pred = r.predict(Xtest)\n",
      "    print i, mean_squared_error(r_pred, Ytest)\n",
      "37/114:\n",
      "mindata = [0, 424, 424*2, 424*3, 424*4, 424*5] # 100%, 90%, 80%, 70%, 60%, 50%\n",
      "for i in range(len(mindata)):\n",
      "    d = dropsame(background).select_dtypes(include=['float64', 'int64'])\n",
      "    d = d.dropna(thresh=mindata[i], axis=1)\n",
      "    for i in range(len(d.columns)):\n",
      "        var = d.columns[i]\n",
      "        if d[var].dtype == 'int64':\n",
      "            d[var] = d[var].fillna(d[var].mode().iloc[0])\n",
      "        else:\n",
      "            d[var] = d[var].fillna(d[var].mean())\n",
      "        print d.isnull().sum().sum()\n",
      "    Xtrain, Xtest, Ytrain, Ytest, IDtrain, IDtest = makeXY('gpa', d)\n",
      "    r = RandomForestRegressor(n_estimators=100)\n",
      "    r.fit(Xtrain, Ytrain)\n",
      "    r_pred = r.predict(Xtest)\n",
      "    print i, mean_squared_error(r_pred, Ytest)\n",
      "38/1:\n",
      "import pandas as pd\n",
      "import matplotlib.pyplot as plt\n",
      "import seaborn as sns\n",
      "import re\n",
      "import numpy as np\n",
      "import math\n",
      "import ff\n",
      "38/2:\n",
      "from sklearn.linear_model import LinearRegression, RidgeCV, LassoCV, MultiTaskLasso, ElasticNetCV, MultiTaskElasticNet\n",
      "from sklearn.ensemble import RandomForestRegressor, AdaBoostRegressor\n",
      "from sklearn.gaussian_process import GaussianProcessRegressor\n",
      "from sklearn.svm import SVC, LinearSVC, SVR\n",
      "from sklearn.metrics import mean_squared_error, brier_score_loss\n",
      "from sklearn.feature_selection import SelectFromModel\n",
      "38/3:\n",
      "data = pd.read_csv('output.csv', low_memory=False)\n",
      "data.info()\n",
      "38/4: data.head()\n",
      "38/5:\n",
      "def dropsame(d):\n",
      "    nunique = d.apply(pd.Series.nunique)\n",
      "    cols_to_drop = nunique[nunique == 1].index\n",
      "    return d.drop(cols_to_drop, axis=1)\n",
      "38/6:\n",
      "# drop columns where every value is the same\n",
      "nunique = data.apply(pd.Series.nunique)\n",
      "cols_to_drop = nunique[nunique == 1].index\n",
      "data = data.drop(cols_to_drop, axis=1)\n",
      "data.head()\n",
      "38/7:\n",
      "# drop highly correlated features\n",
      "# corr_matrix = data.corr().abs()\n",
      "# upper = corr_matrix.where(np.triu(np.ones(corr_matrix.shape), k=1).astype(np.bool))\n",
      "# to_drop = [column for column in upper.columns if any(upper[column] > 0.98)]\n",
      "# data_corr = data.drop(data.columns[to_drop], axis=1)\n",
      "# data_corr.head()\n",
      "38/8:\n",
      "background = pd.read_csv('background.csv', low_memory=False)\n",
      "background.info()\n",
      "38/9: background.head()\n",
      "38/10: data2 = dropsame(background).select_dtypes(include=['float64', 'int64'])\n",
      "38/11: data2.head()\n",
      "38/12:\n",
      "# drop columns if more than 90% are NaN\n",
      "data2 = data2.dropna(thresh=424, axis=1)\n",
      "data2.head()\n",
      "38/13: data2.isnull().sum().sum()\n",
      "38/14:\n",
      "# impute\n",
      "for i in range(len(data2.columns)):\n",
      "    var = data2.columns[i]\n",
      "    if data2[var].dtype == 'int64':\n",
      "        data2[var] = data2[var].fillna(data2[var].mode().iloc[0])\n",
      "    else:\n",
      "        data2[var] = data2[var].fillna(data2[var].mean())\n",
      "38/15: data2.isnull().sum().sum()\n",
      "38/16:\n",
      "d0 = dropsame(background).select_dtypes(include=['float64', 'int64'])\n",
      "mindata = [0, 424, 424*2, 424*3, 424*4, 424*5] # 100%, 90%, 80%, 70%, 60%, 50%\n",
      "for i in range(len(mindata)):\n",
      "    d = d0.dropna(thresh=mindata[i], axis=1)\n",
      "    for i in range(len(d.columns)):\n",
      "        var = d.columns[i]\n",
      "        if d[var].dtype == 'int64':\n",
      "            d[var] = d[var].fillna(d[var].mode().iloc[0])\n",
      "        else:\n",
      "            d[var] = d[var].fillna(d[var].mean())\n",
      "        print d.isnull().sum().sum()\n",
      "    Xtrain, Xtest, Ytrain, Ytest, IDtrain, IDtest = makeXY('gpa', d)\n",
      "    r = RandomForestRegressor(n_estimators=100)\n",
      "    r.fit(Xtrain, Ytrain)\n",
      "    r_pred = r.predict(Xtest)\n",
      "    print i, mean_squared_error(r_pred, Ytest)\n",
      "38/17:\n",
      "background = pd.read_csv('background.csv', low_memory=False)\n",
      "background.info()\n",
      "38/18: background.head()\n",
      "38/19:\n",
      "d0 = dropsame(background).select_dtypes(include=['float64', 'int64'])\n",
      "mindata = [0, 424, 424*2, 424*3, 424*4, 424*5] # 100%, 90%, 80%, 70%, 60%, 50%\n",
      "for i in range(len(mindata)):\n",
      "    d = d0.dropna(thresh=mindata[i], axis=1)\n",
      "    for i in range(len(d.columns)):\n",
      "        var = d.columns[i]\n",
      "        if d[var].dtype == 'int64':\n",
      "            d[var] = d[var].fillna(d[var].mode().iloc[0])\n",
      "        else:\n",
      "            d[var] = d[var].fillna(d[var].mean())\n",
      "        print d.isnull().sum().sum()\n",
      "    Xtrain, Xtest, Ytrain, Ytest, IDtrain, IDtest = makeXY('gpa', d)\n",
      "    r = RandomForestRegressor(n_estimators=100)\n",
      "    r.fit(Xtrain, Ytrain)\n",
      "    r_pred = r.predict(Xtest)\n",
      "    print i, mean_squared_error(r_pred, Ytest)\n",
      "38/20:\n",
      "mindata = [0, 424, 424*2, 424*3, 424*4, 424*5] # 100%, 90%, 80%, 70%, 60%, 50%\n",
      "for i in range(len(mindata)):\n",
      "    d0 = dropsame(background).select_dtypes(include=['float64', 'int64'])\n",
      "    d0 = d0.dropna(thresh=mindata[i], axis=1)\n",
      "    for i in range(len(d0.columns)):\n",
      "        var = d0.columns[i]\n",
      "        if d0[var].dtype == 'int64':\n",
      "            d0[var] = d0[var].fillna(d0[var].mode().iloc[0])\n",
      "        else:\n",
      "            d0[var] = d0[var].fillna(d0[var].mean())\n",
      "        print d0.isnull().sum().sum()\n",
      "    Xtrain, Xtest, Ytrain, Ytest, IDtrain, IDtest = makeXY('gpa', d)\n",
      "    r = RandomForestRegressor(n_estimators=100)\n",
      "    r.fit(Xtrain, Ytrain)\n",
      "    r_pred = r.predict(Xtest)\n",
      "    print i, mean_squared_error(r_pred, Ytest)\n",
      "38/21:\n",
      "mindata = [0, 424, 424*2, 424*3, 424*4, 424*5] # 100%, 90%, 80%, 70%, 60%, 50%\n",
      "for i in range(len(mindata)):\n",
      "    d0 = dropsame(background).select_dtypes(include=['float64', 'int64'])\n",
      "    d0 = d0.dropna(thresh=mindata[i], axis=1)\n",
      "    for i in range(len(d0.columns)):\n",
      "        var = d0.columns[i]\n",
      "        if d0[var].dtype == 'int64':\n",
      "            d0[var] = d0[var].fillna(d0[var].mode().iloc[0])\n",
      "        else:\n",
      "            d0[var] = d0[var].fillna(d0[var].mean())\n",
      "    print d0.isnull().sum().sum()\n",
      "    Xtrain, Xtest, Ytrain, Ytest, IDtrain, IDtest = makeXY('gpa', d)\n",
      "    r = RandomForestRegressor(n_estimators=100)\n",
      "    r.fit(Xtrain, Ytrain)\n",
      "    r_pred = r.predict(Xtest)\n",
      "    print i, mean_squared_error(r_pred, Ytest)\n",
      "38/22:\n",
      "# make X and Y dataframes for test and train from inputdata \n",
      "def makeXY(field, inputdata):\n",
      "    IDs_train = []\n",
      "    Y_train = []\n",
      "    for i in range(len(train)):\n",
      "        if not np.isnan(train[field][i]):\n",
      "            IDs_train.append(train['challengeID'][i]-1)\n",
      "            Y_train.append(train[field][i])\n",
      "    IDs_test = []\n",
      "    Y_test = []\n",
      "    for i in range(len(test)):\n",
      "        if not np.isnan(test[field][i]):\n",
      "            IDs_test.append(test['challengeID'][i]-1)\n",
      "            Y_test.append(test[field][i])\n",
      "    X_train = inputdata.drop('challengeID', axis=1).loc[IDs_train].select_dtypes(include=['float64', 'int64'])\n",
      "    X_test = inputdata.drop('challengeID', axis=1).loc[IDs_test].select_dtypes(include=['float64', 'int64'])\n",
      "    return X_train, X_test, Y_train, Y_test, IDs_train, IDs_test\n",
      "38/23:\n",
      "background = pd.read_csv('background.csv', low_memory=False)\n",
      "background.info()\n",
      "38/24:\n",
      "mindata = [0, 424, 424*2, 424*3, 424*4, 424*5] # 100%, 90%, 80%, 70%, 60%, 50%\n",
      "for i in range(len(mindata)):\n",
      "    d0 = dropsame(background).select_dtypes(include=['float64', 'int64'])\n",
      "    d0 = d0.dropna(thresh=mindata[i], axis=1)\n",
      "    for i in range(len(d0.columns)):\n",
      "        var = d0.columns[i]\n",
      "        if d0[var].dtype == 'int64':\n",
      "            d0[var] = d0[var].fillna(d0[var].mode().iloc[0])\n",
      "        else:\n",
      "            d0[var] = d0[var].fillna(d0[var].mean())\n",
      "    print d0.isnull().sum().sum()\n",
      "    Xtrain, Xtest, Ytrain, Ytest, IDtrain, IDtest = makeXY('gpa', d)\n",
      "    r = RandomForestRegressor(n_estimators=100)\n",
      "    r.fit(Xtrain, Ytrain)\n",
      "    r_pred = r.predict(Xtest)\n",
      "    print i, mean_squared_error(r_pred, Ytest)\n",
      "38/25:\n",
      "mindata = [0, 424, 424*2, 424*3, 424*4, 424*5] # 100%, 90%, 80%, 70%, 60%, 50%\n",
      "for i in range(len(mindata)):\n",
      "    d0 = dropsame(background).select_dtypes(include=['float64', 'int64'])\n",
      "    d0 = d0.dropna(thresh=mindata[i], axis=1)\n",
      "    for j in range(len(d0.columns)):\n",
      "        var = d0.columns[j]\n",
      "        if d0[var].dtype == 'int64':\n",
      "            d0[var] = d0[var].fillna(d0[var].mode().iloc[0])\n",
      "        else:\n",
      "            d0[var] = d0[var].fillna(d0[var].mean())\n",
      "    print d0.isnull().sum().sum()\n",
      "    Xtrain, Xtest, Ytrain, Ytest, IDtrain, IDtest = makeXY('gpa', d)\n",
      "    r = RandomForestRegressor(n_estimators=100)\n",
      "    r.fit(Xtrain, Ytrain)\n",
      "    r_pred = r.predict(Xtest)\n",
      "    print i, mean_squared_error(r_pred, Ytest)\n",
      "38/26: data2 = dropsame(background).select_dtypes(include=['float64', 'int64'])\n",
      "38/27: data2.head()\n",
      "38/28:\n",
      "# drop columns if more than 90% are NaN\n",
      "data2 = data2.dropna(thresh=424, axis=1)\n",
      "data2.head()\n",
      "38/29: data2.isnull().sum().sum()\n",
      "38/30:\n",
      "# impute\n",
      "for i in range(len(data2.columns)):\n",
      "    var = data2.columns[i]\n",
      "    if data2[var].dtype == 'int64':\n",
      "        data2[var] = data2[var].fillna(data2[var].mode().iloc[0])\n",
      "    else:\n",
      "        data2[var] = data2[var].fillna(data2[var].mean())\n",
      "38/31: data2.isnull().sum().sum()\n",
      "38/32:\n",
      "mindata = [0, 424, 424*2, 424*3, 424*4, 424*5] # 100%, 90%, 80%, 70%, 60%, 50%\n",
      "for i in range(len(mindata)):\n",
      "    d0 = dropsame(background).select_dtypes(include=['float64', 'int64']).dropna(thresh=mindata[i], axis=1)\n",
      "    for j in range(len(d0.columns)):\n",
      "        var = d0.columns[j]\n",
      "        if d0[var].dtype == 'int64':\n",
      "            d0[var] = d0[var].fillna(d0[var].mode().iloc[0])\n",
      "        else:\n",
      "            d0[var] = d0[var].fillna(d0[var].mean())\n",
      "    print d0.isnull().sum().sum()\n",
      "    Xtrain, Xtest, Ytrain, Ytest, IDtrain, IDtest = makeXY('gpa', d0)\n",
      "    r = RandomForestRegressor(n_estimators=100)\n",
      "    r.fit(Xtrain, Ytrain)\n",
      "    r_pred = r.predict(Xtest)\n",
      "    print i, mean_squared_error(r_pred, Ytest)\n",
      "38/33:\n",
      "train = pd.read_csv('train.csv')\n",
      "train.info()\n",
      "38/34:\n",
      "test = pd.read_csv('test.csv')\n",
      "test.info()\n",
      "38/35:\n",
      "train = pd.read_csv('train.csv')\n",
      "train.info()\n",
      "38/36:\n",
      "test = pd.read_csv('test.csv')\n",
      "test.info()\n",
      "38/37:\n",
      "background = pd.read_csv('background.csv', low_memory=False)\n",
      "background.info()\n",
      "38/38:\n",
      "mindata = [0, 424, 424*2, 424*3, 424*4, 424*5] # 100%, 90%, 80%, 70%, 60%, 50%\n",
      "for i in range(len(mindata)):\n",
      "    d0 = dropsame(background).select_dtypes(include=['float64', 'int64']).dropna(thresh=mindata[i], axis=1)\n",
      "    for j in range(len(d0.columns)):\n",
      "        var = d0.columns[j]\n",
      "        if d0[var].dtype == 'int64':\n",
      "            d0[var] = d0[var].fillna(d0[var].mode().iloc[0])\n",
      "        else:\n",
      "            d0[var] = d0[var].fillna(d0[var].mean())\n",
      "    print d0.isnull().sum().sum()\n",
      "    Xtrain, Xtest, Ytrain, Ytest, IDtrain, IDtest = makeXY('gpa', d0)\n",
      "    r = RandomForestRegressor(n_estimators=100)\n",
      "    r.fit(Xtrain, Ytrain)\n",
      "    r_pred = r.predict(Xtest)\n",
      "    print i, mean_squared_error(r_pred, Ytest)\n",
      "38/39: d0.head()\n",
      "38/40: d0.isnull().sum().sum()\n",
      "38/41:\n",
      "for i in range(len(d0.columns)):\n",
      "    var = d0.columns[i]\n",
      "    if d0[var].dtype == 'int64':\n",
      "        d0[var] = d0[var].fillna(d0[var].mode().iloc[0])\n",
      "    else:\n",
      "        d0[var] = d0[var].fillna(d0[var].mean())\n",
      "38/42: d0.isnull().sum().sum()\n",
      "38/43:\n",
      "def dropsame(d):\n",
      "    nunique = d.apply(pd.Series.nunique)\n",
      "    cols_to_drop = nunique[nunique == 1].index\n",
      "    return d.drop(cols_to_drop, axis=1).copy()\n",
      "38/44:\n",
      "mindata = [0, 424, 424*2, 424*3, 424*4, 424*5] # 100%, 90%, 80%, 70%, 60%, 50%\n",
      "for i in range(len(mindata)):\n",
      "    d0 = dropsame(background).select_dtypes(include=['float64', 'int64']).dropna(thresh=mindata[i], axis=1)\n",
      "    for j in range(len(d0.columns)):\n",
      "        var = d0.columns[j]\n",
      "        if d0[var].dtype == 'int64':\n",
      "            d0[var] = d0[var].fillna(d0[var].mode().iloc[0])\n",
      "        else:\n",
      "            d0[var] = d0[var].fillna(d0[var].mean())\n",
      "    print d0.isnull().sum().sum()\n",
      "    Xtrain, Xtest, Ytrain, Ytest, IDtrain, IDtest = makeXY('gpa', d0)\n",
      "    r = RandomForestRegressor(n_estimators=100)\n",
      "    r.fit(Xtrain, Ytrain)\n",
      "    r_pred = r.predict(Xtest)\n",
      "    print i, mean_squared_error(r_pred, Ytest)\n",
      "38/45:\n",
      "for i in range(len(d0.columns)):\n",
      "    var = d0.columns[i]\n",
      "    if d0[var].dtype == 'int64':\n",
      "        d0[var] = d0[var].fillna(d0[var].mode().iloc[0])\n",
      "    else:\n",
      "        d0[var] = d0[var].fillna(d0[var].mean())\n",
      "38/46: d0.isnull().sum().sum()\n",
      "38/47: data2 = dropsame(background).select_dtypes(include=['float64', 'int64'])\n",
      "38/48:\n",
      "# drop columns if more than 90% are NaN\n",
      "data2 = data2.dropna(thresh=424, axis=1)\n",
      "38/49: data2.isnull().sum().sum()\n",
      "38/50:\n",
      "# impute\n",
      "for i in range(len(data2.columns)):\n",
      "    var = data2.columns[i]\n",
      "    if data2[var].dtype == 'int64':\n",
      "        data2[var] = data2[var].fillna(data2[var].mode().iloc[0])\n",
      "    else:\n",
      "        data2[var] = data2[var].fillna(data2[var].mean())\n",
      "38/51: data2.isnull().sum().sum()\n",
      "38/52: d0 = dropsame(background).select_dtypes(include=['float64', 'int64'])\n",
      "38/53:\n",
      "for i in range(len(d0.columns)):\n",
      "    var = d0.columns[i]\n",
      "    if d0[var].dtype == 'int64':\n",
      "        d0[var] = d0[var].fillna(d0[var].mode().iloc[0])\n",
      "    else:\n",
      "        d0[var] = d0[var].fillna(d0[var].mean())\n",
      "38/54: d0.isnull().sum().sum()\n",
      "39/1:\n",
      "import pandas as pd\n",
      "import matplotlib.pyplot as plt\n",
      "import seaborn as sns\n",
      "import re\n",
      "import numpy as np\n",
      "import math\n",
      "import ff\n",
      "39/2:\n",
      "from sklearn.linear_model import LinearRegression, RidgeCV, LassoCV, MultiTaskLasso, ElasticNetCV, MultiTaskElasticNet\n",
      "from sklearn.ensemble import RandomForestRegressor, AdaBoostRegressor\n",
      "from sklearn.gaussian_process import GaussianProcessRegressor\n",
      "from sklearn.svm import SVC, LinearSVC, SVR\n",
      "from sklearn.metrics import mean_squared_error, brier_score_loss\n",
      "from sklearn.feature_selection import SelectFromModel\n",
      "39/3:\n",
      "data = pd.read_csv('output.csv', low_memory=False)\n",
      "data.info()\n",
      "39/4: data.head()\n",
      "39/5:\n",
      "def dropsame(dat):\n",
      "    nunique = dat.apply(pd.Series.nunique)\n",
      "    cols_to_drop = nunique[nunique == 1].index\n",
      "    return dat.drop(cols_to_drop, axis=1).copy()\n",
      "39/6:\n",
      "# drop columns where every value is the same\n",
      "nunique = data.apply(pd.Series.nunique)\n",
      "cols_to_drop = nunique[nunique == 1].index\n",
      "data = data.drop(cols_to_drop, axis=1)\n",
      "data.head()\n",
      "39/7:\n",
      "# drop highly correlated features\n",
      "# corr_matrix = data.corr().abs()\n",
      "# upper = corr_matrix.where(np.triu(np.ones(corr_matrix.shape), k=1).astype(np.bool))\n",
      "# to_drop = [column for column in upper.columns if any(upper[column] > 0.98)]\n",
      "# data_corr = data.drop(data.columns[to_drop], axis=1)\n",
      "# data_corr.head()\n",
      "39/8:\n",
      "# make X and Y dataframes for test and train from inputdata \n",
      "def makeXY(field, inputdata):\n",
      "    IDs_train = []\n",
      "    Y_train = []\n",
      "    for i in range(len(train)):\n",
      "        if not np.isnan(train[field][i]):\n",
      "            IDs_train.append(train['challengeID'][i]-1)\n",
      "            Y_train.append(train[field][i])\n",
      "    IDs_test = []\n",
      "    Y_test = []\n",
      "    for i in range(len(test)):\n",
      "        if not np.isnan(test[field][i]):\n",
      "            IDs_test.append(test['challengeID'][i]-1)\n",
      "            Y_test.append(test[field][i])\n",
      "    X_train = inputdata.drop('challengeID', axis=1).loc[IDs_train].select_dtypes(include=['float64', 'int64'])\n",
      "    X_test = inputdata.drop('challengeID', axis=1).loc[IDs_test].select_dtypes(include=['float64', 'int64'])\n",
      "    return X_train, X_test, Y_train, Y_test, IDs_train, IDs_test\n",
      "39/9:\n",
      "background = pd.read_csv('background.csv', low_memory=False)\n",
      "background.info()\n",
      "39/10: background.head()\n",
      "39/11:\n",
      "train = pd.read_csv('train.csv')\n",
      "train.info()\n",
      "39/12:\n",
      "test = pd.read_csv('test.csv')\n",
      "test.info()\n",
      "39/13: d0 = dropsame(background).select_dtypes(include=['float64', 'int64'])\n",
      "39/14:\n",
      "for i in range(len(d0.columns)):\n",
      "    var = d0.columns[i]\n",
      "    if d0[var].dtype == 'int64':\n",
      "        d0[var] = d0[var].fillna(d0[var].mode().iloc[0])\n",
      "    else:\n",
      "        d0[var] = d0[var].fillna(d0[var].mean())\n",
      "39/15: d0.isnull().sum().sum()\n",
      "39/16: data2 = dropsame(background).select_dtypes(include=['float64', 'int64'])\n",
      "39/17: data2.head()\n",
      "39/18:\n",
      "# drop columns if more than 90% are NaN\n",
      "data2 = data2.dropna(thresh=424, axis=1)\n",
      "39/19: data2.isnull().sum().sum()\n",
      "39/20:\n",
      "# impute\n",
      "for i in range(len(data2.columns)):\n",
      "    var = data2.columns[i]\n",
      "    if data2[var].dtype == 'int64':\n",
      "        data2[var] = data2[var].fillna(data2[var].mode().iloc[0])\n",
      "    else:\n",
      "        data2[var] = data2[var].fillna(data2[var].mean())\n",
      "39/21: data2.isnull().sum().sum()\n",
      "39/22: d0 = dropsame(background).select_dtypes(include=['float64', 'int64'])\n",
      "39/23: d0 = d0.dropna(thresh=424, axis=1)\n",
      "39/24:\n",
      "for i in range(len(d0.columns)):\n",
      "    var = d0.columns[i]\n",
      "    if d0[var].dtype == 'int64':\n",
      "        d0[var] = d0[var].fillna(d0[var].mode().iloc[0])\n",
      "    else:\n",
      "        d0[var] = d0[var].fillna(d0[var].mean())\n",
      "39/25: d0.isnull().sum().sum()\n",
      "39/26:\n",
      "mindata = [0, 424, 424*2, 424*3, 424*4, 424*5] # 100%, 90%, 80%, 70%, 60%, 50%\n",
      "for i in range(len(mindata)):\n",
      "    d0 = dropsame(background).select_dtypes(include=['float64', 'int64'])\n",
      "    d0 = d0.dropna(thresh=mindata[i], axis=1)\n",
      "    for j in range(len(d0.columns)):\n",
      "        var = d0.columns[j]\n",
      "        if d0[var].dtype == 'int64':\n",
      "            d0[var] = d0[var].fillna(d0[var].mode().iloc[0])\n",
      "        else:\n",
      "            d0[var] = d0[var].fillna(d0[var].mean())\n",
      "    print d0.isnull().sum().sum()\n",
      "    Xtrain, Xtest, Ytrain, Ytest, IDtrain, IDtest = makeXY('gpa', d0)\n",
      "    r = RandomForestRegressor(n_estimators=100)\n",
      "    r.fit(Xtrain, Ytrain)\n",
      "    r_pred = r.predict(Xtest)\n",
      "    print i, mean_squared_error(r_pred, Ytest)\n",
      "39/27:\n",
      "mindata = [0, 424, 424*2, 424*3, 424*4, 424*5] # 100%, 90%, 80%, 70%, 60%, 50%\n",
      "for i in range(len(mindata)):\n",
      "    d0 = dropsame(background).copy().select_dtypes(include=['float64', 'int64'])\n",
      "    d0 = d0.dropna(thresh=mindata[i], axis=1)\n",
      "    for j in range(len(d0.columns)):\n",
      "        var = d0.columns[j]\n",
      "        if d0[var].dtype == 'int64':\n",
      "            d0[var] = d0[var].fillna(d0[var].mode().iloc[0])\n",
      "        else:\n",
      "            d0[var] = d0[var].fillna(d0[var].mean())\n",
      "    print d0.isnull().sum().sum()\n",
      "    Xtrain, Xtest, Ytrain, Ytest, IDtrain, IDtest = makeXY('gpa', d0)\n",
      "    r = RandomForestRegressor(n_estimators=100)\n",
      "    r.fit(Xtrain, Ytrain)\n",
      "    r_pred = r.predict(Xtest)\n",
      "    print i, mean_squared_error(r_pred, Ytest)\n",
      "39/28: d0 = dropsame(background).select_dtypes(include=['float64', 'int64'])\n",
      "39/29: d0 = d0.dropna(thresh=0, axis=1)\n",
      "39/30:\n",
      "for i in range(len(d0.columns)):\n",
      "    var = d0.columns[i]\n",
      "    if d0[var].dtype == 'int64':\n",
      "        d0[var] = d0[var].fillna(d0[var].mode().iloc[0])\n",
      "    else:\n",
      "        d0[var] = d0[var].fillna(d0[var].mean())\n",
      "39/31: d0.isnull().sum().sum()\n",
      "39/32: d0 = dropsame(background).select_dtypes(include=['float64', 'int64'])\n",
      "39/33: d0 = d0.dropna(thresh=1, axis=1)\n",
      "39/34:\n",
      "for i in range(len(d0.columns)):\n",
      "    var = d0.columns[i]\n",
      "    if d0[var].dtype == 'int64':\n",
      "        d0[var] = d0[var].fillna(d0[var].mode().iloc[0])\n",
      "    else:\n",
      "        d0[var] = d0[var].fillna(d0[var].mean())\n",
      "39/35: d0.isnull().sum().sum()\n",
      "39/36:\n",
      "mindata = [1, 424, 424*2, 424*3, 424*4, 424*5] # 100%, 90%, 80%, 70%, 60%, 50%\n",
      "for i in range(len(mindata)):\n",
      "    d0 = dropsame(background).select_dtypes(include=['float64', 'int64'])\n",
      "    d0 = d0.dropna(thresh=mindata[i], axis=1)\n",
      "    for j in range(len(d0.columns)):\n",
      "        var = d0.columns[j]\n",
      "        if d0[var].dtype == 'int64':\n",
      "            d0[var] = d0[var].fillna(d0[var].mode().iloc[0])\n",
      "        else:\n",
      "            d0[var] = d0[var].fillna(d0[var].mean())\n",
      "    print d0.isnull().sum().sum()\n",
      "    Xtrain, Xtest, Ytrain, Ytest, IDtrain, IDtest = makeXY('gpa', d0)\n",
      "    r = RandomForestRegressor(n_estimators=100)\n",
      "    r.fit(Xtrain, Ytrain)\n",
      "    r_pred = r.predict(Xtest)\n",
      "    print i, mean_squared_error(r_pred, Ytest)\n",
      "39/37:\n",
      "x = [1, .9, .8, .7, .6, .5]\n",
      "y = [0.35185, 0.35481, 0.35339, 0.35593, 0.35286, 0.35354]\n",
      "plot.plot(x, y, \"o\")\n",
      "39/38:\n",
      "x = [1, .9, .8, .7, .6, .5]\n",
      "y = [0.35185, 0.35481, 0.35339, 0.35593, 0.35286, 0.35354]\n",
      "plt.plot(x, y, \"o\")\n",
      "39/39:\n",
      "x = [1, .9, .8, .7, .6, .5]\n",
      "y = [0.35185, 0.35481, 0.35339, 0.35593, 0.35286, 0.35354]\n",
      "plt.plot(x, y)\n",
      "39/40:\n",
      "# mindata = [1, 424, 424*2, 424*3, 424*4, 424*5] # 100%, 90%, 80%, 70%, 60%, 50%\n",
      "mindata = [424*x for x in [6, 7, 8, 9]]\n",
      "mse = []\n",
      "for i in range(len(mindata)):\n",
      "    d0 = dropsame(background).select_dtypes(include=['float64', 'int64'])\n",
      "    d0 = d0.dropna(thresh=mindata[i], axis=1)\n",
      "    for j in range(len(d0.columns)):\n",
      "        var = d0.columns[j]\n",
      "        if d0[var].dtype == 'int64':\n",
      "            d0[var] = d0[var].fillna(d0[var].mode().iloc[0])\n",
      "        else:\n",
      "            d0[var] = d0[var].fillna(d0[var].mean())\n",
      "    Xtrain, Xtest, Ytrain, Ytest, IDtrain, IDtest = makeXY('gpa', d0)\n",
      "    r = RandomForestRegressor(n_estimators=100)\n",
      "    r.fit(Xtrain, Ytrain)\n",
      "    r_pred = r.predict(Xtest)\n",
      "    e = mean_squared_error(r_pred, Ytest)\n",
      "    mse.append(e)\n",
      "    print i, e\n",
      "39/41:\n",
      "x = [1, .9, .8, .7, .6, .5, .4, .3, .2, .1]\n",
      "y = [0.35185, 0.35481, 0.35339, 0.35593, 0.35286, 0.35354, 0.35802, 0.35052, 0.35176, 0.35141]\n",
      "plt.plot(x, y)\n",
      "39/42: Xtrain.shape\n",
      "39/43: Xtest.shape\n",
      "39/44: Xtrain.shape\n",
      "39/45:\n",
      "# drop columns if more than 90% are NaN\n",
      "data2 = data2.dropna(thresh=424*7, axis=1)\n",
      "39/46: data2.isnull().sum().sum()\n",
      "39/47:\n",
      "# impute\n",
      "for i in range(len(data2.columns)):\n",
      "    var = data2.columns[i]\n",
      "    if data2[var].dtype == 'int64':\n",
      "        data2[var] = data2[var].fillna(data2[var].mode().iloc[0])\n",
      "    else:\n",
      "        data2[var] = data2[var].fillna(data2[var].mean())\n",
      "39/48: data2.isnull().sum().sum()\n",
      "39/49: data2 = dropsame(background).select_dtypes(include=['float64', 'int64'])\n",
      "39/50: data2.head()\n",
      "39/51:\n",
      "# drop columns if more than 90% are NaN\n",
      "data2 = data2.dropna(thresh=424*7, axis=1)\n",
      "39/52: data2.isnull().sum().sum()\n",
      "39/53:\n",
      "# impute\n",
      "for i in range(len(data2.columns)):\n",
      "    var = data2.columns[i]\n",
      "    if data2[var].dtype == 'int64':\n",
      "        data2[var] = data2[var].fillna(data2[var].mode().iloc[0])\n",
      "    else:\n",
      "        data2[var] = data2[var].fillna(data2[var].mean())\n",
      "39/54: data2.isnull().sum().sum()\n",
      "39/55: Xgpa_train2, Xgpa_test2, Ygpa_train2, Ygpa_test2, IDgpa_train2, IDgpa_test2 = makeXY('gpa', data2)\n",
      "39/56:\n",
      "rf2 = RandomForestRegressor(n_estimators=100)\n",
      "rf2.fit(Xgpa_train2, Ygpa_train2)\n",
      "rf2_pred = rf2.predict(Xgpa_test2)\n",
      "mean_squared_error(rf2_pred, Ygpa_test2)\n",
      "39/57:\n",
      "n = [20*(x+1) for x in range(10)]\n",
      "mse = []\n",
      "for i in range(len(n)):\n",
      "    r = RandomForestRegressor(n_estimators=n[i])\n",
      "    r.fit(Xgpa_train2, Ygpa_train2)\n",
      "    r_pred = r.predict(Xgpa_test2)\n",
      "    e = mean_squared_error(r_pred, Ygpa_test2)\n",
      "    mse.append(e)\n",
      "    print i, e\n",
      "26/618:\n",
      "arrtop = getarr(Gtop)\n",
      "regs(arrtop)\n",
      "26/619:\n",
      "arrtop = getarr(Gtop)\n",
      "arrtop[4] = [0 if np.isnan(a) else a for a in arrtop[4]]\n",
      "regs(arrtop)\n",
      "39/58:\n",
      "n = [20*x for x in [8,9,10]]\n",
      "mse = []\n",
      "for i in range(len(n)):\n",
      "    r = RandomForestRegressor(n_estimators=n[i])\n",
      "    r.fit(Xgpa_train2, Ygpa_train2)\n",
      "    r_pred = r.predict(Xgpa_test2)\n",
      "    e = mean_squared_error(r_pred, Ygpa_test2)\n",
      "    mse.append(e)\n",
      "    print i, e\n",
      "26/620: arrtop = getarr(Gtop)\n",
      "26/621: arrtop[4]\n",
      "26/622: arrtop[4] = [0 if a == None else a for a in arrtop[4]]\n",
      "26/623: arrtop[4]\n",
      "26/624: regs(arrtop)\n",
      "26/625:\n",
      "for i in range(len(arrtop)):\n",
      "    arrtop[i] = [0 if a == None else a for a in arrtop[i]]\n",
      "26/626: regs(arrtop)\n",
      "26/627:\n",
      "arrlow = getarr(Glow)\n",
      "for i in range(len(arrlow)):\n",
      "    arrlow[i] = [0 if a == None else a for a in arrlow[i]]\n",
      "regs(arrlow)\n",
      "26/628: arrlow[4]\n",
      "26/629:\n",
      "def getarr(graph, alist):\n",
      "    degree = nx.degree_centrality(graph)\n",
      "    degrees = [degree.get(a) for a in alist]\n",
      "    eigenvector = nx.eigenvector_centrality(graph)\n",
      "    eigenvectors = [eigenvector.get(a) for a in alist]\n",
      "    close = nx.closeness_centrality(graph)\n",
      "    closes = [close.get(a) for a in alist]\n",
      "    between = nx.betweenness_centrality(graph)\n",
      "    betweens = [between.get(a) for a in alist]\n",
      "    effective = nx.algorithms.structuralholes.effective_size(graph)\n",
      "    effectives = [effective.get(a) for a in alist]\n",
      "    cluster = nx.algorithms.cluster.clustering(graph)\n",
      "    clusters = [cluster.get(a) for a in alist]\n",
      "    return [degrees, eigenvectors, closes, betweens, effectives, clusters]\n",
      "26/630:\n",
      "def regs(arrs, box):\n",
      "    for arr in arrs:\n",
      "        slope, intercept, r_value, p_value, std_err = stats.linregress(arr,box)\n",
      "        print p_value, r_value\n",
      "26/631: arrtop = getarr(Gtop, Gtop.nodes())\n",
      "39/59:\n",
      "n = [20*x for x in [9,10]]\n",
      "mse = []\n",
      "for i in range(len(n)):\n",
      "    r = RandomForestRegressor(n_estimators=n[i])\n",
      "    r.fit(Xgpa_train2, Ygpa_train2)\n",
      "    r_pred = r.predict(Xgpa_test2)\n",
      "    e = mean_squared_error(r_pred, Ygpa_test2)\n",
      "    mse.append(e)\n",
      "    print i, e\n",
      "26/632: arrtop[4] = [0 if np.isnan(a) else a for a in arrtop[4]]\n",
      "26/633: boxtop = [boxofficedict.get(a) for a in Gtop.nodes()]\n",
      "26/634: regs(arrtop, boxtop)\n",
      "26/635:\n",
      "arrlow = getarr(Glow, Glow.nodes())\n",
      "arrlow[4] = [0 if np.isnan(a) else a for a in arrlow[4]]\n",
      "boxlow = [boxofficedict.get(a) for a in Glow.nodes()]\n",
      "regs(arrlow, boxlow)\n",
      "39/60:\n",
      "y = [0.358418, 0.361059, 0.357367, 0.352876, 0.350095, 0.349639, 0.351140, 0.351885, 0.349676]\n",
      "plt.plot([20*(x+1) for x in range(9)], y)\n",
      "39/61:\n",
      "def n_estimators():\n",
      "    n = [20*x for x in range(10)]\n",
      "    mse = []\n",
      "    for i in range(len(n)):\n",
      "        r = RandomForestRegressor(n_estimators=n[i])\n",
      "        r.fit(Xgpa_train2, Ygpa_train2)\n",
      "        r_pred = r.predict(Xgpa_test2)\n",
      "        e = mean_squared_error(r_pred, Ygpa_test2)\n",
      "        mse.append(e)\n",
      "        print i, e\n",
      "39/62: # n_estimators(Xgpa_train2, Ygpa_train2, Xgpa_test2, Ygpa_test2)\n",
      "39/63:\n",
      "def n_estimators(Xtrain, Ytrain, Xtest, Ytest):\n",
      "    n = [20*x for x in range(10)]\n",
      "    mse = []\n",
      "    for i in range(len(n)):\n",
      "        r = RandomForestRegressor(n_estimators=n[i])\n",
      "        r.fit(Xtrain, Ytrain)\n",
      "        r_pred = r.predict(Xtest)\n",
      "        e = mean_squared_error(r_pred, Ytest)\n",
      "        mse.append(e)\n",
      "        print i, e\n",
      "    return mse\n",
      "39/64: Xgrit_train2, Xgrit_test2, Ygrit_train2, Ygrit_test2, IDgrit_train2, IDgrit_test2 = makeXY('grit', data2)\n",
      "39/65: n_estimators(Xgpa_train2, Ygpa_train2, Xgpa_test2, Ygrit_test2)\n",
      "39/66: n_estimators(Xgpa_train2, Ygpa_train2, Xgpa_test2, Ygrit_test2)\n",
      "39/67:\n",
      "def n_estimators(Xtrain, Ytrain, Xtest, Ytest):\n",
      "    n = [20*(x+1) for x in range(9)]\n",
      "    mse = []\n",
      "    for i in range(len(n)):\n",
      "        r = RandomForestRegressor(n_estimators=n[i])\n",
      "        r.fit(Xtrain, Ytrain)\n",
      "        r_pred = r.predict(Xtest)\n",
      "        e = mean_squared_error(r_pred, Ytest)\n",
      "        mse.append(e)\n",
      "        print i, e\n",
      "    return mse\n",
      "39/68: n_estimators(Xgpa_train2, Ygpa_train2, Xgpa_test2, Ygrit_test2)\n",
      "39/69: n_estimators(Xgrit_train2, Ygrit_train2, Xgrit_test2, Ygrit_test2)\n",
      "39/70:\n",
      "def n_estimators(Xtrain, Ytrain, Xtest, Ytest):\n",
      "    n = [20*(x+1) for x in range(9)]\n",
      "    mse = []\n",
      "    for i in range(len(n)):\n",
      "        r = RandomForestRegressor(n_estimators=n[i])\n",
      "        r.fit(Xtrain, Ytrain)\n",
      "        r_pred = r.predict(Xtest)\n",
      "        e = mean_squared_error(r_pred, Ytest)\n",
      "        mse.append(e)\n",
      "        print i, e\n",
      "39/71: n_estimators(Xgrit_train2, Ygrit_train2, Xgrit_test2, Ygrit_test2)\n",
      "39/72:\n",
      "def n_estimators(Xtrain, Ytrain, Xtest, Ytest, n):\n",
      "#     n = [20*(x+1) for x in range(9)]\n",
      "    mse = []\n",
      "    for i in range(len(n)):\n",
      "        r = RandomForestRegressor(n_estimators=n[i])\n",
      "        r.fit(Xtrain, Ytrain)\n",
      "        r_pred = r.predict(Xtest)\n",
      "        e = mean_squared_error(r_pred, Ytest)\n",
      "        mse.append(e)\n",
      "        print i, e\n",
      "39/73: n_estimators(Xgrit_train2, Ygrit_train2, Xgrit_test2, Ygrit_test2, [60, 80])\n",
      "39/74: n_estimators(Xgrit_train2, Ygrit_train2, Xgrit_test2, Ygrit_test2, [100, 120])\n",
      "39/75: n_estimators(Xgrit_train2, Ygrit_train2, Xgrit_test2, Ygrit_test2, [140, 160])\n",
      "39/76: n_estimators(Xgrit_train2, Ygrit_train2, Xgrit_test2, Ygrit_test2, [180])\n",
      "39/77:\n",
      "y = [0.248987, 0.246823, 0.246088, 0.241562, 0.239581, 0.242524, 0.240435]\n",
      "plt.plot([20*(x+1) for x in range(7)], y)\n",
      "39/78:\n",
      "y = [0.248987, 0.246823, 0.246088, 0.241562, 0.239581, 0.242524, 0.241\n",
      "     435]\n",
      "plt.plot([20*(x+1) for x in range(7)], y)\n",
      "39/79:\n",
      "y = [0.248987, 0.246823, 0.246088, 0.241562, 0.239581, 0.242524, 0.241435]\n",
      "plt.plot([20*(x+1) for x in range(7)], y)\n",
      "39/80: n_estimators(Xgrit_train2, Ygrit_train2, Xgrit_test2, Ygrit_test2, [20, 40, 60, 80, 100, 120, 140])\n",
      "39/81: # n_estimators(Xgrit_train2, Ygrit_train2, Xgrit_test2, Ygrit_test2, [20, 40, 60, 80, 100, 120, 140])\n",
      "39/82: # n_estimators(Xgrit_train2, Ygrit_train2, Xgrit_test2, Ygrit_test2, [20, 40, 60, 80, 100, 120, 140, 160])\n",
      "39/83: Xmh_train2, Xmh_test2, Ymh_train2, Ymh_test2, IDmh_train2, IDmh_test2 = makeXY('materialHardship', data2)\n",
      "39/84: n_estimators(Xmh_train2, Ymh_train2, Xmh_test2, Ymh_test2, [20, 40])\n",
      "26/636:\n",
      "lowdict = {}\n",
      "for (u, v) in G.edges():\n",
      "    if u in academylist and v not in academylist:\n",
      "        if v in lowdict:\n",
      "            lowdict[v] += 1\n",
      "        else:\n",
      "            lowdict[v] = 1\n",
      "    elif v in academylist and u not in academylist:\n",
      "        if u in lowdict:\n",
      "            lowdict[u] += 1\n",
      "        else:\n",
      "            lowdict[u] = 1\n",
      "low = [1 if a in lowdict else 0 for a in Glow.nodes()]\n",
      "slope, intercept, r_value, p_value, std_err = stats.linregress(low,boxlow)\n",
      "print p_value, r_value\n",
      "26/637: sum(low)\n",
      "26/638: sum(low), len(low)\n",
      "26/639:\n",
      "low2 = [lowdict.get(a) if a in lowdict else 0 for a in Glow.nodes()]\n",
      "slope, intercept, r_value, p_value, std_err = stats.linregress(low2,boxlow)\n",
      "print p_value, r_value\n",
      "26/640: sum(low)\n",
      "26/641: sum(low2)\n",
      "26/642: sum(low2), np.mean(low)\n",
      "26/643: sum(low), len(low), np.mean(low)\n",
      "26/644: sum(low2), np.mean(low2), np.std(low2)\n",
      "26/645: plot.plot(low2, boxlow)\n",
      "26/646: plot.plot(low2, boxlow, \"o\")\n",
      "26/647:\n",
      "plot.figure()\n",
      "plot.plot(low2, boxlow, \"o\")\n",
      "plot.plot(low2, [slope*x + intercept for x in low2], \"black\")\n",
      "plot.title(\"Box Office Gross vs Number of Top Actors Connected To\")\n",
      "plot.show()\n",
      "39/85: n_estimators(Xmh_train2, Ymh_train2, Xmh_test2, Ymh_test2, [60, 80])\n",
      "26/648: Gtop.degree\n",
      "26/649:\n",
      "top2 = [Gtop.degree.get(a) for a in Gtop.nodes()]\n",
      "slope, intercept, r_value, p_value, std_err = stats.linregress(low2,boxtop)\n",
      "print p_value, r_value\n",
      "26/650:\n",
      "top2 = [Gtop.degree[a] for a in Gtop.nodes()]\n",
      "slope, intercept, r_value, p_value, std_err = stats.linregress(low2,boxtop)\n",
      "print p_value, r_value\n",
      "26/651:\n",
      "top2 = [Gtop.degree[a] for a in Gtop.nodes()]\n",
      "slope, intercept, r_value, p_value, std_err = stats.linregress(top2,boxtop)\n",
      "print p_value, r_value\n",
      "26/652: np.mean(top2), np.std(top2)\n",
      "26/653: arrtop = getarr(G, Gtop.nodes())\n",
      "26/654: arrtop[4] = [0 if np.isnan(a) else a for a in arrtop[4]]\n",
      "26/655: regs(arrtop, boxtop)\n",
      "26/656:\n",
      "arrlow = getarr(G, Glow.nodes())\n",
      "arrlow[4] = [0 if np.isnan(a) else a for a in arrlow[4]]\n",
      "boxlow = [boxofficedict.get(a) for a in Glow.nodes()]\n",
      "regs(arrlow, boxlow)\n",
      "39/86: n_estimators(Xmh_train2, Ymh_train2, Xmh_test2, Ymh_test2, [100, 120])\n",
      "26/657:\n",
      "arrlow = getarr(G, Glow.nodes())\n",
      "arrlow[4] = [0 if np.isnan(a) else a for a in arrlow[4]]\n",
      "boxlow = [boxofficedict.get(a) for a in Glow.nodes()]\n",
      "regs(arrlow, boxlow)\n",
      "39/87: n_estimators(Xmh_train2, Ymh_train2, Xmh_test2, Ymh_test2, [100, 120])\n",
      "39/88:\n",
      "y = [0.021223, 0.020338, 0.020317, 0.020410, 0.020149]\n",
      "plt.plot([20*(x+1) for x in range(len(y))], y)\n",
      "39/89:\n",
      "y = [0.021223, 0.020338, 0.020317, 0.020400, 0.020149]\n",
      "plt.plot([20*(x+1) for x in range(len(y))], y)\n",
      "39/90:\n",
      "y = [0.021223, 0.020338, 0.020317, 0.020320, 0.020149]\n",
      "plt.plot([20*(x+1) for x in range(len(y))], y)\n",
      "39/91:\n",
      "y = [0.021223, 0.020338, 0.020317, 0.020390, 0.020149]\n",
      "plt.plot([20*(x+1) for x in range(len(y))], y)\n",
      "39/92: n_estimators(Xmh_train2, Ymh_train2, Xmh_test2, Ymh_test2, [120])\n",
      "39/93: n_estimators(Xmh_train2, Ymh_train2, Xmh_test2, Ymh_test2, [140])\n",
      "39/94:\n",
      "y = [0.021223, 0.020338, 0.020317, 0.020390, 0.020149, 0.020059, 0.020137]\n",
      "plt.plot([20*(x+1) for x in range(len(y))], y)\n",
      "39/95: n_estimators(Xmh_train2, Ymh_train2, Xmh_test2, Ymh_test2, [160])\n",
      "39/96:\n",
      "y = [0.021223, 0.020338, 0.020317, 0.020390, 0.020149, 0.020059, 0.020137, 0.020284]\n",
      "plt.plot([20*(x+1) for x in range(len(y))], y)\n",
      "39/97: # n_estimators(Xmh_train2, Ymh_train2, Xmh_test2, Ymh_test2, [20*(x+1) for x in range(8)])\n",
      "39/98:\n",
      "y = [0.021223, 0.020338, 0.020317, 0.020390, 0.020149, 0.020059, 0.020137, 0.020284]\n",
      "plt.plot([20*(x+1) for x in range(len(y))], y)\n",
      "39/99:\n",
      "lrR = RidgeCV(cv=5, normalize=True)\n",
      "lrR.fit(Xgpa_train, Ygpa_train)\n",
      "lrR_pred = lrR.predict(Xgpa_test)\n",
      "mean_squared_error(lrR_pred, Ygpa_test)\n",
      "39/100: Xgpa_train, Xgpa_test, Ygpa_train, Ygpa_test, IDgpa_train, IDgpa_test = makeXY('gpa', data)\n",
      "39/101:\n",
      "lrR = RidgeCV(cv=5, normalize=True)\n",
      "lrR.fit(Xgpa_train, Ygpa_train)\n",
      "lrR_pred = lrR.predict(Xgpa_test)\n",
      "mean_squared_error(lrR_pred, Ygpa_test)\n",
      "39/102:\n",
      "lrL = LassoCV(max_iter=10000, cv=5)\n",
      "lrL.fit(Xgpa_train, Ygpa_train)\n",
      "lrL_pred = lrL.predict(Xgpa_test)\n",
      "mean_squared_error(lrL_pred, Ygpa_test)\n",
      "39/103:\n",
      "gp = GaussianProcessRegressor(normalize_y=True)\n",
      "gp.fit(Xgpa_train, Ygpa_train)\n",
      "gp_pred = gp.predict(Xgpa_test)\n",
      "mean_squared_error(gp_pred, Ygpa_test)\n",
      "39/104:\n",
      "lrR_grit = RidgeCV(cv=5, normalize=True)\n",
      "lrR_grit.fit(Xgrit_train, Ygrit_train)\n",
      "lrR_grit_pred = lrR_grit.predict(Xgrit_test)\n",
      "mean_squared_error(lrR_grit_pred, Ygrit_test)\n",
      "39/105: Xgrit_train, Xgrit_test, Ygrit_train, Ygrit_test, IDgrit_train, IDgrit_test = makeXY('grit', data)\n",
      "39/106:\n",
      "lrR_grit = RidgeCV(cv=5, normalize=True)\n",
      "lrR_grit.fit(Xgrit_train, Ygrit_train)\n",
      "lrR_grit_pred = lrR_grit.predict(Xgrit_test)\n",
      "mean_squared_error(lrR_grit_pred, Ygrit_test)\n",
      "39/107:\n",
      "lrR_grit = RidgeCV(cv=5, normalize=True, iid=True)\n",
      "lrR_grit.fit(Xgrit_train, Ygrit_train)\n",
      "lrR_grit_pred = lrR_grit.predict(Xgrit_test)\n",
      "mean_squared_error(lrR_grit_pred, Ygrit_test)\n",
      "39/108:\n",
      "lrR_grit = RidgeCV(cv=5, normalize=True)\n",
      "lrR_grit.fit(Xgrit_train, Ygrit_train)\n",
      "lrR_grit_pred = lrR_grit.predict(Xgrit_test)\n",
      "mean_squared_error(lrR_grit_pred, Ygrit_test)\n",
      "39/109: Xe_train2, Xe_test2, Ye_train2, Ye_test2, IDe_train2, IDe_test2 = makeXY('eviction', data2)\n",
      "39/110:\n",
      "from sklearn.linear_model import LinearRegression, RidgeCV, LassoCV, MultiTaskLasso, ElasticNetCV, MultiTaskElasticNet\n",
      "from sklearn.ensemble import RandomForestRegressor, RandomForestClassifier, AdaBoostRegressor, AdaBoostClassifier\n",
      "from sklearn.gaussian_process import GaussianProcessRegressor, GaussianProcessClassifier\n",
      "from sklearn.svm import SVC, LinearSVC, SVR\n",
      "from sklearn.metrics import mean_squared_error, brier_score_loss\n",
      "from sklearn.feature_selection import SelectFromModel\n",
      "39/111:\n",
      "rf_e = RandomForestClassifier(n_estimators=100)\n",
      "rf_e.fit(Xe_train, Ye_train)\n",
      "rf_e_pred = rf_grit.predict(Xe_test)\n",
      "mean_squared_error(rf_e_pred, Ye_test)\n",
      "39/112:\n",
      "rf_e = RandomForestClassifier(n_estimators=100)\n",
      "rf_e.fit(Xe_train2, Ye_train2)\n",
      "rf_e_pred = rf_grit.predict(Xe_test2)\n",
      "mean_squared_error(rf_e_pred, Ye_test2)\n",
      "39/113:\n",
      "rf_e = RandomForestClassifier(n_estimators=100)\n",
      "rf_e.fit(Xe_train2, Ye_train2)\n",
      "rf_e_pred = rf_e.predict(Xe_test2)\n",
      "mean_squared_error(rf_e_pred, Ye_test2)\n",
      "39/114:\n",
      "ab_e = RandomForestClassifier(n_estimators=100)\n",
      "ab_e.fit(Xe_train2, Ye_train2)\n",
      "ab_e_pred = ab_e.predict(Xe_test2)\n",
      "mean_squared_error(ab_e_pred, Ye_test2)\n",
      "39/115:\n",
      "rf_e = RandomForestClassifier(n_estimators=100)\n",
      "rf_e.fit(Xe_train2, Ye_train2)\n",
      "rf_e_pred = rf_e.predict(Xe_test2)\n",
      "brier_loss(rf_e_pred, Ye_test2)\n",
      "39/116:\n",
      "rf_e = RandomForestClassifier(n_estimators=100)\n",
      "rf_e.fit(Xe_train2, Ye_train2)\n",
      "rf_e_pred = rf_e.predict(Xe_test2)\n",
      "brier_score_loss(rf_e_pred, Ye_test2)\n",
      "39/117:\n",
      "ab_e = RandomForestClassifier(n_estimators=100)\n",
      "ab_e.fit(Xe_train2, Ye_train2)\n",
      "ab_e_pred = ab_e.predict(Xe_test2)\n",
      "brier_score_loss(ab_e_pred, Ye_test2)\n",
      "39/118:\n",
      "ab_e = AdaBoostClassifier(n_estimators=100)\n",
      "ab_e.fit(Xe_train2, Ye_train2)\n",
      "ab_e_pred = ab_e.predict(Xe_test2)\n",
      "brier_score_loss(ab_e_pred, Ye_test2)\n",
      "39/119:\n",
      "sv_e = SVC(kernel='rbf', gamma='auto')\n",
      "sv_e.fit(Xe_train, Ye_train)\n",
      "sv_e_pred = sv_e.predict(Xe_test)\n",
      "mean_squared_error(sv_e_pred, Ye_test)\n",
      "39/120:\n",
      "sv_e = SVC(kernel='rbf', gamma='auto')\n",
      "sv_e.fit(Xe_train2, Ye_train2)\n",
      "sv_e_pred = sv_e.predict(Xe_test2)\n",
      "brier_score_error(sv_e_pred, Ye_test2)\n",
      "39/121:\n",
      "sv_e = SVC(kernel='rbf', gamma='auto')\n",
      "sv_e.fit(Xe_train2, Ye_train2)\n",
      "sv_e_pred = sv_e.predict(Xe_test2)\n",
      "brier_score_loss(sv_e_pred, Ye_test2)\n",
      "39/122: Xl_train2, Xl_test2, Yl_train2, Yl_test2, IDl_train2, IDl_test2 = makeXY('layoff', data2)\n",
      "39/123:\n",
      "rf_l = RandomForestClassifier(n_estimators=100)\n",
      "rf_l.fit(Xl_train2, Yl_train2)\n",
      "rf_l_pred = rf_l.predict(Xl_test2)\n",
      "brier_score_loss(rf_l_pred, Yl_test2)\n",
      "39/124:\n",
      "sv_e = SVC(kernel='linear', gamma='auto')\n",
      "sv_e.fit(Xe_train2, Ye_train2)\n",
      "sv_e_pred = sv_e.predict(Xe_test2)\n",
      "brier_score_loss(sv_e_pred, Ye_test2)\n",
      "39/125:\n",
      "sv_e = SVC(kernel='rbf', gamma='auto')\n",
      "sv_e.fit(Xe_train2, Ye_train2)\n",
      "sv_e_pred = sv_e.predict(Xe_test2)\n",
      "brier_score_loss(sv_e_pred, Ye_test2)\n",
      "39/126:\n",
      "sv_l = SVC(kernel='rbf', gamma='auto')\n",
      "sv_l.fit(Xl_train2, Yl_train2)\n",
      "sv_l_pred = sv_l.predict(Xl_test2)\n",
      "brier_score_loss(sv_l_pred, Yl_test2)\n",
      "39/127:\n",
      "def split(d, trainingProportion):\n",
      "d = shuffle(d)\n",
      "splitInd = int(d.shape[0] * trainingProportion)\n",
      "train = d.iloc[0:splitInd,]\n",
      "test = d.iloc[splitInd:,]\n",
      "return train, test\n",
      "39/128:\n",
      "train, validation = split(fullTrain, .6)\n",
      "print(train.shape)\n",
      "print(validation.shape)\n",
      "39/129:\n",
      "def split(d, trainingProportion):\n",
      "d = shuffle(d)\n",
      "splitInd = int(d.shape[0] * trainingProportion)\n",
      "train = d.iloc[0:splitInd,]\n",
      "test = d.iloc[splitInd:,]\n",
      "return train, test\n",
      "39/130:\n",
      "def split(d, trainingProportion):\n",
      " shuffle(d)\n",
      "splitInd = int(d.shape[0] * trainingProportion)\n",
      "train = d.iloc[0:splitInd,]\n",
      "test = d.iloc[splitInd:,]\n",
      "return train, test\n",
      "39/131: # Hold Out Validation\n",
      "39/132:\n",
      "lrL = LassoCV(max_iter=1000, cv=5)\n",
      "lrL.fit(Xgpa_train, Ygpa_train)\n",
      "lrL_pred = lrL.predict(Xgpa_test)\n",
      "mean_squared_error(lrL_pred, Ygpa_test)\n",
      "39/133:\n",
      "lrL = LassoCV(max_iter=1000, cv=5, normalize=True)\n",
      "lrL.fit(Xgpa_train, Ygpa_train)\n",
      "lrL_pred = lrL.predict(Xgpa_test)\n",
      "mean_squared_error(lrL_pred, Ygpa_test)\n",
      "39/134:\n",
      "lrL = LassoCV(max_iter=10000, cv=5, normalize=True)\n",
      "lrL.fit(Xgpa_train, Ygpa_train)\n",
      "lrL_pred = lrL.predict(Xgpa_test)\n",
      "mean_squared_error(lrL_pred, Ygpa_test)\n",
      "39/135:\n",
      "lrEN = ElasticNetCV(max_iter=10000, cv=5, normalize=True)\n",
      "lrEN.fit(Xgpa_train, Ygpa_train)\n",
      "lrEN_pred = lrEN.predict(Xgpa_test)\n",
      "mean_squared_error(lrEN_pred, Ygpa_test)\n",
      "39/136:\n",
      "ab = AdaBoostRegressor(RandomForestRegressor(n_estimators=100), n_estimators=100)\n",
      "ab.fit(Xgpa_train, Ygpa_train)\n",
      "ab_pred = ab.predict(Xgpa_test)\n",
      "mean_squared_error(ab_pred, Ygpa_test)\n",
      "39/137:\n",
      "ab = AdaBoostRegressor(n_estimators=100)\n",
      "ab.fit(Xgpa_train, Ygpa_train)\n",
      "ab_pred = ab.predict(Xgpa_test)\n",
      "mean_squared_error(ab_pred, Ygpa_test)\n",
      "41/1:\n",
      "import pandas as pd\n",
      "import matplotlib.pyplot as plt\n",
      "import seaborn as sns\n",
      "import re\n",
      "import numpy as np\n",
      "import math\n",
      "import ff\n",
      "41/2:\n",
      "from sklearn.linear_model import LinearRegression, RidgeCV, LassoCV, MultiTaskLasso, ElasticNetCV, MultiTaskElasticNet\n",
      "from sklearn.ensemble import RandomForestRegressor, AdaBoostRegressor\n",
      "from sklearn.gaussian_process import GaussianProcessRegressor\n",
      "from sklearn.svm import SVC, LinearSVC, SVR\n",
      "from sklearn.metrics import mean_squared_error, brier_score_loss\n",
      "from sklearn.feature_selection import SelectFromModel\n",
      "\n",
      "from sklearn.utils import shuffle\n",
      "41/3:\n",
      "data = pd.read_csv('output.csv', low_memory=False)\n",
      "data.info()\n",
      "41/4:\n",
      "data = pd.read_csv('output.csv', low_memory=False)\n",
      "data.info()\n",
      "39/138:\n",
      "gp = GaussianProcessRegressor(n_restarts_optimizer=5, normalize_y=True)\n",
      "gp.fit(Xgpa_train, Ygpa_train)\n",
      "gp_pred = gp.predict(Xgpa_test)\n",
      "mean_squared_error(gp_pred, Ygpa_test)\n",
      "39/139:\n",
      "gp = GaussianProcessRegressor(n_restarts_optimizer=5, normalize_y=True)\n",
      "gp.fit(Xgpa_train, Ygpa_train)\n",
      "gp_pred = gp.predict(Xgpa_test)\n",
      "mean_squared_error(gp_pred, Ygpa_test)\n",
      "39/140:\n",
      "gp = GaussianProcessRegressor(n_restarts_optimizer=2, normalize_y=True)\n",
      "gp.fit(Xgpa_train, Ygpa_train)\n",
      "gp_pred = gp.predict(Xgpa_test)\n",
      "mean_squared_error(gp_pred, Ygpa_test)\n",
      "39/141:\n",
      "def fitmodels(X_train, X_test, Y_train, Y_test, models):\n",
      "    ret = []\n",
      "    for m in models:\n",
      "        m.fit(X_train, Y_train)\n",
      "        m_pred = m.predict(X_test)\n",
      "        mse = mean_squared_error(m_pred, Y_test)\n",
      "        ret.append((m, mse))\n",
      "        print mse\n",
      "    return ret\n",
      "39/142:\n",
      "y = [0.358418, 0.361059, 0.357367, 0.352876, 0.350095, 0.349639, 0.351140, 0.351885]\n",
      "plt.plot([20*(x+1) for x in range(8)], y)\n",
      "39/143:\n",
      "models = [LinearRegression(normalize=True), \\\n",
      "          RidgeCV(cv=5, normalize=True), \\\n",
      "          LassoCV(max_iter=10000, cv=5, normalize=True), \\\n",
      "          ElasticCV(max_iter=10000, cv=5, normalize=True), \\\n",
      "          RandomForestRegressor(n_estimators=100), \\\n",
      "          GaussianProcessRegressor(n_restarts_optimizer=2, normalize_y=True), \\\n",
      "          AdaBoostRegressor(n_estimators=100), \\\n",
      "          SVR(kernel='rbf', gamma='auto')\n",
      "         ]\n",
      "39/144:\n",
      "models = [LinearRegression(normalize=True), \\\n",
      "          RidgeCV(cv=5, normalize=True), \\\n",
      "          LassoCV(max_iter=10000, cv=5, normalize=True), \\\n",
      "          ElasticNetCV(max_iter=10000, cv=5, normalize=True), \\\n",
      "          RandomForestRegressor(n_estimators=100), \\\n",
      "          GaussianProcessRegressor(n_restarts_optimizer=2, normalize_y=True), \\\n",
      "          AdaBoostRegressor(n_estimators=100), \\\n",
      "          SVR(kernel='rbf', gamma='auto')\n",
      "         ]\n",
      "39/145: grits = fitmodels(Xgrit_train, Xgrit_test, Ygrit_train, Ygrit_test, models)\n",
      "39/146:\n",
      "gpa2models = [LinearRegression(normalize=True), \\\n",
      "          RidgeCV(cv=5, normalize=True), \\\n",
      "          LassoCV(max_iter=10000, cv=5, normalize=True), \\\n",
      "          ElasticNetCV(max_iter=10000, cv=5, normalize=True), \\\n",
      "          RandomForestRegressor(n_estimators=100), \\\n",
      "          GaussianProcessRegressor(n_restarts_optimizer=2, normalize_y=True), \\\n",
      "          AdaBoostRegressor(n_estimators=100), \\\n",
      "          SVR(kernel='rbf', gamma='auto')\n",
      "         ]\n",
      "39/147: gpas = fitmodels(Xgpa_train2, Xgpa_test2, Ygpa_train2, Ygpa_test2, gpa2models)\n",
      "39/148:\n",
      "mhmodels = [LinearRegression(normalize=True), \\\n",
      "          RidgeCV(cv=5, normalize=True), \\\n",
      "          LassoCV(max_iter=10000, cv=5, normalize=True), \\\n",
      "          ElasticNetCV(max_iter=10000, cv=5, normalize=True), \\\n",
      "          AdaBoostRegressor(n_estimators=100), \\\n",
      "          RandomForestRegressor(n_estimators=100), \\\n",
      "          GaussianProcessRegressor(n_restarts_optimizer=2, normalize_y=True), \\\n",
      "          SVR(kernel='rbf', gamma='auto')\n",
      "         ]\n",
      "39/149: mh = fitmodels(Xmh_train2, Xmh_test2, Ymh_train2, Ymh_test2, mhmodels)\n",
      "39/150: Xmh_train, Xmh_test, Ymh_train, Ymh_test, IDmh_train, IDmh_test = makeXY('materialHardship', data)\n",
      "39/151: mh = fitmodels(Xmh_train, Xmh_test, Ymh_train, Ymh_test, mhmodels)\n",
      "39/152: gpas = fitmodels(Xgpa_train2, Xgpa_test2, Ygpa_train2, Ygpa_test2, gpa2models)\n",
      "39/153: mh\n",
      "39/154: mh[5]\n",
      "39/155: mh[5][0]\n",
      "39/156:\n",
      "# features sorted by importance\n",
      "featuresMH = zip(Xmh_train.columns, mh[5][0].feature_importances_)\n",
      "sorted(featuresMH, key=lambda x: x[1], reverse=True)\n",
      "39/157:\n",
      "data3 = dropsame(background)\n",
      "strs = data3.select_dtypes(\"object\")\n",
      "nu2 = strs.nunique()\n",
      "keep = nu2[nu2 < 5].index\n",
      "data3[keep] = data3[keep].apply(lambda x: pd.factorize(x)[0])\n",
      "data3[keep].head()\n",
      "39/158:\n",
      "notkeep = nu2[nu2 >= 5].index\n",
      "data3 = data3.drop(notkeep, axis=1)\n",
      "numeric = data3._get_numeric_data()\n",
      "numeric[numeric < 0] = np.nan\n",
      "39/159: data3.info()\n",
      "39/160:\n",
      "data3 = data3.dropna(thresh=424*7, axis=1)\n",
      "for i in range(len(data3.columns)):\n",
      "    var = data3.columns[i]\n",
      "    if data3[var].dtype == 'int64':\n",
      "        data3[var] = data3[var].fillna(data3[var].mode().iloc[0])\n",
      "    else:\n",
      "        data3[var] = data3[var].fillna(data3[var].mean())\n",
      "39/161: data3.isnull().sum().sum()\n",
      "39/162: data3.info()\n",
      "39/163: data2.info()\n",
      "39/164: Xgpa_train3, Xgpa_test3, Ygpa_train3, Ygpa_test3, IDgpa_train3, IDgpa_test3 = makeXY('gpa', data3)\n",
      "39/165:\n",
      "gpa3models = [LinearRegression(normalize=True), \\\n",
      "          RidgeCV(cv=5, normalize=True), \\\n",
      "          LassoCV(max_iter=10000, cv=5, normalize=True), \\\n",
      "          ElasticNetCV(max_iter=10000, cv=5, normalize=True), \\\n",
      "          AdaBoostRegressor(n_estimators=100), \\\n",
      "          RandomForestRegressor(n_estimators=100), \\\n",
      "          GaussianProcessRegressor(n_restarts_optimizer=2, normalize_y=True), \\\n",
      "          SVR(kernel='rbf', gamma='auto')\n",
      "         ]\n",
      "39/166: gpas3 = fitmodels(Xgpa_train3, Xgpa_test3, Ygpa_train3, Ygpa_test3, gpa3models)\n",
      "39/167:\n",
      "from sklearn.linear_model import LinearRegression, RidgeCV, LassoCV, ElasticNetCV, SGDClassifier, MultiTaskElasticNet\n",
      "from sklearn.ensemble import RandomForestRegressor, RandomForestClassifier, AdaBoostRegressor, AdaBoostClassifier\n",
      "from sklearn.gaussian_process import GaussianProcessRegressor, GaussianProcessClassifier\n",
      "from sklearn.svm import SVC, LinearSVC, SVR\n",
      "from sklearn.metrics import mean_squared_error, brier_score_loss\n",
      "from sklearn.feature_selection import SelectFromModel\n",
      "39/168:\n",
      "emodels = [SGDClassifier(loss='log', penalty='none'), \\\n",
      "          RidgeClassifier(ormalize=True), \\\n",
      "          SGDClassifier(loss='log', penalty='l2'), \\\n",
      "          SGDClassifier(loss='log', penalty='elasticnet'), \\\n",
      "          AdaBoostClassifier(n_estimators=100), \\\n",
      "          RandomForestClassifier(n_estimators=100), \\\n",
      "          GaussianProcessClassifier(n_restarts_optimizer=2, normalize_y=True), \\\n",
      "          SVC(kernel='rbf', gamma='auto')\n",
      "         ]\n",
      "39/169:\n",
      "emodels = [SGDClassifier(loss='log', penalty='none'), \\\n",
      "          SGDClassifier(loss='log', penalty='l2'), \\\n",
      "          SGDClassifier(loss='log', penalty='l1'), \\\n",
      "          SGDClassifier(loss='log', penalty='elasticnet'), \\\n",
      "          AdaBoostClassifier(n_estimators=100), \\\n",
      "          RandomForestClassifier(n_estimators=100), \\\n",
      "          GaussianProcessClassifier(n_restarts_optimizer=2, normalize_y=True), \\\n",
      "          SVC(kernel='rbf', gamma='auto')\n",
      "         ]\n",
      "39/170:\n",
      "emodels = [SGDClassifier(loss='log', penalty='none'), \\\n",
      "          SGDClassifier(loss='log', penalty='l2'), \\\n",
      "          SGDClassifier(loss='log', penalty='l1'), \\\n",
      "          SGDClassifier(loss='log', penalty='elasticnet'), \\\n",
      "          AdaBoostClassifier(n_estimators=100), \\\n",
      "          RandomForestClassifier(n_estimators=100), \\\n",
      "          GaussianProcessClassifier(n_restarts_optimizer=2), \\\n",
      "          SVC(kernel='rbf', gamma='auto')\n",
      "         ]\n",
      "39/171: Xe_train, Xe_test, Ye_train, Ye_test, IDe_train, IDe_test = makeXY('eviction', data)\n",
      "39/172: e = fitmodels(Xe_train, Xe_test, Ye_train, Ye_test)\n",
      "39/173: e = fitmodels(Xe_train, Xe_test, Ye_train, Ye_test, emodels)\n",
      "39/174:\n",
      "rf_e = RandomForestClassifier(n_estimators=100)\n",
      "rf_e.fit(Xe_train2, Ye_train2)\n",
      "rf_e_pred = rf_e.predicta(Xe_test2)\n",
      "brier_score_loss(rf_e_pred, Ye_test2)\n",
      "39/175:\n",
      "rf_e = RandomForestClassifier(n_estimators=100)\n",
      "rf_e.fit(Xe_train2, Ye_train2)\n",
      "rf_e_pred = rf_e.proba(Xe_test2)\n",
      "brier_score_loss(rf_e_pred, Ye_test2)\n",
      "39/176:\n",
      "rf_e = RandomForestClassifier(n_estimators=100)\n",
      "rf_e.fit(Xe_train2, Ye_train2)\n",
      "rf_e_pred = rf_e.predict_proba(Xe_test2)\n",
      "brier_score_loss(rf_e_pred, Ye_test2)\n",
      "39/177:\n",
      "rf_e = RandomForestClassifier(n_estimators=100)\n",
      "rf_e.fit(Xe_train2, Ye_train2)\n",
      "rf_e_pred = rf_e.predict_proba(Xe_test2)[:,1]\n",
      "brier_score_loss(rf_e_pred, Ye_test2)\n",
      "39/178:\n",
      "rf_e = RandomForestClassifier(n_estimators=100)\n",
      "rf_e.fit(Xe_train2, Ye_train2)\n",
      "rf_e_pred = rf_e.predict_proba(Xe_test2)[:,1]\n",
      "brier_score_loss(Ye_test2, rf_e_pred)\n",
      "39/179:\n",
      "def fitclassifiers(X_train, X_test, Y_train, Y_test, models):\n",
      "    ret = []\n",
      "    for m in models:\n",
      "        m.fit(X_train, Y_train)\n",
      "        m_pred = m.predict_proba(X_test)\n",
      "        brier = brier_score_loss(Y_test, m_pred)\n",
      "        ret.append((m, brier))\n",
      "        print brier\n",
      "    return ret\n",
      "39/180: e = fitclassifiers(Xe_train, Xe_test, Ye_train, Ye_test, emodels)\n",
      "39/181:\n",
      "def fitclassifiers(X_train, X_test, Y_train, Y_test, models):\n",
      "    ret = []\n",
      "    for m in models:\n",
      "        m.fit(X_train, Y_train)\n",
      "        m_pred = m.predict_proba(X_test)[:,1]\n",
      "        brier = brier_score_loss(Y_test, m_pred)\n",
      "        ret.append((m, brier))\n",
      "        print brier\n",
      "    return ret\n",
      "39/182: e = fitclassifiers(Xe_train, Xe_test, Ye_train, Ye_test, emodels)\n",
      "39/183:\n",
      "ab_e = AdaBoostClassifier(n_estimators=100)\n",
      "ab_e.fit(Xe_train2, Ye_train2)\n",
      "ab_e_pred = ab_e.predict_proba(Xe_test2)\n",
      "brier_score_loss(Ye_test2, ab_e_pred)\n",
      "39/184:\n",
      "ab_e = AdaBoostClassifier(n_estimators=100)\n",
      "ab_e.fit(Xe_train2, Ye_train2)\n",
      "ab_e_pred = ab_e.predict_proba(Xe_test2)[:,1]\n",
      "brier_score_loss(Ye_test2, ab_e_pred)\n",
      "39/185: brier_score_loss(Ye_test2, sv_e_pred)\n",
      "39/186:\n",
      "ab_e = AdaBoostClassifier(n_estimators=100)\n",
      "ab_e.fit(Xe_train2, Ye_train2)\n",
      "ab_e_pred = ab_e.decision_function(Xe_test2)[:,1]\n",
      "brier_score_loss(Ye_test2, ab_e_pred)\n",
      "39/187:\n",
      "ab_e = AdaBoostClassifier(n_estimators=100)\n",
      "ab_e.fit(Xe_train2, Ye_train2)\n",
      "ab_e_pred = ab_e.decision_function(Xe_test2)\n",
      "brier_score_loss(Ye_test2, ab_e_pred)\n",
      "39/188:\n",
      "sv_e = SVC(kernel='rbf', gamma='auto')\n",
      "sv_e.fit(Xe_train2, Ye_train2)\n",
      "sv_e_pred = sv_e.decision_function(Xe_test2)\n",
      "brier_score_loss(sv_e_pred, Ye_test2)\n",
      "39/189:\n",
      "sv_e = SVC(kernel='rbf', gamma='auto')\n",
      "sv_e.fit(Xe_train2, Ye_train2)\n",
      "sv_e_pred = sv_e.predict_proba(Xe_test2)\n",
      "brier_score_loss(sv_e_pred, Ye_test2)\n",
      "39/190:\n",
      "sv_e = SVC(kernel='rbf', gamma='auto', probability=True)\n",
      "sv_e.fit(Xe_train2, Ye_train2)\n",
      "sv_e_pred = sv_e.predict_proba(Xe_test2)\n",
      "brier_score_loss(sv_e_pred, Ye_test2)\n",
      "39/191:\n",
      "sv_e = SVC(kernel='rbf', gamma='auto', probability=True)\n",
      "sv_e.fit(Xe_train2, Ye_train2)\n",
      "sv_e_pred = sv_e.predict_proba(Xe_test2)[:,1]\n",
      "brier_score_loss(sv_e_pred, Ye_test2)\n",
      "39/192: e = fitclassifiers(Xe_train, Xe_test, Ye_train, Ye_test, emodels)\n",
      "39/193:\n",
      "from sklearn.linear_model import LinearRegression, RidgeCV, LassoCV, ElasticNetCV, SGDClassifier, MultiTaskElasticNet\n",
      "from sklearn.ensemble import RandomForestRegressor, RandomForestClassifier, AdaBoostRegressor, AdaBoostClassifier\n",
      "from sklearn.gaussian_process import GaussianProcessRegressor, GaussianProcessClassifier\n",
      "from sklearn.svm import SVC, LinearSVC, SVR\n",
      "from sklearn.metrics import mean_squared_error, brier_score_loss\n",
      "from sklearn.feature_selection import SelectFromModel\n",
      "from sklearn.preprocessing import normalize\n",
      "39/194: e = fitclassifiers(normalize(Xe_train), normalize(Xe_test), Ye_train, Ye_test, emodels)\n",
      "39/195:\n",
      "ab_e = AdaBoostClassifier(n_estimators=100)\n",
      "ab_e.fit(Xe_train2, Ye_train2)\n",
      "ab_e_pred = ab_e.predict_proba(Xe_test2)\n",
      "brier_score_loss(Ye_test2, ab_e_pred)\n",
      "39/196:\n",
      "emodels = [SGDClassifier(loss='log', penalty='none'), \\\n",
      "          SGDClassifier(loss='log', penalty='l2'), \\\n",
      "          SGDClassifier(loss='log', penalty='l1'), \\\n",
      "          SGDClassifier(loss='log', penalty='elasticnet'), \\\n",
      "          AdaBoostClassifier(n_estimators=100), \\\n",
      "          RandomForestClassifier(n_estimators=100), \\\n",
      "          GaussianProcessClassifier(n_restarts_optimizer=2), \\\n",
      "          SVC(kernel='rbf', gamma='auto', probability=True)\n",
      "         ]\n",
      "39/197: e = fitclassifiers(normalize(Xe_train), normalize(Xe_test), Ye_train, Ye_test, emodels)\n",
      "39/198:\n",
      "ab_e = AdaBoostClassifier(n_estimators=100)\n",
      "ab_e.fit(Xe_train2, Ye_train2)\n",
      "ab_e_pred = ab_e.predict(Xe_test2)\n",
      "brier_score_loss(Ye_test2, ab_e_pred)\n",
      "39/199:\n",
      "def fitclassifiers(X_train, X_test, Y_train, Y_test, models):\n",
      "    ret = []\n",
      "    count = 0\n",
      "    for m in models:\n",
      "        m.fit(X_train, Y_train)\n",
      "        if count != 4:\n",
      "            m_pred = m.predict_proba(X_test)[:,1]\n",
      "        else:\n",
      "            m_pred = m.predict(X_test)\n",
      "        brier = brier_score_loss(Y_test, m_pred)\n",
      "        ret.append((m, brier))\n",
      "        print brier\n",
      "        count = count + 1\n",
      "    return ret\n",
      "39/200: e = fitclassifiers(normalize(Xe_train), normalize(Xe_test), Ye_train, Ye_test, emodels)\n",
      "39/201: e = fitclassifiers(normalize(Xe_train), normalize(Xe_test), Ye_train, Ye_test, emodels)\n",
      "39/202:\n",
      "lmodels = [SGDClassifier(loss='hinge', penalty='none'), \\\n",
      "          SGDClassifier(loss='hinge', penalty='l2'), \\\n",
      "          SGDClassifier(loss='hinge', penalty='l1'), \\\n",
      "          SGDClassifier(loss='hinge', penalty='elasticnet'), \\\n",
      "          AdaBoostClassifier(n_estimators=100), \\\n",
      "          RandomForestClassifier(n_estimators=100), \\\n",
      "          GaussianProcessClassifier(n_restarts_optimizer=2), \\\n",
      "          SVC(kernel='rbf', gamma='auto', probability=True)\n",
      "         ]\n",
      "39/203: Xl_train, Xl_test, Yl_train, Yl_test, IDl_train, IDl_test = makeXY('layoff', data)\n",
      "39/204: e = fitclassifiers(normalize(Xl_train), normalize(Xl_test), Yl_train, Yl_test, lmodels)\n",
      "39/205:\n",
      "lmodels = [SGDClassifier(loss='log', penalty='none'), \\\n",
      "          SGDClassifier(loss='log', penalty='l2'), \\\n",
      "          SGDClassifier(loss='log', penalty='l1'), \\\n",
      "          SGDClassifier(loss='log', penalty='elasticnet'), \\\n",
      "          AdaBoostClassifier(n_estimators=100), \\\n",
      "          RandomForestClassifier(n_estimators=100), \\\n",
      "          GaussianProcessClassifier(n_restarts_optimizer=2), \\\n",
      "          SVC(kernel='rbf', gamma='auto', probability=True)\n",
      "         ]\n",
      "39/206: e = fitclassifiers(normalize(Xl_train), normalize(Xl_test), Yl_train, Yl_test, lmodels)\n",
      "39/207:\n",
      "JTmodels = [SGDClassifier(loss='log', penalty='none'), \\\n",
      "          SGDClassifier(loss='log', penalty='l2'), \\\n",
      "          SGDClassifier(loss='log', penalty='l1'), \\\n",
      "          SGDClassifier(loss='log', penalty='elasticnet'), \\\n",
      "          AdaBoostClassifier(n_estimators=100), \\\n",
      "          RandomForestClassifier(n_estimators=100), \\\n",
      "          GaussianProcessClassifier(n_restarts_optimizer=2), \\\n",
      "          SVC(kernel='rbf', gamma='auto', probability=True)\n",
      "         ]\n",
      "39/208: Xjt_train, Xjt_test, Yjt_train, Yjt_test, IDjt_train, IDjt_test = makeXY('layoff', data)\n",
      "39/209:\n",
      "jt\n",
      "= fitclassifiers(normalize(Xjt_train), normalize(Xjt_test), Yjt_train, Yjt_test, jtmodels)\n",
      "39/210: jt = fitclassifiers(normalize(Xjt_train), normalize(Xjt_test), Yjt_train, Yjt_test, jtmodels)\n",
      "39/211:\n",
      "jtmodels = [SGDClassifier(loss='log', penalty='none'), \\\n",
      "          SGDClassifier(loss='log', penalty='l2'), \\\n",
      "          SGDClassifier(loss='log', penalty='l1'), \\\n",
      "          SGDClassifier(loss='log', penalty='elasticnet'), \\\n",
      "          AdaBoostClassifier(n_estimators=100), \\\n",
      "          RandomForestClassifier(n_estimators=100), \\\n",
      "          GaussianProcessClassifier(n_restarts_optimizer=2), \\\n",
      "          SVC(kernel='rbf', gamma='auto', probability=True)\n",
      "         ]\n",
      "39/212: jt = fitclassifiers(normalize(Xjt_train), normalize(Xjt_test), Yjt_train, Yjt_test, jtmodels)\n",
      "39/213:\n",
      "jt = fitclassifiers(normalize(Xjt_train), normalize(Xjt_test), Yjt_train, Yjt_test, [SGDClassifier(loss='log', penalty='none'), \\\n",
      "          SGDClassifier(loss='log', penalty='l2'), \\\n",
      "          SGDClassifier(loss='log', penalty='l1'), \\\n",
      "          SGDClassifier(loss='log', penalty='elasticnet')])\n",
      "39/214: e = fitclassifiers(normalize(Xl_train), normalize(Xl_test), Yl_train, Yl_test, lmodels)\n",
      "39/215:\n",
      "e = fitclassifiers(normalize(Xl_train), normalize(Xl_test), Yl_train, Yl_test, [SGDClassifier(loss='log', penalty='none'), \\\n",
      "          SGDClassifier(loss='log', penalty='l2'), \\\n",
      "          SGDClassifier(loss='log', penalty='l1'), \\\n",
      "          SGDClassifier(loss='log', penalty='elasticnet')])\n",
      "39/216:\n",
      "# features sorted by importance\n",
      "featurese = zip(Xe_train.columns, e[5][0].feature_importances_)\n",
      "sorted(featurese, key=lambda x: x[1], reverse=True)\n",
      "39/217: e\n",
      "39/218: e[5]\n",
      "39/219: e = fitclassifiers(normalize(Xe_train), normalize(Xe_test), Ye_train, Ye_test, emodels)\n",
      "39/220: e = fitclassifiers(normalize(Xe_train), normalize(Xe_test), Ye_train, Ye_test, emodels)\n",
      "39/221: e = fitclassifiers(normalize(Xe_train), normalize(Xe_test), Ye_train, Ye_test, emodels)\n",
      "39/222: e[5]\n",
      "39/223:\n",
      "# features sorted by importance\n",
      "featurese = zip(Xe_train.columns, e[5][0].feature_importances_)\n",
      "sorted(featurese, key=lambda x: x[1], reverse=True)\n",
      "39/224: l1 = fitclassifiers(normalize(Xl_train), normalize(Xl_test), Yl_train, Yl_test, lmodels)\n",
      "39/225:\n",
      "# features sorted by importance\n",
      "featuresl = zip(Xl_train.columns, l1[5][0].feature_importances_)\n",
      "sorted(featuresl, key=lambda x: x[1], reverse=True)\n",
      "39/226: jt1 = fitclassifiers(normalize(Xjt_train), normalize(Xjt_test), Yjt_train, Yjt_test, jtmodels)\n",
      "39/227:\n",
      "# features sorted by importance\n",
      "featuresjt = zip(Xjt_train.columns, jt[5][0].feature_importances_)\n",
      "sorted(featuresjt, key=lambda x: x[1], reverse=True)\n",
      "39/228:\n",
      "# features sorted by importance\n",
      "featuresjt = zip(Xjt_train.columns, jt1[5][0].feature_importances_)\n",
      "sorted(featuresjt, key=lambda x: x[1], reverse=True)\n",
      "39/229:\n",
      "y = [0.358418, 0.361059, 0.357367, 0.352876, 0.349639, 0.350095, 0.351140, 0.351885]\n",
      "plt.plot([20*(x+1) for x in range(8)], y)\n",
      "plot.title(\"Validating n_estimators\")\n",
      "39/230:\n",
      "y = [0.358418, 0.361059, 0.357367, 0.352876, 0.349639, 0.350095, 0.351140, 0.351885]\n",
      "plt.plot([20*(x+1) for x in range(8)], y)\n",
      "plt.title(\"Validating n_estimators\")\n",
      "39/231:\n",
      "rfselect = SelectFromModel(RandomForestRegressor(n_estimators=100))\n",
      "Xgpa_train_rf = rfselect.fit_transform(Xgpa_train3, Ygpa_train3)\n",
      "rfs = RandomForestRegressor(n_estimators=100).fit(Xgpa_train_rf, Ygpa_train3)\n",
      "Xgpa_test_rf = rfselect.transform(Xgpa_test3)\n",
      "rfs_pred = rfs.predict(Xgpa_test_rf)\n",
      "print mean_squared_error(rfs_pred, Ygpa_test3)\n",
      "print sum(rfselect.get_support())\n",
      "print rfselect.get_support()[:10]\n",
      "39/232: gpas_rf = fitmodels(Xgpa_train_rf, Xgpa_test_rf, Ygpa_train3, Ygpa_test3, gpa3models)\n",
      "39/233:\n",
      "gpas_rf = fitmodels(Xgpa_train_rf, Xgpa_test_rf, Ygpa_train3, Ygpa_test3, [LinearRegression(normalize=True), \\\n",
      "          RidgeCV(cv=5, normalize=True), \\\n",
      "          AdaBoostRegressor(n_estimators=100), \\\n",
      "          RandomForestRegressor(n_estimators=100), \\\n",
      "          GaussianProcessRegressor(n_restarts_optimizer=2, normalize_y=True), \\\n",
      "          SVR(kernel='rbf', gamma='auto')\n",
      "         ])\n",
      "39/234: Xgrit_train3, Xgrit_test3, Ygrit_train3, Ygrit_test3, IDgrit_train3, IDgrit_test3 = makeXY('grit', data3)\n",
      "39/235: grits3 = fitmodels(Xgrit_train3, Xgrit_test3, Ygrit_train3, Ygrit_test3, modelsgrit)\n",
      "39/236:\n",
      "modelsgrit = [LinearRegression(normalize=True), \\\n",
      "          RidgeCV(cv=5, normalize=True), \\\n",
      "          LassoCV(max_iter=10000, cv=5, normalize=True), \\\n",
      "          ElasticNetCV(max_iter=10000, cv=5, normalize=True), \\\n",
      "          RandomForestRegressor(n_estimators=100), \\\n",
      "          GaussianProcessRegressor(n_restarts_optimizer=2, normalize_y=True), \\\n",
      "          AdaBoostRegressor(n_estimators=100), \\\n",
      "          SVR(kernel='rbf', gamma='auto')\n",
      "         ]\n",
      "39/237: grits3 = fitmodels(Xgrit_train3, Xgrit_test3, Ygrit_train3, Ygrit_test3, modelsgrit)\n",
      "39/238:\n",
      "rfselectgrit = SelectFromModel(RandomForestRegressor(n_estimators=100))\n",
      "Xgrit_train_rf = rfselectgrit.fit_transform(Xgrit_train3, Ygrit_train3)\n",
      "rfsgrit = RandomForestRegressor(n_estimators=100).fit(Xgrit_train_rf, Ygrit_train3)\n",
      "Xgrit_test_rf = rfselect.transform(Xgrit_test3)\n",
      "rfsgrit_pred = rfs.predict(Xgrit_test_rf)\n",
      "print mean_squared_error(rfsgrit_pred, Ygrit_test3)\n",
      "print sum(rfselectgrit.get_support())\n",
      "print rfselectgrit.get_support()[:10]\n",
      "39/239:\n",
      "rfselectmh = SelectFromModel(RandomForestRegressor(n_estimators=100))\n",
      "Xmh_train_rf = rfselectmh.fit_transform(Xmh_train3, Ymh_train3)\n",
      "rfsmh = RandomForestRegressor(n_estimators=100).fit(Xmh_train_rf, Ymh_train3)\n",
      "Xmh_test_rf = rfselect.transform(Xmh_test3)\n",
      "rfsmh_pred = rfs.predict(Xmh_test_rf)\n",
      "print mean_squared_error(rfsmh_pred, Ymh_test3)\n",
      "print sum(rfselectmh.get_support())\n",
      "print rfselectmh.get_support()[:10]\n",
      "39/240: Xmh_train3, Xmh_test3, Ymh_train3, Ymh_test3, IDmh_train3, IDmh_test3 = makeXY('materialHardship', data3)\n",
      "39/241:\n",
      "rfselectmh = SelectFromModel(RandomForestRegressor(n_estimators=100))\n",
      "Xmh_train_rf = rfselectmh.fit_transform(Xmh_train3, Ymh_train3)\n",
      "rfsmh = RandomForestRegressor(n_estimators=100).fit(Xmh_train_rf, Ymh_train3)\n",
      "Xmh_test_rf = rfselect.transform(Xmh_test3)\n",
      "rfsmh_pred = rfs.predict(Xmh_test_rf)\n",
      "print mean_squared_error(rfsmh_pred, Ymh_test3)\n",
      "print sum(rfselectmh.get_support())\n",
      "print rfselectmh.get_support()[:10]\n",
      "39/242: Xe_train3, Xe_test3, Ye_train3, Ye_test3, IDe_train3, IDe_test3 = makeXY('materialHardship', data3)\n",
      "39/243:\n",
      "rfselecte = SelectFromModel(RandomForestClassifier(n_estimators=100))\n",
      "Xe_train_rf = rfselecte.fit_transform(Xe_train3, Ye_train3)\n",
      "rfse = RandomForestClassifier(n_estimators=100).fit(Xe_train_rf, Ye_train3)\n",
      "Xe_test_rf = rfselect.transform(Xe_test3)\n",
      "rfse_pred = rfs.predict(Xe_test_rf)\n",
      "print mean_squared_error(rfse_pred, Ye_test3)\n",
      "print sum(rfselecte.get_support())\n",
      "print rfselecte.get_support()[:10]\n",
      "39/244:\n",
      "modelsgrit = [LinearRegression(normalize=True), \\\n",
      "          RidgeCV(cv=5, normalize=True), \\\n",
      "          LassoCV(max_iter=1000, cv=2, normalize=True), \\\n",
      "          ElasticNetCV(max_iter=1000, cv=2, normalize=True), \\\n",
      "          RandomForestRegressor(n_estimators=100), \\\n",
      "          GaussianProcessRegressor(n_restarts_optimizer=2, normalize_y=True), \\\n",
      "          AdaBoostRegressor(n_estimators=100), \\\n",
      "          SVR(kernel='rbf', gamma='auto')\n",
      "         ]\n",
      "39/245: grits3 = fitmodels(Xgrit_train3, Xgrit_test3, Ygrit_train3, Ygrit_test3, modelsgrit)\n",
      "39/246: rfsgrit = RandomForestRegressor(n_estimators=100).fit(Xgrit_train_rf, Ygrit_train3)\n",
      "39/247:\n",
      "rfsgrit_pred = rfs.predict(Xgrit_test_rf)\n",
      "print mean_squared_error(rfsgrit_pred, Ygrit_test3)\n",
      "39/248:\n",
      "grit_rf = fitmodels(Xgrit_train_rf, Xgrit_test_rf, Ygrit_train3, Ygrit_test3, [LinearRegression(normalize=True), \\\n",
      "          RidgeCV(cv=5, normalize=True), \\\n",
      "          AdaBoostRegressor(n_estimators=100), \\\n",
      "          RandomForestRegressor(n_estimators=100), \\\n",
      "          GaussianProcessRegressor(n_restarts_optimizer=2, normalize_y=True), \\\n",
      "          SVR(kernel='rbf', gamma='auto')\n",
      "         ])\n",
      "39/249:\n",
      "grit_rf = fitmodels(Xgrit_train_rf, Xgrit_test_rf, Ygrit_train3, Ygrit_test3, [LinearRegression(normalize=True), \\\n",
      "          RidgeCV(cv=5, normalize=True), \\\n",
      "          AdaBoostRegressor(n_estimators=100), \\\n",
      "          RandomForestRegressor(n_estimators=100), \\\n",
      "          GaussianProcessRegressor(n_restarts_optimizer=2, normalize_y=True), \\\n",
      "          SVR(kernel='rbf', gamma='auto')\n",
      "         ])\n",
      "39/250: Xgrit_train_rf.shape\n",
      "39/251: Xgrit_train_rf.shape, Xgrit_test_rf.shape\n",
      "39/252: Xgrit_train_rf.shape, Xgrit_test_rf.shape, Ygrit_train3.shape, Ygrit_test3.shape\n",
      "39/253: Xgrit_train_rf.shape, Xgrit_test_rf.shape, len(Ygrit_train3), len(Ygrit_test3)\n",
      "39/254: Xgrit_test_rf = rfselect.transform(Xgrit_test3)\n",
      "39/255: Xgrit_train_rf.shape, Xgrit_test_rf.shape, len(Ygrit_train3), len(Ygrit_test3)\n",
      "39/256: Xgrit_train_rf = rfselect.transform(Xgrit_train3)\n",
      "39/257: Xgrit_train_rf.shape, Xgrit_test_rf.shape, len(Ygrit_train3), len(Ygrit_test3)\n",
      "39/258:\n",
      "rfselectgrit = SelectFromModel(RandomForestRegressor(n_estimators=100))\n",
      "Xgrit_train_rf = rfselectgrit.fit_transform(Xgrit_train3, Ygrit_train3)\n",
      "rfsgrit = RandomForestRegressor(n_estimators=100).fit(Xgrit_train_rf, Ygrit_train3)\n",
      "Xgrit_test_rf = rfselectgrit.transform(Xgrit_test3)\n",
      "rfsgrit_pred = rfsgrit.predict(Xgrit_test_rf)\n",
      "print mean_squared_error(rfsgrit_pred, Ygrit_test3)\n",
      "print sum(rfselectgrit.get_support())\n",
      "print rfselectgrit.get_support()[:10]\n",
      "39/259:\n",
      "grit_rf = fitmodels(Xgrit_train_rf, Xgrit_test_rf, Ygrit_train3, Ygrit_test3, [LinearRegression(normalize=True), \\\n",
      "          RidgeCV(cv=5, normalize=True), \\\n",
      "          AdaBoostRegressor(n_estimators=100), \\\n",
      "          RandomForestRegressor(n_estimators=100), \\\n",
      "          GaussianProcessRegressor(n_restarts_optimizer=2, normalize_y=True), \\\n",
      "          SVR(kernel='rbf', gamma='auto')\n",
      "         ])\n",
      "39/260:\n",
      "rfselectmh = SelectFromModel(RandomForestRegressor(n_estimators=100))\n",
      "Xmh_train_rf = rfselectmh.fit_transform(Xmh_train3, Ymh_train3)\n",
      "rfsmh = RandomForestRegressor(n_estimators=100).fit(Xmh_train_rf, Ymh_train3)\n",
      "Xmh_test_rf = rfselectmh.transform(Xmh_test3)\n",
      "rfsmh_pred = rfsmh.predict(Xmh_test_rf)\n",
      "print mean_squared_error(rfsmh_pred, Ymh_test3)\n",
      "print sum(rfselectmh.get_support())\n",
      "print rfselectmh.get_support()[:10]\n",
      "39/261:\n",
      "mh_rf = fitmodels(Xmh_train_rf, Xmh_test_rf, Ymh_train3, Ymh_test3, [LinearRegression(normalize=True), \\\n",
      "          RidgeCV(cv=5, normalize=True), \\\n",
      "          AdaBoostRegressor(n_estimators=100), \\\n",
      "          RandomForestRegressor(n_estimators=100), \\\n",
      "          GaussianProcessRegressor(n_restarts_optimizer=2, normalize_y=True), \\\n",
      "          SVR(kernel='rbf', gamma='auto')\n",
      "         ])\n",
      "39/262:\n",
      "rfselecte = SelectFromModel(RandomForestClassifier(n_estimators=100))\n",
      "Xe_train_rf = rfselecte.fit_transform(Xe_train3, Ye_train3)\n",
      "rfse = RandomForestClassifier(n_estimators=100).fit(Xe_train_rf, Ye_train3)\n",
      "Xe_test_rf = rfselecte.transform(Xe_test3)\n",
      "rfse_pred = rfse.predict(Xe_test_rf)\n",
      "print mean_squared_error(rfse_pred, Ye_test3)\n",
      "print sum(rfselecte.get_support())\n",
      "print rfselecte.get_support()[:10]\n",
      "39/263:\n",
      "rfselecte = SelectFromModel(RandomForestClassifier(n_estimators=100))\n",
      "Xe_train_rf = rfselecte.fit(Xe_train3, Ye_train3)\n",
      "rfse = RandomForestClassifier(n_estimators=100).fit(Xe_train_rf, Ye_train3)\n",
      "Xe_test_rf = rfselecte.transform(Xe_test3)\n",
      "rfse_pred = rfse.predict(Xe_test_rf)\n",
      "print mean_squared_error(rfse_pred, Ye_test3)\n",
      "print sum(rfselecte.get_support())\n",
      "print rfselecte.get_support()[:10]\n",
      "39/264: Ye_train3\n",
      "39/265: Ye_train2\n",
      "39/266: sum(Ye_train2)\n",
      "39/267: sum(Ye_train3)\n",
      "39/268: Xe_train3, Xe_test3, Ye_train3, Ye_test3, IDe_train3, IDe_test3 = makeXY('materialHardship', data3)\n",
      "39/269: Ye_train3\n",
      "39/270: Ye_train\n",
      "39/271: Xe_train3, Xe_test3, Ye_train3, Ye_test3, IDe_train3, IDe_test3 = makeXY('eviction', data3)\n",
      "39/272:\n",
      "rfselecte = SelectFromModel(RandomForestClassifier(n_estimators=100))\n",
      "Xe_train_rf = rfselecte.fit_transform(Xe_train3, Ye_train3)\n",
      "rfse = RandomForestClassifier(n_estimators=100).fit(Xe_train_rf, Ye_train3)\n",
      "Xe_test_rf = rfselecte.transform(Xe_test3)\n",
      "rfse_pred = rfse.predict(Xe_test_rf)\n",
      "print mean_squared_error(rfse_pred, Ye_test3)\n",
      "print sum(rfselecte.get_support())\n",
      "print rfselecte.get_support()[:10]\n",
      "39/273:\n",
      "emodels3 = [SGDClassifier(loss='log', penalty='none'), \\\n",
      "          SGDClassifier(loss='log', penalty='l2'), \\\n",
      "          SGDClassifier(loss='log', penalty='l1'), \\\n",
      "          SGDClassifier(loss='log', penalty='elasticnet'), \\\n",
      "          AdaBoostClassifier(n_estimators=100), \\\n",
      "          RandomForestClassifier(n_estimators=100), \\\n",
      "          GaussianProcessClassifier(n_restarts_optimizer=2), \\\n",
      "          SVC(kernel='rbf', gamma='auto', probability=True)\n",
      "         ]\n",
      "39/274: e = fitclassifiers(normalize(Xe_train_rf), normalize(Xe_test_rf), Ye_train, Ye_test, emodels)\n",
      "39/275:\n",
      "rfselectl = SelectFromModel(RandomForestRegressor(n_estimators=100))\n",
      "Xl_train_rf = rfselectl.fit_transform(Xl_train3, Yl_train3)\n",
      "rfsl = RandomForestRegressor(n_estimators=100).fit(Xl_train_rf, Yl_train3)\n",
      "Xl_test_rf = rfselectl.transform(Xl_test3)\n",
      "rfsl_pred = rfsl.predict(Xl_test_rf)\n",
      "print mean_squared_error(rfsl_pred, Yl_test3)\n",
      "print sum(rfselectl.get_support())\n",
      "print rfselectl.get_support()[:10]\n",
      "39/276: Xl_train3, Xl_test3, Yl_train3, Yl_test3, IDl_train3, IDl_test3 = makeXY(layoff, data3)\n",
      "39/277: Xl_train3, Xl_test3, Yl_train3, Yl_test3, IDl_train3, IDl_test3 = makeXY('layoff', data3)\n",
      "39/278:\n",
      "rfselectl = SelectFromModel(RandomForestRegressor(n_estimators=100))\n",
      "Xl_train_rf = rfselectl.fit_transform(Xl_train3, Yl_train3)\n",
      "rfsl = RandomForestRegressor(n_estimators=100).fit(Xl_train_rf, Yl_train3)\n",
      "Xl_test_rf = rfselectl.transform(Xl_test3)\n",
      "rfsl_pred = rfsl.predict(Xl_test_rf)\n",
      "print mean_squared_error(rfsl_pred, Yl_test3)\n",
      "print sum(rfselectl.get_support())\n",
      "print rfselectl.get_support()[:10]\n",
      "39/279:\n",
      "lmodels3 = [SGDClassifier(loss='log', penalty='none'), \\\n",
      "          SGDClassifier(loss='log', penalty='l2'), \\\n",
      "          SGDClassifier(loss='log', penalty='l1'), \\\n",
      "          SGDClassifier(loss='log', penalty='elasticnet'), \\\n",
      "          AdaBoostClassifier(n_estimators=100), \\\n",
      "          RandomForestClassifier(n_estimators=100), \\\n",
      "          GaussianProcessClassifier(n_restarts_optimizer=2), \\\n",
      "          SVC(kernel='rbf', gamma='auto', probability=True)\n",
      "         ]\n",
      "39/280: l1 = fitclassifiers(normalize(Xl_train_rf), normalize(Xl_test_rf), Yl_train, Yl_test, lmodels)\n",
      "39/281: Xjt_train3, Xjt_test3, Yjt_train3, Yjt_test3, IDjt_train3, IDjt_test3 = makeXY('jobTraining', data3)\n",
      "39/282: brier_score_loss(Ye_test3, rfse.predict_proba(Xe_test_rf))\n",
      "39/283:\n",
      "rfselectjt = SelectFromModel(RandomForestRegressor(n_estimators=100))\n",
      "Xjt_train_rf = rfselectjt.fit_transform(Xjt_train3, Yjt_train3)\n",
      "rfsjt = RandomForestRegressor(n_estimators=100).fit(Xjt_train_rf, Yjt_train3)\n",
      "Xjt_test_rf = rfselectjt.transform(Xjt_test3)\n",
      "rfsjt_pred = rfsjt.predict(Xjt_test_rf)\n",
      "print mean_squared_error(rfsjt_pred, Yjt_test3)\n",
      "print sum(rfselectjt.get_support())\n",
      "print rfselectjt.get_support()[:10]\n",
      "39/284:\n",
      "jtmodels3 = [SGDClassifier(loss='log', penalty='none'), \\\n",
      "          SGDClassifier(loss='log', penalty='l2'), \\\n",
      "          SGDClassifier(loss='log', penalty='l1'), \\\n",
      "          SGDClassifier(loss='log', penalty='elasticnet'), \\\n",
      "          AdaBoostClassifier(n_estimators=100), \\\n",
      "          RandomForestClassifier(n_estimators=100), \\\n",
      "          GaussianProcessClassifier(n_restarts_optimizer=2), \\\n",
      "          SVC(kernel='rbf', gamma='auto', probability=True)\n",
      "         ]\n",
      "39/285: jt1 = fitclassifiers(normalize(Xjt_train_rf), normalize(Xjt_test_rf), Yjt_train, Yjt_test, jtmodels)\n",
      "39/286: jt3 = fitclassifiers(normalize(Xjt_train_rf), normalize(Xjt_test_rf), Yjt_train, Yjt_test, jtmodels3)\n",
      "39/287: Xjt_train_rf.shape, Xjt_test_rf.shape\n",
      "39/288: jt3 = fitclassifiers(normalize(Xjt_train_rf), normalize(Xjt_test_rf), Yjt_train3, Yjt_test3, jtmodels3)\n",
      "39/289:\n",
      "l1 = fitclassifiers(normalize(Xl_train_rf), normalize(Xl_test_rf), Yl_train, Yl_test, [SGDClassifier(loss='log', penalty='none'), \\\n",
      "          SGDClassifier(loss='log', penalty='l2'), \\\n",
      "          SGDClassifier(loss='log', penalty='l1'), \\\n",
      "          SGDClassifier(loss='log', penalty='elasticnet')])\n",
      "39/290:\n",
      "l1 = fitclassifiers(normalize(Xl_train_rf), normalize(Xl_test_rf), Yl_train, Yl_test, [SGDClassifier(loss='log', penalty='none'), \\\n",
      "          SGDClassifier(loss='log', penalty='l2'), \\\n",
      "          SGDClassifier(loss='log', penalty='l1'), \\\n",
      "          SGDClassifier(loss='log', penalty='elasticnet')])\n",
      "41/5:\n",
      "def split(d, trainingProportion):\n",
      "    d = shuffle(d)\n",
      "    splitInd = int(d.shape[0] * trainingProportion)\n",
      "    train = d.iloc[0:splitInd,]\n",
      "    test = d.iloc[splitInd:,]\n",
      "    return train, test\n",
      "39/291:\n",
      "def split(d, trainingProportion):\n",
      "    d = shuffle(d)\n",
      "    splitInd = int(d.shape[0] * trainingProportion)\n",
      "    train = d.iloc[0:splitInd,]\n",
      "    test = d.iloc[splitInd:,]\n",
      "    return train, test\n",
      "39/292:\n",
      "fullTrain = pd.read_csv('train.csv')\n",
      "fullTrain.info()\n",
      "\n",
      "fullTrainNoNA = dropsame(fullTrain).select_dtypes(include=['float64', 'int64'])\n",
      "fullTrainNoNA.info()\n",
      "train, validation = split(fullTrain, .6)\n",
      "print(train.shape)\n",
      "print(validation.shape)\n",
      "39/293:\n",
      "fullTrain = train.copy()\n",
      "fullTrain.info()\n",
      "fullTrainNoNA = dropsame(fullTrain).select_dtypes(include=['float64', 'int64'])\n",
      "fullTrainNoNA.info()\n",
      "train, validation = split(fullTrain, .6)\n",
      "print(train.shape)\n",
      "print(validation.shape)\n",
      "39/294:\n",
      "from sklearn.utils import shuffle\n",
      "fullTrain = train.copy()\n",
      "fullTrain.info()\n",
      "fullTrainNoNA = dropsame(fullTrain).select_dtypes(include=['float64', 'int64'])\n",
      "fullTrainNoNA.info()\n",
      "train, validation = split(fullTrain, .6)\n",
      "print(train.shape)\n",
      "print(validation.shape)\n",
      "39/295:\n",
      "def doGrit(d, train, test):\n",
      "    Xgrit_train, Xgrit_test, Ygrit_train, Ygrit_test, IDgrit_train, IDgrit_test = makeXY('grit', d, train, test)\n",
      "    print(Xgrit_train.shape, Xgrit_test.shape, len(Ygrit_train), len(Ygrit_test))\n",
      "    rf_grit = RandomForestRegressor(n_estimators=100)\n",
      "    rf_grit.fit(Xgrit_train, Ygrit_train)\n",
      "    rf_grit_pred = rf_grit.predict(Xgrit_test)\n",
      "    return mean_squared_error(rf_grit_pred, Ygrit_test)\n",
      "39/296:\n",
      "def makeDatas(d, p):\n",
      "    myD = d.copy()\n",
      "    myD = dropsame(myD).select_dtypes(include=['float64', 'int64'])\n",
      "    myD = myD.dropna(thresh= int(myD.shape[0]*p), axis=1)\n",
      "    \n",
      "    for i in range(len(myD.columns)):\n",
      "        var = myD.columns[i]\n",
      "        if myD[var].dtype == 'int64':\n",
      "            myD[var] = myD[var].fillna(myD[var].mode().iloc[0])\n",
      "        else:\n",
      "            myD[var] = myD[var].fillna(myD[var].mean())\n",
      "            \n",
      "    return myD\n",
      "39/297:\n",
      "for p in [.7]:\n",
      "    d = makeDatas(background, p)\n",
      "    print(doGrit(d, train, test))\n",
      "39/298: import forestci as fci\n",
      "39/299:\n",
      "lr = LinearRegression(normalize=True)\n",
      "lr.fit(Xgpa_train, Ygpa_train)\n",
      "lr_pred = lr.predict(Xgpa_test)\n",
      "mean_squared_error(lr_pred, Ygpa_test)\n",
      "39/300:\n",
      "rf_gpa = RandomForestRegressor(n_estimators=100)\n",
      "rf_gpa.fit(Xgpa_train, Ygpa_train)\n",
      "rf_gpa_pred = rf_gpa.predict(Xgpa_test)\n",
      "mean_squared_error(rf_gpa_pred, Ygpa_test)\n",
      "39/301: fci.random_forest_error(rf_gpa, Xgpa_train, Xgpa_test)\n",
      "39/302: gpa_var = fci.random_forest_error(rf_gpa, Xgpa_train, Xgpa_test)\n",
      "39/303: plt.errorbar(Xgpa_test, rf_gpa_pred, yerr=np.sqrt(gpa_var), fmt='o')\n",
      "39/304: gpa_var\n",
      "39/305: len(gpa_var)\n",
      "39/306: Xgpa_test.shape\n",
      "39/307: np.sqrt(gpa_var)\n",
      "39/308: len(np.sqrt(gpa_var))\n",
      "39/309: plt.errorbar(Xgpa_test, rf_gpa_pred, yerr=gpa_var, fmt='o')\n",
      "39/310:\n",
      "lr_gpa = LinearRegression(normalize=True)\n",
      "lr_gpa.fit(Xgpa_train, Ygpa_train)\n",
      "lr_gpa_pred = lr.predict(Xgpa_test)\n",
      "mean_squared_error(lr_gpa_pred, Ygpa_test)\n",
      "39/311: Ygpa_test - lr_gpa_pred\n",
      "39/312: gpa_res = Ygpa_test - lr_gpa_pred\n",
      "39/313:\n",
      "gpa_res_train = Ygpa_train - lr_gpa.predict(Xgpa_train)\n",
      "gpa_res_test = Ygpa_test - lr_gpa_pred\n",
      "39/314:\n",
      "lr_gpa_res = LinearRegression(normalize=True)\n",
      "lr_gpa_res.fit(Xgpa_train, gpa_res_train)\n",
      "lr_gpa_res.score(Xgpa_test, gpa_res_test)\n",
      "39/315:\n",
      "lr_gpa_res = LinearRegression(normalize=True)\n",
      "lr_gpa_res.fit(Xgpa_train, gpa_res_train)\n",
      "lr_gpa_res.score(Xgpa_test, gpa_res_test)\n",
      "39/316:\n",
      "lr_grit = LinearRegression(normalize=True)\n",
      "lr_grit.fit(Xgrit_train, Ygrit_train)\n",
      "lr_grit_pred = lr_grit.predict(Xgrit_test)\n",
      "mean_squared_error(lr_grit_pred, Ygrit_test)\n",
      "39/317:\n",
      "grit_res_train = Ygrit_train - lr_grit.predict(Xgrit_train)\n",
      "grit_res_test = Ygrit_test - lr_grit_pred\n",
      "lr_grit_res = LinearRegression(normalize=True)\n",
      "lr_grit_res.fit(Xgrit_train, grit_res_train)\n",
      "lr_grit_res.score(Xgrit_test, grit_res_test)\n",
      "39/318:\n",
      "lr_mh = LinearRegression(normalize=True)\n",
      "lr_mh.fit(Xmh_train, Ymh_train)\n",
      "lr_mh_pred = lr_mh.predict(Xmh_test)\n",
      "mean_squared_error(lr_mh_pred, Ymh_test)\n",
      "39/319:\n",
      "mh_res_train = Ymh_train - lr_mh.predict(Xmh_train)\n",
      "mh_res_test = Ymh_test - lr_mh_pred\n",
      "lr_mh_res = LinearRegression(normalize=True)\n",
      "lr_mh_res.fit(Xmh_train, mh_res_train)\n",
      "lr_mh_res.score(Xmh_test, mh_res_test)\n",
      "39/320: from sklearn.feature_selection import f_regression, SelectKBest\n",
      "39/321:\n",
      "gpa_model = SelectKBest(score_func=f_regression, k=10)\n",
      "gpa_results = model.fit(X_gpa_train, gpa_res_train)\n",
      "\n",
      "print gpa_results.scores_\n",
      "print gpa_results.pvalues_\n",
      "39/322:\n",
      "gpa_model = SelectKBest(score_func=f_regression, k=10)\n",
      "gpa_results = gpa_model.fit(X_gpa_train, gpa_res_train)\n",
      "\n",
      "print gpa_results.scores_\n",
      "print gpa_results.pvalues_\n",
      "39/323:\n",
      "gpa_model = SelectKBest(score_func=f_regression, k=10)\n",
      "gpa_results = gpa_model.fit(Xgpa_train, gpa_res_train)\n",
      "\n",
      "print gpa_results.scores_\n",
      "print gpa_results.pvalues_\n",
      "39/324:\n",
      "gpa_model = SelectKBest(score_func=f_regression, k=100)\n",
      "gpa_results = gpa_model.fit(Xgpa_train, gpa_res_train)\n",
      "\n",
      "print gpa_results.scores_\n",
      "print gpa_results.pvalues_\n",
      "39/325:\n",
      "gpa_model = SelectKBest(score_func=f_regression, k=1000)\n",
      "gpa_results = gpa_model.fit(Xgpa_train, gpa_res_train)\n",
      "\n",
      "print gpa_results.scores_\n",
      "print gpa_results.pvalues_\n",
      "39/326:\n",
      "gpa_model = SelectKBest(score_func=f_regression, k=1000)\n",
      "gpa_results = gpa_model.fit(Xgpa_train, gpa_res_train)\n",
      "\n",
      "print gpa_results.scores_\n",
      "print gpa_results.pvalues_\n",
      "print len(gpa_results.scores_)\n",
      "39/327:\n",
      "gpa_model = SelectKBest(score_func=f_regression, k=1000)\n",
      "gpa_results = gpa_model.fit(Xgpa_train, gpa_res_train)\n",
      "gpa_model.score(Xgpa_train, gpa_res_train)\n",
      "print gpa_results.scores_\n",
      "print gpa_results.pvalues_\n",
      "39/328:\n",
      "gpa_model = SelectKBest(score_func=f_regression, k=1000)\n",
      "gpa_results = gpa_model.fit(Xgpa_train, gpa_res_train)\n",
      "print gpa_results.scores_\n",
      "print gpa_results.pvalues_\n",
      "39/329:\n",
      "features_gpa_res = zip(Xgpa_train.columns, gpa_results.scores_)\n",
      "sorted(features_gpa_res, key=lambda x: x[1], reverse=True)\n",
      "39/330:\n",
      "gpa_model = SelectKBest(score_func=f_regression, k=1000)\n",
      "gpa_results = gpa_model.fit(Xgpa_train, gpa_res_train)\n",
      "print gpa_results.scores_\n",
      "print gpa_results.pvalues_\n",
      "39/331:\n",
      "features_gpa_res = zip(Xgpa_train.columns, gpa_results.pvalues_)\n",
      "sorted(features_gpa_res, key=lambda x: x[1])\n",
      "39/332:\n",
      "features_gpa_res = zip(Xgpa_train.columns, gpa_results.pvalues_)\n",
      "print sorted(features_gpa_res, key=lambda x: x[1])[:10]\n",
      "features_gpa_res = zip(Xgpa_train.columns, gpa_results.scores_)\n",
      "print sorted(features_gpa_res, key=lambda x: x[1], reverse=True)[:10]\n",
      "39/333:\n",
      "features_gpa_res = zip(Xgpa_train.columns, gpa_results.scores_)\n",
      "print sorted(features_gpa_res, key=lambda x: x[1], reverse=True)[:10]\n",
      "39/334:\n",
      "gpa_model = SelectKBest(score_func=f_regression, k=1000)\n",
      "gpa_results = gpa_model.fit(Xgpa_train, gpa_res_train)\n",
      "print gpa_results.scores_\n",
      "39/335:\n",
      "gpa_model = SelectKBest(score_func=f_regression, k=100)\n",
      "gpa_results = gpa_model.fit(Xgpa_train, gpa_res_train)\n",
      "\n",
      "print gpa_results.scores_\n",
      "39/336:\n",
      "gpa_model = SelectKBest(score_func=f_regression, k=100)\n",
      "gpa_results = gpa_model.fit(Xgpa_train, gpa_res_train)\n",
      "print gpa_results.scores_\n",
      "features_gpa_res = zip(Xgpa_train.columns, gpa_results.scores_)\n",
      "print sorted(features_gpa_res, key=lambda x: x[1], reverse=True)[:10]\n",
      "39/337:\n",
      "gpa_model = SelectKBest(score_func=f_regression, k=100)\n",
      "gpa_results = gpa_model.fit(Xgpa_train, gpa_res_train)\n",
      "print gpa_results.scores_\n",
      "features_gpa_res = zip(Xgpa_train.columns, gpa_results.scores_)\n",
      "print sorted(features_gpa_res, key=lambda x: x[1], reverse=True)\n",
      "39/338:\n",
      "gpa_model = SelectKBest(score_func=f_regression, k=100)\n",
      "gpa_results = gpa_model.fit(Xgpa_train, gpa_res_train)\n",
      "print gpa_results.scores_\n",
      "features_gpa_res = zip(Xgpa_train.columns, gpa_results.scores_)\n",
      "sorted(features_gpa_res, key=lambda x: x[1], reverse=True)\n",
      "39/339:\n",
      "gpa_model = SelectKBest(score_func=f_regression, k=100)\n",
      "gpa_results = gpa_model.fit(Xgpa_train, gpa_res_train)\n",
      "print gpa_results.scores_\n",
      "features_gpa_res = zip(Xgpa_train.columns, gpa_results.scores_)\n",
      "sorted(features_gpa_res, key=lambda x: x[1], reverse=True)[:10]\n",
      "39/340:\n",
      "grit_model = SelectKBest(score_func=f_regression, k=100)\n",
      "grit_results = grit_model.fit(Xgrit_train, grit_res_train)\n",
      "print grit_results.scores_\n",
      "features_grit_res = zip(Xgrit_train.columns, grit_results.scores_)\n",
      "sorted(features_grit_res, key=lambda x: x[1], reverse=True)[:10]\n",
      "39/341:\n",
      "mh_model = SelectKBest(score_func=f_regression, k=100)\n",
      "mh_results = mh_model.fit(Xmh_train, mh_res_train)\n",
      "print mh_results.scores_\n",
      "features_mh_res = zip(Xmh_train.columns, mh_results.scores_)\n",
      "sorted(features_mh_res, key=lambda x: x[1], reverse=True)[:10]\n",
      "39/342: ff.select('hv4r19a_3')\n",
      "39/343:\n",
      "def doGrit(d, train, test):\n",
      "    Xgrit_train, Xgrit_test, Ygrit_train, Ygrit_test, IDgrit_train, IDgrit_test = makeXY('grit', d)\n",
      "    print(Xgrit_train.shape, Xgrit_test.shape, len(Ygrit_train), len(Ygrit_test))\n",
      "    rf_grit = RandomForestRegressor(n_estimators=100)\n",
      "    rf_grit.fit(Xgrit_train, Ygrit_train)\n",
      "    rf_grit_pred = rf_grit.predict(Xgrit_test)\n",
      "    return mean_squared_error(rf_grit_pred, Ygrit_test)\n",
      "39/344:\n",
      "def makeDatas(d, p):\n",
      "    myD = d.copy()\n",
      "    myD = dropsame(myD).select_dtypes(include=['float64', 'int64'])\n",
      "    myD = myD.dropna(thresh= int(myD.shape[0]*p), axis=1)\n",
      "    \n",
      "    for i in range(len(myD.columns)):\n",
      "        var = myD.columns[i]\n",
      "        if myD[var].dtype == 'int64':\n",
      "            myD[var] = myD[var].fillna(myD[var].mode().iloc[0])\n",
      "        else:\n",
      "            myD[var] = myD[var].fillna(myD[var].mean())\n",
      "            \n",
      "    return myD\n",
      "39/345:\n",
      "for p in [.7]:\n",
      "    d = makeDatas(background, p)\n",
      "    print(doGrit(d, train, test))\n",
      "39/346:\n",
      "probs1 = [.2,.4,.6,.81,.1,.3,.5,.7,.9,.7]\n",
      "res1 = [0.24340168604651163,0.24358185465116278,0.24377816860465118,0.24302983139534887,\n",
      "       0.24330572674418607,0.24375533720930234,0.24545936046511627,0.23928777325581396, 0.2413143779069767,\n",
      "       0.24336240697674416]\n",
      "plt.plot(probs1, res1, \"o\")\n",
      "plt.show()\n",
      "39/347:\n",
      "probs1 = [.2,.4,.6,.81,.1,.3,.5,.7,.9,.7]\n",
      "res1 = [0.24340168604651163,0.24358185465116278,0.24377816860465118,0.24302983139534887,\n",
      "       0.24330572674418607,0.24375533720930234,0.24545936046511627,0.23928777325581396, 0.2413143779069767,\n",
      "       0.24336240697674416]\n",
      "plt.plot(probs1, res1, \"o\")\n",
      "plt.title(\"Validating columns for grit\")\n",
      "plt.show()\n",
      "39/348:\n",
      "y = [0.358418, 0.361059, 0.357367, 0.352876, 0.349639, 0.350095, 0.351140, 0.351885]\n",
      "plt.plot([20*(x+1) for x in range(8)], y)\n",
      "plt.title(\"GPA: Validating n_estimators\")\n",
      "39/349:\n",
      "y = [0.358418, 0.361059, 0.357367, 0.352876, 0.349639, 0.350095, 0.351140, 0.351885]\n",
      "plt.plot([20*(x+1) for x in range(8)], y)\n",
      "plt.title(\"GPA: Validating n_estimators\")\n",
      "plt.ylabel(\"MSE\")\n",
      "plt.xlabel(\"n_estimators\")\n",
      "plt.show()\n",
      "39/350:\n",
      "x = [1, .9, .8, .7, .6, .5, .4, .3, .2, .1]\n",
      "y = [0.35185, 0.35481, 0.35339, 0.35593, 0.35286, 0.35354, 0.35802, 0.35052, 0.35176, 0.35141]\n",
      "plt.plot(x, y)\n",
      "plt.ylabel(\"MSE\")\n",
      "plt.xlabel(\"Proportion of data required to be non-null\")\n",
      "plt.title(\"GPA: Validating imputation cutoffs\")\n",
      "plt.show()\n",
      "39/351:\n",
      "ks = [100, 1000, 2500, 5000, 7500, 10000, 12000]\n",
      "for i in range(len(ks)):\n",
      "    rfselect = SelectFromModel(RandomForestRegressor(n_estimators=100), max_features=ks[i])\n",
      "    Xgpa_train_rf = rfselect.fit_transform(Xgpa_train, Ygpa_train)\n",
      "    rfs = RandomForestRegressor(n_estimators=100).fit(Xgpa_train_rf, Ygpa_train)\n",
      "    Xgpa_test_rf = rfselect.transform(Xgpa_test)\n",
      "    rfs_pred = rfs.predict(Xgpa_test_rf)\n",
      "    print i, mean_squared_error(rfs_pred, Ygpa_test), sum(rfselect.get_support())\n",
      "39/352:\n",
      "ks = [100, 1000, 2500, 5000, 7500, 10000, 12000]\n",
      "for i in range(len(ks)):\n",
      "    select = SelectKBest(f_regression, k=ks[i])\n",
      "    Xgpa_trains = select.fit_transform(Xgpa_train, Ygpa_train)\n",
      "    rfs = RandomForestRegressor(n_estimators=100).fit(Xgpa_trains, Ygpa_train)\n",
      "    Xgpa_tests = select.transform(Xgpa_test)\n",
      "    rfs_pred = rfs.predict(Xgpa_tests)\n",
      "    print i, mean_squared_error(rfs_pred, Ygpa_test), sum(select.get_support())\n",
      "39/353:\n",
      "plt.plot([100, 1000, 1917, 2500, 5000, 7500, 10000, 12000], [0.36541,0.35262,0.34675,0.35169,0.35174,0.35366,0.35001,0.35002])\n",
      "plt.title(\"GPA: Validating number of features\")\n",
      "plt.xlabel(\"Number of features\")\n",
      "plt.ylabel(\"Best MSE achieved\")\n",
      "plt.show()\n",
      "39/354:\n",
      "plt.plot([100, 1000, 1917, 2500, 5000, 7500, 10000, 12000], [0.36541,0.35262,0.34675,0.34971,0.35174,0.35366,0.35001,0.35002])\n",
      "plt.title(\"GPA: Validating number of features\")\n",
      "plt.xlabel(\"Number of features\")\n",
      "plt.ylabel(\"Best MSE achieved\")\n",
      "plt.show()\n",
      "39/355:\n",
      "plt.plot([100, 1000, 1917, 2500, 5000, 7500, 10000, 12000], [0.36541,0.35262,0.34675,0.34971,0.35112,0.35174,0.35001,0.35002])\n",
      "plt.title(\"GPA: Validating number of features\")\n",
      "plt.xlabel(\"Number of features\")\n",
      "plt.ylabel(\"Best MSE achieved\")\n",
      "plt.show()\n",
      "39/356:\n",
      "plt.plot([100, 1000, 1917, 2500, 5000, 7500, 10000, 12000], [0.36541,0.35262,0.34904,0.34971,0.35112,0.35174,0.35001,0.35002])\n",
      "plt.title(\"GPA: Validating number of features\")\n",
      "plt.xlabel(\"Number of features\")\n",
      "plt.ylabel(\"Best MSE achieved\")\n",
      "plt.show()\n",
      "39/357:\n",
      "plt.plot([100, 1000, 1917, 3500, 5000, 7500, 10000, 12000], [0.36541,0.35262,0.34904,0.34971,0.35112,0.35174,0.35001,0.35002])\n",
      "plt.title(\"GPA: Validating number of features\")\n",
      "plt.xlabel(\"Number of features\")\n",
      "plt.ylabel(\"Best MSE achieved\")\n",
      "plt.show()\n",
      "39/358:\n",
      "y = [0.358418, 0.361059, 0.357367, 0.352876, 0.349639, 0.350095, 0.351140, 0.351885]\n",
      "plt.plot([20*(x+1) for x in range(8)], y)\n",
      "plt.title(\"GPA: Validating n_estimators\")\n",
      "plt.ylabel(\"Best MSE achieved\")\n",
      "plt.xlabel(\"n_estimators\")\n",
      "plt.show()\n",
      "39/359:\n",
      "x = [1, .9, .8, .7, .6, .5, .4, .3, .2, .1]\n",
      "y = [0.35185, 0.35481, 0.35339, 0.35593, 0.35286, 0.35354, 0.35802, 0.35052, 0.35176, 0.35141]\n",
      "plt.plot(x, y)\n",
      "plt.ylabel(\"Best MSE achieved\")\n",
      "plt.xlabel(\"Proportion of data required to be non-null\")\n",
      "plt.title(\"GPA: Validating imputation cutoffs\")\n",
      "plt.show()\n",
      "39/360:\n",
      "y = [0.358418, 0.361059, 0.357367, 0.352876, 0.349639, 0.350095, 0.351140, 0.351885]\n",
      "plt.plot([20*(x+1) for x in range(8)], y)\n",
      "plt.title(\"GPA: Validating number of trees\")\n",
      "plt.ylabel(\"Best MSE achieved\")\n",
      "plt.xlabel(\"n_estimators\")\n",
      "plt.show()\n",
      "42/1:\n",
      "import networkx as nx\n",
      "import csv\n",
      "import matplotlib.pyplot as plot\n",
      "42/2:\n",
      "from bs4 import BeautifulSoup\n",
      "import urllib2\n",
      "42/3:\n",
      "from sklearn.linear_model import LinearRegression, LogisticRegression\n",
      "import numpy as np\n",
      "from scipy import stats\n",
      "42/4:\n",
      "page0 = urllib2.urlopen(\"https://en.wikipedia.org/wiki/List_of_actors_with_Academy_Award_nominations\")\n",
      "soup0 = BeautifulSoup(page0, 'html.parser')\n",
      "42/5: actors0 = soup0.find_all('tr')[11:][:-11]\n",
      "42/6: actors0[0].find('a').text\n",
      "42/7:\n",
      "academylist = []\n",
      "for actor in actors0:\n",
      "    academylist.append(actor.find('a').text)\n",
      "42/8: academylist\n",
      "42/9:\n",
      "page1 = urllib2.urlopen(\"https://www.boxofficemojo.com/people/?view=Actor&p=.htm\")\n",
      "soup1 = BeautifulSoup(page1, 'html.parser')\n",
      "42/10: actors1 = soup1.find_all('tr')[2].find_all('tr')[1:]\n",
      "42/11: actors1\n",
      "42/12: boxofficedict = {}\n",
      "42/13:\n",
      "def addtodict(actors):\n",
      "    for actor in actors:\n",
      "        name = actor.find('b').find('b').text.replace(\",\", \"\")\n",
      "        boxoffice = str(actor.find('td', attrs={'align':'right'}).text[1:]).replace(\",\", \"\")\n",
      "        if \"k\" in boxoffice:\n",
      "            boxoffice = float(boxoffice.replace(\"k\", \"\"))/1000\n",
      "        boxofficedict[name] = float(boxoffice)\n",
      "42/14: boxofficedict\n",
      "42/15:\n",
      "page2 = urllib2.urlopen(\"https://www.boxofficemojo.com/people/?view=Actor&pagenum=2&sort=person&order=ASC&p=.htm\")\n",
      "soup2 = BeautifulSoup(page2, 'html.parser')\n",
      "actors2 = soup2.find_all('tr')[2].find_all('tr')[1:]\n",
      "42/16:\n",
      "page3 = urllib2.urlopen(\"https://www.boxofficemojo.com/people/?view=Actor&pagenum=3&sort=person&order=ASC&p=.htm\")\n",
      "soup3 = BeautifulSoup(page3, 'html.parser')\n",
      "actors3 = soup3.find_all('tr')[2].find_all('tr')[1:]\n",
      "42/17:\n",
      "addtodict(actors1)\n",
      "addtodict(actors2)\n",
      "addtodict(actors3)\n",
      "42/18: len(boxofficedict)\n",
      "42/19: boxofficedict\n",
      "42/20:\n",
      "boxofficesorted = sorted(boxofficedict, key=boxofficedict.get, reverse=True)\n",
      "boxofficesorted\n",
      "42/21:\n",
      "with open('title.basics.tsv') as tsvfile:\n",
      "    reader = csv.reader(tsvfile, delimiter='\\t')\n",
      "    i = 0\n",
      "    for row in reader:\n",
      "        if i < 200:\n",
      "            print(row)\n",
      "            i = i + 1\n",
      "        else:\n",
      "            break\n",
      "42/22:\n",
      "movies = {}\n",
      "with open('title.basics.tsv') as tsvfile:\n",
      "    reader = csv.reader(tsvfile, delimiter='\\t')\n",
      "    i = 0\n",
      "    for row in reader:\n",
      "        movies[row[0]] = row[1]\n",
      "42/23:\n",
      "dict = {}\n",
      "with open('name.basics.tsv') as tsvfile:\n",
      "    reader = csv.reader(tsvfile, delimiter='\\t')\n",
      "    i = 0\n",
      "    for row in reader:\n",
      "        if i < 200:\n",
      "            print(row)\n",
      "            i = i + 1\n",
      "        if i > 1:\n",
      "            dict[row[0]] = row[1]\n",
      "42/24:\n",
      "G = nx.Graph()\n",
      "Gw = nx.Graph()\n",
      "Gm = nx.Graph()\n",
      "Gweighted = nx.Graph()\n",
      "moviecountdict = {}\n",
      "\n",
      "with open('title.principals.tsv') as tsvfile:\n",
      "    reader = csv.reader(tsvfile, delimiter='\\t')\n",
      "    prev = \"\"\n",
      "    actors = []\n",
      "    for row in reader:\n",
      "        if row[3] == 'actor' and row[2] in dict and dict[row[2]] in boxofficedict:\n",
      "            Gm.add_node(dict[row[2]])\n",
      "            G.add_node(dict[row[2]])\n",
      "            Gweighted.add_node(dict[row[2]])\n",
      "            if dict[row[2]] in moviecountdict:\n",
      "                moviecountdict[dict[row[2]]] += 1\n",
      "            else:\n",
      "                moviecountdict[dict[row[2]]] = 1\n",
      "        elif row[3] == 'actress' and row[2] in dict and dict[row[2]] in boxofficedict:\n",
      "            Gw.add_node(dict[row[2]])\n",
      "            G.add_node(dict[row[2]])\n",
      "            Gweighted.add_node(dict[row[2]])\n",
      "            if dict[row[2]] in moviecountdict:\n",
      "                moviecountdict[dict[row[2]]] += 1\n",
      "            else:\n",
      "                moviecountdict[dict[row[2]]] = 1\n",
      "        else:\n",
      "            continue\n",
      "        if row[0] != prev:\n",
      "            for i in range(len(actors)):\n",
      "                for j in range(i, len(actors)):\n",
      "                    a1 = actors[i]\n",
      "                    a2 = actors[j]\n",
      "                    if a1 == a2:\n",
      "                        continue\n",
      "                    G.add_edge(a1, a2)\n",
      "                    if Gweighted.has_edge(a1, a2):\n",
      "                        Gweighted.add_edge(a1, a2, weight=(Gweighted.get_edge_data(a1, a2)['weight'] + 1))\n",
      "                    else:\n",
      "                        Gweighted.add_edge(a1, a2, weight=1)\n",
      "                    if a1 in Gw.nodes and a2 in Gw.nodes:\n",
      "                        Gw.add_edge(a1, a2)\n",
      "                    elif a1 in Gm.nodes and a2 in Gm.nodes:\n",
      "                        Gm.add_edge(a1, a2)\n",
      "            actors = [dict[row[2]]]\n",
      "        else:\n",
      "            actors.append(dict[row[2]])\n",
      "        prev = row[0]\n",
      "42/25:\n",
      "Gtop = nx.Graph()\n",
      "Glow = nx.Graph()\n",
      "for a in G.nodes():\n",
      "    if a in academylist:\n",
      "        Gtop.add_node(a)\n",
      "    else:\n",
      "        Glow.add_node(a)\n",
      "for (u, v) in G.edges():\n",
      "    if u in academylist and v in academylist:\n",
      "        Gtop.add_edge(u, v)\n",
      "    elif u not in academylist and v not in academylist:\n",
      "        Glow.add_edge(u, v)\n",
      "42/26: len(Gtop.nodes()), len(Gtop.edges())\n",
      "42/27: len(Glow.nodes()), len(Glow.edges())\n",
      "42/28:\n",
      "actorw = sorted(Gw.nodes)\n",
      "actorm = sorted(Gm.nodes)\n",
      "42/29: degree_centrality = nx.degree_centrality(G)\n",
      "42/30: sorted(degree_centrality, key=degree_centrality.get, reverse=True)\n",
      "42/31: eigenvector_centrality = nx.eigenvector_centrality(G)\n",
      "42/32: sorted(eigenvector_centrality, key=eigenvector_centrality.get, reverse=True)\n",
      "42/33: closeness_centrality = nx.closeness_centrality(G)\n",
      "42/34: sorted(closeness_centrality, key=closeness_centrality.get, reverse=True)\n",
      "42/35: betweenness_centrality = nx.betweenness_centrality(G)\n",
      "42/36: sorted(betweenness_centrality, key=betweenness_centrality.get, reverse=True)\n",
      "42/37: cluster_coefficient = nx.algorithms.cluster.clustering(G)\n",
      "42/38: sorted(cluster_coefficient, key=cluster_coefficient.get, reverse=True)\n",
      "42/39:\n",
      "for i in range(10):\n",
      "    print sorted(effective_size, key=effective_size.get, reverse=True)[i] + \",\"\n",
      "42/40:\n",
      "cluster_coefficients = [cluster_coefficient.get(x) for x in actorlist]\n",
      "cluster_coefficients\n",
      "42/41:\n",
      "plot.hist(cluster_coefficients)\n",
      "plot.title(\"Cluster Coefficients\")\n",
      "plot.show()\n",
      "np.max(cluster_coefficients), np.mean(cluster_coefficients), np.std(cluster_coefficients)\n",
      "42/42: effective_size = nx.algorithms.structuralholes.effective_size(G)\n",
      "42/43:\n",
      "for i in range(10):\n",
      "    print sorted(effective_size, key=effective_size.get, reverse=True)[i] + \",\"\n",
      "42/44: sorted(effective_size, key=effective_size.get, reverse=True)\n",
      "42/45:\n",
      "degree_centralities = []\n",
      "actorlist = [a for a in actorw]\n",
      "for a in actorm:\n",
      "    actorlist.append(a)\n",
      "for actor in actorlist:\n",
      "    degree_centralities.append(degree_centrality.get(actor))\n",
      "42/46:\n",
      "cluster_coefficients = [cluster_coefficient.get(x) for x in actorlist]\n",
      "cluster_coefficients\n",
      "42/47:\n",
      "plot.hist(cluster_coefficients)\n",
      "plot.title(\"Cluster Coefficients\")\n",
      "plot.show()\n",
      "np.max(cluster_coefficients), np.mean(cluster_coefficients), np.std(cluster_coefficients)\n",
      "42/48:\n",
      "effective_sizes = [0 if np.isnan(effective_size.get(x)) else effective_size.get(x) for x in actorlist]\n",
      "effective_sizes\n",
      "42/49:\n",
      "plot.hist(effective_sizes)\n",
      "plot.title(\"Effective Size\")\n",
      "plot.show()\n",
      "np.max(effective_sizes), np.mean(effective_sizes), np.std(effective_sizes)\n",
      "42/50: len(actorw)\n",
      "42/51: actorlist\n",
      "42/52: degree_centralities\n",
      "42/53:\n",
      "plot.hist(degree_centralities)\n",
      "plot.title(\"Degree Centrality\")\n",
      "plot.show()\n",
      "np.max(degree_centralities), np.mean(degree_centralities), np.std(degree_centralities)\n",
      "42/54:\n",
      "plot.hist(degree_centralities[:313])\n",
      "plot.title(\"Degree Centrality - Female Actors\")\n",
      "plot.show()\n",
      "np.median(degree_centralities[:313]), np.mean(degree_centralities[:313]), np.std(degree_centralities[:313])\n",
      "42/55:\n",
      "plot.hist(degree_centralities[313:])\n",
      "plot.title(\"Degree Centrality - Male Actors\")\n",
      "plot.show()\n",
      "np.median(degree_centralities[313:]), np.mean(degree_centralities[313:]), np.std(degree_centralities[313:])\n",
      "42/56:\n",
      "boxofficegross = []\n",
      "for actor in actorlist:\n",
      "    boxofficegross.append(boxofficedict[actor])\n",
      "42/57: stats.ranksums(degree_centralities[313:], degree_centralities[:313])\n",
      "42/58:\n",
      "plot.hist(boxofficegross[:313])\n",
      "np.median(boxofficegross[:313]), np.mean(boxofficegross[:313]), np.std(boxofficegross[:313])\n",
      "42/59:\n",
      "plot.hist(boxofficegross[313:])\n",
      "np.median(boxofficegross[313:]), np.mean(boxofficegross[313:]), np.std(boxofficegross[313:])\n",
      "42/60: stats.ranksums(boxofficegross[:313], boxofficegross[313:])\n",
      "42/61: stats.ranksums(boxofficegross[313:], boxofficegross[:313])\n",
      "42/62: stats.ranksum(boxtop, boxlow)\n",
      "42/63: boxtop = [boxofficedict.get(a) for a in Gtop.nodes()]\n",
      "42/64:\n",
      "arrlow = getarr(G, Glow.nodes())\n",
      "arrlow[4] = [0 if np.isnan(a) else a for a in arrlow[4]]\n",
      "boxlow = [boxofficedict.get(a) for a in Glow.nodes()]\n",
      "regs(arrlow, boxlow)\n",
      "42/65:\n",
      "def getarr(graph, alist):\n",
      "    degree = nx.degree_centrality(graph)\n",
      "    degrees = [degree.get(a) for a in alist]\n",
      "    eigenvector = nx.eigenvector_centrality(graph)\n",
      "    eigenvectors = [eigenvector.get(a) for a in alist]\n",
      "    close = nx.closeness_centrality(graph)\n",
      "    closes = [close.get(a) for a in alist]\n",
      "    between = nx.betweenness_centrality(graph)\n",
      "    betweens = [between.get(a) for a in alist]\n",
      "    effective = nx.algorithms.structuralholes.effective_size(graph)\n",
      "    effectives = [effective.get(a) for a in alist]\n",
      "    cluster = nx.algorithms.cluster.clustering(graph)\n",
      "    clusters = [cluster.get(a) for a in alist]\n",
      "    return [degrees, eigenvectors, closes, betweens, effectives, clusters]\n",
      "42/66:\n",
      "arrlow = getarr(G, Glow.nodes())\n",
      "arrlow[4] = [0 if np.isnan(a) else a for a in arrlow[4]]\n",
      "boxlow = [boxofficedict.get(a) for a in Glow.nodes()]\n",
      "regs(arrlow, boxlow)\n",
      "42/67: stats.ranksum(boxtop, boxlow)\n",
      "42/68: stats.ranksums(boxtop, boxlow)\n",
      "42/69: arrtop = getarr(G, Gtop.nodes())\n",
      "42/70:\n",
      "for i in range(len(arrlow)):\n",
      "    print stats.ranksums(arrlow[i], arrtop[i])\n",
      "39/361:\n",
      "def numfeatures(Xtrain, Ytrain):\n",
      "    ks = [100, 1000, 2500, 5000, 7500, 10000, 12000]\n",
      "    for i in range(len(ks)):\n",
      "        select = SelectKBest(f_regression, k=ks[i])\n",
      "        Xtrains = select.fit_transform(Xtrain, Ytrain)\n",
      "        rfs = RandomForestRegressor(n_estimators=100).fit(Xtrains, Ytrain)\n",
      "        Xtests = select.transform(Xtest)\n",
      "        rfs_pred = rfs.predict(Xtests)\n",
      "        print i, mean_squared_error(rfs_pred, Ytest), sum(select.get_support())\n",
      "39/362: numfeatures(Xgrit_train, Ygrit_train, Xgrit_test, Ygrit_test)\n",
      "39/363:\n",
      "def numfeatures(Xtrain, Ytrain, Xtest, Ytest):\n",
      "    ks = [100, 1000, 2500, 5000, 7500, 10000, 12000]\n",
      "    for i in range(len(ks)):\n",
      "        select = SelectKBest(f_regression, k=ks[i])\n",
      "        Xtrains = select.fit_transform(Xtrain, Ytrain)\n",
      "        rfs = RandomForestRegressor(n_estimators=100).fit(Xtrains, Ytrain)\n",
      "        Xtests = select.transform(Xtest)\n",
      "        rfs_pred = rfs.predict(Xtests)\n",
      "        print i, mean_squared_error(rfs_pred, Ytest), sum(select.get_support())\n",
      "39/364:\n",
      "data4 = dropsame(background)\n",
      "strs = data4.select_dtypes(\"object\")\n",
      "nu2 = strs.nunique()\n",
      "keep = nu2[nu2 < 5].index\n",
      "data4[keep] = data4[keep].apply(lambda x: pd.factorize(x)[0])\n",
      "39/365:\n",
      "data4 = dropsame(background)\n",
      "strs4 = data4.select_dtypes(\"object\")\n",
      "nu4 = strs4.nunique()\n",
      "keep4 = nu4[nu4 < 5].index\n",
      "data4[keep4] = data4[keep4].apply(lambda x: pd.factorize(x)[0])\n",
      "39/366:\n",
      "notkeep4 = nu4[nu4 >= 5].index\n",
      "data4 = data4.drop(notkeep4, axis=1)\n",
      "numeric4 = data4._get_numeric_data()\n",
      "numeric4[numeric4 < 0] = np.nan\n",
      "39/367:\n",
      "from sklearn.linear_model import LinearRegression, RidgeCV, LassoCV, ElasticNetCV, SGDClassifier, MultiTaskElasticNet\n",
      "from sklearn.ensemble import RandomForestRegressor, RandomForestClassifier, AdaBoostRegressor, AdaBoostClassifier\n",
      "from sklearn.gaussian_process import GaussianProcessRegressor, GaussianProcessClassifier\n",
      "from sklearn.svm import SVC, LinearSVC, SVR\n",
      "from sklearn.metrics import mean_squared_error, brier_score_loss\n",
      "from sklearn.feature_selection import SelectFromModel, SelectKBest, f_regression\n",
      "from sklearn.preprocessing import normalize\n",
      "from sklearn.impute import IterativeImputer\n",
      "39/368:\n",
      "from sklearn.linear_model import LinearRegression, RidgeCV, LassoCV, ElasticNetCV, SGDClassifier, MultiTaskElasticNet\n",
      "from sklearn.ensemble import RandomForestRegressor, RandomForestClassifier, AdaBoostRegressor, AdaBoostClassifier\n",
      "from sklearn.gaussian_process import GaussianProcessRegressor, GaussianProcessClassifier\n",
      "from sklearn.svm import SVC, LinearSVC, SVR\n",
      "from sklearn.metrics import mean_squared_error, brier_score_loss\n",
      "from sklearn.feature_selection import SelectFromModel, SelectKBest, f_regression\n",
      "from sklearn.preprocessing import normalize\n",
      "39/369: from sklearn.impute import IterativeImputer\n",
      "39/370: from sklearn.impute import BasicImputer\n",
      "39/371: from sklearn.impute import SimpleImputer\n",
      "39/372: from sklearn.impute import IterativeImputer\n",
      "39/373: from fancyimpute import IterativeImputer\n",
      "39/374: import tensorflow as tf\n",
      "39/375: import tensorflow as tf\n",
      "39/376: from fancyimpute import IterativeImputer\n",
      "39/377: from fancyimpute import IterativeImputer\n",
      "39/378: from fancyimpute import IterativeImputer\n",
      "39/379: import tensorflow as tf\n",
      "43/1: from fancyimpute import IterativeImputer\n",
      "43/2:\n",
      "import pandas as pd\n",
      "import matplotlib.pyplot as plt\n",
      "import seaborn as sns\n",
      "import re\n",
      "import numpy as np\n",
      "import math\n",
      "import ff\n",
      "43/3:\n",
      "from sklearn.linear_model import LinearRegression, RidgeCV, LassoCV, ElasticNetCV, SGDClassifier, MultiTaskElasticNet\n",
      "from sklearn.ensemble import RandomForestRegressor, RandomForestClassifier, AdaBoostRegressor, AdaBoostClassifier\n",
      "from sklearn.gaussian_process import GaussianProcessRegressor, GaussianProcessClassifier\n",
      "from sklearn.svm import SVC, LinearSVC, SVR\n",
      "from sklearn.metrics import mean_squared_error, brier_score_loss\n",
      "from sklearn.feature_selection import SelectFromModel, SelectKBest, f_regression\n",
      "from sklearn.preprocessing import normalize\n",
      "43/4:\n",
      "data = pd.read_csv('output.csv', low_memory=False)\n",
      "data.info()\n",
      "43/5:\n",
      "data = pd.read_csv('output.csv', low_memory=False)\n",
      "data.info()\n",
      "43/6:\n",
      "def dropsame(dat):\n",
      "    nunique = dat.apply(pd.Series.nunique)\n",
      "    cols_to_drop = nunique[nunique == 1].index\n",
      "    return dat.drop(cols_to_drop, axis=1).copy()\n",
      "43/7:\n",
      "# make X and Y dataframes for test and train from inputdata \n",
      "def makeXY(field, inputdata):\n",
      "    IDs_train = []\n",
      "    Y_train = []\n",
      "    for i in range(len(train)):\n",
      "        if not np.isnan(train[field][i]):\n",
      "            IDs_train.append(train['challengeID'][i]-1)\n",
      "            Y_train.append(train[field][i])\n",
      "    IDs_test = []\n",
      "    Y_test = []\n",
      "    for i in range(len(test)):\n",
      "        if not np.isnan(test[field][i]):\n",
      "            IDs_test.append(test['challengeID'][i]-1)\n",
      "            Y_test.append(test[field][i])\n",
      "    X_train = inputdata.drop('challengeID', axis=1).loc[IDs_train].select_dtypes(include=['float64', 'int64'])\n",
      "    X_test = inputdata.drop('challengeID', axis=1).loc[IDs_test].select_dtypes(include=['float64', 'int64'])\n",
      "    return X_train, X_test, Y_train, Y_test, IDs_train, IDs_test\n",
      "43/8:\n",
      "background = pd.read_csv('background.csv', low_memory=False)\n",
      "background.info()\n",
      "43/9:\n",
      "train = pd.read_csv('train.csv')\n",
      "train.info()\n",
      "43/10:\n",
      "test = pd.read_csv('test.csv')\n",
      "test.info()\n",
      "43/11:\n",
      "data4 = dropsame(background)\n",
      "strs4 = data4.select_dtypes(\"object\")\n",
      "nu4 = strs4.nunique()\n",
      "keep4 = nu4[nu4 < 5].index\n",
      "data4[keep4] = data4[keep4].apply(lambda x: pd.factorize(x)[0])\n",
      "43/12:\n",
      "notkeep4 = nu4[nu4 >= 5].index\n",
      "data4 = data4.drop(notkeep4, axis=1)\n",
      "numeric4 = data4._get_numeric_data()\n",
      "numeric4[numeric4 < 0] = np.nan\n",
      "43/13: data4 = IterativeImputer().fit_transform(data4)\n",
      "43/14:\n",
      "notkeep4 = nu4[nu4 >= 5].index\n",
      "data4 = data4.drop(notkeep4, axis=1)\n",
      "numeric4 = data4._get_numeric_data()\n",
      "numeric4[numeric4 < 0] = np.nan\n",
      "43/15:\n",
      "data4 = dropsame(background)\n",
      "strs4 = data4.select_dtypes(\"object\")\n",
      "nu4 = strs4.nunique()\n",
      "keep4 = nu4[nu4 < 5].index\n",
      "data4[keep4] = data4[keep4].apply(lambda x: pd.factorize(x)[0])\n",
      "43/16:\n",
      "notkeep4 = nu4[nu4 >= 5].index\n",
      "data4 = data4.drop(notkeep4, axis=1)\n",
      "numeric4 = data4._get_numeric_data()\n",
      "numeric4[numeric4 < 0] = np.nan\n",
      "43/17: data4 = data4.dropna(thresh=424*7, axis=1)\n",
      "43/18: data4.info()\n",
      "43/19: data4.isnull().sum().sum()\n",
      "43/20: data4 = data4.dropna(thresh=424*8, axis=1)\n",
      "43/21: data4.isnull().sum().sum()\n",
      "43/22: data4 = data4.dropna(thresh=424*9, axis=1)\n",
      "43/23: data4.isnull().sum().sum()\n",
      "43/24: data4 = IterativeImputer().fit_transform(data4)\n",
      "43/25: data4.info()\n",
      "43/26: data4 = IterativeImputer().fit_transform(data4)\n",
      "43/27: data4.isnull().sum().sum()\n",
      "43/28: Xgpa_train4, Xgpa_test4, Ygpa_train4, Ygpa_test4, IDgpa_train4, IDgpa_test4 = makeXY('gpa', data4)\n",
      "43/29: data4.shape\n",
      "43/30:\n",
      "data4df = dropsame(background)\n",
      "strs4 = data4df.select_dtypes(\"object\")\n",
      "nu4 = strs4.nunique()\n",
      "keep4 = nu4[nu4 < 5].index\n",
      "data4df[keep4] = data4df[keep4].apply(lambda x: pd.factorize(x)[0])\n",
      "43/31:\n",
      "notkeep4 = nu4[nu4 >= 5].index\n",
      "data4df = data4df.drop(notkeep4, axis=1)\n",
      "numeric4 = data4df._get_numeric_data()\n",
      "numeric4[numeric4 < 0] = np.nan\n",
      "43/32: data4df = data4df.dropna(thresh=424*9, axis=1)\n",
      "43/33: data4df.isnull().sum().sum()\n",
      "43/34: data4df.shape\n",
      "43/35: print pd.DataFrame(data=data4, index=data4df.index, columns=data4df.columns)\n",
      "43/36: pd.DataFrame(data=data4, index=data4df.index, columns=data4df.columns)\n",
      "43/37: d4 = pd.DataFrame(data=data4, index=data4df.index, columns=data4df.columns)\n",
      "43/38: Xgpa_train4, Xgpa_test4, Ygpa_train4, Ygpa_test4, IDgpa_train4, IDgpa_test4 = makeXY('gpa', d4)\n",
      "43/39: gpas4 = fitmodels(Xgpa_train4, Xgpa_test4, Ygpa_train4, Ygpa_test4, gpa4models)\n",
      "43/40:\n",
      "def fitmodels(X_train, X_test, Y_train, Y_test, models):\n",
      "    ret = []\n",
      "    for m in models:\n",
      "        m.fit(X_train, Y_train)\n",
      "        m_pred = m.predict(X_test)\n",
      "        mse = mean_squared_error(m_pred, Y_test)\n",
      "        ret.append((m, mse))\n",
      "        print mse\n",
      "    return ret\n",
      "43/41:\n",
      "gpa4models = [LinearRegression(normalize=True), \\\n",
      "          RidgeCV(cv=5, normalize=True), \\\n",
      "          LassoCV(max_iter=10000, cv=5, normalize=True), \\\n",
      "          ElasticNetCV(max_iter=10000, cv=5, normalize=True), \\\n",
      "          AdaBoostRegressor(n_estimators=100), \\\n",
      "          RandomForestRegressor(n_estimators=100), \\\n",
      "          GaussianProcessRegressor(n_restarts_optimizer=2, normalize_y=True), \\\n",
      "          SVR(kernel='rbf', gamma='auto')\n",
      "         ]\n",
      "43/42: gpas4 = fitmodels(Xgpa_train4, Xgpa_test4, Ygpa_train4, Ygpa_test4, gpa4models)\n",
      "43/43:\n",
      "gpas_4 = fitmodels(Xgpa_train4, Xgpa_test4, Ygpa_train4, Ygpa_test4, [LinearRegression(normalize=True), \\\n",
      "          RidgeCV(cv=5, normalize=True), \\\n",
      "          AdaBoostRegressor(n_estimators=100), \\\n",
      "          RandomForestRegressor(n_estimators=100), \\\n",
      "          GaussianProcessRegressor(n_restarts_optimizer=2, normalize_y=True), \\\n",
      "          SVR(kernel='rbf', gamma='auto')\n",
      "         ])\n",
      "43/44: Xgpa_train, Xgpa_test, Ygpa_train, Ygpa_test, IDgpa_train, IDgpa_test = makeXY('gpa', data)\n",
      "43/45: from sklearn.feature_selection import f_regression, SelectKBest\n",
      "43/46:\n",
      "lr_gpa = LinearRegression(normalize=True)\n",
      "lr_gpa.fit(Xgpa_train, Ygpa_train)\n",
      "lr_gpa_pred = lr_gpa.predict(Xgpa_test)\n",
      "mean_squared_error(lr_gpa_pred, Ygpa_test)\n",
      "43/47:\n",
      "gpa_res_train = Ygpa_train - lr_gpa.predict(Xgpa_train)\n",
      "gpa_res_test = Ygpa_test - lr_gpa_pred\n",
      "43/48:\n",
      "lr_gpa_res = LinearRegression(normalize=True)\n",
      "lr_gpa_res.fit(Xgpa_train, gpa_res_train)\n",
      "lr_gpa_res.score(Xgpa_test, gpa_res_test)\n",
      "43/49:\n",
      "gpa_model = SelectKBest(score_func=f_regression, k=100)\n",
      "gpa_results = gpa_model.fit(Xgpa_train, gpa_res_train)\n",
      "print gpa_results.scores_\n",
      "features_gpa_res = zip(Xgpa_train.columns, gpa_results.scores_)\n",
      "sorted(features_gpa_res, key=lambda x: x[1], reverse=True)[:10]\n",
      "43/50:\n",
      "gpa_model = SelectKBest(score_func=f_regression, k=100)\n",
      "gpa_results = gpa_model.fit(Xgpa_train, gpa_res_train)\n",
      "print gpa_results.scores_\n",
      "features_gpa_res = zip(Xgpa_train.columns, gpa_results.scores_)\n",
      "# sorted(features_gpa_res, key=lambda x: x[1], reverse=True)[:10]\n",
      "43/51: Xgpa_train.shape, Xgpa_test.shape, len(Ygpa_train), len(Ygpa_test)\n",
      "43/52:\n",
      "data = pd.read_csv('output.csv', low_memory=False)\n",
      "data.info()\n",
      "43/53: Xgpa_train, Xgpa_test, Ygpa_train, Ygpa_test, IDgpa_train, IDgpa_test = makeXY('gpa', data)\n",
      "43/54: Xgpa_train.shape, Xgpa_test.shape, len(Ygpa_train), len(Ygpa_test)\n",
      "43/55:\n",
      "# drop columns where every value is the same\n",
      "nunique = data.apply(pd.Series.nunique)\n",
      "cols_to_drop = nunique[nunique == 1].index\n",
      "data = data.drop(cols_to_drop, axis=1)\n",
      "data.head()\n",
      "43/56: Xgpa_train, Xgpa_test, Ygpa_train, Ygpa_test, IDgpa_train, IDgpa_test = makeXY('gpa', data)\n",
      "43/57: Xgpa_train.shape, Xgpa_test.shape, len(Ygpa_train), len(Ygpa_test)\n",
      "43/58:\n",
      "gpa_model = SelectKBest(score_func=f_regression, k=100)\n",
      "gpa_results = gpa_model.fit(Xgpa_train, gpa_res_train)\n",
      "print gpa_results.scores_\n",
      "features_gpa_res = zip(Xgpa_train.columns, gpa_results.scores_)\n",
      "sorted(features_gpa_res, key=lambda x: x[1], reverse=True)[:10]\n",
      "43/59:\n",
      "gpa_model = SelectKBest(score_func=f_regression, k=100)\n",
      "gpa_results = gpa_model.fit(Xgpa_train, gpa_res_train)\n",
      "print gpa_results.scores_\n",
      "features_gpa_res = zip(Xgpa_train.columns, gpa_results.scores_)\n",
      "sorted(features_gpa_res, key=lambda x: x[1], reverse=True)[:10]\n",
      "43/60:\n",
      "gpa_model = SelectKBest(score_func=f_regression, k=100)\n",
      "gpa_results = gpa_model.fit(Xgpa_train, gpa_res_train)\n",
      "print gpa_results.scores_.isnull().sum()\n",
      "features_gpa_res = zip(Xgpa_train.columns, gpa_results.scores_)\n",
      "sorted(features_gpa_res, key=lambda x: x[1], reverse=True)[:10]\n",
      "43/61:\n",
      "gpa_model = SelectKBest(score_func=f_regression, k=100)\n",
      "gpa_results = gpa_model.fit(Xgpa_train, gpa_res_train)\n",
      "print gpa_results.scores_\n",
      "len([x for x in gpa_results.scores_ if not np.isnan(x)]) - len(gpa_results.scores_)\n",
      "features_gpa_res = zip(Xgpa_train.columns, gpa_results.scores_)\n",
      "sorted(features_gpa_res, key=lambda x: x[1], reverse=True)[:10]\n",
      "43/62:\n",
      "gpa_model = SelectKBest(score_func=f_regression, k=100)\n",
      "gpa_results = gpa_model.fit(Xgpa_train, gpa_res_train)\n",
      "print gpa_results.scores_\n",
      "print len([x for x in gpa_results.scores_ if not np.isnan(x)]) - len(gpa_results.scores_)\n",
      "features_gpa_res = zip(Xgpa_train.columns, gpa_results.scores_)\n",
      "sorted(features_gpa_res, key=lambda x: x[1], reverse=True)[:10]\n",
      "43/63:\n",
      "gpa_model = SelectKBest(score_func=f_regression, k=100)\n",
      "gpa_results = gpa_model.fit(Xgpa_train, gpa_res_train)\n",
      "print gpa_results.scores_\n",
      "features_gpa_res = zip(Xgpa_train.columns, gpa_results.scores_)\n",
      "sorted(features_gpa_res, key=lambda x: x[1], reverse=True)[:10]\n",
      "43/64:\n",
      "gpa_model = SelectKBest(score_func=f_regression, k=100)\n",
      "gpa_results = gpa_model.fit(Xgpa_train, gpa_res_train)\n",
      "print gpa_results.scores_\n",
      "arr = [x for x in zip(Xgpa_train.columns, gpa_results.scores_) if not np.isnan(x)]\n",
      "features_gpa_res = zip(Xgpa_train.columns, gpa_results.scores_)\n",
      "sorted(arr, key=lambda x: x[1], reverse=True)[:10]\n",
      "43/65:\n",
      "gpa_model = SelectKBest(score_func=f_regression, k=100)\n",
      "gpa_results = gpa_model.fit(Xgpa_train, gpa_res_train)\n",
      "print gpa_results.scores_\n",
      "arr = [x for x in zip(Xgpa_train.columns, gpa_results.scores_) if not np.isnan(x[1])]\n",
      "features_gpa_res = zip(Xgpa_train.columns, gpa_results.scores_)\n",
      "sorted(arr, key=lambda x: x[1], reverse=True)[:10]\n",
      "43/66: Xgrit_train, Xgrit_test, Ygrit_train, Ygrit_test, IDgrit_train, IDgrit_test = makeXY('grit', data)\n",
      "43/67: Xgrit_train.shape, Xgrit_test.shape, len(Ygrit_train), len(Ygrit_test)\n",
      "43/68: Xmh_train, Xmh_test, Ymh_train, Ymh_test, IDmh_train, IDmh_test = makeXY('materialHardship', data)\n",
      "43/69:\n",
      "lr_grit = LinearRegression(normalize=True)\n",
      "lr_grit.fit(Xgrit_train, Ygrit_train)\n",
      "lr_grit_pred = lr_grit.predict(Xgrit_test)\n",
      "mean_squared_error(lr_grit_pred, Ygrit_test)\n",
      "43/70:\n",
      "grit_res_train = Ygrit_train - lr_grit.predict(Xgrit_train)\n",
      "grit_res_test = Ygrit_test - lr_grit_pred\n",
      "lr_grit_res = LinearRegression(normalize=True)\n",
      "lr_grit_res.fit(Xgrit_train, grit_res_train)\n",
      "lr_grit_res.score(Xgrit_test, grit_res_test)\n",
      "43/71:\n",
      "grit_model = SelectKBest(score_func=f_regression, k=100)\n",
      "grit_results = grit_model.fit(Xgrit_train, grit_res_train)\n",
      "print grit_results.scores_\n",
      "features_grit_res = [x for x in zip(Xgrit_train.columns, grit_results.scores_ if not np.isnan(x[1]))\n",
      "sorted(features_grit_res, key=lambda x: x[1], reverse=True)[:10]\n",
      "43/72:\n",
      "grit_model = SelectKBest(score_func=f_regression, k=100)\n",
      "grit_results = grit_model.fit(Xgrit_train, grit_res_train)\n",
      "print grit_results.scores_\n",
      "features_grit_res = [x for x in zip(Xgrit_train.columns, grit_results.scores_ if not np.isnan(x[1]))]\n",
      "sorted(features_grit_res, key=lambda x: x[1], reverse=True)[:10]\n",
      "43/73:\n",
      "grit_model = SelectKBest(score_func=f_regression, k=100)\n",
      "grit_results = grit_model.fit(Xgrit_train, grit_res_train)\n",
      "print grit_results.scores_\n",
      "features_grit_res = [x for x in zip(Xgrit_train.columns, grit_results.scores_) if not np.isnan(x[1])]\n",
      "sorted(features_grit_res, key=lambda x: x[1], reverse=True)[:10]\n",
      "43/74:\n",
      "lr_mh = LinearRegression(normalize=True)\n",
      "lr_mh.fit(Xmh_train, Ymh_train)\n",
      "lr_mh_pred = lr_mh.predict(Xmh_test)\n",
      "mean_squared_error(lr_mh_pred, Ymh_test)\n",
      "43/75:\n",
      "mh_res_train = Ymh_train - lr_mh.predict(Xmh_train)\n",
      "mh_res_test = Ymh_test - lr_mh_pred\n",
      "lr_mh_res = LinearRegression(normalize=True)\n",
      "lr_mh_res.fit(Xmh_train, mh_res_train)\n",
      "lr_mh_res.score(Xmh_test, mh_res_test)\n",
      "43/76:\n",
      "mh_model = SelectKBest(score_func=f_regression, k=100)\n",
      "mh_results = mh_model.fit(Xmh_train, mh_res_train)\n",
      "print mh_results.scores_\n",
      "features_mh_res = [x for x in zip(Xmh_train.columns, mh_results.scores_) if not np.isnan(x[1])]\n",
      "sorted(features_mh_res, key=lambda x: x[1], reverse=True)[:10]\n",
      "43/77: lr_gpa_res.coefs_\n",
      "43/78: lr_gpa_res.coef_\n",
      "43/79: zip(Xgpa_train.columns, lr_gpa_res.coef_)\n",
      "43/80: dict(zip(Xgpa_train.columns, lr_gpa_res.coef_))\n",
      "43/81: gpacoefs = dict(zip(Xgpa_train.columns, lr_gpa_res.coef_))\n",
      "43/82: gpa_coefs = dict(zip(Xgpa_train.columns, lr_gpa_res.coef_))\n",
      "43/83: gpa_coefs['f3j62']\n",
      "43/84:\n",
      "for x in ['f3j62', 'm1b23c', 'f1j4', 'f1j11', 'f1j7a']:\n",
      "    print gpa_coefs[x]\n",
      "43/85: np.max(lr_gpa_res.coef_)\n",
      "43/86: grit_coefs = dict(zip(Xgrit_train.columns, lr_grit_res.coef_))\n",
      "43/87:\n",
      "for x in ['m1b23c', 'm1b23d', 'm1b23e', 'm1a10', 'cm1marf']:\n",
      "    print grit_coefs[x]\n",
      "43/88: mh_coefs = dict(zip(Xmh_train.columns, lr_mh_res.coef_))\n",
      "43/89:\n",
      "for x in ['m4j3a', 'cm1hhinc', 'm3i3c', 'm2h11', 'm2h8e']:\n",
      "    print mh_coefs[x]\n",
      "44/1:\n",
      "import pandas as pd\n",
      "import matplotlib.pyplot as plt\n",
      "import seaborn as sns\n",
      "import re\n",
      "import numpy as np\n",
      "import math\n",
      "import ff\n",
      "44/2:\n",
      "from sklearn.linear_model import LinearRegression, RidgeCV, LassoCV, ElasticNetCV, SGDClassifier, MultiTaskElasticNet\n",
      "from sklearn.ensemble import RandomForestRegressor, RandomForestClassifier, AdaBoostRegressor, AdaBoostClassifier\n",
      "from sklearn.gaussian_process import GaussianProcessRegressor, GaussianProcessClassifier\n",
      "from sklearn.svm import SVC, LinearSVC, SVR\n",
      "from sklearn.metrics import mean_squared_error, brier_score_loss\n",
      "from sklearn.feature_selection import SelectFromModel, SelectKBest, f_regression\n",
      "from sklearn.preprocessing import normalize\n",
      "44/3: from fancyimpute import IterativeImputer\n",
      "44/4:\n",
      "data = pd.read_csv('output.csv', low_memory=False)\n",
      "data.info()\n",
      "44/5: data.head()\n",
      "44/6:\n",
      "def dropsame(dat):\n",
      "    nunique = dat.apply(pd.Series.nunique)\n",
      "    cols_to_drop = nunique[nunique == 1].index\n",
      "    return dat.drop(cols_to_drop, axis=1).copy()\n",
      "44/7:\n",
      "# drop columns where every value is the same\n",
      "nunique = data.apply(pd.Series.nunique)\n",
      "cols_to_drop = nunique[nunique == 1].index\n",
      "data = data.drop(cols_to_drop, axis=1)\n",
      "data.head()\n",
      "44/8:\n",
      "# drop highly correlated features\n",
      "# corr_matrix = data.corr().abs()\n",
      "# upper = corr_matrix.where(np.triu(np.ones(corr_matrix.shape), k=1).astype(np.bool))\n",
      "# to_drop = [column for column in upper.columns if any(upper[column] > 0.98)]\n",
      "# data_corr = data.drop(data.columns[to_drop], axis=1)\n",
      "# data_corr.head()\n",
      "44/9:\n",
      "# make X and Y dataframes for test and train from inputdata \n",
      "def makeXY(field, inputdata):\n",
      "    IDs_train = []\n",
      "    Y_train = []\n",
      "    for i in range(len(train)):\n",
      "        if not np.isnan(train[field][i]):\n",
      "            IDs_train.append(train['challengeID'][i]-1)\n",
      "            Y_train.append(train[field][i])\n",
      "    IDs_test = []\n",
      "    Y_test = []\n",
      "    for i in range(len(test)):\n",
      "        if not np.isnan(test[field][i]):\n",
      "            IDs_test.append(test['challengeID'][i]-1)\n",
      "            Y_test.append(test[field][i])\n",
      "    X_train = inputdata.drop('challengeID', axis=1).loc[IDs_train].select_dtypes(include=['float64', 'int64'])\n",
      "    X_test = inputdata.drop('challengeID', axis=1).loc[IDs_test].select_dtypes(include=['float64', 'int64'])\n",
      "    return X_train, X_test, Y_train, Y_test, IDs_train, IDs_test\n",
      "44/10:\n",
      "background = pd.read_csv('background.csv', low_memory=False)\n",
      "background.info()\n",
      "44/11: background.head()\n",
      "44/12:\n",
      "train = pd.read_csv('train.csv')\n",
      "train.info()\n",
      "44/13:\n",
      "test = pd.read_csv('test.csv')\n",
      "test.info()\n",
      "44/14:\n",
      "# mindata = [1, 424, 424*2, 424*3, 424*4, 424*5] # 100%, 90%, 80%, 70%, 60%, 50%\n",
      "mindata = [424*x for x in [6, 7, 8, 9]]\n",
      "mse = []\n",
      "for i in range(len(mindata)):\n",
      "    d0 = dropsame(background).select_dtypes(include=['float64', 'int64'])\n",
      "    d0 = d0.dropna(thresh=mindata[i], axis=1)\n",
      "    for j in range(len(d0.columns)):\n",
      "        var = d0.columns[j]\n",
      "        if d0[var].dtype == 'int64':\n",
      "            d0[var] = d0[var].fillna(d0[var].mode().iloc[0])\n",
      "        else:\n",
      "            d0[var] = d0[var].fillna(d0[var].mean())\n",
      "    Xtrain, Xtest, Ytrain, Ytest, IDtrain, IDtest = makeXY('gpa', d0)\n",
      "    r = RandomForestRegressor(n_estimators=100)\n",
      "    r.fit(Xtrain, Ytrain)\n",
      "    r_pred = r.predict(Xtest)\n",
      "    e = mean_squared_error(r_pred, Ytest)\n",
      "    mse.append(e)\n",
      "    print i, e\n",
      "45/1:\n",
      "x = [1, .9, .8, .7, .6, .5, .4, .3, .2, .1]\n",
      "y = [0.35185, 0.35481, 0.35339, 0.35593, 0.35286, 0.35354, 0.35802, 0.35052, 0.35176, 0.35141]\n",
      "plt.plot(x, y)\n",
      "plt.ylabel(\"MSE achieved\")\n",
      "plt.xlabel(\"Proportion of data required to be non-null\")\n",
      "plt.title(\"GPA: Validating imputation cutoffs\")\n",
      "plt.show()\n",
      "45/2:\n",
      "import pandas as pd\n",
      "import matplotlib.pyplot as plt\n",
      "import seaborn as sns\n",
      "import re\n",
      "import numpy as np\n",
      "import math\n",
      "import ff\n",
      "45/3:\n",
      "x = [1, .9, .8, .7, .6, .5, .4, .3, .2, .1]\n",
      "y = [0.35185, 0.35481, 0.35339, 0.35593, 0.35286, 0.35354, 0.35802, 0.35052, 0.35176, 0.35141]\n",
      "plt.plot(x, y)\n",
      "plt.ylabel(\"MSE achieved\")\n",
      "plt.xlabel(\"Proportion of data required to be non-null\")\n",
      "plt.title(\"GPA: Validating imputation cutoffs\")\n",
      "plt.show()\n",
      "45/4:\n",
      "plt.plot([100, 1000, 1917, 3500, 5000, 7500, 10000, 12000], [0.36541,0.35262,0.34904,0.34971,0.35112,0.35174,0.35001,0.35002])\n",
      "plt.title(\"GPA: Validating number of features\")\n",
      "plt.xlabel(\"Number of features\")\n",
      "plt.ylabel(\"MSE achieved\")\n",
      "plt.show()\n",
      "45/5:\n",
      "y = [0.358418, 0.361059, 0.357367, 0.352876, 0.349639, 0.350095, 0.351140, 0.351885]\n",
      "plt.plot([20*(x+1) for x in range(8)], y)\n",
      "plt.title(\"GPA: Validating number of trees\")\n",
      "plt.ylabel(\"MSE achieved\")\n",
      "plt.xlabel(\"n_estimators\")\n",
      "plt.show()\n",
      "46/1:\n",
      "import pandas as pd\n",
      "import matplotlib.pyplot as plt\n",
      "import seaborn as sns\n",
      "import re\n",
      "import numpy as np\n",
      "import math\n",
      "import ff\n",
      "46/2:\n",
      "from sklearn.linear_model import LinearRegression, RidgeCV, LassoCV, ElasticNetCV, SGDClassifier, MultiTaskElasticNet\n",
      "from sklearn.ensemble import RandomForestRegressor, RandomForestClassifier, AdaBoostRegressor, AdaBoostClassifier\n",
      "from sklearn.gaussian_process import GaussianProcessRegressor, GaussianProcessClassifier\n",
      "from sklearn.svm import SVC, LinearSVC, SVR\n",
      "from sklearn.metrics import mean_squared_error, brier_score_loss\n",
      "from sklearn.feature_selection import SelectFromModel, SelectKBest, f_regression\n",
      "from sklearn.preprocessing import normalize\n",
      "46/3: from fancyimpute import IterativeImputer\n",
      "46/4:\n",
      "data = pd.read_csv('output.csv', low_memory=False)\n",
      "data.info()\n",
      "46/5: data.head()\n",
      "46/6:\n",
      "def dropsame(dat):\n",
      "    nunique = dat.apply(pd.Series.nunique)\n",
      "    cols_to_drop = nunique[nunique == 1].index\n",
      "    return dat.drop(cols_to_drop, axis=1).copy()\n",
      "46/7:\n",
      "# drop columns where every value is the same\n",
      "nunique = data.apply(pd.Series.nunique)\n",
      "cols_to_drop = nunique[nunique == 1].index\n",
      "data = data.drop(cols_to_drop, axis=1)\n",
      "data.head()\n",
      "46/8:\n",
      "# drop highly correlated features\n",
      "# corr_matrix = data.corr().abs()\n",
      "# upper = corr_matrix.where(np.triu(np.ones(corr_matrix.shape), k=1).astype(np.bool))\n",
      "# to_drop = [column for column in upper.columns if any(upper[column] > 0.98)]\n",
      "# data_corr = data.drop(data.columns[to_drop], axis=1)\n",
      "# data_corr.head()\n",
      "46/9:\n",
      "# make X and Y dataframes for test and train from inputdata \n",
      "def makeXY(field, inputdata):\n",
      "    IDs_train = []\n",
      "    Y_train = []\n",
      "    for i in range(len(train)):\n",
      "        if not np.isnan(train[field][i]):\n",
      "            IDs_train.append(train['challengeID'][i]-1)\n",
      "            Y_train.append(train[field][i])\n",
      "    IDs_test = []\n",
      "    Y_test = []\n",
      "    for i in range(len(test)):\n",
      "        if not np.isnan(test[field][i]):\n",
      "            IDs_test.append(test['challengeID'][i]-1)\n",
      "            Y_test.append(test[field][i])\n",
      "    X_train = inputdata.drop('challengeID', axis=1).loc[IDs_train].select_dtypes(include=['float64', 'int64'])\n",
      "    X_test = inputdata.drop('challengeID', axis=1).loc[IDs_test].select_dtypes(include=['float64', 'int64'])\n",
      "    return X_train, X_test, Y_train, Y_test, IDs_train, IDs_test\n",
      "46/10:\n",
      "background = pd.read_csv('background.csv', low_memory=False)\n",
      "background.info()\n",
      "46/11: background.head()\n",
      "46/12:\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train = pd.read_csv('train.csv')\n",
      "train.info()\n",
      "46/13:\n",
      "test = pd.read_csv('test.csv')\n",
      "test.info()\n",
      "46/14:\n",
      "data3 = dropsame(background)\n",
      "strs = data3.select_dtypes(\"object\")\n",
      "nu2 = strs.nunique()\n",
      "keep = nu2[nu2 < 5].index\n",
      "data3[keep] = data3[keep].apply(lambda x: pd.factorize(x)[0])\n",
      "46/15:\n",
      "notkeep = nu2[nu2 >= 5].index\n",
      "data3 = data3.drop(notkeep, axis=1)\n",
      "numeric = data3._get_numeric_data()\n",
      "numeric[numeric < 0] = np.nan\n",
      "46/16:\n",
      "data3 = data3.dropna(thresh=424*7, axis=1)\n",
      "for i in range(len(data3.columns)):\n",
      "    var = data3.columns[i]\n",
      "    if data3[var].dtype == 'int64':\n",
      "        data3[var] = data3[var].fillna(data3[var].mode().iloc[0])\n",
      "    else:\n",
      "        data3[var] = data3[var].fillna(data3[var].mean())\n",
      "46/17: data3.info()\n",
      "46/18: Xgpa_train, Xgpa_test, Ygpa_train, Ygpa_test, IDgpa_train, IDgpa_test = makeXY('gpa', data)\n",
      "46/19: Xgpa_train3, Xgpa_test3, Ygpa_train3, Ygpa_test3, IDgpa_train3, IDgpa_test3 = makeXY('gpa', data3)\n",
      "46/20: Xgrit_train, Xgrit_test, Ygrit_train, Ygrit_test, IDgrit_train, IDgrit_test = makeXY('grit', data)\n",
      "46/21: Xgrit_train3, Xgrit_test3, Ygrit_train3, Ygrit_test3, IDgrit_train3, IDgrit_test3 = makeXY('grit', data3)\n",
      "46/22: Xmh_train, Xmh_test, Ymh_train, Ymh_test, IDmh_train, IDmh_test = makeXY('materialHardship', data)\n",
      "46/23: Xmh_train3, Xmh_test3, Ymh_train3, Ymh_test3, IDmh_train3, IDmh_test3 = makeXY('materialHardship', data3)\n",
      "46/24: Xe_train, Xe_test, Ye_train, Ye_test, IDe_train, IDe_test = makeXY('eviction', data)\n",
      "46/25: Xe_train3, Xe_test3, Ye_train3, Ye_test3, IDe_train3, IDe_test3 = makeXY('eviction', data3)\n",
      "46/26: Xl_train, Xl_test, Yl_train, Yl_test, IDl_train, IDl_test = makeXY('layoff', data)\n",
      "46/27: Xl_train3, Xl_test3, Yl_train3, Yl_test3, IDl_train3, IDl_test3 = makeXY('layoff', data3)\n",
      "46/28: Xjt_train, Xjt_test, Yjt_train, Yjt_test, IDjt_train, IDjt_test = makeXY('layoff', data)\n",
      "46/29: Xjt_train3, Xjt_test3, Yjt_train3, Yjt_test3, IDjt_train3, IDjt_test3 = makeXY('jobTraining', data3)\n",
      "46/30:\n",
      "def fitclassifiers(X_train, X_test, Y_train, Y_test, models):\n",
      "    ret = []\n",
      "    count = 0\n",
      "    for m in models:\n",
      "        m.fit(X_train, Y_train)\n",
      "        if count != 4:\n",
      "            m_pred = m.predict_proba(X_test)[:,1]\n",
      "        else:\n",
      "            m_pred = m.predict(X_test)\n",
      "        brier = brier_score_loss(Y_test, m_pred)\n",
      "        ret.append((m, brier))\n",
      "        tn, fp, fn, tp = confusion_matrix(Y_test, m_pred).ravel()\n",
      "        acc = float(tp + tn)/(tp+tn+fp+fn)\n",
      "        precision = (float(tp)/(tp+fp))\n",
      "        recall = (float(tp)/(tp+fn))\n",
      "        f = 2*precision*recall/(precision + recall)\n",
      "        print count\n",
      "        print \"brier \" + str(brier)\n",
      "        print tn, fp, fn, tp\n",
      "        print \"accuracy \" + str(acc)\n",
      "        print \"precision \" + str(prec)\n",
      "        print \"recall \" + str(rec)\n",
      "        print \"f1 \" + str(f1)\n",
      "        count = count + 1\n",
      "    return ret\n",
      "46/31:\n",
      "emodels = [SGDClassifier(loss='log', penalty='none'), \\\n",
      "          SGDClassifier(loss='log', penalty='l2'), \\\n",
      "          SGDClassifier(loss='log', penalty='l1'), \\\n",
      "          SGDClassifier(loss='log', penalty='elasticnet'), \\\n",
      "          AdaBoostClassifier(n_estimators=100), \\\n",
      "          RandomForestClassifier(n_estimators=100), \\\n",
      "          GaussianProcessClassifier(n_restarts_optimizer=2), \\\n",
      "          SVC(kernel='rbf', gamma='auto', probability=True)\n",
      "         ]\n",
      "46/32: e = fitclassifiers(normalize(Xe_train), normalize(Xe_test), Ye_train, Ye_test, emodels)\n",
      "46/33: from sklearn.metrics import confusion_matrix\n",
      "46/34: e = fitclassifiers(normalize(Xe_train), normalize(Xe_test), Ye_train, Ye_test, emodels)\n",
      "46/35:\n",
      "def fitclassifiers(X_train, X_test, Y_train, Y_test, models):\n",
      "    ret = []\n",
      "    count = 0\n",
      "    for m in models:\n",
      "        m.fit(X_train, Y_train)\n",
      "        if count != 4:\n",
      "            m_pred = m.predict_proba(X_test)[:,1]\n",
      "            m_proba = m_pred\n",
      "        else:\n",
      "            m_pred = m.predict(X_test)\n",
      "            m_proba = m.predict_proba(X_test)[:,1]\n",
      "        brier = brier_score_loss(Y_test, m_pred)\n",
      "        ret.append((m, brier))\n",
      "        tn, fp, fn, tp = confusion_matrix(Y_test, m_proba).ravel()\n",
      "        acc = float(tp + tn)/(tp+tn+fp+fn)\n",
      "        precision = (float(tp)/(tp+fp))\n",
      "        recall = (float(tp)/(tp+fn))\n",
      "        f = 2*precision*recall/(precision + recall)\n",
      "        print count\n",
      "        print \"brier \" + str(brier)\n",
      "        print tn, fp, fn, tp\n",
      "        print \"accuracy \" + str(acc)\n",
      "        print \"precision \" + str(prec)\n",
      "        print \"recall \" + str(rec)\n",
      "        print \"f1 \" + str(f1)\n",
      "        count = count + 1\n",
      "    return ret\n",
      "46/36: e = fitclassifiers(normalize(Xe_train), normalize(Xe_test), Ye_train, Ye_test, emodels)\n",
      "46/37:\n",
      "def fitclassifiers(X_train, X_test, Y_train, Y_test, models):\n",
      "    ret = []\n",
      "    count = 0\n",
      "    for m in models:\n",
      "        m.fit(X_train, Y_train)\n",
      "        if count != 4:\n",
      "            m_pred = m.predict_proba(X_test)[:,1]\n",
      "            m_proba = m_pred\n",
      "        else:\n",
      "            m_pred = m.predict(X_test)\n",
      "            m_proba = m.predict_proba(X_test)[:,1]\n",
      "        brier = brier_score_loss(Y_test, m_pred)\n",
      "        ret.append((m, brier))\n",
      "        print m_proba\n",
      "        tn, fp, fn, tp = confusion_matrix(Y_test, m_proba).ravel()\n",
      "        acc = float(tp + tn)/(tp+tn+fp+fn)\n",
      "        precision = (float(tp)/(tp+fp))\n",
      "        recall = (float(tp)/(tp+fn))\n",
      "        f = 2*precision*recall/(precision + recall)\n",
      "        print count\n",
      "        print \"brier \" + str(brier)\n",
      "        print tn, fp, fn, tp\n",
      "        print \"accuracy \" + str(acc)\n",
      "        print \"precision \" + str(prec)\n",
      "        print \"recall \" + str(rec)\n",
      "        print \"f1 \" + str(f1)\n",
      "        count = count + 1\n",
      "    return ret\n",
      "46/38: e = fitclassifiers(normalize(Xe_train), normalize(Xe_test), Ye_train, Ye_test, emodels)\n",
      "46/39:\n",
      "def fitclassifiers(X_train, X_test, Y_train, Y_test, models):\n",
      "    ret = []\n",
      "    count = 0\n",
      "    for m in models:\n",
      "        m.fit(X_train, Y_train)\n",
      "        if count != 4:\n",
      "            m_pred = m.predict_proba(X_test)[:,1]\n",
      "        else:\n",
      "            m_pred = m.predict(X_test)\n",
      "        brier = brier_score_loss(Y_test, m_pred)\n",
      "        ret.append((m, brier))\n",
      "        print m_pred\n",
      "        tn, fp, fn, tp = confusion_matrix(Y_test, m_pred).ravel()\n",
      "        acc = float(tp + tn)/(tp+tn+fp+fn)\n",
      "        precision = (float(tp)/(tp+fp))\n",
      "        recall = (float(tp)/(tp+fn))\n",
      "        f = 2*precision*recall/(precision + recall)\n",
      "        print count\n",
      "        print \"brier \" + str(brier)\n",
      "        print tn, fp, fn, tp\n",
      "        print \"accuracy \" + str(acc)\n",
      "        print \"precision \" + str(prec)\n",
      "        print \"recall \" + str(rec)\n",
      "        print \"f1 \" + str(f1)\n",
      "        count = count + 1\n",
      "    return ret\n",
      "46/40: e = fitclassifiers(normalize(Xe_train), normalize(Xe_test), Ye_train, Ye_test, emodels)\n",
      "46/41:\n",
      "def fitclassifiers(X_train, X_test, Y_train, Y_test, models):\n",
      "    ret = []\n",
      "    count = 0\n",
      "    for m in models:\n",
      "        m.fit(X_train, Y_train)\n",
      "        if count != 4:\n",
      "            m_pred = m.predict_proba(X_test)[:,1]\n",
      "            m_pred2 = [0 if x < 0.5 else 1 for x in m_pred]\n",
      "        else:\n",
      "            m_pred = m.predict(X_test)\n",
      "            m_pred2 = m_pred\n",
      "        brier = brier_score_loss(Y_test, m_pred)\n",
      "        ret.append((m, brier))\n",
      "        print m_pred\n",
      "        tn, fp, fn, tp = confusion_matrix(Y_test, m_pred2).ravel()\n",
      "        acc = float(tp + tn)/(tp+tn+fp+fn)\n",
      "        precision = (float(tp)/(tp+fp))\n",
      "        recall = (float(tp)/(tp+fn))\n",
      "        f = 2*precision*recall/(precision + recall)\n",
      "        print count\n",
      "        print \"brier \" + str(brier)\n",
      "        print tn, fp, fn, tp\n",
      "        print \"accuracy \" + str(acc)\n",
      "        print \"precision \" + str(prec)\n",
      "        print \"recall \" + str(rec)\n",
      "        print \"f1 \" + str(f1)\n",
      "        count = count + 1\n",
      "    return ret\n",
      "46/42:\n",
      "def fitclassifiers(X_train, X_test, Y_train, Y_test, models):\n",
      "    ret = []\n",
      "    count = 0\n",
      "    for m in models:\n",
      "        m.fit(X_train, Y_train)\n",
      "        if count != 4:\n",
      "            m_pred = m.predict_proba(X_test)[:,1]\n",
      "            m_pred2 = [0 if x < 0.5 else 1 for x in m_pred]\n",
      "        else:\n",
      "            m_pred = m.predict(X_test)\n",
      "            m_pred2 = m_pred\n",
      "        brier = brier_score_loss(Y_test, m_pred)\n",
      "        ret.append((m, brier))\n",
      "        print m_pred2\n",
      "        tn, fp, fn, tp = confusion_matrix(Y_test, m_pred2).ravel()\n",
      "        acc = float(tp + tn)/(tp+tn+fp+fn)\n",
      "        precision = (float(tp)/(tp+fp))\n",
      "        recall = (float(tp)/(tp+fn))\n",
      "        f = 2*precision*recall/(precision + recall)\n",
      "        print count\n",
      "        print \"brier \" + str(brier)\n",
      "        print tn, fp, fn, tp\n",
      "        print \"accuracy \" + str(acc)\n",
      "        print \"precision \" + str(prec)\n",
      "        print \"recall \" + str(rec)\n",
      "        print \"f1 \" + str(f1)\n",
      "        count = count + 1\n",
      "    return ret\n",
      "46/43: e = fitclassifiers(normalize(Xe_train), normalize(Xe_test), Ye_train, Ye_test, emodels)\n",
      "46/44:\n",
      "def fitclassifiers(X_train, X_test, Y_train, Y_test, models):\n",
      "    ret = []\n",
      "    count = 0\n",
      "    for m in models:\n",
      "        m.fit(X_train, Y_train)\n",
      "        if count != 4:\n",
      "            m_pred = m.predict_proba(X_test)[:,1]\n",
      "            m_pred2 = [0 if x < 0.5 else 1 for x in m_pred]\n",
      "        else:\n",
      "            m_pred = m.predict(X_test)\n",
      "            m_pred2 = m_pred\n",
      "        brier = brier_score_loss(Y_test, m_pred)\n",
      "        ret.append((m, brier))\n",
      "        tn, fp, fn, tp = confusion_matrix(Y_test, m_pred2).ravel()\n",
      "        acc = float(tp + tn)/(tp+tn+fp+fn)\n",
      "        prec = (float(tp)/(tp+fp))\n",
      "        rec = (float(tp)/(tp+fn))\n",
      "        f = 2*precision*recall/(precision + recall)\n",
      "        print count\n",
      "        print \"brier \" + str(brier)\n",
      "        print tn, fp, fn, tp\n",
      "        print \"accuracy \" + str(acc)\n",
      "        print \"precision \" + str(prec)\n",
      "        print \"recall \" + str(rec)\n",
      "        print \"f1 \" + str(f)\n",
      "        count = count + 1\n",
      "    return ret\n",
      "46/45: e = fitclassifiers(normalize(Xe_train), normalize(Xe_test), Ye_train, Ye_test, emodels)\n",
      "46/46:\n",
      "def fitclassifiers(X_train, X_test, Y_train, Y_test, models):\n",
      "    ret = []\n",
      "    count = 0\n",
      "    for m in models:\n",
      "        m.fit(X_train, Y_train)\n",
      "        if count != 4:\n",
      "            m_pred = m.predict_proba(X_test)[:,1]\n",
      "            m_pred2 = [0 if x < 0.5 else 1 for x in m_pred]\n",
      "        else:\n",
      "            m_pred = m.predict(X_test)\n",
      "            m_pred2 = m_pred\n",
      "        brier = brier_score_loss(Y_test, m_pred)\n",
      "        ret.append((m, brier))\n",
      "        tn, fp, fn, tp = confusion_matrix(Y_test, m_pred2).ravel()\n",
      "        acc = float(tp + tn)/(tp+tn+fp+fn)\n",
      "        prec = (float(tp)/(tp+fp))\n",
      "        rec = (float(tp)/(tp+fn))\n",
      "        f = 2*prec*rec/(prec + rec)\n",
      "        print count\n",
      "        print \"brier \" + str(brier)\n",
      "        print tn, fp, fn, tp\n",
      "        print \"accuracy \" + str(acc)\n",
      "        print \"precision \" + str(prec)\n",
      "        print \"recall \" + str(rec)\n",
      "        print \"f1 \" + str(f)\n",
      "        count = count + 1\n",
      "    return ret\n",
      "46/47: e = fitclassifiers(normalize(Xe_train), normalize(Xe_test), Ye_train, Ye_test, emodels)\n",
      "46/48:\n",
      "def fitclassifiers(X_train, X_test, Y_train, Y_test, models):\n",
      "    ret = []\n",
      "    count = 0\n",
      "    for m in models:\n",
      "        m.fit(X_train, Y_train)\n",
      "        if count != 4:\n",
      "            m_pred = m.predict_proba(X_test)[:,1]\n",
      "            m_pred2 = [0 if x < 0.5 else 1 for x in m_pred]\n",
      "        else:\n",
      "            m_pred = m.predict(X_test)\n",
      "            m_pred2 = m_pred\n",
      "        brier = brier_score_loss(Y_test, m_pred)\n",
      "        ret.append((m, brier))\n",
      "        tn, fp, fn, tp = confusion_matrix(Y_test, m_pred2).ravel()\n",
      "        acc = float(tp + tn)/(tp+tn+fp+fn)\n",
      "        prec = (float(tp)/(tp+fp))\n",
      "        rec = (float(tp)/(tp+fn))\n",
      "        f = np.nan if prec+rec == 0 else 2*prec*rec/(prec + rec)\n",
      "        print count\n",
      "        print \"brier \" + str(brier)\n",
      "        print tn, fp, fn, tp\n",
      "        print \"accuracy \" + str(acc)\n",
      "        print \"precision \" + str(prec)\n",
      "        print \"recall \" + str(rec)\n",
      "        print \"f1 \" + str(f)\n",
      "        count = count + 1\n",
      "    return ret\n",
      "46/49: e = fitclassifiers(normalize(Xe_train), normalize(Xe_test), Ye_train, Ye_test, emodels)\n",
      "46/50:\n",
      "def fitclassifiers(X_train, X_test, Y_train, Y_test, models):\n",
      "    ret = []\n",
      "    count = 0\n",
      "    for m in models:\n",
      "        m.fit(X_train, Y_train)\n",
      "        if count != 4:\n",
      "            m_pred = m.predict_proba(X_test)[:,1]\n",
      "            m_pred2 = [0 if x < 0.5 else 1 for x in m_pred]\n",
      "        else:\n",
      "            m_pred = m.predict(X_test)\n",
      "            m_pred2 = m_pred\n",
      "        brier = brier_score_loss(Y_test, m_pred)\n",
      "        ret.append((m, brier))\n",
      "        tn, fp, fn, tp = confusion_matrix(Y_test, m_pred2).ravel()\n",
      "        acc = float(tp + tn)/(tp+tn+fp+fn)\n",
      "        prec = np.nan if tp+fp == 0 else (float(tp)/(tp+fp))\n",
      "        rec = np.nan if tp+fn == 0 else (float(tp)/(tp+fn))\n",
      "        f = np.nan if prec+rec == 0 else 2*prec*rec/(prec + rec)\n",
      "        print count\n",
      "        print \"brier \" + str(brier)\n",
      "        print tn, fp, fn, tp\n",
      "        print \"accuracy \" + str(acc)\n",
      "        print \"precision \" + str(prec)\n",
      "        print \"recall \" + str(rec)\n",
      "        print \"f1 \" + str(f)\n",
      "        count = count + 1\n",
      "    return ret\n",
      "46/51: e = fitclassifiers(normalize(Xe_train), normalize(Xe_test), Ye_train, Ye_test, emodels)\n",
      "46/52: from sklearn.metrics import confusion_matrix, roc_auc_score\n",
      "46/53:\n",
      "def fitclassifiers(X_train, X_test, Y_train, Y_test, models):\n",
      "    ret = []\n",
      "    count = 0\n",
      "    for m in models:\n",
      "        m.fit(X_train, Y_train)\n",
      "        if count != 4:\n",
      "            m_pred2 = m.predict_proba(X_test)[:,1]\n",
      "            m_pred = [0 if x < 0.5 else 1 for x in m_pred2]\n",
      "        else:\n",
      "            m_pred = m.predict(X_test)\n",
      "            m_pred2 = m_predict_proba(X_test)[:,1]\n",
      "        brier = brier_score_loss(Y_test, m_pred)\n",
      "        ret.append((m, brier, m_pred, m_pred2))\n",
      "        tn, fp, fn, tp = confusion_matrix(Y_test, m_pred).ravel()\n",
      "        auroc = roc_auc_score(Y_test, m_pred2)\n",
      "        acc = float(tp + tn)/(tp+tn+fp+fn)\n",
      "        prec = np.nan if tp+fp == 0 else (float(tp)/(tp+fp))\n",
      "        rec = np.nan if tp+fn == 0 else (float(tp)/(tp+fn))\n",
      "        f = np.nan if prec+rec == 0 else 2*prec*rec/(prec + rec)\n",
      "        print count\n",
      "        print \"brier \" + str(brier)\n",
      "        print tn, fp, fn, tp\n",
      "        print \"accuracy \" + str(acc)\n",
      "        print \"precision \" + str(prec)\n",
      "        print \"recall \" + str(rec)\n",
      "        print \"f1 \" + str(f)\n",
      "        print \"auroc \" + str(auroc)\n",
      "        count = count + 1\n",
      "    return ret\n",
      "46/54: e = fitclassifiers(normalize(Xe_train), normalize(Xe_test), Ye_train, Ye_test, emodels)\n",
      "46/55:\n",
      "def fitclassifiers(X_train, X_test, Y_train, Y_test, models):\n",
      "    ret = []\n",
      "    count = 0\n",
      "    for m in models:\n",
      "        m.fit(X_train, Y_train)\n",
      "        if count != 4:\n",
      "            m_pred2 = m.predict_proba(X_test)[:,1]\n",
      "            m_pred = [0 if x < 0.5 else 1 for x in m_pred2]\n",
      "        else:\n",
      "            m_pred = m.predict(X_test)\n",
      "            m_pred2 = m.predict_proba(X_test)[:,1]\n",
      "        brier = brier_score_loss(Y_test, m_pred)\n",
      "        ret.append((m, brier, m_pred, m_pred2))\n",
      "        tn, fp, fn, tp = confusion_matrix(Y_test, m_pred).ravel()\n",
      "        auroc = roc_auc_score(Y_test, m_pred2)\n",
      "        acc = float(tp + tn)/(tp+tn+fp+fn)\n",
      "        prec = np.nan if tp+fp == 0 else (float(tp)/(tp+fp))\n",
      "        rec = np.nan if tp+fn == 0 else (float(tp)/(tp+fn))\n",
      "        f = np.nan if prec+rec == 0 else 2*prec*rec/(prec + rec)\n",
      "        print count\n",
      "        print \"brier \" + str(brier)\n",
      "        print tn, fp, fn, tp\n",
      "        print \"accuracy \" + str(acc)\n",
      "        print \"precision \" + str(prec)\n",
      "        print \"recall \" + str(rec)\n",
      "        print \"f1 \" + str(f)\n",
      "        print \"auroc \" + str(auroc)\n",
      "        count = count + 1\n",
      "    return ret\n",
      "46/56: e = fitclassifiers(normalize(Xe_train), normalize(Xe_test), Ye_train, Ye_test, emodels)\n",
      "46/57: modelnames = [\"OLS\", \"Ridge\", \"Lasso\", \"Elastic Net\", \"AdaBoost\", \"Random Forest\", \"Gaussian Process\", \"SVM\"]\n",
      "46/58:\n",
      "def plotroc(arr, Ytest, title):\n",
      "    plot.figure()\n",
      "    count = 0\n",
      "    for a in arr:\n",
      "        fpr, tpr, _ = roc_curve(Ytest, a[3])\n",
      "        plot.plot(fpr, tpr, label=modelnames[count])\n",
      "    plot.legend()\n",
      "    plot.xlabel(\"FPR\")\n",
      "    plot.ylabel(\"TPR\")\n",
      "    plot.title(\"ROC curves for \" + title)\n",
      "    pylab.plot(np.linspace(0, 1, 100), np.linspace(0, 1, 100), ls='dashed', c='k')\n",
      "46/59: plotroc(e)\n",
      "46/60: plotroc(e, Ye_test, \"Eviction: Full\")\n",
      "46/61:\n",
      "def plotroc(arr, Ytest, title):\n",
      "    plt.figure()\n",
      "    count = 0\n",
      "    for a in arr:\n",
      "        fpr, tpr, _ = roc_curve(Ytest, a[3])\n",
      "        plt.plot(fpr, tpr, label=modelnames[count])\n",
      "    plt.legend()\n",
      "    plt.xlabel(\"FPR\")\n",
      "    plt.ylabel(\"TPR\")\n",
      "    plt.title(\"ROC curves for \" + title)\n",
      "    pylab.plot(np.linspace(0, 1, 100), np.linspace(0, 1, 100), ls='dashed', c='k')\n",
      "46/62: plotroc(e, Ye_test, \"Eviction: Full\")\n",
      "46/63: from sklearn.metrics import confusion_matrix, roc_auc_score, roc_curve\n",
      "46/64: plotroc(e, Ye_test, \"Eviction: Full\")\n",
      "46/65:\n",
      "from sklearn.metrics import confusion_matrix, roc_auc_score, roc_curve\n",
      "import pylab\n",
      "46/66: plotroc(e, Ye_test, \"Eviction: Full\")\n",
      "46/67:\n",
      "def plotroc(arr, Ytest, title):\n",
      "    plt.figure()\n",
      "    count = 0\n",
      "    for a in arr:\n",
      "        fpr, tpr, _ = roc_curve(Ytest, a[3])\n",
      "        plt.plot(fpr, tpr, label=modelnames[count])\n",
      "    count = count + 1\n",
      "    plt.legend()\n",
      "    plt.xlabel(\"FPR\")\n",
      "    plt.ylabel(\"TPR\")\n",
      "    plt.title(\"ROC curves for \" + title)\n",
      "    pylab.plot(np.linspace(0, 1, 100), np.linspace(0, 1, 100), ls='dashed', c='k')\n",
      "46/68: plotroc(e, Ye_test, \"Eviction: Full\")\n",
      "46/69: plotroc(e, Ye_test, \"Eviction: Full\")\n",
      "46/70:\n",
      "def plotroc(arr, Ytest, title):\n",
      "    plt.figure()\n",
      "    count = 0\n",
      "    for a in arr:\n",
      "        fpr, tpr, _ = roc_curve(Ytest, a[3])\n",
      "        plt.plot(fpr, tpr, label=modelnames[count])\n",
      "        count = count + 1\n",
      "    plt.legend()\n",
      "    plt.xlabel(\"FPR\")\n",
      "    plt.ylabel(\"TPR\")\n",
      "    plt.title(\"ROC curves for \" + title)\n",
      "    pylab.plot(np.linspace(0, 1, 100), np.linspace(0, 1, 100), ls='dashed', c='k')\n",
      "46/71: plotroc(e, Ye_test, \"Eviction: Full\")\n",
      "46/72: e = fitclassifiers(normalize(Xe_train_rf), normalize(Xe_test_rf), Ye_train, Ye_test, emodels)\n",
      "46/73:\n",
      "rfselecte = SelectFromModel(RandomForestClassifier(n_estimators=100))\n",
      "Xe_train_rf = rfselecte.fit_transform(Xe_train3, Ye_train3)\n",
      "rfse = RandomForestClassifier(n_estimators=100).fit(Xe_train_rf, Ye_train3)\n",
      "Xe_test_rf = rfselecte.transform(Xe_test3)\n",
      "rfse_pred = rfse.predict(Xe_test_rf)\n",
      "print mean_squared_error(rfse_pred, Ye_test3)\n",
      "print sum(rfselecte.get_support())\n",
      "print rfselecte.get_support()[:10]\n",
      "46/74:\n",
      "emodels3 = [SGDClassifier(loss='log', penalty='none'), \\\n",
      "          SGDClassifier(loss='log', penalty='l2'), \\\n",
      "          SGDClassifier(loss='log', penalty='l1'), \\\n",
      "          SGDClassifier(loss='log', penalty='elasticnet'), \\\n",
      "          AdaBoostClassifier(n_estimators=100), \\\n",
      "          RandomForestClassifier(n_estimators=100), \\\n",
      "          GaussianProcessClassifier(n_restarts_optimizer=2), \\\n",
      "          SVC(kernel='rbf', gamma='auto', probability=True)\n",
      "         ]\n",
      "46/75: e = fitclassifiers(normalize(Xe_train_rf), normalize(Xe_test_rf), Ye_train, Ye_test, emodels)\n",
      "46/76: plotroc(e, Ye_test, \"Eviction: Reduced\")\n",
      "46/77: sum(Ye_test)\n",
      "46/78: sum(Ye_test), len(Ye_test)\n",
      "46/79: e[4]\n",
      "46/80: e[4], e[5]\n",
      "46/81: e[4]\n",
      "46/82: e[5]\n",
      "46/83: e[5][3]\n",
      "46/84: e[5][3].mean(), e[5][3].std()\n",
      "46/85: e[4][3].mean(), e[4][3].std()\n",
      "46/86: e[3][3].mean(), e[3][3].std()\n",
      "46/87:\n",
      "lmodels = [SGDClassifier(loss='log', penalty='none'), \\\n",
      "          SGDClassifier(loss='log', penalty='l2'), \\\n",
      "          SGDClassifier(loss='log', penalty='l1'), \\\n",
      "          SGDClassifier(loss='log', penalty='elasticnet'), \\\n",
      "          AdaBoostClassifier(n_estimators=100), \\\n",
      "          RandomForestClassifier(n_estimators=100), \\\n",
      "          GaussianProcessClassifier(n_restarts_optimizer=2), \\\n",
      "          SVC(kernel='rbf', gamma='auto', probability=True)\n",
      "         ]\n",
      "46/88: l1 = fitclassifiers(normalize(Xl_train), normalize(Xl_test), Yl_train, Yl_test, lmodels)\n",
      "46/89: plotroc(l1, Yl_test, \"Layoff: Full\")\n",
      "46/90: l1[4][3]\n",
      "46/91: l1[5][3]\n",
      "46/92: l1[5][3].mean(), l1[4][3].mean()\n",
      "46/93:\n",
      "lmodels3 = [SGDClassifier(loss='log', penalty='none'), \\\n",
      "          SGDClassifier(loss='log', penalty='l2'), \\\n",
      "          SGDClassifier(loss='log', penalty='l1'), \\\n",
      "          SGDClassifier(loss='log', penalty='elasticnet'), \\\n",
      "          AdaBoostClassifier(n_estimators=100), \\\n",
      "          RandomForestClassifier(n_estimators=100), \\\n",
      "          GaussianProcessClassifier(n_restarts_optimizer=2), \\\n",
      "          SVC(kernel='rbf', gamma='auto', probability=True)\n",
      "         ]\n",
      "46/94: l1 = fitclassifiers(normalize(Xl_train_rf), normalize(Xl_test_rf), Yl_train, Yl_test, lmodels3)\n",
      "46/95:\n",
      "rfselectl = SelectFromModel(RandomForestRegressor(n_estimators=100))\n",
      "Xl_train_rf = rfselectl.fit_transform(Xl_train3, Yl_train3)\n",
      "rfsl = RandomForestRegressor(n_estimators=100).fit(Xl_train_rf, Yl_train3)\n",
      "Xl_test_rf = rfselectl.transform(Xl_test3)\n",
      "rfsl_pred = rfsl.predict(Xl_test_rf)\n",
      "print mean_squared_error(rfsl_pred, Yl_test3)\n",
      "print sum(rfselectl.get_support())\n",
      "print rfselectl.get_support()[:10]\n",
      "46/96:\n",
      "lmodels3 = [SGDClassifier(loss='log', penalty='none'), \\\n",
      "          SGDClassifier(loss='log', penalty='l2'), \\\n",
      "          SGDClassifier(loss='log', penalty='l1'), \\\n",
      "          SGDClassifier(loss='log', penalty='elasticnet'), \\\n",
      "          AdaBoostClassifier(n_estimators=100), \\\n",
      "          RandomForestClassifier(n_estimators=100), \\\n",
      "          GaussianProcessClassifier(n_restarts_optimizer=2), \\\n",
      "          SVC(kernel='rbf', gamma='auto', probability=True)\n",
      "         ]\n",
      "46/97: l1 = fitclassifiers(normalize(Xl_train_rf), normalize(Xl_test_rf), Yl_train, Yl_test, lmodels3)\n",
      "46/98: plotroc(l2, Yl_test, \"Layoff: Reduced\")\n",
      "46/99: plotroc(l1, Yl_test, \"Layoff: Reduced\")\n",
      "46/100:\n",
      "jtmodels = [SGDClassifier(loss='log', penalty='none'), \\\n",
      "          SGDClassifier(loss='log', penalty='l2'), \\\n",
      "          SGDClassifier(loss='log', penalty='l1'), \\\n",
      "          SGDClassifier(loss='log', penalty='elasticnet'), \\\n",
      "          AdaBoostClassifier(n_estimators=100), \\\n",
      "          RandomForestClassifier(n_estimators=100), \\\n",
      "          GaussianProcessClassifier(n_restarts_optimizer=2), \\\n",
      "          SVC(kernel='rbf', gamma='auto', probability=True)\n",
      "         ]\n",
      "46/101: jt1 = fitclassifiers(normalize(Xjt_train), normalize(Xjt_test), Yjt_train, Yjt_test, jtmodels)\n",
      "46/102: plotroc(jt1, Yjt_test, \"JT: Reduced\")\n",
      "46/103: plotroc(jt1, Yjt_test, \"JT: Full\")\n",
      "46/104:\n",
      "rfselectjt = SelectFromModel(RandomForestRegressor(n_estimators=100))\n",
      "Xjt_train_rf = rfselectjt.fit_transform(Xjt_train3, Yjt_train3)\n",
      "rfsjt = RandomForestRegressor(n_estimators=100).fit(Xjt_train_rf, Yjt_train3)\n",
      "Xjt_test_rf = rfselectjt.transform(Xjt_test3)\n",
      "rfsjt_pred = rfsjt.predict(Xjt_test_rf)\n",
      "print mean_squared_error(rfsjt_pred, Yjt_test3)\n",
      "print sum(rfselectjt.get_support())\n",
      "print rfselectjt.get_support()[:10]\n",
      "46/105:\n",
      "jtmodels3 = [SGDClassifier(loss='log', penalty='none'), \\\n",
      "          SGDClassifier(loss='log', penalty='l2'), \\\n",
      "          SGDClassifier(loss='log', penalty='l1'), \\\n",
      "          SGDClassifier(loss='log', penalty='elasticnet'), \\\n",
      "          AdaBoostClassifier(n_estimators=100), \\\n",
      "          RandomForestClassifier(n_estimators=100), \\\n",
      "          GaussianProcessClassifier(n_restarts_optimizer=2), \\\n",
      "          SVC(kernel='rbf', gamma='auto', probability=True)\n",
      "         ]\n",
      "46/106: jt3 = fitclassifiers(normalize(Xjt_train_rf), normalize(Xjt_test_rf), Yjt_train3, Yjt_test3, jtmodels3)\n",
      "46/107: plotroc(jt3, Yjt_test, \"JT: Full\")\n",
      "46/108: plotroc(jt3, Yjt_test3, \"JT: Full\")\n",
      "46/109:\n",
      "def plotroc(arr, Ytest, title):\n",
      "    plt.figure()\n",
      "    count = 0\n",
      "    for a in arr:\n",
      "        fpr, tpr, _ = roc_curve(Ytest, a[3])\n",
      "        plt.plot(fpr, tpr, label=modelnames[count])\n",
      "        count = count + 1\n",
      "    plt.legend()\n",
      "    plt.xlabel(\"FPR\")\n",
      "    plt.ylabel(\"TPR\")\n",
      "    plt.title(\"ROC curves for \" + title)\n",
      "    pylab.plot(np.linspace(0, 1, 100), np.linspace(0, 1, 100), ls='dashed', c='k')\n",
      "46/110: e1 = fitclassifiers(normalize(Xe_train), normalize(Xe_test), Ye_train, Ye_test, emodels)\n",
      "46/111: plotroc(e1, Ye_test, \"Eviction: Full\")\n",
      "46/112: l1 = fitclassifiers(normalize(Xl_train), normalize(Xl_test), Yl_train, Yl_test, lmodels)\n",
      "46/113: plotroc(l1, Yl_test, \"Layoff: Full\")\n",
      "46/114: plotroc(jt3, Yjt_test3, \"JT: Reduced\")\n",
      "46/115: sum(Yl_test)\n",
      "46/116: sum(Yl_test), len(Yl_test)\n",
      "46/117: 211./994\n",
      "46/118: 1-211./994\n",
      "46/119: sum(Yjt_test3), len(Yjt_test3)\n",
      "46/120: 271./1104\n",
      "46/121: 1-271./1104\n",
      "46/122: sum(Yjt_test), len(Yjt_test)\n",
      "46/123: Xjt_train, Xjt_test, Yjt_train, Yjt_test, IDjt_train, IDjt_test = makeXY('jobTraining', data)\n",
      "46/124:\n",
      "jtmodels = [SGDClassifier(loss='log', penalty='none'), \\\n",
      "          SGDClassifier(loss='log', penalty='l2'), \\\n",
      "          SGDClassifier(loss='log', penalty='l1'), \\\n",
      "          SGDClassifier(loss='log', penalty='elasticnet'), \\\n",
      "          AdaBoostClassifier(n_estimators=100), \\\n",
      "          RandomForestClassifier(n_estimators=100), \\\n",
      "          GaussianProcessClassifier(n_restarts_optimizer=2), \\\n",
      "          SVC(kernel='rbf', gamma='auto', probability=True)\n",
      "         ]\n",
      "46/125: jt1 = fitclassifiers(normalize(Xjt_train), normalize(Xjt_test), Yjt_train, Yjt_test, jtmodels)\n",
      "48/1:\n",
      "import networkx as nx\n",
      "import csv\n",
      "import matplotlib.pyplot as plot\n",
      "48/2:\n",
      "from bs4 import BeautifulSoup\n",
      "import urllib2\n",
      "48/3:\n",
      "from sklearn.linear_model import LinearRegression, LogisticRegression\n",
      "import numpy as np\n",
      "from scipy import stats\n",
      "48/4:\n",
      "page0 = urllib2.urlopen(\"https://en.wikipedia.org/wiki/List_of_actors_with_Academy_Award_nominations\")\n",
      "soup0 = BeautifulSoup(page0, 'html.parser')\n",
      "48/5: actors0 = soup0.find_all('tr')[11:][:-11]\n",
      "48/6: actors0[0].find('a').text\n",
      "48/7:\n",
      "academylist = []\n",
      "for actor in actors0:\n",
      "    academylist.append(actor.find('a').text)\n",
      "48/8: academylist\n",
      "48/9:\n",
      "page1 = urllib2.urlopen(\"https://www.boxofficemojo.com/people/?view=Actor&p=.htm\")\n",
      "soup1 = BeautifulSoup(page1, 'html.parser')\n",
      "48/10: actors1 = soup1.find_all('tr')[2].find_all('tr')[1:]\n",
      "48/11: actors1\n",
      "48/12: boxofficedict = {}\n",
      "48/13:\n",
      "def addtodict(actors):\n",
      "    for actor in actors:\n",
      "        name = actor.find('b').find('b').text.replace(\",\", \"\")\n",
      "        boxoffice = str(actor.find('td', attrs={'align':'right'}).text[1:]).replace(\",\", \"\")\n",
      "        if \"k\" in boxoffice:\n",
      "            boxoffice = float(boxoffice.replace(\"k\", \"\"))/1000\n",
      "        boxofficedict[name] = float(boxoffice)\n",
      "48/14: boxofficedict\n",
      "48/15:\n",
      "page2 = urllib2.urlopen(\"https://www.boxofficemojo.com/people/?view=Actor&pagenum=2&sort=person&order=ASC&p=.htm\")\n",
      "soup2 = BeautifulSoup(page2, 'html.parser')\n",
      "actors2 = soup2.find_all('tr')[2].find_all('tr')[1:]\n",
      "48/16:\n",
      "page3 = urllib2.urlopen(\"https://www.boxofficemojo.com/people/?view=Actor&pagenum=3&sort=person&order=ASC&p=.htm\")\n",
      "soup3 = BeautifulSoup(page3, 'html.parser')\n",
      "actors3 = soup3.find_all('tr')[2].find_all('tr')[1:]\n",
      "48/17:\n",
      "addtodict(actors1)\n",
      "addtodict(actors2)\n",
      "addtodict(actors3)\n",
      "48/18: len(boxofficedict)\n",
      "48/19: boxofficedict\n",
      "48/20:\n",
      "boxofficesorted = sorted(boxofficedict, key=boxofficedict.get, reverse=True)\n",
      "boxofficesorted\n",
      "48/21:\n",
      "with open('title.basics.tsv') as tsvfile:\n",
      "    reader = csv.reader(tsvfile, delimiter='\\t')\n",
      "    i = 0\n",
      "    for row in reader:\n",
      "        if i < 200:\n",
      "            print(row)\n",
      "            i = i + 1\n",
      "        else:\n",
      "            break\n",
      "48/22:\n",
      "movies = {}\n",
      "with open('title.basics.tsv') as tsvfile:\n",
      "    reader = csv.reader(tsvfile, delimiter='\\t')\n",
      "    i = 0\n",
      "    for row in reader:\n",
      "        movies[row[0]] = row[1]\n",
      "48/23:\n",
      "dict = {}\n",
      "with open('name.basics.tsv') as tsvfile:\n",
      "    reader = csv.reader(tsvfile, delimiter='\\t')\n",
      "    i = 0\n",
      "    for row in reader:\n",
      "        if i < 200:\n",
      "            print(row)\n",
      "            i = i + 1\n",
      "        if i > 1:\n",
      "            dict[row[0]] = row[1]\n",
      "48/24:\n",
      "G = nx.Graph()\n",
      "Gw = nx.Graph()\n",
      "Gm = nx.Graph()\n",
      "Gweighted = nx.Graph()\n",
      "moviecountdict = {}\n",
      "\n",
      "with open('title.principals.tsv') as tsvfile:\n",
      "    reader = csv.reader(tsvfile, delimiter='\\t')\n",
      "    prev = \"\"\n",
      "    actors = []\n",
      "    for row in reader:\n",
      "        if row[3] == 'actor' and row[2] in dict and dict[row[2]] in boxofficedict:\n",
      "            Gm.add_node(dict[row[2]])\n",
      "            G.add_node(dict[row[2]])\n",
      "            Gweighted.add_node(dict[row[2]])\n",
      "            if dict[row[2]] in moviecountdict:\n",
      "                moviecountdict[dict[row[2]]] += 1\n",
      "            else:\n",
      "                moviecountdict[dict[row[2]]] = 1\n",
      "        elif row[3] == 'actress' and row[2] in dict and dict[row[2]] in boxofficedict:\n",
      "            Gw.add_node(dict[row[2]])\n",
      "            G.add_node(dict[row[2]])\n",
      "            Gweighted.add_node(dict[row[2]])\n",
      "            if dict[row[2]] in moviecountdict:\n",
      "                moviecountdict[dict[row[2]]] += 1\n",
      "            else:\n",
      "                moviecountdict[dict[row[2]]] = 1\n",
      "        else:\n",
      "            continue\n",
      "        if row[0] != prev:\n",
      "            for i in range(len(actors)):\n",
      "                for j in range(i, len(actors)):\n",
      "                    a1 = actors[i]\n",
      "                    a2 = actors[j]\n",
      "                    if a1 == a2:\n",
      "                        continue\n",
      "                    G.add_edge(a1, a2)\n",
      "                    if Gweighted.has_edge(a1, a2):\n",
      "                        Gweighted.add_edge(a1, a2, weight=(Gweighted.get_edge_data(a1, a2)['weight'] + 1))\n",
      "                    else:\n",
      "                        Gweighted.add_edge(a1, a2, weight=1)\n",
      "                    if a1 in Gw.nodes and a2 in Gw.nodes:\n",
      "                        Gw.add_edge(a1, a2)\n",
      "                    elif a1 in Gm.nodes and a2 in Gm.nodes:\n",
      "                        Gm.add_edge(a1, a2)\n",
      "            actors = [dict[row[2]]]\n",
      "        else:\n",
      "            actors.append(dict[row[2]])\n",
      "        prev = row[0]\n",
      "48/25:\n",
      "def kweighted(G_old, k, show):\n",
      "    G_new = nx.Graph()\n",
      "    for (u, v, d) in G_old.edges(data=True):\n",
      "        if show:\n",
      "            print u,v,d\n",
      "        if d['weight'] >= k:\n",
      "            G_new.add_edge(u, v)\n",
      "    return G_new\n",
      "48/26:\n",
      "actorw = sorted(Gw.nodes)\n",
      "actorm = sorted(Gm.nodes)\n",
      "48/27:\n",
      "degree_centralities = []\n",
      "actorlist = [a for a in actorw]\n",
      "for a in actorm:\n",
      "    actorlist.append(a)\n",
      "for actor in actorlist:\n",
      "    degree_centralities.append(degree_centrality.get(actor))\n",
      "48/28: degree_centrality = nx.degree_centrality(G)\n",
      "48/29:\n",
      "degree_centralities = []\n",
      "actorlist = [a for a in actorw]\n",
      "for a in actorm:\n",
      "    actorlist.append(a)\n",
      "for actor in actorlist:\n",
      "    degree_centralities.append(degree_centrality.get(actor))\n",
      "48/30:\n",
      "dict = {}\n",
      "birthdict = {}\n",
      "with open('name.basics.tsv') as tsvfile:\n",
      "    reader = csv.reader(tsvfile, delimiter='\\t')\n",
      "    i = 0\n",
      "    for row in reader:\n",
      "        if i < 200:\n",
      "            print(row)\n",
      "            i = i + 1\n",
      "        if i > 1:\n",
      "            dict[row[0]] = row[1]\n",
      "            birthdict[row[1]] = row[2]\n",
      "48/31: print birthdict[:10]\n",
      "48/32:\n",
      "i = 0\n",
      "for a in birthdict:\n",
      "    if i > 10:\n",
      "        break\n",
      "    i = i + 1\n",
      "    print a, birthdict[a]\n",
      "48/33:\n",
      "i = 0\n",
      "for a in actorlist:\n",
      "    if i > 10:\n",
      "        break\n",
      "    i = i + 1\n",
      "    print a, birthdict[a]\n",
      "48/34:\n",
      "for a in actorlist:\n",
      "    if birthdict[a] == '\\N':\n",
      "        print a\n",
      "48/35:\n",
      "dict = {}\n",
      "birthdict = {}\n",
      "with open('name.basics.tsv') as tsvfile:\n",
      "    reader = csv.reader(tsvfile, delimiter='\\t')\n",
      "    i = 0\n",
      "    for row in reader:\n",
      "        if i < 200:\n",
      "            print(row)\n",
      "            i = i + 1\n",
      "        if i > 1:\n",
      "            dict[row[0]] = row[1]\n",
      "            birthdict[row[1]] = row[2]\n",
      "48/36:\n",
      "for a in actorlist:\n",
      "    if birthdict[a] == '\\N':\n",
      "        print a\n",
      "48/37:\n",
      "i = 0\n",
      "for a in actorlist:\n",
      "    if birthdict[a] == '\\N':\n",
      "        print a, birthdict[a]\n",
      "        i = i+1\n",
      "        \n",
      "print i\n",
      "48/38:\n",
      "dict = {}\n",
      "birthdict = {}\n",
      "with open('name.basics.tsv') as tsvfile:\n",
      "    reader = csv.reader(tsvfile, delimiter='\\t')\n",
      "    i = 0\n",
      "    for row in reader:\n",
      "        if i < 200:\n",
      "            print(row)\n",
      "            i = i + 1\n",
      "        if i > 1:\n",
      "            dict[row[0]] = row[1]\n",
      "            birthdict[row[1]] = row[2]\n",
      "            if row[1] == 'Abigail Breslin':\n",
      "                print row[2]\n",
      "48/39:\n",
      "i = 0\n",
      "for a in actorlist:\n",
      "    if birthdict[a] == '\\N':\n",
      "        print a, birthdict[a]\n",
      "        i = i+1\n",
      "        \n",
      "print i\n",
      "48/40:\n",
      "dict = {}\n",
      "birthdict = {}\n",
      "with open('name.basics.tsv') as tsvfile:\n",
      "    reader = csv.reader(tsvfile, delimiter='\\t')\n",
      "    i = 0\n",
      "    for row in reader:\n",
      "        if i < 200:\n",
      "            print(row)\n",
      "            i = i + 1\n",
      "        if i > 1:\n",
      "            dict[row[0]] = row[1]\n",
      "            if row[1] not in birthdict:\n",
      "                birthdict[row[1]] = row[2]\n",
      "48/41:\n",
      "i = 0\n",
      "for a in actorlist:\n",
      "    if birthdict[a] == '\\N':\n",
      "        print a, birthdict[a]\n",
      "        i = i+1\n",
      "        \n",
      "print i\n",
      "48/42:\n",
      "dict = {}\n",
      "birthdict = {}\n",
      "with open('name.basics.tsv') as tsvfile:\n",
      "    reader = csv.reader(tsvfile, delimiter='\\t')\n",
      "    i = 0\n",
      "    for row in reader:\n",
      "        if i < 200:\n",
      "            print(row)\n",
      "            i = i + 1\n",
      "        if i > 1:\n",
      "            dict[row[0]] = row[1]\n",
      "            if 'actor' in row[4] or 'actress' in row[4]:\n",
      "                birthdict[row[1]] = row[2]\n",
      "48/43:\n",
      "i = 0\n",
      "ages = [2019-birthdict[a] for a in actorlist]\n",
      "for a in actorlist:\n",
      "    if birthdict[a] == '\\N':\n",
      "        print a, birthdict[a]\n",
      "        i = i+1\n",
      "        \n",
      "print i\n",
      "48/44:\n",
      "i = 0\n",
      "ages = [2019-int(birthdict[a]) for a in actorlist]\n",
      "for a in actorlist:\n",
      "    if birthdict[a] == '\\N':\n",
      "        print a, birthdict[a]\n",
      "        i = i+1\n",
      "        \n",
      "print i\n",
      "48/45:\n",
      "i = 0\n",
      "for a in actorlist:\n",
      "    if birthdict[a] == '\\N':\n",
      "        print a, birthdict[a]\n",
      "        i = i+1\n",
      "        \n",
      "print i\n",
      "48/46: 'actor' in 'actor,soundtrack'\n",
      "48/47:\n",
      "dict = {}\n",
      "birthdict = {}\n",
      "with open('name.basics.tsv') as tsvfile:\n",
      "    reader = csv.reader(tsvfile, delimiter='\\t')\n",
      "    i = 0\n",
      "    for row in reader:\n",
      "        if i < 200:\n",
      "            print(row)\n",
      "            i = i + 1\n",
      "        if i > 1:\n",
      "            dict[row[0]] = row[1]\n",
      "            birthdict[row[0]] = row[2]\n",
      "48/48:\n",
      "G = nx.Graph()\n",
      "Gw = nx.Graph()\n",
      "Gm = nx.Graph()\n",
      "Gweighted = nx.Graph()\n",
      "moviecountdict = {}\n",
      "agedict = {}\n",
      "with open('title.principals.tsv') as tsvfile:\n",
      "    reader = csv.reader(tsvfile, delimiter='\\t')\n",
      "    prev = \"\"\n",
      "    actors = []\n",
      "    for row in reader:\n",
      "        if row[3] == 'actor' and row[2] in dict and dict[row[2]] in boxofficedict:\n",
      "            Gm.add_node(dict[row[2]])\n",
      "            G.add_node(dict[row[2]])\n",
      "            Gweighted.add_node(dict[row[2]])\n",
      "            if dict[row[2]] in moviecountdict:\n",
      "                moviecountdict[dict[row[2]]] += 1\n",
      "            else:\n",
      "                moviecountdict[dict[row[2]]] = 1\n",
      "            agedict[dict[row[2]]] = 0 if birthdict[row[2]] == '\\N' else 2019-int(birthdict[row[2]])\n",
      "        elif row[3] == 'actress' and row[2] in dict and dict[row[2]] in boxofficedict:\n",
      "            Gw.add_node(dict[row[2]])\n",
      "            G.add_node(dict[row[2]])\n",
      "            Gweighted.add_node(dict[row[2]])\n",
      "            if dict[row[2]] in moviecountdict:\n",
      "                moviecountdict[dict[row[2]]] += 1\n",
      "            else:\n",
      "                moviecountdict[dict[row[2]]] = 1\n",
      "            agedict[dict[row[2]]] = 0 if birthdict[row[2]] == '\\N' else 2019-int(birthdict[row[2]])\n",
      "        else:\n",
      "            continue\n",
      "        if row[0] != prev:\n",
      "            for i in range(len(actors)):\n",
      "                for j in range(i, len(actors)):\n",
      "                    a1 = actors[i]\n",
      "                    a2 = actors[j]\n",
      "                    if a1 == a2:\n",
      "                        continue\n",
      "                    G.add_edge(a1, a2)\n",
      "                    if Gweighted.has_edge(a1, a2):\n",
      "                        Gweighted.add_edge(a1, a2, weight=(Gweighted.get_edge_data(a1, a2)['weight'] + 1))\n",
      "                    else:\n",
      "                        Gweighted.add_edge(a1, a2, weight=1)\n",
      "                    if a1 in Gw.nodes and a2 in Gw.nodes:\n",
      "                        Gw.add_edge(a1, a2)\n",
      "                    elif a1 in Gm.nodes and a2 in Gm.nodes:\n",
      "                        Gm.add_edge(a1, a2)\n",
      "            actors = [dict[row[2]]]\n",
      "        else:\n",
      "            actors.append(dict[row[2]])\n",
      "        prev = row[0]\n",
      "48/49:\n",
      "i = 0\n",
      "for a in agedict:\n",
      "    i = i + 1\n",
      "    if i > 10:\n",
      "        break\n",
      "    print a, agedict[a]\n",
      "48/50:\n",
      "for a in agedict:\n",
      "    if a in actorlist:\n",
      "        print a, agedict[a]\n",
      "48/51: print len([a for a in actorlist if agedict[a] == 0)\n",
      "48/52: print len([a if agedict[a] == 0 for a in actorlist )\n",
      "48/53: print len([agedict[a] == 0 for a in actorlist )\n",
      "48/54: print len([agedict[a] == 0 for a in actorlist if agedict[a] == 0])\n",
      "48/55: print np.median(agedict)\n",
      "48/56: print np.median([agedict[a] for a in agedict if agedict[a] > 0])\n",
      "48/57: print np.mean([agedict[a] for a in agedict if agedict[a] > 0])\n",
      "48/58: print np.min([agedict[a] for a in agedict if agedict[a] > 0])\n",
      "48/59: print np.max([agedict[a] for a in agedict if agedict[a] > 0])\n",
      "48/60: print np.median([agedict[a] for a in agedict if agedict[a] > 0])\n",
      "48/61:\n",
      "dict = {}\n",
      "birthdict = {}\n",
      "deathdict = {}\n",
      "with open('name.basics.tsv') as tsvfile:\n",
      "    reader = csv.reader(tsvfile, delimiter='\\t')\n",
      "    i = 0\n",
      "    for row in reader:\n",
      "        if i < 200:\n",
      "            print(row)\n",
      "            i = i + 1\n",
      "        if i > 1:\n",
      "            dict[row[0]] = row[1]\n",
      "            birthdict[row[0]] = row[2]\n",
      "            if row[3] != '\\N':\n",
      "                deathdict[row[0]] = row[3]\n",
      "48/62: len(deathdict)\n",
      "48/63: len([deathdict[a] for a in actorlist])\n",
      "48/64: len([deathdict[a] if a in deathdict for a in actorlist])\n",
      "48/65: len([deathdict[a] for a in actorlist if a in deathdict])\n",
      "48/66: print a for a in agedict if agedict[a] == 0\n",
      "48/67:\n",
      "for a in agedict:\n",
      "    if agedict[a] == 0:\n",
      "        print a\n",
      "48/68:\n",
      "missing = []\n",
      "for a in agedict:\n",
      "    if agedict[a] == 0:\n",
      "        missing.append(a)\n",
      "48/69:\n",
      "ages = [1973, 1964, 1968, 1966, 1997, 1972, 1984, 1954, 1951, 1965, 1958, 1988, 1989, 1931, 1968, 1990, 1966, 1928, 1972, 1969, 1974, 1969, 1976, 1922, 1997, 1995]\n",
      "missing = zip(missing, ages)\n",
      "48/70:\n",
      "for a in agedict:\n",
      "    if agedict[a] == 0:\n",
      "        agedict[a] = missing[a]\n",
      "\n",
      "print sorted(agedict)\n",
      "48/71:\n",
      "for a in agedict:\n",
      "    if agedict[a] == 0:\n",
      "        agedict[a] = missing[a]\n",
      "\n",
      "print sorted([agedict[a] for a in agedict])\n",
      "48/72:\n",
      "ages = [1973, 1964, 1968, 1966, 1997, 1972, 1984, 1954, 1951, 1965, 1958, 1988, 1989, 1931, 1968, 1990, 1966, 1928, 1972, 1969, 1974, 1969, 1976, 1922, 1997, 1995]\n",
      "missing = dict(zip(missing, ages))\n",
      "48/73:\n",
      "ages = [1973, 1964, 1968, 1966, 1997, 1972, 1984, 1954, 1951, 1965, 1958, 1988, 1989, 1931, 1968, 1990, 1966, 1928, 1972, 1969, 1974, 1969, 1976, 1922, 1997, 1995]\n",
      "missing = zip(missing, ages)\n",
      "48/74: print missing\n",
      "48/75:\n",
      "missing = []\n",
      "for a in agedict:\n",
      "    if agedict[a] == 0:\n",
      "        missing.append(a)\n",
      "48/76:\n",
      "ages = [1973, 1964, 1968, 1966, 1997, 1972, 1984, 1954, 1951, 1965, 1958, 1988, 1989, 1931, 1968, 1990, 1966, 1928, 1972, 1969, 1974, 1969, 1976, 1922, 1997, 1995]\n",
      "missing = zip(missing, ages)\n",
      "48/77: print missing\n",
      "48/78: print dict(missing)\n",
      "48/79: dict((k, v) for k, v in missing)\n",
      "48/80:\n",
      "for i in range(len(missing)):\n",
      "    agedict[missing[i][0]] = 2019-missing[i][1]\n",
      "\n",
      "print sorted([agedict[a] for a in agedict])\n",
      "48/81:\n",
      "for i in range(len(missing)):\n",
      "    agedict[missing[i][0]] = 2019-missing[i][1]\n",
      "48/82: print np.median([agedict[a] for a in agedict if agedict[a] > 0])\n",
      "48/83: print np.min([agedict[a] for a in agedict if agedict[a] > 0])\n",
      "48/84: print np.max([agedict[a] for a in agedict if agedict[a] > 0])\n",
      "48/85: print np.mean([agedict[a] for a in agedict if agedict[a] > 0])\n",
      "48/86: print np.mean([agedict[a] for a in agedict])\n",
      "48/87: print np.median([agedict[a] for a in agedict])\n",
      "48/88: print np.min([agedict[a] for a in agedict])\n",
      "48/89: print np.max([agedict[a] for a in agedict])\n",
      "48/90: print len([agedict[a] for a in agedict])\n",
      "48/91: print np.median([agedict[a] for a in agedict])\n",
      "48/92: old = [a for a in actorlist if agedict[a] >= 47]\n",
      "48/93: len(old)\n",
      "48/94: young = [a for a in actorlist if agedict[a] < 47]\n",
      "48/95: arrold = getarr(G, old)\n",
      "48/96:\n",
      "def getarr(graph, alist):\n",
      "    degree = nx.degree_centrality(graph)\n",
      "    degrees = [degree.get(a) for a in alist]\n",
      "    eigenvector = nx.eigenvector_centrality(graph)\n",
      "    eigenvectors = [eigenvector.get(a) for a in alist]\n",
      "    close = nx.closeness_centrality(graph)\n",
      "    closes = [close.get(a) for a in alist]\n",
      "    between = nx.betweenness_centrality(graph)\n",
      "    betweens = [between.get(a) for a in alist]\n",
      "    effective = nx.algorithms.structuralholes.effective_size(graph)\n",
      "    effectives = [effective.get(a) for a in alist]\n",
      "    cluster = nx.algorithms.cluster.clustering(graph)\n",
      "    clusters = [cluster.get(a) for a in alist]\n",
      "    return [degrees, eigenvectors, closes, betweens, effectives, clusters]\n",
      "48/97: arrold = getarr(G, old)\n",
      "48/98: arryoung = getarr(G, young)\n",
      "48/99: regs(arrold, boxofficegross)\n",
      "48/100:\n",
      "def regs(arrs, box):\n",
      "    for arr in arrs:\n",
      "        slope, intercept, r_value, p_value, std_err = stats.linregress(arr,box)\n",
      "        print p_value, r_value\n",
      "48/101: regs(arrold, boxofficegross)\n",
      "48/102:\n",
      "boxofficegross = []\n",
      "for actor in actorlist:\n",
      "    boxofficegross.append(boxofficedict[actor])\n",
      "48/103: boxofficegross\n",
      "48/104: regs(arrold, boxofficegross)\n",
      "48/105: regs(arrold, [boxofficedict.get(a) for a in old])\n",
      "48/106: regs(arryoung, [boxofficedict.get(a) for a in young])\n",
      "48/107:\n",
      "def genderanksum(x):\n",
      "    xm = np.array(x[313:])\n",
      "    xf = np.array(x[:313])\n",
      "    print stats.ranksums(xm, xf)\n",
      "48/108:\n",
      "for x in [degree_centralities, eigenvector_centralities, closeness_centralities, betweenness_centralities, cluster_coefficients]:\n",
      "    genderranksum(x)\n",
      "48/109:\n",
      "cluster_coefficients = [cluster_coefficient.get(x) for x in actorlist]\n",
      "cluster_coefficients\n",
      "48/110: cluster_coefficient = nx.algorithms.cluster.clustering(G)\n",
      "48/111:\n",
      "cluster_coefficients = [cluster_coefficient.get(x) for x in actorlist]\n",
      "cluster_coefficients\n",
      "48/112: eigenvector_centrality = nx.eigenvector_centrality(G)\n",
      "48/113: closeness_centrality = nx.closeness_centrality(G)\n",
      "48/114: betweenness_centrality = nx.betweenness_centrality(G)\n",
      "48/115:\n",
      "for x in [degree_centralities, eigenvector_centralities, closeness_centralities, betweenness_centralities, cluster_coefficients]:\n",
      "    genderranksum(x)\n",
      "48/116:\n",
      "eigenvector_centralities = []\n",
      "for actor in actorlist:\n",
      "    eigenvector_centralities.append(eigenvector_centrality.get(actor))\n",
      "48/117:\n",
      "betweenness_centralities = []\n",
      "for actor in actorlist:\n",
      "    betweenness_centralities.append(betweenness_centrality.get(actor))\n",
      "x_b = np.array(betweenness_centralities)\n",
      "y = np.array(boxofficegross)\n",
      "slope, intercept, r_value, p_value, std_err = stats.linregress(x_b,y)\n",
      "slope, intercept, r_value, p_value, std_err\n",
      "48/118:\n",
      "closeness_centralities = []\n",
      "for actor in actorlist:\n",
      "    closeness_centralities.append(closeness_centrality.get(actor))\n",
      "x_c = np.array(closeness_centralities)\n",
      "y = np.array(boxofficegross)\n",
      "slope, intercept, r_value, p_value, std_err = stats.linregress(x_c,y)\n",
      "slope, intercept, r_value, p_value, std_err\n",
      "48/119:\n",
      "for x in [degree_centralities, eigenvector_centralities, closeness_centralities, betweenness_centralities, cluster_coefficients]:\n",
      "    genderranksum(x)\n",
      "48/120:\n",
      "def genderranksum(x):\n",
      "    xm = np.array(x[313:])\n",
      "    xf = np.array(x[:313])\n",
      "    print stats.ranksums(xm, xf)\n",
      "48/121:\n",
      "for x in [degree_centralities, eigenvector_centralities, closeness_centralities, betweenness_centralities, cluster_coefficients]:\n",
      "    genderranksum(x)\n",
      "48/122:\n",
      "for i in range(len(arrold)):\n",
      "    print stats.ranksums(arrold[i], arryoung[i])\n",
      "49/1:\n",
      "import numpy as np\n",
      "import pandas as pd\n",
      "import matplotlib.pyplot as plt\n",
      "%matplotlib inline\n",
      "\n",
      "\n",
      "# increase the width of the pandas dataframe to allow scrolling through all columns\n",
      "\n",
      "pd.options.display.max_columns = 100\n",
      "49/2: profile_df = pd.read_csv(\"profiles.csv\")\n",
      "49/3:\n",
      "print profile_df.shape[0], \"\"\n",
      "print profile_df.shape[1], \"Columns\"\n",
      "profile_df.head()\n",
      "50/1:\n",
      "import numpy as np\n",
      "import pandas as pd\n",
      "import matplotlib.pyplot as plt\n",
      "%matplotlib inline\n",
      "\n",
      "\n",
      "# increase the width of the pandas dataframe to allow scrolling through all columns\n",
      "\n",
      "pd.options.display.max_columns = 100\n",
      "50/2: profile_df = pd.read_csv(\"profiles.csv\")\n",
      "50/3:\n",
      "print profile_df.shape[0], \"\"\n",
      "print profile_df.shape[1], \"Columns\"\n",
      "profile_df.head()\n",
      "50/4: profile_df[:20]\n",
      "50/5:\n",
      "categorical_columns = ['body_type',\n",
      "                      'diet',\n",
      "                      'drinks',\n",
      "                      'drugs',\n",
      "                      'education',\n",
      "                      'ethnicity',\n",
      "                      'job',\n",
      "                      'offspring',\n",
      "                      'orientation',\n",
      "                      'pets',\n",
      "                      'religion',\n",
      "                       'sex',\n",
      "                       'sign',\n",
      "                       'smokes',\n",
      "                       'speaks',\n",
      "                       'status']\n",
      "profile_num_df = pd.get_dummies(profile_df, \n",
      "                                columns = categorical_columns)\n",
      "print profile_num_df.shape\n",
      "profile_num_df\n",
      "50/6:\n",
      "dataset = fetch_20newsgroups(subset='all', categories=categories,\n",
      "                             shuffle=True, random_state=42)\n",
      "50/7:\n",
      "dataset = fetch_20newsgroups(subset='all', categories=categories,\n",
      "                             shuffle=True, random_state=42)\n",
      "dataset = fetch_20newsgroups(subset='all', categories=categories,\n",
      "                             shuffle=True, random_state=42)\n",
      "50/8:\n",
      "from sklearn.datasets import fetch_20newsgroups\n",
      "dataset = fetch_20newsgroups(subset='all', categories=categories,\n",
      "                             shuffle=True, random_state=42)\n",
      "50/9:\n",
      "from sklearn.datasets import fetch_20newsgroups\n",
      "dataset = fetch_20newsgroups(subset='all',\n",
      "                             shuffle=True, random_state=42)\n",
      "50/10: dataset.data\n",
      "50/11: dataset.data.shape\n",
      "50/12:\n",
      "from sklearn.feature_extraction.text import TfidfVectorizer\n",
      "vectorizer = TfidfVectorizer(max_df=0.5, max_features=opts.n_features,\n",
      "                                 min_df=2, stop_words='english',\n",
      "                                 use_idf=opts.use_idf)\n",
      "50/13:\n",
      "from sklearn.feature_extraction.text import TfidfVectorizer\n",
      "vectorizer = TfidfVectorizer(max_df=0.5, max_features=opts.n_features,\n",
      "                                 min_df=2, stop_words='english')\n",
      "50/14:\n",
      "from sklearn.feature_extraction.text import TfidfVectorizer\n",
      "vectorizer = TfidfVectorizer(max_df=0.5, min_df=2, stop_words='english')\n",
      "50/15: X = vectorizer.fit_transform(profile_df['essay0'])\n",
      "50/16: X = vectorizer.fit_transform(profile_df['essay0'].notna)\n",
      "50/17: X = vectorizer.fit_transform(profile_df.dropna(subset=['essay0'])['essay0'])\n",
      "50/18: profile_df.shape\n",
      "50/19: X\n",
      "50/20: X.shape\n",
      "50/21: km = KMeans()\n",
      "50/22:\n",
      "from sklearn.cluster import KMeans\n",
      "km = KMeans()\n",
      "50/23: km.fit(X)\n",
      "50/24: profile_df.dropna(subset=['essay0'])['essay0']\n",
      "50/25: profile_df.dropna(subset=['essay0'])['essay0'].shape\n",
      "50/26:\n",
      "from sklearn.feature_extraction.text import TfidfVectorizer\n",
      "vectorizer = TfidfVectorizer(max_df=0.5, min_df=2, max_features=200, stop_words='english')\n",
      "50/27: from sklearn.feature_extraction.text import TfidfVectorizer\n",
      "50/28: vectorizer = TfidfVectorizer(max_df=0.5, min_df=2, max_features=200, stop_words='english')\n",
      "50/29: X = vectorizer.fit_transform(profile_df.dropna(subset=['essay0'])['essay0'])\n",
      "50/30: X.shape\n",
      "50/31:\n",
      "from sklearn.cluster import KMeans\n",
      "km = KMeans()\n",
      "50/32: km.fit(X)\n",
      "50/33: km.cluster_centers_\n",
      "50/34: order_centroids = km.cluster_centers_.argsort()[:, ::-1]\n",
      "50/35: terms = vectorizer.get_feature_names()\n",
      "50/36:\n",
      "for i in range(true_k):\n",
      "        print(\"Cluster %d:\" % i, end='')\n",
      "        for ind in order_centroids[i, :10]:\n",
      "            print(' %s' % terms[ind], end='')\n",
      "        print()\n",
      "50/37:\n",
      "for i in range(true_k):\n",
      "        print(\"Cluster %d:\" % i, end='')\n",
      "        for ind in order_centroids[i, :10]:\n",
      "            print(' %s' % terms[ind])\n",
      "        print()\n",
      "50/38:\n",
      "for i in range(true_k):\n",
      "        print(\"Cluster %d:\" % i)\n",
      "        for ind in order_centroids[i, :10]:\n",
      "            print(' %s' % terms[ind])\n",
      "        print()\n",
      "50/39:\n",
      "for i in range(8):\n",
      "        print(\"Cluster %d:\" % i)\n",
      "        for ind in order_centroids[i, :10]:\n",
      "            print(' %s' % terms[ind])\n",
      "        print()\n",
      "50/40: profile_df.dropna(subset=['essay0'])['essay0']\n",
      "50/41: profile_df.dropna(subset=['essay0'])['essay0'][:10]\n",
      "50/42:\n",
      "from sklearn.feature_extraction.text import TfidfVectorizer\n",
      "from nltk.corpus import stopwords\n",
      "from sklearn.cluster import KMeans\n",
      "50/43:\n",
      "def getclusters(essay, n_features, n_clusters):\n",
      "    vectorizer = TfidfVectorizer(max_df=0.5, min_df=2, max_features=n_features, stop_words='english')\n",
      "    X = vectorizer.fit_transform(profile_df.dropna(subset=[essay])[essay])\n",
      "    km = KMeans()\n",
      "    km.fit(X)\n",
      "    order_centroids = km.cluster_centers_.argsort()[:, ::-1]\n",
      "    terms = vectorizer.get_feature_names()\n",
      "    for i in range(n_clusters):\n",
      "        print(\"Cluster %d:\" % i)\n",
      "        for ind in order_centroids[i, :10]:\n",
      "            print(' %s' % terms[ind])\n",
      "        print()\n",
      "    return vectorizer, X, km\n",
      "50/44: getclusters('essay0', 200, 4)\n",
      "50/45: v0, X0, km0 = getclusters('essay0', 200, 4)\n",
      "50/46: essays = profile_df.iloc[:, np.array(['essay' in i for i in np.array(list(profile_df))])]\n",
      "50/47:\n",
      "essaysL = []\n",
      "def add(df, r,c):\n",
      "if df.iloc[r,c] == df.iloc[r,c]:\n",
      "return df.iloc[r,c]\n",
      "\n",
      "essaysL = list(map(lambda c: list(map(lambda r : add(essays, r, c), range(essays.shape[0]))), range(essays.shape[1])))\n",
      "50/48:\n",
      "essaysL = []\n",
      "def add(df, r,c):\n",
      "    if df.iloc[r,c] == df.iloc[r,c]:\n",
      "return df.iloc[r,c]\n",
      "\n",
      "essaysL = list(map(lambda c: list(map(lambda r : add(essays, r, c), range(essays.shape[0]))), range(essays.shape[1])))\n",
      "50/49:\n",
      "essaysL = []\n",
      "def add(df, r,c):\n",
      "    if df.iloc[r,c] == df.iloc[r,c]:\n",
      "        return df.iloc[r,c]\n",
      "\n",
      "essaysL = list(map(lambda c: list(map(lambda r : add(essays, r, c), range(essays.shape[0]))), range(essays.shape[1])))\n",
      "50/50: essaysL\n",
      "50/51:\n",
      "essaysL = []\n",
      "def add(df, r,c):\n",
      "    if not pd.isnull(df.iloc[r,c]):\n",
      "        return df.iloc[r,c]\n",
      "\n",
      "essaysL = list(map(lambda c: list(map(lambda r : add(essays, r, c), range(essays.shape[0]))), range(essays.shape[1])))\n",
      "50/52:\n",
      "def tocategorical(essay, km):\n",
      "    res = []\n",
      "    count = 0\n",
      "    for i in range(len(essay)):\n",
      "        if not pd.isnull(essay[i]):\n",
      "            res.append(km.labels_[count])\n",
      "            count += 1\n",
      "        else:\n",
      "            res.append(np.nan)\n",
      "    return res\n",
      "50/53: tocategorical(profile_df['essay0'], km0)\n",
      "50/54:\n",
      "def tocategorical(essay, kmeans):\n",
      "    res = []\n",
      "    count = 0\n",
      "    for i in range(len(essay)):\n",
      "        if not pd.isnull(essay[i]):\n",
      "            res.append(kmeans.labels_[count])\n",
      "            count += 1\n",
      "        else:\n",
      "            res.append(np.nan)\n",
      "    return res\n",
      "50/55: tocategorical(profile_df['essay0'], km0)\n",
      "50/56: km0\n",
      "50/57:\n",
      "def getclusters(essay, n_features, n_clusters):\n",
      "    vectorizer = TfidfVectorizer(max_df=0.5, min_df=2, max_features=n_features, \\\n",
      "                                 stop_words=stopwords.words('english').extend(['href', 'ilink', 'br']))\n",
      "    X = vectorizer.fit_transform(profile_df.dropna(subset=[essay])[essay])\n",
      "    km = KMeans(n_clusters=n_clusters)\n",
      "    km.fit(X)\n",
      "    order_centroids = km.cluster_centers_.argsort()[:, ::-1]\n",
      "    terms = vectorizer.get_feature_names()\n",
      "    for i in range(n_clusters):\n",
      "        print(\"Cluster %d:\" % i)\n",
      "        for ind in order_centroids[i, :10]:\n",
      "            print(' %s' % terms[ind])\n",
      "        print()\n",
      "    return vectorizer, X, km\n",
      "50/58: v0, X0, km0 = getclusters('essay0', 200, 4)\n",
      "50/59:\n",
      "def getclusters(essay, n_features, n_clusters):\n",
      "    vectorizer = TfidfVectorizer(max_df=0.5, min_df=2, max_features=n_features, \\\n",
      "                                 stop_words=stopwords.words('english').extend(['href', 'ilink', 'br']))\n",
      "    X = vectorizer.fit_transform(profile_df.dropna(subset=[essay])[essay])\n",
      "    km = KMeans(n_clusters=n_clusters)\n",
      "    km.fit(X)\n",
      "    order_centroids = km.cluster_centers_.argsort()[:, ::-1]\n",
      "    terms = vectorizer.get_feature_names()\n",
      "    for i in range(n_clusters):\n",
      "        print(\"Cluster %d:\" % i)\n",
      "        for ind in order_centroids[i, :10]:\n",
      "            print(' %s' % terms[ind])\n",
      "        print()\n",
      "    return vectorizer, X, km\n",
      "50/60:\n",
      "def getclusters(essay, n_features, n_clusters):\n",
      "    vectorizer = TfidfVectorizer(max_df=0.5, min_df=2, max_features=n_features, \\\n",
      "                                 stop_words=(stopwords.words('english') + ['href', 'ilink', 'br']))\n",
      "    X = vectorizer.fit_transform(profile_df.dropna(subset=[essay])[essay])\n",
      "    km = KMeans(n_clusters=n_clusters)\n",
      "    km.fit(X)\n",
      "    order_centroids = km.cluster_centers_.argsort()[:, ::-1]\n",
      "    terms = vectorizer.get_feature_names()\n",
      "    for i in range(n_clusters):\n",
      "        print(\"Cluster %d:\" % i)\n",
      "        for ind in order_centroids[i, :10]:\n",
      "            print(' %s' % terms[ind])\n",
      "        print()\n",
      "    return vectorizer, X, km\n",
      "51/1:\n",
      "import numpy as np\n",
      "import pandas as pd\n",
      "import matplotlib.pyplot as plt\n",
      "%matplotlib inline\n",
      "51/2:\n",
      "from sklearn.feature_extraction.text import TfidfVectorizer\n",
      "from nltk.corpus import stopwords\n",
      "from sklearn.cluster import KMeans\n",
      "51/3: profile_df = pd.read_csv(\"profiles.csv\")\n",
      "51/4:\n",
      "essays = profile_df.iloc[:, np.array(['essay' in i for i in np.array(list(profile_df))])]\n",
      "# essays = essays2.iloc[:100, :]\n",
      "51/5:\n",
      "essaysL = []\n",
      "def add(df, r,c):\n",
      "    if not pd.isnull(df.iloc[r,c]):\n",
      "        return df.iloc[r,c]\n",
      "\n",
      "essaysL = list(map(lambda c: list(filter(lambda i : not pd.isnull(i), map(lambda r : essays.iloc[r,c], range(essays.shape[0])))), range(essays.shape[1])))\n",
      "hasEssayL = list(map(lambda c: list(map(lambda r : not pd.isnull(essays.iloc[r,c]), range(essays.shape[0]))), range(essays.shape[1])))\n",
      "51/6:\n",
      "def getclusters(essayL, n_features, n_clusters):\n",
      "    vectorizer = TfidfVectorizer(max_df=0.5, min_df=2, \\\n",
      "                                 stop_words=(stopwords.words('english') +['href', 'ilink', 'br']),\\\n",
      "                                 max_features=n_features, analyzer = 'word')\n",
      "    X = vectorizer.fit_transform(essayL)\n",
      "    km = KMeans(n_clusters)\n",
      "    km.fit(X)\n",
      "    order_centroids = km.cluster_centers_.argsort()[:, ::-1]\n",
      "    terms = vectorizer.get_feature_names()\n",
      "    for i in range(n_clusters):\n",
      "        print(\"Cluster %d:\" % i)\n",
      "        for ind in order_centroids[i, :10]:\n",
      "            print(' %s' % terms[ind])\n",
      "        print()\n",
      "    return vectorizer, X, km\n",
      "51/7:\n",
      "def tocategorical(essay, hasEssay, kmeans):\n",
      "    res = []\n",
      "    count = 0\n",
      "    for i in range(len(hasEssay)):\n",
      "        if hasEssay[i]:\n",
      "            res.append(kmeans.labels_[count])\n",
      "            count += 1\n",
      "        else:\n",
      "            res.append(np.nan)\n",
      "    return res\n",
      "51/8:\n",
      "%%time\n",
      "\n",
      "def clusterAssignments(i):\n",
      "    v0, X0, km0 = getclusters(essaysL[i], 250, 4)\n",
      "    return tocategorical(essaysL[i], hasEssayL[i], km0)\n",
      "\n",
      "essayClusters = list(map(lambda i : clusterAssignments(i), range(len(essaysL))))\n",
      "51/9: essayClusters\n",
      "51/10: essayClusters.shape\n",
      "51/11: len(essayClusters)\n",
      "51/12: len(essayClusters), len(essayClusters[0])\n",
      "51/13: profile_df\n",
      "51/14: profile_df.shape\n",
      "51/15: profile_df\n",
      "51/16:\n",
      "categorical_columns = ['body_type',\n",
      "                      'diet',\n",
      "                      'drinks',\n",
      "                      'drugs',\n",
      "                      'education',\n",
      "                      'ethnicity',\n",
      "                      'job',\n",
      "                      'offspring',\n",
      "                      'orientation',\n",
      "                      'pets',\n",
      "                      'religion',\n",
      "                       'sex',\n",
      "                       'sign',\n",
      "                       'smokes',\n",
      "                       'speaks',\n",
      "                       'status']\n",
      "profile_num_df = pd.get_dummies(profile_df, \n",
      "                                columns = categorical_columns)\n",
      "print profile_num_df.shape\n",
      "profile_num_df\n",
      "51/17:\n",
      "import numpy as np\n",
      "import pandas as pd\n",
      "import matplotlib.pyplot as plt\n",
      "%matplotlib inline\n",
      "pd.options.display.max_columns = 100\n",
      "51/18:\n",
      "categorical_columns = ['body_type',\n",
      "                      'diet',\n",
      "                      'drinks',\n",
      "                      'drugs',\n",
      "                      'education',\n",
      "                      'ethnicity',\n",
      "                      'job',\n",
      "                      'offspring',\n",
      "                      'orientation',\n",
      "                      'pets',\n",
      "                      'religion',\n",
      "                       'sex',\n",
      "                       'sign',\n",
      "                       'smokes',\n",
      "                       'speaks',\n",
      "                       'status']\n",
      "profile_num_df = pd.get_dummies(profile_df, \n",
      "                                columns = categorical_columns)\n",
      "print profile_num_df.shape\n",
      "profile_num_df\n",
      "51/19: type(profile_num_df['last_online'])\n",
      "51/20: profile_num_df = profile_num_df.drop('location')\n",
      "51/21: profile_num_df = profile_num_df.drop('location', axis=1)\n",
      "51/22: print profile_num_df.shape\n",
      "51/23: profile_num_df['last_online'] = pd.to_numeric(profile_num_df['last_online'])\n",
      "51/24: type(profile_num_df['last_online'][0])\n",
      "51/25: profile_num_df['last_online'] = pd.to_datetime(profile_num_df['last_online'])\n",
      "51/26: profile_num_df['last_online']\n",
      "51/27: profile_num_df['last_online'] = pd.to_numeric(profile_num_df['last_online'])\n",
      "51/28: profile_num_df['last_online']\n",
      "51/29: profile_num_df\n",
      "51/30:\n",
      "from itertools import compress\n",
      "profiles = profile_num_df.deepcopy()\n",
      "a = np.array(['essay' in i for i in np.array(list(profile_num_df))])\n",
      "indexA = np.array(list(range(len(a))))[a]\n",
      "indexA\n",
      "for i in range(len(indexA)):\n",
      "profiles.iloc[:, indexA[i]] = essayClusters[i]\n",
      "51/31:\n",
      "from itertools import compress\n",
      "profiles = profile_num_df.deepcopy()\n",
      "a = np.array(['essay' in i for i in np.array(list(profile_num_df))])\n",
      "indexA = np.array(list(range(len(a))))[a]\n",
      "indexA\n",
      "for i in range(len(indexA)):\n",
      "    profiles.iloc[:, indexA[i]] = essayClusters[i]\n",
      "51/32:\n",
      "from itertools import compress\n",
      "profiles = profile_num_df.copy()\n",
      "a = np.array(['essay' in i for i in np.array(list(profile_num_df))])\n",
      "indexA = np.array(list(range(len(a))))[a]\n",
      "indexA\n",
      "for i in range(len(indexA)):\n",
      "    profiles.iloc[:, indexA[i]] = essayClusters[i]\n",
      "51/33: profiles\n",
      "51/34: profiles = pd.get_dummies\n",
      "51/35: profiles = pd.get_dummies([i for i in np.array(list(profile_df)) if 'essay' in i])\n",
      "51/36: profiles\n",
      "51/37:\n",
      "from itertools import compress\n",
      "profiles = profile_num_df.copy()\n",
      "a = np.array(['essay' in i for i in np.array(list(profile_num_df))])\n",
      "indexA = np.array(list(range(len(a))))[a]\n",
      "indexA\n",
      "for i in range(len(indexA)):\n",
      "    profiles.iloc[:, indexA[i]] = essayClusters[i]\n",
      "51/38: profiles = pd.get_dummies(profiles, columns=[i for i in np.array(list(profile_df)) if 'essay' in i])\n",
      "51/39: profiles\n",
      "51/40: profiles.to_csv(\"profiledf.csv\")\n",
      "52/1:\n",
      "import numpy as np\n",
      "import pandas as pd\n",
      "import matplotlib.pyplot as plt\n",
      "%matplotlib inline\n",
      "pd.options.display.max_columns = 100\n",
      "52/2:\n",
      "from sklearn.feature_extraction.text import TfidfVectorizer\n",
      "from nltk.corpus import stopwords\n",
      "from sklearn.cluster import KMeans\n",
      "52/3: profile_df = pd.read_csv(\"profiledf.csv\")\n",
      "52/4: profile_df.head()\n",
      "52/5: profile_df = profile_df.drop('Unnamed: 0')\n",
      "52/6: profile_df = profile_df.drop('Unnamed: 0', axis=1)\n",
      "52/7: profile_df.head()\n",
      "52/8: km = KMeans()\n",
      "52/9:\n",
      "total = 0\n",
      "for i in range(len(profile_df.columns)):\n",
      "    if profile_df[profile_df.columns[i]].sum() < 10:\n",
      "        total += 1\n",
      "print total\n",
      "52/10: profile_df.columns\n",
      "50/61:\n",
      "colcounts = []\n",
      "for c in range(len(categorical_columns)):\n",
      "    for i in range(len(profile_num_df.columns)):\n",
      "        if categorical_columns[c] in profile_num_df.columns[i]:\n",
      "            counts += 1\n",
      "    colcounts.append(counts)\n",
      "for i in range(len(categorical_columns)):\n",
      "    print categorical_columns[i], colcounts[i]\n",
      "50/62:\n",
      "colcounts = []\n",
      "for c in range(len(categorical_columns)):\n",
      "    counts = 0\n",
      "    for i in range(len(profile_num_df.columns)):\n",
      "        if categorical_columns[c] in profile_num_df.columns[i]:\n",
      "            counts += 1\n",
      "    colcounts.append(counts)\n",
      "for i in range(len(categorical_columns)):\n",
      "    print categorical_columns[i], colcounts[i]\n",
      "52/11: [a for a in profile_df.columns if \"speaks\" in a]\n",
      "52/12: [a for a in profile_df.columns if \"speaks_english\" in a]\n",
      "52/13: len([a for a in profile_df.columns if \"speaks_english\" in a])\n",
      "52/14: [a for a in profile_df.columns if \"speaks_english\" in a]\n",
      "52/15: len(profile_df)\n",
      "52/16:\n",
      "len(profile_df)\n",
      "profile_df[0]\n",
      "52/17:\n",
      "len(profile_df)\n",
      "profile_df.iloc[0]\n",
      "52/18:\n",
      "len(profile_df)\n",
      "# profile_df.iloc[0]\n",
      "52/19:\n",
      "def makeindicator(keyword):\n",
      "    res = []\n",
      "    a = np.array([keyword in i for i in np.array(list(profile_df))])\n",
      "    indexA = np.array(list(range(len(a))))[a]\n",
      "    print indexA\n",
      "    for i in range(len(profile_df)):\n",
      "        indic = 0\n",
      "        for j in range(len(indexA)):\n",
      "            if profile_df.iloc[:, indexA[j]]:\n",
      "                indic = 1\n",
      "        res.append(indic)\n",
      "52/20: makeindicator(\"speaks_english\")\n",
      "52/21:\n",
      "def makeindicator(keyword):\n",
      "    res = []\n",
      "    a = np.array([keyword in i for i in np.array(list(profile_df))])\n",
      "    indexA = np.array(list(range(len(a))))[a]\n",
      "    print indexA\n",
      "    for i in range(len(profile_df)):\n",
      "        indic = 0\n",
      "        for j in range(len(indexA)):\n",
      "            if profile_df.iloc[i, indexA[j]]:\n",
      "                indic = 1\n",
      "        res.append(indic)\n",
      "52/22: makeindicator(\"speaks_english\")\n",
      "52/23:\n",
      "def makeindicator(keyword):\n",
      "    res = []\n",
      "    a = np.array([keyword in i for i in np.array(list(profile_df))])\n",
      "    indexA = np.array(list(range(len(a))))[a]\n",
      "    for i in range(len(profile_df)):\n",
      "        indic = 0\n",
      "        for j in range(len(indexA)):\n",
      "            if profile_df.iloc[i, indexA[j]]:\n",
      "                indic = 1\n",
      "        res.append(indic)\n",
      "    return res\n",
      "52/24: speaks_eng = makeindicator(\"speaks_english\")\n",
      "52/25:\n",
      "def makeindicator(keyword):\n",
      "    res = []\n",
      "    a = np.array([keyword in i for i in np.array(list(profile_df))])\n",
      "    indexA = np.array(list(range(len(a))))[a]\n",
      "    for i in range(len(profile_df)):\n",
      "        indic = 0\n",
      "        for j in range(len(indexA)):\n",
      "            if profile_df.iloc[i, indexA[j]]:\n",
      "                indic = 1\n",
      "                break\n",
      "        res.append(indic)\n",
      "    return res\n",
      "52/26: speaks_eng = makeindicator(\"speaks_english\")\n",
      "52/27:\n",
      "def makeindicator(keyword):\n",
      "    res = []\n",
      "    a = np.array([keyword in i for i in np.array(list(profile_df))])\n",
      "    indexA = np.array(list(range(len(a))))[a]\n",
      "    len(indexA)\n",
      "    for i in range(len(profile_df)):\n",
      "        indic = 0\n",
      "        for j in range(len(indexA)):\n",
      "            if profile_df.iloc[i, indexA[j]]:\n",
      "                indic = 1\n",
      "                break\n",
      "        res.append(indic)\n",
      "    return res\n",
      "52/28: speaks_eng = makeindicator(\"speaks_english\")\n",
      "52/29:\n",
      "def makeindicator(keyword):\n",
      "    res = []\n",
      "    a = np.array([keyword in i for i in np.array(list(profile_df))])\n",
      "    indexA = np.array(list(range(len(a))))[a]\n",
      "    print len(indexA)\n",
      "    for i in range(len(profile_df)):\n",
      "        indic = 0\n",
      "        for j in range(len(indexA)):\n",
      "            if profile_df.iloc[i, indexA[j]]:\n",
      "                indic = 1\n",
      "                break\n",
      "        res.append(indic)\n",
      "    return res\n",
      "52/30: speaks_eng = makeindicator(\"speaks_english\")\n",
      "52/31: speaks_eng = makeindicator(\"speaks_swahili\")\n",
      "52/32: speaks_eng = makeindicator(\"swahili\")\n",
      "52/33:\n",
      "def makeindicator(keyword):\n",
      "    res = []\n",
      "    a = np.array([keyword in i for i in np.array(list(profile_df))])\n",
      "    indexA = np.array(list(range(len(a))))[a]\n",
      "    print len(indexA)\n",
      "    for i in range(len(profile_df)):\n",
      "        indic = 0\n",
      "        print i\n",
      "        for j in range(len(indexA)):\n",
      "            if profile_df.iloc[i, indexA[j]]:\n",
      "                indic = 1\n",
      "                break\n",
      "        res.append(indic)\n",
      "    return res\n",
      "52/34: speaks_eng = makeindicator(\"swahili\")\n",
      "52/35:\n",
      "def makeindicator(keyword):\n",
      "    res = []\n",
      "    a = np.array([keyword in i for i in np.array(list(profile_df))])\n",
      "    indexA = np.array(list(range(len(a))))[a]\n",
      "    print len(indexA)\n",
      "    for i in range(len(profile_df)):\n",
      "        indic = 0\n",
      "        if i % 10000 == 0:\n",
      "            print i\n",
      "        for j in range(len(indexA)):\n",
      "            if profile_df.iloc[i, indexA[j]]:\n",
      "                indic = 1\n",
      "                break\n",
      "        res.append(indic)\n",
      "    return res\n",
      "52/36: speaks_eng = makeindicator(\"speaks_english\")\n",
      "52/37:\n",
      "def makeindicator(keyword):\n",
      "    res = []\n",
      "    a = np.array([keyword in i for i in np.array(list(profile_df))])\n",
      "    indexA = np.array(list(range(len(a))))[a]\n",
      "    print len(indexA)\n",
      "    for i in range(len(profile_df)):\n",
      "        indic = 0\n",
      "        if i % 5000 == 0:\n",
      "            print i\n",
      "        for j in range(len(indexA)):\n",
      "            if profile_df.iloc[i, indexA[j]]:\n",
      "                indic = 1\n",
      "                break\n",
      "        res.append(indic)\n",
      "    return res\n",
      "52/38: speaks_eng = makeindicator(\"speaks_english\")\n",
      "52/39:\n",
      "def makeindicator(keyword):\n",
      "    res = []\n",
      "    a = np.array([keyword in i for i in np.array(list(profile_df))])\n",
      "    indexA = np.array(list(range(len(a))))[a]\n",
      "    print len(indexA)\n",
      "    for i in range(len(profile_df)):\n",
      "        indic = 0\n",
      "        if i % 5000 == 0:\n",
      "            print i\n",
      "        for j in range(len(indexA)):\n",
      "            if profile_df.iloc[i, indexA[j]] == 1:\n",
      "                indic = 1\n",
      "                break\n",
      "        res.append(indic)\n",
      "    return res\n",
      "52/40: speaks_eng = makeindicator(\"speaks_english\")\n",
      "52/41:\n",
      "def makeindicator(keyword):\n",
      "    res = []\n",
      "    a = np.array([keyword in i for i in np.array(list(profile_df))])\n",
      "    indexA = np.array(list(range(len(a))))[a]\n",
      "    print len(indexA)\n",
      "    for i in range(len(profile_df)):\n",
      "        indic = 0\n",
      "        if i % 1000 == 0:\n",
      "            print i\n",
      "        for j in range(len(indexA)):\n",
      "            if profile_df.iloc[i, indexA[j]] == 1:\n",
      "                indic = 1\n",
      "                break\n",
      "        res.append(indic)\n",
      "    return res\n",
      "52/42: speaks_eng = makeindicator(\"speaks_english\")\n",
      "52/43:\n",
      "def jloop(indexA, i):\n",
      "    for j in range(len(indexA)):\n",
      "        if profile_df.iloc[i, indexA[j]] == 1:\n",
      "            return 1\n",
      "    return 0\n",
      "def makeindicator(keyword):\n",
      "    res = []\n",
      "    a = np.array([keyword in i for i in np.array(list(profile_df))])\n",
      "    indexA = np.array(list(range(len(a))))[a]\n",
      "    print len(indexA)\n",
      "    map(lambda i: jloop(indexA, i), range(len(profile_df))\n",
      "52/44:\n",
      "def jloop(indexA, i):\n",
      "    for j in range(len(indexA)):\n",
      "        if profile_df.iloc[i, indexA[j]] == 1:\n",
      "            return 1\n",
      "    return 0\n",
      "def makeindicator(keyword):\n",
      "    res = []\n",
      "    a = np.array([keyword in i for i in np.array(list(profile_df))])\n",
      "    indexA = np.array(list(range(len(a))))[a]\n",
      "    print len(indexA)\n",
      "    map(lambda i: jloop(indexA, i), range(len(profile_df)))\n",
      "52/45:\n",
      "def jloop(indexA, i):\n",
      "    for j in range(len(indexA)):\n",
      "        if profile_df.iloc[i, indexA[j]] == 1:\n",
      "            return 1\n",
      "    return 0\n",
      "def makeindicator(keyword):\n",
      "    res = []\n",
      "    a = np.array([keyword in i for i in np.array(list(profile_df))])\n",
      "    indexA = np.array(list(range(len(a))))[a]\n",
      "    print len(indexA)\n",
      "    return list(map(lambda i: jloop(indexA, i), range(len(profile_df))))\n",
      "52/46: speaks_eng = makeindicator(\"speaks_english\")\n",
      "52/47:\n",
      "def makeindicator(df, keyword):\n",
      "res = []\n",
      "a = np.array([keyword in i for i in np.array(list(profile_df))])\n",
      "indexA = np.array(list(range(len(a))))[a]\n",
      "df = df.iloc[:, indexA]\n",
      "print(df.shape)\n",
      "return list(df.any(axis = 1)==1)\n",
      "52/48:\n",
      "def makeindicator(df, keyword):\n",
      "    res = []\n",
      "    a = np.array([keyword in i for i in np.array(list(profile_df))])\n",
      "    indexA = np.array(list(range(len(a))))[a]\n",
      "    df = df.iloc[:, indexA]\n",
      "    print(df.shape)\n",
      "    return list(df.any(axis = 1)==1)\n",
      "52/49: speaks_eng = makeindicator(\"speaks_english\")\n",
      "52/50: speaks_eng = makeindicator(profile_df, \"speaks_english\")\n",
      "52/51: print len(speaks_eng), sum(speaks_eng)\n",
      "52/52: speaks_eng = makeindicator(profile_df, \"speaks_english (fluently)\")\n",
      "52/53: profile_df.shape\n",
      "52/54:\n",
      "def checkwords(keywords, text):\n",
      "    for k in keywords:\n",
      "        if k in text:\n",
      "            return True\n",
      "    return False\n",
      "def makeindicator2(df, keywords):\n",
      "    res = []\n",
      "    a = np.array([checkwords(keywords, i) for i in np.array(list(profile_df))])\n",
      "    indexA = np.array(list(range(len(a))))[a]\n",
      "    df = df.iloc[:, indexA]\n",
      "    print(df.shape)\n",
      "    return list(df.any(axis = 1)==1)\n",
      "53/1:\n",
      "import numpy as np\n",
      "import pandas as pd\n",
      "import matplotlib.pyplot as plt\n",
      "%matplotlib inline\n",
      "from copy import deepcopy\n",
      "53/2:\n",
      "from sklearn.cluster import KMeans\n",
      "profiles = pd.read_csv('profiledf.csv')\n",
      "profiles = profiles.iloc[:, 1:]\n",
      "53/3: # sign & matters? (24)\n",
      "53/4:\n",
      "# fluent in -> english, romance, programming, african, asiatic, eastern european, middle eastern, other\n",
      "# 8 total\n",
      "\n",
      "\n",
      "\n",
      "# countEnglish = 0\n",
      "# countEth = 0\n",
      "# languages = {}\n",
      "# for i in list(profiles):\n",
      "#     for j in i.split(','):\n",
      "#         if 'fluently' in j and 'english':\n",
      "            \n",
      "\n",
      "# print(languages)\n",
      "# d_view = [ (v,k) for k,v in languages.items() ]\n",
      "# d_view.sort(reverse=True) # natively sort tuples by first element\n",
      "# sumLanguages = 0\n",
      "# for v,k in d_view:\n",
      "#     sumLanguages+=v'malay'\n",
      "#     print(\"%s: %d\" % (k,v))\n",
      "# print(sumLanguages)\n",
      "\n",
      "romance = ['italian', 'spanish', 'french', 'portuguese', 'catalan', 'occitan', 'sardinian']\n",
      "programming = ['C++', 'lisp']\n",
      "african = ['afrikaans']\n",
      "asiatic = ['tamil', 'cebuano', 'chinese', 'tagalog', 'japanese', 'vietnamese', 'korean', \\\n",
      "           'thai', 'indonesian', 'bengali', 'sanskrit', 'khmer', 'cebuano' \\\n",
      "          'malay', 'gujarati', 'mongolian', 'ilongo', 'tibetan', 'rotuman']\n",
      "eastern_european= ['swedish', 'yiddish', 'russian', 'german', 'polish', \\\n",
      "                   'greek',  'serbian', 'slovak', 'chechen', 'romanian', 'czech', \\\n",
      "                   'hungarian', 'danish', 'bulgarian', 'ukrainian', \\\n",
      "                  'norwegian', 'finnish', 'slovak', \\\n",
      "                  'lithuanian', 'latvian', 'basque', 'albanian', 'estonian', 'belarusan', \\\n",
      "                  'slovenian', 'georgian', 'frisian']\n",
      "middle_eastern = ['arabic', 'turkish', 'hindi', 'hebrew', 'farsi', 'arabic', 'urdu', 'persian', \\\n",
      "                 'armenian']\n",
      "english = ['english']\n",
      "other = ['dutch', 'other', 'sign language', 'ancient greek', 'latin', 'esperanto',\\\n",
      "        'irish', 'icelandic', 'hawaiian', 'maori', 'breton', 'welsh']\n",
      "53/5:\n",
      "def makeindicator(df, keyword):\n",
      "    df2 = deepcopy(df)\n",
      "    res = []\n",
      "    a = np.array([keyword in i for i in np.array(list(df2))])\n",
      "    indexA = np.array(list(range(len(a))))[a]\n",
      "    df2 = df2.iloc[:, indexA]\n",
      "    return list(df2.any(axis = 1)==1)\n",
      "53/6:\n",
      "%%time\n",
      "\n",
      "languages = {}\n",
      "languages['romances'] = romance\n",
      "languages['programming'] = programming\n",
      "languages['african'] = african\n",
      "languages['eastern_european'] = eastern_european\n",
      "languages['middle_eastern'] = middle_eastern\n",
      "languages['english'] =english\n",
      "languages['other'] =other\n",
      "\n",
      "for langName, lang in languages.items():\n",
      "    a = doLangGroup(lang)\n",
      "    print(langName) + sum(a)/len(a)\n",
      "53/7:\n",
      "\n",
      "def doLangGroup(langGroup):\n",
      "    \n",
      "    indic = np.repeat(False, profile_df.shape[0])\n",
      "    for lang in langGroup:\n",
      "        falseIndexes = np.array(list(range(len(indic))))[~indic]\n",
      "        indic[falseIndexes] = makeindicator(deepcopy(profile_df.iloc[falseIndexes, :]),lang +' (fluently)')\n",
      "    return indic\n",
      "53/8: profile_df_orig = deepcopy(profiles)\n",
      "53/9: profile_df = profiles\n",
      "53/10:\n",
      "%%time\n",
      "\n",
      "languages = {}\n",
      "languages['romances'] = romance\n",
      "languages['programming'] = programming\n",
      "languages['african'] = african\n",
      "languages['eastern_european'] = eastern_european\n",
      "languages['middle_eastern'] = middle_eastern\n",
      "languages['english'] =english\n",
      "languages['other'] =other\n",
      "\n",
      "for langName, lang in languages.items():\n",
      "    a = doLangGroup(lang)\n",
      "    print(langName) + sum(a)/len(a)\n",
      "52/55: print(\"a\") + 2\n",
      "53/11:\n",
      "%%time\n",
      "\n",
      "languages = {}\n",
      "languages['romances'] = romance\n",
      "languages['programming'] = programming\n",
      "languages['african'] = african\n",
      "languages['eastern_european'] = eastern_european\n",
      "languages['middle_eastern'] = middle_eastern\n",
      "languages['english'] =english\n",
      "languages['other'] =other\n",
      "\n",
      "for langName, lang in languages.items():\n",
      "    a = doLangGroup(lang)\n",
      "    print(langName + \" \" + str(sum(a)/len(a)))\n",
      "53/12:\n",
      "%%time\n",
      "\n",
      "languages = {}\n",
      "languages['romances'] = romance\n",
      "languages['programming'] = programming\n",
      "languages['african'] = african\n",
      "languages['eastern_european'] = eastern_european\n",
      "languages['middle_eastern'] = middle_eastern\n",
      "languages['english'] =english\n",
      "languages['other'] =other\n",
      "\n",
      "for langName, lang in languages.items():\n",
      "    a = doLangGroup(lang)\n",
      "    print(langName + \" \" + str(float(sum(a))/len(a)))\n",
      "53/13:\n",
      "d = {}\n",
      "for i in list(profiles):\n",
      "    if 'sign' in i and 'speaks' not in i:\n",
      "        d[i] = sum(profiles[i])\n",
      "#         print(i + \": \"  + str(sum(profiles[i])))\n",
      "d_view = [ (v,k) for k,v in d.items() ]\n",
      "d_view.sort(reverse=True) # natively sort tuples by first element\n",
      "sumLanguages = 0\n",
      "for v,k in d_view:\n",
      "    sumLanguages+=v\n",
      "    print(\"%s: %d\" % (k,v))\n",
      "print(sumLanguages)\n",
      "53/14:\n",
      "def makeindicator_eth(df, keyword):\n",
      "    df2 = deepcopy(df)\n",
      "    res = []\n",
      "    a = np.array([('ethnicity' in i and ',' in i and keyword in i) for i in np.array(list(df2))])\n",
      "    indexA = np.array(list(range(len(a))))[a]\n",
      "    df2 = df2.iloc[:, indexA]\n",
      "    return list(df2.any(axis = 1)==1)\n",
      "53/15:\n",
      "def doEthnicGroup(langGroup):\n",
      "    \n",
      "    indic = np.repeat(False, profile_df.shape[0])\n",
      "    for lang in langGroup:\n",
      "        falseIndexes = np.array(list(range(len(indic))))[~indic]\n",
      "        indic[falseIndexes] = makeindicator_eth(deepcopy(profile_df.iloc[falseIndexes, :]),lang)\n",
      "    return indic\n",
      "53/16:\n",
      "ethnicities = {}\n",
      "ethnicities['asian mix'] = ['asian', 'pacific islander']\n",
      "ethnicities['hispanic/latin mix'] = ['hispanic / latin']\n",
      "ethnicities['black mix'] = ['black']\n",
      "ethnicities['indian mix'] = ['indian']\n",
      "ethnicities['middle eastern mix'] = ['middle eastern']\n",
      "for ethName, eth in ethnicities.items():\n",
      "    a = doEthnicGroup(eth)\n",
      "53/17:\n",
      "ethnicities = {}\n",
      "ethnicities['asian mix'] = ['asian', 'pacific islander']\n",
      "ethnicities['hispanic/latin mix'] = ['hispanic / latin']\n",
      "ethnicities['black mix'] = ['black']\n",
      "ethnicities['indian mix'] = ['indian']\n",
      "ethnicities['middle eastern mix'] = ['middle eastern']\n",
      "ethnicitiesL = []\n",
      "for ethName, eth in ethnicities.items():\n",
      "    ethnicitiesL.append(doEthnicGroup(eth))\n",
      "53/18: print ethnicitiesL\n",
      "53/19: print ethnicitiesL.shape\n",
      "53/20: print len(ethnicitiesL[0])\n",
      "53/21: print len(ethnicitiesL)\n",
      "53/22:\n",
      "def makeindicator_sign(df, keyword):\n",
      "    df2 = deepcopy(df)\n",
      "    res = []\n",
      "    a = np.array([('sign' in i and 'speaks' not in i and keyword in i) for i in np.array(list(df2))])\n",
      "    indexA = np.array(list(range(len(a))))[a]\n",
      "    df2 = df2.iloc[:, indexA]\n",
      "    return list(df2.any(axis = 1)==1)\n",
      "53/23:\n",
      "def doSignGroup(langGroup):\n",
      "    \n",
      "    indic = np.repeat(False, profile_df.shape[0])\n",
      "    for lang in langGroup:\n",
      "        falseIndexes = np.array(list(range(len(indic))))[~indic]\n",
      "        indic[falseIndexes] = makeindicator_sign(deepcopy(profile_df.iloc[falseIndexes, :]),lang)\n",
      "    return indic\n",
      "53/24:\n",
      "signsL = []\n",
      "for signName, sign in signs.items():\n",
      "    signsL.append(doSignGroup(sign))\n",
      "53/25:\n",
      "signs = {}\n",
      "signs['not serious'] = ['but it', 'fun to think about']\n",
      "signs['leo'] = ['leo']\n",
      "signs['libra'] = ['libra']\n",
      "signs['cancer'] = ['cancer']\n",
      "signs['virgo'] = ['virgo']\n",
      "signs['scorpio'] = ['scorpio']\n",
      "signs['gemini'] = ['gemini']\n",
      "signs['taurus'] = ['taurus']\n",
      "signs['aries'] = ['aries']\n",
      "signs['pisces'] = ['pisces']\n",
      "signs['aquarius'] = ['aquarius']\n",
      "signs['sagittarius'] = ['sagittarius']\n",
      "signs['capricorn'] = ['capricorn']\n",
      "53/26:\n",
      "signsL = []\n",
      "for signName, sign in signs.items():\n",
      "    signsL.append(doSignGroup(sign))\n",
      "53/27: print len(signsL[9])\n",
      "53/28: print sum(signsL[0])\n",
      "53/29: print sum(signsL[2])\n",
      "53/30:\n",
      "for i in range(len(signsL))\n",
      "    print sum(signsL[i]), len(signsL[i])\n",
      "53/31:\n",
      "for i in range(len(signsL))\n",
      "    print sum(signsL[i]), double(sum(signsL[i]))/len(signsL[i])\n",
      "53/32:\n",
      "for i in range(len(signsL)):\n",
      "    print sum(signsL[i]), double(sum(signsL[i]))/len(signsL[i])\n",
      "53/33:\n",
      "for i in range(len(signsL)):\n",
      "    print sum(signsL[i]), float(sum(signsL[i]))/len(signsL[i])\n",
      "53/34:\n",
      "signsL = {}\n",
      "for signName, sign in signs.items():\n",
      "    signsL[signName] = doSignGroup(sign)\n",
      "53/35:\n",
      "for signName, sign in signsL.items():\n",
      "    print signName, sum(sign), float(sum(sign))/len(sign)\n",
      "53/36:\n",
      "ethnicities = {}\n",
      "ethnicities['asian mix'] = ['asian', 'pacific islander']\n",
      "ethnicities['hispanic/latin mix'] = ['hispanic / latin']\n",
      "ethnicities['black mix'] = ['black']\n",
      "ethnicities['indian mix'] = ['indian']\n",
      "ethnicities['middle eastern mix'] = ['middle eastern']\n",
      "ethnicitiesL = {}\n",
      "for ethName, eth in ethnicities.items():\n",
      "    ethnicitiesL[ethName] = doEthnicGroup(eth)\n",
      "53/37:\n",
      "for ethName, eth in ethnicities.items():\n",
      "    print ethnicitiesL[ethName], sum(eth), float(sum(eth))/len(eth)\n",
      "53/38:\n",
      "for ethName, eth in ethnicitiesL.items():\n",
      "    print ethnicitiesL[ethName], sum(eth), float(sum(eth))/len(eth)\n",
      "53/39:\n",
      "for ethName, eth in ethnicitiesL.items():\n",
      "    print ethName, sum(eth), float(sum(eth))/len(eth)\n",
      "53/40:\n",
      "religions = {}\n",
      "religions[\"religion_serious\"] = ['serious about it']\n",
      "religions[\"other\"] = ['other']\n",
      "religions[\"judaism\"] = [\"judaism\"]\n",
      "religions[\"islam\"] = ['islam']\n",
      "religions[\"hinduism\"] = ['hinduism']\n",
      "religions[\"christianity\"] = ['christianity']\n",
      "religions[\"catholicism\"] = ['catholicism']\n",
      "religions[\"buddhism\"] = ['buddhism']\n",
      "religions[\"atheism\"] = ['atheism']\n",
      "religions[\"agnosticism\"] = ['agnosticism']\n",
      "53/41:\n",
      "def doReligionGroup(religionGroup):\n",
      "    indic = np.repeat(False, profile_df.shape[0])\n",
      "    for religion in religionGroup:\n",
      "        falseIndexes = np.array(list(range(len(indic))))[~indic]\n",
      "        indic[falseIndexes] = makeindicator(deepcopy(profile_df.iloc[falseIndexes, :]),religion)\n",
      "    return indic\n",
      "53/42:\n",
      "religionIndic = {}\n",
      "for religionName, religion in religions.items():\n",
      "    a = doReligionGroup(religion)\n",
      "    religionIndic[religionName] = a\n",
      "    print(religionName + \": \"+ str(sum(a)/len(a)))\n",
      "53/43:\n",
      "religionIndic = {}\n",
      "for religionName, religion in religions.items():\n",
      "    a = doReligionGroup(religion)\n",
      "    religionIndic[religionName] = a\n",
      "    print(religionName + \": \"+ str(float(sum(a))/len(a)))\n",
      "53/44:\n",
      "# dropped hs, hs, college, masters, med, lawyer, ph.d\n",
      "# student\n",
      "education ={}\n",
      "\n",
      "education['dropped_hs'] = ['education_dropped out of high school']\n",
      "education['hs'] = ['education_graduated from high school', 'education_dropped out of college/university',\\\n",
      "'education_two-year college', 'education_working on college/university', 'education_dropped out of two-year college',\\\n",
      "'education_working on two-year college']\n",
      "education['college'] = ['education_college/university', 'education_dropped out of law school',\\\n",
      "'education_dropped out of masters program', 'education_dropped out of med school', \\\n",
      "'education_dropped out of ph.d program', 'education_dropped out of space camp',\\\n",
      "'education_graduated from college/university', 'education_graduated from two-year college'\n",
      "]\n",
      "education['masters'] = ['education_graduated from masters program']\n",
      "education['law'] = ['education_graduated from law school']\n",
      "education['med'] = ['education_graduated from med school']\n",
      "education['phd'] = ['education_graduated from ph.d program', 'job_education / academia']\n",
      "education['student'] = ['education_working on college/university', 'education_working on high school',\\\n",
      "'education_working on law school', 'education_working on masters program',\n",
      "'education_working on med school', 'education_working on ph.d program', \\\n",
      "'education_working on space camp', 'education_working on two-year college']\n",
      "53/45:\n",
      "# dropped hs, hs, college, masters, med, lawyer, ph.d\n",
      "# student\n",
      "education ={}\n",
      "\n",
      "education['dropped_hs'] = ['education_dropped out of high school']\n",
      "education['hs'] = ['education_graduated from high school', 'education_dropped out of college/university',\\\n",
      "'education_two-year college', 'education_working on college/university', 'education_dropped out of two-year college',\\\n",
      "'education_working on two-year college']\n",
      "education['college'] = ['education_college/university', 'education_dropped out of law school',\\\n",
      "'education_dropped out of masters program', 'education_dropped out of med school', \\\n",
      "'education_dropped out of ph.d program', 'education_dropped out of space camp',\\\n",
      "'education_graduated from college/university', 'education_graduated from two-year college'\n",
      "]\n",
      "education['masters'] = ['education_graduated from masters program']\n",
      "education['law'] = ['education_graduated from law school']\n",
      "education['med'] = ['education_graduated from med school']\n",
      "education['phd'] = ['education_graduated from ph.d program', 'job_education / academia']\n",
      "education['student'] = ['education_working on college/university', 'education_working on high school',\\\n",
      "'education_working on law school', 'education_working on masters program',\\\n",
      "'education_working on med school', 'education_working on ph.d program', \\\n",
      "'education_working on space camp', 'education_working on two-year college']\n",
      "53/46:\n",
      "# dropped hs, hs, college, masters, med, lawyer, ph.d\n",
      "# student\n",
      "education ={}\n",
      "\n",
      "education['dropped_hs'] = ['education_dropped out of high school']\n",
      "education['hs'] = ['education_graduated from high school', 'education_dropped out of college/university',\\\n",
      "'education_two-year college', 'education_working on college/university', 'education_dropped out of two-year college',\\\n",
      "'education_working on two-year college']\n",
      "education['college'] = ['education_college/university', 'education_dropped out of law school',\\\n",
      "'education_dropped out of masters program', 'education_dropped out of med school', \\\n",
      "'education_dropped out of ph.d program', 'education_dropped out of space camp',\\\n",
      "'education_graduated from college/university', 'education_graduated from two-year college']\n",
      "education['masters'] = ['education_graduated from masters program']\n",
      "education['law'] = ['education_graduated from law school']\n",
      "education['med'] = ['education_graduated from med school']\n",
      "education['phd'] = ['education_graduated from ph.d program', 'job_education / academia']\n",
      "education['student'] = ['education_working on college/university', 'education_working on high school',\\\n",
      "'education_working on law school', 'education_working on masters program',\\\n",
      "'education_working on med school', 'education_working on ph.d program', \\\n",
      "'education_working on space camp', 'education_working on two-year college']\n",
      "53/47:\n",
      "# dropped hs, hs, college, masters, med, lawyer, ph.d\n",
      "# student\n",
      "education ={}\n",
      "\n",
      "education['dropped_hs'] = ['education_dropped out of high school']\n",
      "education['hs'] = ['education_graduated from high school', 'education_dropped out of college/university', \\\n",
      "'education_two-year college', 'education_working on college/university', 'education_dropped out of two-year college',\\\n",
      "'education_working on two-year college']\n",
      "education['college'] = ['education_college/university', 'education_dropped out of law school', \\\n",
      "'education_dropped out of masters program', 'education_dropped out of med school', \\\n",
      "'education_dropped out of ph.d program', 'education_dropped out of space camp',\\\n",
      "'education_graduated from college/university', 'education_graduated from two-year college']\n",
      "education['masters'] = ['education_graduated from masters program']\n",
      "education['law'] = ['education_graduated from law school']\n",
      "education['med'] = ['education_graduated from med school']\n",
      "education['phd'] = ['education_graduated from ph.d program', 'job_education / academia']\n",
      "education['student'] = ['education_working on college/university', 'education_working on high school',\\\n",
      "'education_working on law school', 'education_working on masters program',\\\n",
      "'education_working on med school', 'education_working on ph.d program', \\\n",
      "'education_working on space camp', 'education_working on two-year college']\n",
      "53/48:\n",
      "# dropped hs, hs, college, masters, med, lawyer, ph.d\n",
      "# student\n",
      "education ={}\n",
      "\n",
      "education['dropped_hs'] = ['education_dropped out of high school']\n",
      "education['hs'] = ['education_graduated from high school', 'education_dropped out of college/university', \\\n",
      "     'education_two-year college', 'education_working on college/university', 'education_dropped out of two-year college',\\\n",
      "     'education_working on two-year college']\n",
      "education['college'] = ['education_college/university', 'education_dropped out of law school', \\\n",
      "'education_dropped out of masters program', 'education_dropped out of med school', \\\n",
      "'education_dropped out of ph.d program', 'education_dropped out of space camp',\\\n",
      "'education_graduated from college/university', 'education_graduated from two-year college']\n",
      "education['masters'] = ['education_graduated from masters program']\n",
      "education['law'] = ['education_graduated from law school']\n",
      "education['med'] = ['education_graduated from med school']\n",
      "education['phd'] = ['education_graduated from ph.d program', 'job_education / academia']\n",
      "education['student'] = ['education_working on college/university', 'education_working on high school',\\\n",
      "'education_working on law school', 'education_working on masters program',\\\n",
      "'education_working on med school', 'education_working on ph.d program', \\\n",
      "'education_working on space camp', 'education_working on two-year college']\n",
      "53/49:\n",
      "# dropped hs, hs, college, masters, med, lawyer, ph.d\n",
      "# student\n",
      "education ={}\n",
      "\n",
      "education['dropped_hs'] = ['education_dropped out of high school']\n",
      "education['hs'] = ['education_graduated from high school', 'education_dropped out of college/university', \\\n",
      "     'education_two-year college', 'education_working on college/university', 'education_dropped out of two-year college',\\\n",
      "     'education_working on two-year college']\n",
      "education['college'] = ['education_college/university', 'education_dropped out of law school', \\\n",
      "    'education_dropped out of masters program', 'education_dropped out of med school', \\\n",
      "    'education_dropped out of ph.d program', 'education_dropped out of space camp',\\\n",
      "    'education_graduated from college/university', 'education_graduated from two-year college']\n",
      "education['masters'] = ['education_graduated from masters program']\n",
      "education['law'] = ['education_graduated from law school']\n",
      "education['med'] = ['education_graduated from med school']\n",
      "education['phd'] = ['education_graduated from ph.d program', 'job_education / academia']\n",
      "education['student'] = ['education_working on college/university', 'education_working on high school',\\\n",
      "    'education_working on law school', 'education_working on masters program',\\\n",
      "    'education_working on med school', 'education_working on ph.d program', \\\n",
      "    'education_working on space camp', 'education_working on two-year college']\n",
      "53/50:\n",
      "educationIndic = {}\n",
      "for religionName, religion in education.items():\n",
      "a = doReligionGroup(religion)\n",
      "religionIndic[religionName] = a\n",
      "print(religionName + \": \"+ str(float(sum(a))/len(a)))\n",
      "53/51:\n",
      "educationIndic = {}\n",
      "for religionName, religion in education.items():\n",
      "    a = doReligionGroup(religion)\n",
      "    religionIndic[religionName] = a\n",
      "    print(religionName + \": \"+ str(float(sum(a))/len(a)))\n",
      "53/52:\n",
      "df2 = deepcopy(profile_df)\n",
      "a = np.array([('speaks' in i or 'education' in 'sign' in i or 'religion' in i) for i in np.array(list(df2))])\n",
      "profiles2 = df2.drop(a, axis=1)\n",
      "53/53:\n",
      "df2 = deepcopy(profile_df)\n",
      "a = np.array([i if ('speaks' in i or 'education' in 'sign' in i or 'religion' in i) for i in np.array(list(df2))])\n",
      "profiles2 = df2.drop(a, axis=1)\n",
      "53/54:\n",
      "df2 = deepcopy(profile_df)\n",
      "a = np.array([i for i in np.array(list(df2)) if ('speaks' in i or 'education' in 'sign' in i or 'religion' in i)])\n",
      "profiles2 = df2.drop(a, axis=1)\n",
      "53/55:\n",
      "a = np.array([i for i in np.array(list(df2)) if ('ethnicity' in i and ',' in i)])\n",
      "profiles2 = profiles2.drop(a, axis=1)\n",
      "53/56: profiles2.shape\n",
      "53/57: profiles2.head()\n",
      "53/58:\n",
      "# speaks\n",
      "speaks1 = pd.read_csv('language1Indic.csv')\n",
      "speaks2 = pd.read_csv('language1Indic2.csv')\n",
      "53/59: speaks1.shape\n",
      "53/60:\n",
      "for i in range(3):\n",
      "    profiles2[list(speaks1)[i]] = speaks1.iloc[:,i]\n",
      "53/61: profiles2.shape\n",
      "53/62:\n",
      "for i in range(4):\n",
      "    profiles2[list(speaks2)[i]] = speaks2.iloc[:,i]\n",
      "53/63: profiles2.head()\n",
      "53/64:\n",
      "# education\n",
      "for educName, educ in education.items():\n",
      "    profiles2[educName] = educ\n",
      "53/65:\n",
      "# education\n",
      "for educName, educ in educationIndic.items():\n",
      "    profiles2[educName] = educ\n",
      "53/66: profiles2.head()\n",
      "53/67: profiles2.shape\n",
      "53/68:\n",
      "educationIndic = {}\n",
      "for religionName, religion in education.items():\n",
      "    a = doReligionGroup(religion)\n",
      "    educationIndic[religionName] = a\n",
      "    print(religionName + \": \"+ str(float(sum(a))/len(a)))\n",
      "53/69:\n",
      "religionIndic = {}\n",
      "for religionName, religion in religions.items():\n",
      "    a = doReligionGroup(religion)\n",
      "    religionIndic[religionName] = a\n",
      "    print(religionName + \": \"+ str(float(sum(a))/len(a)))\n",
      "53/70:\n",
      "# education\n",
      "for educName, educ in educationIndic.items():\n",
      "    profiles2[educName] = educ\n",
      "53/71: profiles2.shape\n",
      "53/72:\n",
      "# religion\n",
      "for religionName, religion in religionIndic.items():\n",
      "    profiles2[religionName] = religion\n",
      "53/73:\n",
      "# sign\n",
      "for signName, sign in signsL.items():\n",
      "    profiles2[signName] = sign\n",
      "53/74: profiles2.shape\n",
      "53/75:\n",
      "# ethnicity\n",
      "for ethName, eth in ethnicitiesL.items():\n",
      "    profiles2[ethName] = eth\n",
      "53/76: profiles2.shape\n",
      "53/77:\n",
      "profiles2.replace(True, 1)\n",
      "profiles2.replace(False, 0)\n",
      "53/78:\n",
      "profiles2 = profiles2.replace(True, 1)\n",
      "profiles2 = profiles2.replace(False, 0)\n",
      "53/79: profiles2.head()\n",
      "53/80:\n",
      "profiles2 = profiles2.replace(to_replace=True, value=1)\n",
      "profiles2 = profiles2.replace(False, 0)\n",
      "53/81: profiles2.head()\n",
      "53/82: profiles2.to_csv(\"profiles2.csv\")\n",
      "53/83: profiles2.columns\n",
      "53/84: print profiles2.columns\n",
      "53/85: print list(profiles2.columns)\n",
      "53/86:\n",
      "typeVals = profiles2.dtypes\n",
      "typeVals = typeVals.replace('bool', 'int64')\n",
      "profiles2.astype(typeVals)\n",
      "53/87:\n",
      "typeVals = profiles2.dtypes\n",
      "typeVals = typeVals.replace('bool', 'int64')\n",
      "profiles2 = profiles2.astype(typeVals)\n",
      "53/88: profiles2.head()\n",
      "53/89: profiles2.to_csv(\"profiles2.csv\")\n",
      "54/1:\n",
      "import numpy as np\n",
      "import pandas as pd\n",
      "import matplotlib.pyplot as plt\n",
      "%matplotlib inline\n",
      "from copy import deepcopy\n",
      "54/2: profile_df = pd.read_csv('profiles2.csv')\n",
      "54/3: profile_df\n",
      "54/4: profile_df.head()\n",
      "54/5: profile_df = pd.read_csv('profiles2.csv')[:,1:]\n",
      "54/6: profile_df = pd.read_csv('profiles2.csv')\n",
      "54/7: profile_df = profile_df.drop(\"Unnamed: 0\")\n",
      "54/8: profile_df = profile_df.drop(profile_df.columns[0])\n",
      "54/9: profile_df = profile_df.drop(profile_df.columns[0], axis=1)\n",
      "54/10: profile_df.head()\n",
      "54/11:\n",
      "import numpy as np\n",
      "import pandas as pd\n",
      "import matplotlib.pyplot as plt\n",
      "%matplotlib inline\n",
      "from copy import deepcopy\n",
      "from sklearn.preprocessing import MinMaxScaler\n",
      "54/12:\n",
      "toscale = ['age', 'height', 'income', 'last_online']\n",
      "profiles = deepcopy(profile_df)\n",
      "for i in range(len(toscale)):\n",
      "    scaler = MinMaxScaler(feature_range=(0, 1))\n",
      "    profiles[toscale[i]] = scaler.fit_transform(profiles[toscale[i]])\n",
      "54/13:\n",
      "toscale = ['age', 'height', 'income', 'last_online']\n",
      "profiles = deepcopy(profile_df)\n",
      "for i in range(len(toscale)):\n",
      "    scaler = MinMaxScaler(feature_range=(0, 1))\n",
      "    profiles[toscale[i]] = scaler.fit_transform(profiles[toscale[i]].reshape(-1, 1))\n",
      "54/14:\n",
      "toscale = ['age', 'height', 'income', 'last_online']\n",
      "profiles = deepcopy(profile_df)\n",
      "for i in range(len(toscale)):\n",
      "    scaler = MinMaxScaler(feature_range=(0, 1))\n",
      "    profiles[toscale[i]] = scaler.fit_transform(np.array(profiles[toscale[i]]).reshape(-1, 1))\n",
      "54/15: profiles.head()\n",
      "54/16:\n",
      "# most important colmns\n",
      "# rank 1: sex, orientation, languages(?), drugs, alcohol\n",
      "# rank 2: age, status, religion-importance, horoscope-importance, income\n",
      "# rank 3: education, ethnicity\n",
      "# rank 4: pets, offspring, religion, horoscope,\n",
      "# rank 5: essays\n",
      "53/90:\n",
      "df2 = deepcopy(profile_df)\n",
      "a = np.array([i for i in np.array(list(df2)) if ('speaks' in i or 'education' in 'sign' in i or 'religion' in i)])\n",
      "profiles2 = df2.drop(a, axis=1)\n",
      "53/91: profiles2.shape\n",
      "53/92:\n",
      "from sklearn.cluster import KMeans\n",
      "profiles = pd.read_csv('profiledf.csv')\n",
      "profiles = profiles.iloc[:, 1:]\n",
      "54/17:\n",
      "ranks = [['sex', 'orientation', 'speaks', 'drugs', 'alcohol'], \n",
      "        ['age', 'status', 'religion_serious', 'not serious', 'income'], \\\n",
      "        ['education', 'ethnicity'], \\\n",
      "        ['pets', 'offspring', 'religion', 'sign'], \\\n",
      "        ['essay']]\n",
      "54/18: profile_df = pd.read_csv('profiles2.csv').drop(profile_df.columns[0], axis=1)\n",
      "54/19: profile_df.head()\n",
      "54/20: profile_df = pd.read_csv('profiles2.csv').drop(profile_df.columns[0], axis=1)\n",
      "54/21: profile_df.head()\n",
      "54/22:\n",
      "toscale = ['age', 'height', 'income', 'last_online']\n",
      "profiles = deepcopy(profile_df)\n",
      "for i in range(len(toscale)):\n",
      "    scaler = MinMaxScaler(feature_range=(0, 1))\n",
      "    profiles[toscale[i]] = scaler.fit_transform(np.array(profiles[toscale[i]]).reshape(-1, 1))\n",
      "54/23: profiles.head()\n",
      "54/24: \"a_b\".split('_')[0]\n",
      "54/25:\n",
      "for i in range(len(profiles.columns)):\n",
      "    category = profiles.columns[i].split('_')[0]\n",
      "    for j in range(len(ranks)):\n",
      "        if category in ranks[j]:\n",
      "            profiles[profiles.columns[i]] = profiles[profiles.columns[i]]*j\n",
      "54/26: profiles\n",
      "54/27: profile_df = pd.read_csv('profiles2.csv').drop(profile_df.columns[0], axis=1)\n",
      "54/28: profile_df.head()\n",
      "54/29:\n",
      "toscale = ['age', 'height', 'income', 'last_online']\n",
      "profiles = deepcopy(profile_df)\n",
      "for i in range(len(toscale)):\n",
      "    scaler = MinMaxScaler(feature_range=(0, 1))\n",
      "    profiles[toscale[i]] = scaler.fit_transform(np.array(profiles[toscale[i]]).reshape(-1, 1))\n",
      "54/30: profile_df = pd.read_csv('profiles2.csv').drop(profile_df.columns[0], axis=1)\n",
      "54/31: profile_df.head()\n",
      "54/32:\n",
      "toscale = ['age', 'height', 'income', 'last_online']\n",
      "profiles = deepcopy(profile_df)\n",
      "for i in range(len(toscale)):\n",
      "    scaler = MinMaxScaler(feature_range=(0, 1))\n",
      "    profiles[toscale[i]] = scaler.fit_transform(np.array(profiles[toscale[i]]).reshape(-1, 1))\n",
      "54/33: profiles.head()\n",
      "54/34:\n",
      "ranks = [['sex', 'orientation', 'speaks', 'drugs', 'drinks'], \n",
      "        ['age', 'status', 'not', 'income', 'body'], \\\n",
      "        ['education', 'ethnicity'], \\\n",
      "        ['pets', 'offspring', 'religion', 'sign', 'diet'], \\\n",
      "        ['essay']]\n",
      "54/35:\n",
      "for i in range(len(profiles.columns)):\n",
      "    category = profiles.columns[i].split('_')[0]\n",
      "    for j in range(len(ranks)):\n",
      "        if category in ranks[j]:\n",
      "            profiles[profiles.columns[i]] = profiles[profiles.columns[i]]*(2**(4-i))\n",
      "54/36: profiles\n",
      "54/37: profile_df = pd.read_csv('profiles2.csv').drop(profile_df.columns[0], axis=1)\n",
      "54/38:\n",
      "for i in range(len(profiles.columns)):\n",
      "    category = profiles.columns[i].split('_')[0]\n",
      "    for j in range(len(ranks)):\n",
      "        if category in ranks[j]:\n",
      "            profiles[profiles.columns[i]] = profiles[profiles.columns[i]]*(2**(4-j))\n",
      "            continue\n",
      "54/39: profiles\n",
      "54/40: profile_df = pd.read_csv('profiles2.csv').drop(profile_df.columns[0], axis=1)\n",
      "54/41: profile_df.head()\n",
      "54/42:\n",
      "toscale = ['age', 'height', 'income', 'last_online']\n",
      "profiles = deepcopy(profile_df)\n",
      "for i in range(len(toscale)):\n",
      "    scaler = MinMaxScaler(feature_range=(0, 1))\n",
      "    profiles[toscale[i]] = scaler.fit_transform(np.array(profiles[toscale[i]]).reshape(-1, 1))\n",
      "54/43: profiles.head()\n",
      "54/44:\n",
      "# most important colmns\n",
      "# rank1 = sex, orientation, languages(?), drugs, alcohol\n",
      "# rank2 = age, status, religion-importance, horoscope-importance, income\n",
      "# rank3 = education, ethnicity\n",
      "# rank4 = ['pets', 'offspring', 'religion', sign,\n",
      "# rank5 = ['essay']\n",
      "54/45:\n",
      "ranks = [['sex', 'orientation', 'speaks', 'drugs', 'drinks'], \n",
      "        ['age', 'status', 'not', 'income', 'body'], \\\n",
      "        ['education', 'ethnicity'], \\\n",
      "        ['pets', 'offspring', 'religion', 'sign', 'diet'], \\\n",
      "        ['essay']]\n",
      "54/46:\n",
      "for i in range(len(profiles.columns)):\n",
      "    category = profiles.columns[i].split('_')[0]\n",
      "    for j in range(len(ranks)):\n",
      "        if category in ranks[j]:\n",
      "            profiles[profiles.columns[i]] = profiles[profiles.columns[i]]*(2**(4-j))\n",
      "            continue\n",
      "54/47: profiles\n",
      "54/48: profiles['religion_serious'] = profiles['religion_serious']*4\n",
      "54/49: profiles\n",
      "54/50: profiles.head()\n",
      "54/51: km = KMeans()\n",
      "54/52:\n",
      "import numpy as np\n",
      "import pandas as pd\n",
      "import matplotlib.pyplot as plt\n",
      "%matplotlib inline\n",
      "from copy import deepcopy\n",
      "from sklearn.preprocessing import MinMaxScaler\n",
      "from sklearn.clustering import KMeans\n",
      "54/53:\n",
      "import numpy as np\n",
      "import pandas as pd\n",
      "import matplotlib.pyplot as plt\n",
      "%matplotlib inline\n",
      "from copy import deepcopy\n",
      "from sklearn.preprocessing import MinMaxScaler\n",
      "from sklearn.cluster import KMeans\n",
      "54/54:\n",
      "n = [2,3,4]\n",
      "kmL = []\n",
      "for i in range(len(n)):\n",
      "    km = KMeans(n_cluster = n[i])\n",
      "    kmL.append(km.fit(profiles))\n",
      "54/55:\n",
      "n = [2,3,4]\n",
      "kmL = []\n",
      "for i in range(len(n)):\n",
      "    km = KMeans(n_clusters = n[i])\n",
      "    kmL.append(km.fit(profiles))\n",
      "54/56: profiles = profiles.fillna(profiles.mean().iloc[0])\n",
      "54/57:\n",
      "n = [2,3,4]\n",
      "kmL = []\n",
      "for i in range(len(n)):\n",
      "    km = KMeans(n_clusters = n[i])\n",
      "    kmL.append(km.fit(profiles))\n",
      "54/58:\n",
      "order_centroids = km.cluster_centers_.argsort()[:, ::-1]\n",
      "terms = profiles.columns\n",
      "for i in range(4):\n",
      "    print(\"Cluster %d:\" % i)\n",
      "    for ind in order_centroids[i, :10]:\n",
      "        print(' %s' % terms[ind])\n",
      "    print()\n",
      "55/1:\n",
      "import numpy as np\n",
      "import pandas as pd\n",
      "import matplotlib.pyplot as plt\n",
      "%matplotlib inline\n",
      "from copy import deepcopy\n",
      "from sklearn.preprocessing import MinMaxScaler\n",
      "from sklearn.cluster import KMeans\n",
      "import seaborn as sb\n",
      "55/2: profile_df = pd.read_csv('profiles2.csv').drop(profile_df.columns[0], axis=1)\n",
      "55/3:\n",
      "profile_df = pd.read_csv('profiles2.csv')\n",
      "profile_df = profile_df.drop(profile_df.columns[0], axis=1)\n",
      "55/4: profile_df.head()\n",
      "55/5:\n",
      "toscale = ['age', 'height', 'income', 'last_online']\n",
      "profiles = deepcopy(profile_df)\n",
      "for i in range(len(toscale)):\n",
      "    scaler = MinMaxScaler(feature_range=(0, 1))\n",
      "    profiles[toscale[i]] = scaler.fit_transform(np.array(profiles[toscale[i]]).reshape(-1, 1))\n",
      "55/6: profiles.head()\n",
      "55/7:\n",
      "# most important colmns\n",
      "# rank1 = sex, orientation, languages(?), drugs, alcohol\n",
      "# rank2 = age, status, religion-importance, horoscope-importance, income\n",
      "# rank3 = education, ethnicity\n",
      "# rank4 = ['pets', 'offspring', 'religion', sign,\n",
      "# rank5 = ['essay']\n",
      "55/8:\n",
      "ranks = [['sex', 'orientation', 'speaks', 'drugs', 'drinks'], \n",
      "        ['age', 'status', 'not', 'income', 'body'], \\\n",
      "        ['education', 'ethnicity'], \\\n",
      "        ['pets', 'offspring', 'religion', 'sign', 'diet'], \\\n",
      "        ['essay']]\n",
      "55/9:\n",
      "for i in range(len(profiles.columns)):\n",
      "    category = profiles.columns[i].split('_')[0]\n",
      "    for j in range(len(ranks)):\n",
      "        if category in ranks[j]:\n",
      "            profiles[profiles.columns[i]] = profiles[profiles.columns[i]]*(2**(4-j))\n",
      "            continue\n",
      "55/10: profiles['religion_serious'] = profiles['religion_serious']*4\n",
      "55/11: profiles.head()\n",
      "55/12: profiles = profiles.fillna(profiles.mean().iloc[0])\n",
      "55/13:\n",
      "n = [2,3,4]\n",
      "kmL = []\n",
      "for i in range(len(n)):\n",
      "    km = KMeans(n_clusters = n[i])\n",
      "    kmL.append(km.fit(profiles))\n",
      "55/14:\n",
      "for i in range(len(profiles.columns)):\n",
      "    print profiles.columns[i] + \":\"\n",
      "    for j in range(len(n)):\n",
      "         print \"j: \" + kmL[j]\n",
      "55/15:\n",
      "for i in range(len(profiles.columns)):\n",
      "    print profiles.columns[i] + \":\"\n",
      "    for j in range(len(n)):\n",
      "         print \"j: \" + kmL[j][i]\n",
      "55/16: centers = [k.cluster_centers_ for k in kmL]\n",
      "55/17:\n",
      "for i in range(len(profiles.columns)):\n",
      "    print profiles.columns[i] + \":\"\n",
      "    for j in range(len(n)):\n",
      "         print \"j: \" + kmL[j][i]\n",
      "55/18:\n",
      "for i in range(len(profiles.columns)):\n",
      "    print profiles.columns[i] + \":\"\n",
      "    for j in range(len(n)):\n",
      "         print \"j: \" + centers[j][i]\n",
      "55/19: centers.shape\n",
      "55/20: len(centers)\n",
      "55/21: len(centers[0])\n",
      "55/22: centers[0]\n",
      "55/23:\n",
      "for i in range(len(profiles.columns)):\n",
      "    print profiles.columns[i] + \":\"\n",
      "    for j in range(len(n)):\n",
      "         print \"j: \" + centers[0][j][i]\n",
      "55/24:\n",
      "for i in range(len(profiles.columns)):\n",
      "    print profiles.columns[i] + \":\"\n",
      "    for j in range(len(n)):\n",
      "         print \"j: \" + str(centers[0][j][i])\n",
      "55/25:\n",
      "for i in range(len(profiles.columns)):\n",
      "    print profiles.columns[i] + \":\"\n",
      "    for j in range(2):\n",
      "         print \"j: \" + str(centers[0][j][i])\n",
      "55/26:\n",
      "for i in range(len(profiles.columns)):\n",
      "    print profiles.columns[i] + \":\"\n",
      "    for j in range(2):\n",
      "         print str(j) + \": \" + str(centers[0][j][i])\n",
      "55/27:\n",
      "def comparefeatures(k, center):\n",
      "    for i in range(len(profiles.columns)):\n",
      "        print profiles.columns[i] + \":\"\n",
      "        for j in range(k):\n",
      "             print str(j) + \": \" + str(center[j][i])\n",
      "55/28: comparefeatures(2, centers[0])\n",
      "55/29: comparefeatures(3, centers[1])\n",
      "55/30: comparefeatures(4, centers[2])\n",
      "55/31:\n",
      "ranks = [['sex', 'orientation', 'speaks', 'drugs', 'drinks', 'smokes'], \n",
      "        ['age', 'status', 'not', 'income', 'body', 'height'], \\\n",
      "        ['education', 'ethnicity'], \\\n",
      "        ['pets', 'offspring', 'religion', 'sign', 'diet'], \\\n",
      "        ['essay']]\n",
      "55/32:\n",
      "profile_df = pd.read_csv('profiles2.csv')\n",
      "profile_df = profile_df.drop(profile_df.columns[0], axis=1)\n",
      "55/33: profile_df.head()\n",
      "55/34:\n",
      "toscale = ['age', 'height', 'income', 'last_online']\n",
      "profiles = deepcopy(profile_df)\n",
      "for i in range(len(toscale)):\n",
      "    scaler = MinMaxScaler(feature_range=(0, 1))\n",
      "    profiles[toscale[i]] = scaler.fit_transform(np.array(profiles[toscale[i]]).reshape(-1, 1))\n",
      "55/35: profiles.head()\n",
      "55/36:\n",
      "# most important colmns\n",
      "# rank1 = sex, orientation, languages(?), drugs, alcohol\n",
      "# rank2 = age, status, religion-importance, horoscope-importance, income, body, height\n",
      "# rank3 = education, ethnicity\n",
      "# rank4 = ['pets', 'offspring', 'religion', sign,\n",
      "# rank5 = ['essay']\n",
      "55/37:\n",
      "ranks = [['sex', 'orientation', 'speaks', 'drugs', 'drinks', 'smokes'], \n",
      "        ['age', 'status', 'not', 'income', 'body', 'height'], \\\n",
      "        ['education', 'ethnicity'], \\\n",
      "        ['pets', 'offspring', 'religion', 'sign', 'diet'], \\\n",
      "        ['essay']]\n",
      "55/38:\n",
      "for i in range(len(profiles.columns)):\n",
      "    category = profiles.columns[i].split('_')[0]\n",
      "    for j in range(len(ranks)):\n",
      "        if category in ranks[j]:\n",
      "            profiles[profiles.columns[i]] = profiles[profiles.columns[i]]*(2**(4-j))\n",
      "            continue\n",
      "55/39: profiles['religion_serious'] = profiles['religion_serious']*4\n",
      "55/40: profiles.head()\n",
      "55/41: profiles = profiles.fillna(profiles.mean().iloc[0])\n",
      "55/42:\n",
      "n = [2,3,4]\n",
      "kmL = []\n",
      "for i in range(len(n)):\n",
      "    km = KMeans(n_clusters = n[i])\n",
      "    kmL.append(km.fit(profiles))\n",
      "55/43: centers = [k.cluster_centers_ for k in kmL]\n",
      "55/44:\n",
      "def comparefeatures(k, center):\n",
      "    for i in range(len(profiles.columns)):\n",
      "        print profiles.columns[i] + \":\"\n",
      "        for j in range(k):\n",
      "             print str(j) + \": \" + str(center[j][i])\n",
      "55/45:\n",
      "def comparefeatures(k, centers):\n",
      "    featuredict = {}\n",
      "    for i in range(len(profiles.columns)):\n",
      "        f = profiles.columns[i]\n",
      "        fL = []\n",
      "        for j in range(k):\n",
      "            fL.append(centers[j][i])\n",
      "        featuredict[f] = (fL, np.std(fL))\n",
      "    featuredict = sorted(featuredict.items(), key=lambda kv: kv[1][1])\n",
      "    for k,v in featuredict:\n",
      "        print k + \": \", v[0], v[1]\n",
      "55/46: comparefeatures(2, centersL[0])\n",
      "55/47: centersL = [k.cluster_centers_ for k in kmL]\n",
      "55/48:\n",
      "def comparefeatures(k, centers):\n",
      "    featuredict = {}\n",
      "    for i in range(len(profiles.columns)):\n",
      "        f = profiles.columns[i]\n",
      "        fL = []\n",
      "        for j in range(k):\n",
      "            fL.append(centers[j][i])\n",
      "        featuredict[f] = (fL, np.std(fL))\n",
      "    featuredict = sorted(featuredict.items(), key=lambda kv: kv[1][1])\n",
      "    for k,v in featuredict:\n",
      "        print k + \": \", v[0], v[1]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "55/49: comparefeatures(2, centersL[0])\n",
      "55/50:\n",
      "def comparefeatures(k, centers):\n",
      "    featuredict = {}\n",
      "    for i in range(len(profiles.columns)):\n",
      "        f = profiles.columns[i]\n",
      "        fL = []\n",
      "        for j in range(k):\n",
      "            fL.append(centers[j][i])\n",
      "        featuredict[f] = (fL, np.std(fL))\n",
      "    featuredict = sorted(featuredict.items(), key=lambda kv: kv[1][1], reverse=True)\n",
      "    for k,v in featuredict:\n",
      "        print k + \": \", v[0], v[1]\n",
      "55/51: comparefeatures(2, centersL[0])\n",
      "55/52: comparefeatures(3, centersL[1])\n",
      "55/53: comparefeatures(4, centersL[2])\n",
      "55/54:\n",
      "for i in range(len(kmL)):\n",
      "    print kmL.inertia\n",
      "55/55:\n",
      "for i in range(len(kmL)):\n",
      "    print kmL[i].inertia\n",
      "55/56:\n",
      "for i in range(len(kmL)):\n",
      "    print kmL[i].inertia_\n",
      "55/57:\n",
      "n = [2,3,4,5,6,8,10]\n",
      "kmL = []\n",
      "for i in range(len(n)):\n",
      "    km = KMeans(n_clusters = n[i])\n",
      "    kmL.append(km.fit(profiles))\n",
      "55/58: centersL = [k.cluster_centers_ for k in kmL]\n",
      "55/59:\n",
      "for i in range(len(kmL)):\n",
      "    print kmL[i].inertia_\n",
      "55/60:\n",
      "for i in range(len(kmL)):\n",
      "    print kmL[i].inertia_\n",
      "plot.plot(n, [log(x.inertia_) for x in kmL], \"o\")\n",
      "55/61:\n",
      "for i in range(len(kmL)):\n",
      "    print kmL[i].inertia_\n",
      "plt.plot(n, [log(x.inertia_) for x in kmL], \"o\")\n",
      "55/62:\n",
      "for i in range(len(kmL)):\n",
      "    print kmL[i].inertia_\n",
      "plt.plot(n, [np.log(x.inertia_) for x in kmL], \"o\")\n",
      "55/63:\n",
      "for i in range(len(kmL)):\n",
      "    print kmL[i].inertia_\n",
      "plt.plot(n, [np.log(x.inertia_) for x in kmL], \"-o\")\n",
      "55/64:\n",
      "n = [i+1 for i in range(15)]\n",
      "kmL = []\n",
      "for i in range(len(n)):\n",
      "    km = KMeans(n_clusters = n[i])\n",
      "    kmL.append(km.fit(profiles))\n",
      "55/65: centersL = [k.cluster_centers_ for k in kmL]\n",
      "55/66:\n",
      "def comparefeatures(k, centers):\n",
      "    featuredict = {}\n",
      "    for i in range(len(profiles.columns)):\n",
      "        f = profiles.columns[i]\n",
      "        fL = []\n",
      "        for j in range(k):\n",
      "            fL.append(centers[j][i])\n",
      "        featuredict[f] = (fL, np.std(fL))\n",
      "    featuredict = sorted(featuredict.items(), key=lambda kv: kv[1][1], reverse=True)\n",
      "    for k,v in featuredict:\n",
      "        print k + \": \", v[0], v[1]\n",
      "55/67:\n",
      "for i in range(len(kmL)):\n",
      "    print kmL[i].inertia_\n",
      "plt.plot(n, [np.log(x.inertia_) for x in kmL], \"-o\")\n",
      "55/68:\n",
      "reference = np.random.rand(*profiles.shape)\n",
      "reference_inertia = []\n",
      "for k in range(1, 16):\n",
      "    clustering = KMeans()\n",
      "    local_inertia = []\n",
      "    for _ in range(5):\n",
      "        clustering.n_clusters = k\n",
      "        assignments = clustering.fit_predict(reference)\n",
      "        local_inertia.append(compute_inertia(assignments, reference))\n",
      "    reference_inertia.append(np.mean(local_inertia))\n",
      "55/69:\n",
      "reference = np.random.rand(*profiles.shape)\n",
      "reference_inertia = []\n",
      "for k in range(1, 16):\n",
      "    clustering = KMeans()\n",
      "    local_inertia = []\n",
      "    for _ in range(5):\n",
      "        clustering.n_clusters = k\n",
      "        assignments = clustering.fit_predict(reference)\n",
      "        local_inertia.append(clustering.intertia_)\n",
      "    reference_inertia.append(np.mean(local_inertia))\n",
      "55/70:\n",
      "reference = np.random.rand(*profiles.shape)\n",
      "reference_inertia = []\n",
      "for k in range(1, 16):\n",
      "    clustering = KMeans()\n",
      "    local_inertia = []\n",
      "    for _ in range(5):\n",
      "        clustering.n_clusters = k\n",
      "        assignments = clustering.fit_predict(reference)\n",
      "        local_inertia.append(clustering.inertia_)\n",
      "    reference_inertia.append(np.mean(local_inertia))\n",
      "55/71:\n",
      "reference = np.random.rand(*profiles.shape)\n",
      "reference_inertia = []\n",
      "for k in range(2, 16):\n",
      "    clustering = KMeans()\n",
      "    local_inertia = []\n",
      "    for _ in range(5):\n",
      "        clustering.n_clusters = k\n",
      "        assignments = clustering.fit_predict(reference)\n",
      "        local_inertia.append(clustering.inertia_)\n",
      "    reference_inertia.append(np.mean(local_inertia))\n",
      "55/72:\n",
      "reference = np.random.rand(*profiles.shape)\n",
      "reference_inertia = []\n",
      "for k in range(2, 16):\n",
      "    clustering = KMeans()\n",
      "    local_inertia = []\n",
      "    for _ in range(2):\n",
      "        clustering.n_clusters = k\n",
      "        assignments = clustering.fit_predict(reference)\n",
      "        local_inertia.append(clustering.inertia_)\n",
      "    reference_inertia.append(np.mean(local_inertia))\n",
      "55/73:\n",
      "reference = np.random.rand(*profiles.shape)\n",
      "reference_inertia = []\n",
      "for k in range(2, 16):\n",
      "    clustering = KMeans()\n",
      "    local_inertia = []\n",
      "    for _ in range(2):\n",
      "        clustering.n_clusters = k\n",
      "        assignments = clustering.fit_predict(reference)\n",
      "        local_inertia.append(clustering.inertia_)\n",
      "    m = np.mean(local_inertia)\n",
      "    print k, m\n",
      "    reference_inertia.append(m)\n",
      "55/74: plt.plot(n, [np.log(kmL[i-1].inertia_)-np.log(reference_inertia[i-1].inertia_) for i in n])\n",
      "55/75: plt.plot(n, [np.log(kmL[i-1].inertia_)-np.log(reference_inertia[i-1]) for i in n])\n",
      "55/76: plt.plot(n, [np.log(kmL[i-2].inertia_)-np.log(reference_inertia[i-2]) for i in n])\n",
      "55/77:\n",
      "n = [i+2 for i in range(15)]\n",
      "kmL = []\n",
      "for i in range(len(n)):\n",
      "    km = KMeans(n_clusters = n[i])\n",
      "    kmL.append(km.fit(profiles))\n",
      "55/78:\n",
      "n = [i+2 for i in range(14)]\n",
      "kmL = []\n",
      "for i in range(len(n)):\n",
      "    km = KMeans(n_clusters = n[i])\n",
      "    kmL.append(km.fit(profiles))\n",
      "55/79: centersL = [k.cluster_centers_ for k in kmL]\n",
      "55/80: comparefeatures(2, centersL[0])\n",
      "55/81: comparefeatures(3, centersL[1])\n",
      "55/82: comparefeatures(4, centersL[2])\n",
      "55/83:\n",
      "for i in range(len(kmL)):\n",
      "    print kmL[i].inertia_\n",
      "plt.plot(n, [np.log(x.inertia_) for x in kmL], \"-o\")\n",
      "55/84: plt.plot(n, [np.log(kmL[i-2].inertia_)-np.log(reference_inertia[i-2]) for i in n])\n",
      "55/85:\n",
      "toscale = ['age', 'height', 'income', 'last_online']\n",
      "profiles0 = deepcopy(profile_df)\n",
      "for i in range(len(toscale)):\n",
      "    scaler = MinMaxScaler(feature_range=(0, 1))\n",
      "    profiles0[toscale[i]] = scaler.fit_transform(np.array(profiles0[toscale[i]]).reshape(-1, 1))\n",
      "55/86: plt.plot(n, [np.log(reference_inertia[i-2]-np.log(kmL[i-2].inertia_)) for i in n])\n",
      "55/87: plt.plot(n, [np.log(reference_inertia[i-2]-np.log(kmL[i-2].inertia_)) for i in n], )\n",
      "55/88: plt.plot(n, [np.log(reference_inertia[i-2]-np.log(kmL[i-2].inertia_)) for i in n], \"-o\")\n",
      "55/89: pd.min(profiles0)\n",
      "55/90: np.min(profiles0)\n",
      "55/91: np.min(profiles0).min()\n",
      "55/92: np.max(profiles0).max()\n",
      "55/93: np.isnull().sum().sum()\n",
      "55/94: profiles0.isnull().sum().sum()\n",
      "55/95: np.max(profiles0.fillna(profiles.mean().iloc[0]))\n",
      "55/96: np.max(profiles0.fillna(profiles.mean(axis=1).iloc[0]))\n",
      "55/97: profiles0 = profiles0.fillna(profiles.mean(axis=1).iloc[0])\n",
      "55/98:\n",
      "toscale = ['age', 'height', 'income', 'last_online']\n",
      "profiles0 = deepcopy(profile_df)\n",
      "for i in range(len(toscale)):\n",
      "    scaler = MinMaxScaler(feature_range=(0, 1))\n",
      "    profiles0[toscale[i]] = scaler.fit_transform(np.array(profiles0[toscale[i]]).reshape(-1, 1))\n",
      "55/99: profiles0 = profiles0.fillna(profiles0.mean(axis=1).iloc[0])\n",
      "55/100:\n",
      "n = [i+2 for i in range(14)]\n",
      "kmL0 = []\n",
      "for i in range(len(n)):\n",
      "    km = KMeans(n_clusters = n[i])\n",
      "    kmL0.append(km.fit(profiles))\n",
      "55/101: centersL0 = [k.cluster_centers_ for k in kmL0]\n",
      "55/102:\n",
      "for i in range(len(kmL0)):\n",
      "    print kmL0[i].inertia_\n",
      "plt.plot(n, [np.log(x.inertia_) for x in kmL0], \"-o\")\n",
      "55/103:\n",
      "n = [i+2 for i in range(14)]\n",
      "kmL0 = []\n",
      "for i in range(len(n)):\n",
      "    km = KMeans(n_clusters = n[i])\n",
      "    kmL0.append(km.fit(profiles0))\n",
      "55/104: centersL0 = [k.cluster_centers_ for k in kmL0]\n",
      "55/105:\n",
      "for i in range(len(kmL0)):\n",
      "    print kmL0[i].inertia_\n",
      "plt.plot(n, [np.log(x.inertia_) for x in kmL0], \"-o\")\n",
      "55/106: plt.plot(n, [np.log(reference_inertia[i-2]-np.log(kmL[i-2].inertia_)) for i in n], \"-o\")\n",
      "55/107: plt.plot(n, [np.log(reference_inertia[i-2]-np.log(kmL0[i-2].inertia_)) for i in n], \"-o\")\n",
      "55/108: plt.plot(n, [np.log(reference_inertia[i-2]-np.log(kmL0[i-2].inertia_)) for i in n], \"-o\")\n",
      "55/109: kmL\n",
      "55/110: kmL.inertia_\n",
      "55/111: kmL[0].inertia_\n",
      "55/112: np.log(reference_inertia[0])-np.log(kmL[0].inertia_)\n",
      "55/113: np.log(reference_inertia[0])-np.log(kmL0[0].inertia_)\n",
      "55/114: plt.plot(n, [np.log(reference_inertia[i-2])-np.log(kmL[i-2].inertia_) for i in n], \"-o\")\n",
      "55/115: plt.plot(n, [np.log(reference_inertia[i-2])-np.log(kmL0[i-2].inertia_) for i in n], \"-o\")\n",
      "55/116:\n",
      "km20 = KMeans(n_clusters = 20)\n",
      "km20.inertia_\n",
      "55/117:\n",
      "km20 = KMeans(n_clusters = 20).fit(profiles0)\n",
      "km20.inertia_\n",
      "55/118:\n",
      "ref20 = KMeans(n_clusters = 20).fit(np.random.rand(*profiles0.shape))\n",
      "ref20.inertia_\n",
      "55/119:\n",
      "for i in range(len(kmL0)):\n",
      "    print kmL0[i].inertia_\n",
      "plt.plot(n.append(20), [np.log(x.inertia_) for x in kmL0].append(np.log(km20.inertia_)), \"-o\")\n",
      "55/120:\n",
      "for i in range(len(kmL0)):\n",
      "    print kmL0[i].inertia_\n",
      "plt.plot([n, 20], [np.log(x.inertia_) for x in kmL0, np.log(km20.inertia_)], \"-o\")\n",
      "55/121:\n",
      "for i in range(len(kmL0)):\n",
      "    print kmL0[i].inertia_\n",
      "plt.plot(n + [20], [np.log(x.inertia_) for x in kmL0] + [np.log(km20.inertia_)], \"-o\")\n",
      "55/122: n = [i+2 for i in range(14)]\n",
      "55/123:\n",
      "for i in range(len(kmL0)):\n",
      "    print kmL0[i].inertia_\n",
      "plt.plot(n + [20], [np.log(x.inertia_) for x in kmL0] + [np.log(km20.inertia_)], \"-o\")\n",
      "55/124: plt.plot(n+[20], [np.log(reference_inertia[i-2])-np.log(kmL0[i-2].inertia_) for i in n] + [np.log(ref20.inertia_)-np.log(km20.inertia_)], \"-o\")\n",
      "55/125:\n",
      "def getgap(k):\n",
      "    km = KMeans(n_clusters = k).fit(profiles0)\n",
      "    ref = KMeans(n_clusters = k).fit(np.random.rand(*profiles0.shape))\n",
      "    return np.log(ref.inertia_) - np.log(km.inertia_)\n",
      "55/126: getgap(30)\n",
      "55/127: g30 = getgap(30)\n",
      "55/128: g40 = getgap(40)\n",
      "55/129:\n",
      "for i in range(len(kmL0)):\n",
      "    print kmL0[i].inertia_\n",
      "plt.plot(n + [20, 30], [np.log(x.inertia_) for x in kmL0] + [np.log(km20.inertia_), g30], \"-o\")\n",
      "55/130: plt.plot(n+[20, 30], [np.log(reference_inertia[i-2])-np.log(kmL0[i-2].inertia_) for i in n] + [np.log(ref20.inertia_)-np.log(km20.inertia_), g30], \"-o\")\n",
      "55/131:\n",
      "for i in range(len(kmL0)):\n",
      "    print kmL0[i].inertia_\n",
      "plt.plot(n + [20], [np.log(x.inertia_) for x in kmL0] + [np.log(km20.inertia_)], \"-o\")\n",
      "56/1:\n",
      "import numpy as np\n",
      "import pandas as pd\n",
      "import matplotlib.pyplot as plt\n",
      "%matplotlib inline\n",
      "from copy import deepcopy\n",
      "from sklearn.preprocessing import MinMaxScaler\n",
      "from sklearn.cluster import KMeans\n",
      "import seaborn as sb\n",
      "56/2:\n",
      "profile_df = pd.read_csv('profiles2.csv')\n",
      "profile_df = profile_df.drop(profile_df.columns[0], axis=1)\n",
      "56/3: profile_df.head()\n",
      "56/4:\n",
      "toscale = ['age', 'height', 'income', 'last_online']\n",
      "profiles = deepcopy(profile_df)\n",
      "for i in range(len(toscale)):\n",
      "    scaler = MinMaxScaler(feature_range=(0, 1))\n",
      "    profiles[toscale[i]] = scaler.fit_transform(np.array(profiles[toscale[i]]).reshape(-1, 1))\n",
      "56/5: profiles = profiles.fillna(profiles.mean(axis=1).iloc[0])\n",
      "56/6: profiles.head()\n",
      "57/1:\n",
      "import numpy as np\n",
      "import pandas as pd\n",
      "import matplotlib.pyplot as plt\n",
      "%matplotlib inline\n",
      "from copy import deepcopy\n",
      "from sklearn.preprocessing import MinMaxScaler\n",
      "from sklearn.cluster import KMeans\n",
      "import seaborn as sb\n",
      "57/2:\n",
      "profile_df = pd.read_csv('profiles2.csv')\n",
      "profile_df = profile_df.drop(profile_df.columns[0], axis=1)\n",
      "57/3: profile_df.head()\n",
      "57/4: profiles['income'].replace(-1, profiles['income'].mean())\n",
      "57/5: profile_df['income'].replace(-1, profile_df['income'].mean())\n",
      "57/6: profile_df['income'].replace(-1, profile_df[profile_df['income'] != -1].mean())\n",
      "57/7: profile_df['income'].replace(-1, profile_df[profile_df['income'] != -1]['income'].mean())\n",
      "57/8: profile_df['age', 'income', 'job']\n",
      "57/9: profile_df['age', 'income']\n",
      "57/10: profile_df[['age', 'income']]\n",
      "57/11: profile_df[['age', 'income', 'job']]\n",
      "57/12: hasincome = profile_df[profile_df['income'] != -1]\n",
      "57/13:\n",
      "hasincome = profile_df[profile_df['income'] != -1]\n",
      "hasincome.head()\n",
      "57/14: np.corr(hasincome['income'], hasincome['age'])\n",
      "57/15: np.correlate(hasincome['income'], hasincome['age'])\n",
      "57/16: hasincome[['income', 'age']].corr()\n",
      "57/17: hasincome[['income'] + [i for i in hasincome.columns if \"job\" in i]].corr()\n",
      "57/18: np.mean(hasincome['income'])\n",
      "57/19: np.mean(hasincome['income']), np.median(hasincome['income'])\n",
      "57/20: profile_df['income'].replace(-1, profile_df[profile_df['income'] != -1]['income'].meduan())\n",
      "57/21: profile_df['income'].replace(-1, hasincome['income'].median())\n",
      "57/22: profile_df2 = profile_df['income'].replace(-1, hasincome['income'].median())\n",
      "57/23:\n",
      "profile_df2 = profile_df['income'].replace(-1, hasincome['income'].median())\n",
      "profile_df2.head()\n",
      "57/24: profile_df.head()\n",
      "57/25:\n",
      "toscale = ['age', 'height', 'income', 'last_online']\n",
      "profiles = deepcopy(profile_df)\n",
      "for i in range(len(toscale)):\n",
      "    scaler = MinMaxScaler(feature_range=(0, 1))\n",
      "    profiles[toscale[i]] = scaler.fit_transform(np.array(profiles[toscale[i]]).reshape(-1, 1))\n",
      "57/26:\n",
      "toscale = ['age', 'height', 'income', 'last_online']\n",
      "profiles = deepcopy(profile_df2)\n",
      "for i in range(len(toscale)):\n",
      "    scaler = MinMaxScaler(feature_range=(0, 1))\n",
      "    profiles[toscale[i]] = scaler.fit_transform(np.array(profiles[toscale[i]]).reshape(-1, 1))\n",
      "57/27:\n",
      "profile_df2 = deepcopy(profile_df)\n",
      "profile_df2 = profile_df2['income'].replace(-1, hasincome['income'].median())\n",
      "profile_df2.head()\n",
      "57/28:\n",
      "profile_df2 = deepcopy(profile_df)\n",
      "profile_df2['income'] = profile_df2['income'].replace(-1, hasincome['income'].median())\n",
      "profile_df2.head()\n",
      "57/29:\n",
      "toscale = ['age', 'height', 'income', 'last_online']\n",
      "profiles = deepcopy(profile_df2)\n",
      "for i in range(len(toscale)):\n",
      "    scaler = MinMaxScaler(feature_range=(0, 1))\n",
      "    profiles[toscale[i]] = scaler.fit_transform(np.array(profiles[toscale[i]]).reshape(-1, 1))\n",
      "57/30: profiles = profiles.isnull().sum().sum()\n",
      "57/31: profiles.isnull().sum().sum()\n",
      "57/32:\n",
      "profile_df2 = deepcopy(profile_df)\n",
      "profile_df2['income'] = profile_df2['income'].replace(-1, hasincome['income'].median())\n",
      "profile_df2.head()\n",
      "57/33:\n",
      "toscale = ['age', 'height', 'income', 'last_online']\n",
      "profiles = deepcopy(profile_df2)\n",
      "for i in range(len(toscale)):\n",
      "    scaler = MinMaxScaler(feature_range=(0, 1))\n",
      "    profiles[toscale[i]] = scaler.fit_transform(np.array(profiles[toscale[i]]).reshape(-1, 1))\n",
      "57/34: profiles.isnull().sum().sum()\n",
      "57/35: profiles = profiles.fillna(profiles.mean(axis=1).iloc[0])\n",
      "57/36: profiles.isnull().sum().sum()\n",
      "57/37:\n",
      "def comparefeatures(k, centers):\n",
      "    featuredict = {}\n",
      "    for i in range(len(profiles.columns)):\n",
      "        f = profiles.columns[i]\n",
      "        fL = []\n",
      "        for j in range(k):\n",
      "            fL.append(centers[j][i])\n",
      "        featuredict[f] = (fL, np.std(fL))\n",
      "    featuredict = sorted(featuredict.items(), key=lambda kv: kv[1][1], reverse=True)\n",
      "    for k,v in featuredict:\n",
      "        print k + \": \", v[0], v[1]\n",
      "57/38:\n",
      "n = [2*i for i in range(1, 5)]\n",
      "kmL = []\n",
      "for i in range(len(n)):\n",
      "    km = KMeans(n_clusters = n[i])\n",
      "    kmL.append(km.fit(profiles))\n",
      "57/39: centersL = [k.cluster_centers_ for k in kmL]\n",
      "57/40: plot.plot(n, [k.inertia_ for k in kmL])\n",
      "57/41: plt.plot(n, [k.inertia_ for k in kmL])\n",
      "57/42:\n",
      "n = [2*i for i in range(1, 7)]\n",
      "kmL = []\n",
      "for i in range(len(n)):\n",
      "    km = KMeans(n_clusters = n[i])\n",
      "    kmL.append(km.fit(profiles))\n",
      "57/43: centersL = [k.cluster_centers_ for k in kmL]\n",
      "57/44: plt.plot(n, [k.inertia_ for k in kmL])\n",
      "57/45: comparefeatures(2, centersL[0])\n",
      "57/46: comparefeatures(4, centersL[1])\n",
      "57/47: comparefeatures(6, centersL[2])\n",
      "57/48:\n",
      "n = [2*i for i in range(1, 10)]\n",
      "kmL = []\n",
      "for i in range(len(n)):\n",
      "    km = KMeans(n_clusters = n[i])\n",
      "    kmL.append(km.fit(profiles))\n",
      "57/49:\n",
      "centersL = [k.cluster_centers_ for k in kmL]\n",
      "plt.plot(n, [k.inertia_ for k in kmL])\n",
      "57/50:\n",
      "centersL = [k.cluster_centers_ for k in kmL]\n",
      "plt.plot(n, [k.inertia_ for k in kmL], \"-o\")\n",
      "57/51:\n",
      "ref18 = KMeans(n_clusters = 18).fit(np.random.rand(*profiles.shape))\n",
      "ref18.inertia_\n",
      "57/52:\n",
      "profiles1 = deepcopy(profiles)\n",
      "for i in range(len(profiles1.columns)):\n",
      "    category = profiles1.columns[i].split('_')[0]\n",
      "    for j in range(len(ranks)):\n",
      "        if category in ranks[j]:\n",
      "            profiles1[profiles1.columns[i]] = profiles1[profiles1.columns[i]]*(2**(4-j))\n",
      "            continue\n",
      "57/53: refs = [1395751.4649308836, 1391175.2449794328,1388048.6996975695,1385661.1282712892,1383690.338041902,1381997.52775248,1380555.684706425,1377665.1144632318]\n",
      "57/54: plot.plot(n, [np.log(refs[i])-np.log(kmL[i].inertia_) for i in range(len(n))])\n",
      "57/55: plt.plot(n, [np.log(refs[i])-np.log(kmL[i].inertia_) for i in range(len(n))])\n",
      "57/56: len(n), len(refs), len(kmL)\n",
      "57/57:\n",
      "ref16 = KMeans(n_clusters = 16).fit(np.random.rand(*profiles.shape))\n",
      "ref16.inertia_\n",
      "57/58: refs = [1395751.4649308836, 1391175.2449794328,1388048.6996975695,1385661.1282712892,1383690.338041902,1381997.52775248,1380555.684706425,1378236.5336240623,1377665.1144632318]\n",
      "57/59: len(n), len(refs), len(kmL)\n",
      "57/60: plt.plot(n, [np.log(refs[i])-np.log(kmL[i].inertia_) for i in range(len(n))])\n",
      "57/61: refs2 = [1395751.4649308836-2000*x for x in range(len(n))]\n",
      "57/62: plt.plot(n, [np.log(refs2[i])-np.log(kmL[i].inertia_) for i in range(len(n))])\n",
      "57/63: refs2 = [1395751.4649308836-20000*x for x in range(len(n))]\n",
      "57/64: plt.plot(n, [np.log(refs2[i])-np.log(kmL[i].inertia_) for i in range(len(n))])\n",
      "57/65: refs2 = [1395751.4649308836-10000*x for x in range(len(n))]\n",
      "57/66: plt.plot(n, [np.log(refs2[i])-np.log(kmL[i].inertia_) for i in range(len(n))])\n",
      "57/67: plt.plot(n, [np.log(refs[i])-np.log(kmL[i].inertia_) for i in range(len(n))])\n",
      "57/68:\n",
      "ranks = [['sex', 'orientation', 'speaks', 'drugs', 'drinks', 'smokes'], \n",
      "        ['age', 'status', 'not', 'income', 'body', 'height'], \\\n",
      "        ['education', 'ethnicity'], \\\n",
      "        ['pets', 'offspring', 'religion', 'sign', 'diet'], \\\n",
      "        ['essay']]\n",
      "57/69:\n",
      "profiles1 = deepcopy(profiles)\n",
      "for i in range(len(profiles1.columns)):\n",
      "    category = profiles1.columns[i].split('_')[0]\n",
      "    for j in range(len(ranks)):\n",
      "        if category in ranks[j]:\n",
      "            profiles1[profiles1.columns[i]] = profiles1[profiles1.columns[i]]*(2**(4-j))\n",
      "            continue\n",
      "57/70: profiles1['religion_serious'] = profiles1['religion_serious']*4\n",
      "57/71: profiles1.head()\n",
      "57/72: hasincome[['income'] + [x for x in hasincome.columns if \"job\" in x]].corr()\n",
      "57/73:\n",
      "import seaborn as sns\n",
      "def makeViolin(col):\n",
      "    columns = list(filter(lambda c: c.startswith(col), list(profiles)))\n",
      "    count = len(columns)\n",
      "    if count == 0:\n",
      "        raise Exception('no columns!')\n",
      "    if count > 5:\n",
      "        if count%5==0:\n",
      "            fig, axs = plt.subplots(ncols=5,nrows=int(count/5), figsize = (200, 200))\n",
      "        else:\n",
      "            fig, axs = plt.subplots(ncols=5,nrows=int(count/5)+1, figsize = (200, 200))\n",
      "    else:\n",
      "        fig, axs = plt.subplots(ncols=count)      \n",
      "    \n",
      "    if count == 1:\n",
      "        for c in columns:\n",
      "            sns.violinplot(x=kmL[1].labels_, y=c, data=profiles)\n",
      "            return\n",
      "    print(axs.shape)\n",
      "    if count <=5:\n",
      "        tempCount = 0\n",
      "        for c in columns:\n",
      "            sns.violinplot(x=kmL[1].labels_, y=c, data=profiles)  \n",
      "            with sns.axes_style(\"white\"):\n",
      "                sns.violinplot(x=kmL[1].labels_, y=c, data=profiles, ax=axs[tempCount])\n",
      "                tempCount+=1\n",
      "    else:\n",
      "        tempCount = 0\n",
      "        for c in columns:    \n",
      "            with sns.axes_style(\"white\"):\n",
      "                sns.violinplot(x=kmL[1].labels_, y=c, data=profiles, ax=axs[int(tempCount/5), tempCount%5])\n",
      "                tempCount+=1\n",
      "57/74: makeViolin(\"sex\")\n",
      "57/75: makeViolin(\"income\")\n",
      "57/76:\n",
      "import seaborn as sns\n",
      "def makeViolin(col, k):\n",
      "    columns = list(filter(lambda c: c.startswith(col), list(profiles)))\n",
      "    count = len(columns)\n",
      "    if count == 0:\n",
      "        raise Exception('no columns!')\n",
      "    if count > 5:\n",
      "        if count%5==0:\n",
      "            fig, axs = plt.subplots(ncols=5,nrows=int(count/5), figsize = (200, 200))\n",
      "        else:\n",
      "            fig, axs = plt.subplots(ncols=5,nrows=int(count/5)+1, figsize = (200, 200))\n",
      "    else:\n",
      "        fig, axs = plt.subplots(ncols=count)      \n",
      "    \n",
      "    if count == 1:\n",
      "        for c in columns:\n",
      "            sns.violinplot(x=kmL[k].labels_, y=c, data=profiles)\n",
      "            return\n",
      "    print(axs.shape)\n",
      "    if count <=5:\n",
      "        tempCount = 0\n",
      "        for c in columns:\n",
      "            sns.violinplot(x=kmL[k].labels_, y=c, data=profiles)  \n",
      "            with sns.axes_style(\"white\"):\n",
      "                sns.violinplot(x=kmL[k].labels_, y=c, data=profiles, ax=axs[tempCount])\n",
      "                tempCount+=1\n",
      "    else:\n",
      "        tempCount = 0\n",
      "        for c in columns:    \n",
      "            with sns.axes_style(\"white\"):\n",
      "                sns.violinplot(x=kmL[k].labels_, y=c, data=profiles, ax=axs[int(tempCount/5), tempCount%5])\n",
      "                tempCount+=1\n",
      "57/77: makeViolin(\"income\", 2)\n",
      "57/78: makeViolin(\"income\", 4)\n",
      "57/79: makeViolin(\"income\", 3)\n",
      "59/1:\n",
      "import numpy as np\n",
      "import pandas as pd\n",
      "import matplotlib.pyplot as plt\n",
      "%matplotlib inline\n",
      "from copy import deepcopy\n",
      "from sklearn.preprocessing import MinMaxScaler\n",
      "from sklearn.cluster import KMeans\n",
      "import seaborn as sb\n",
      "59/2:\n",
      "profile_df = pd.read_csv('profiles2.csv')\n",
      "profile_df = profile_df.drop(profile_df.columns[0], axis=1)\n",
      "59/3: profile_df.head()\n",
      "59/4:\n",
      "hasincome = profile_df[profile_df['income'] != -1]\n",
      "hasincome.head()\n",
      "59/5: hasincome[['income'] + [x for x in hasincome.columns if \"job\" in x]].corr()\n",
      "59/6: np.mean(hasincome['income']), np.median(hasincome['income'])\n",
      "59/7:\n",
      "profile_df2 = deepcopy(profile_df)\n",
      "profile_df2['income'] = profile_df2['income'].replace(-1, hasincome['income'].median())\n",
      "profile_df2.head()\n",
      "59/8:\n",
      "toscale = ['age', 'height', 'income', 'last_online']\n",
      "profiles = deepcopy(profile_df2)\n",
      "for i in range(len(toscale)):\n",
      "    scaler = MinMaxScaler(feature_range=(0, 1))\n",
      "    profiles[toscale[i]] = scaler.fit_transform(np.array(profiles[toscale[i]]).reshape(-1, 1))\n",
      "59/9: profiles = profiles.fillna(profiles.mean(axis=1).iloc[0])\n",
      "59/10: profiles.head()\n",
      "59/11:\n",
      "# most important colmns\n",
      "# rank1 = sex, orientation, languages(?), drugs, alcohol\n",
      "# rank2 = age, status, religion-importance, horoscope-importance, income, body, height\n",
      "# rank3 = education, ethnicity\n",
      "# rank4 = ['pets', 'offspring', 'religion', sign,\n",
      "# rank5 = ['essay']\n",
      "59/12:\n",
      "ranks = [['sex', 'orientation', 'speaks', 'drugs', 'drinks', 'smokes'], \n",
      "        ['age', 'status', 'not', 'income', 'body', 'height'], \\\n",
      "        ['education', 'ethnicity'], \\\n",
      "        ['pets', 'offspring', 'religion', 'sign', 'diet'], \\\n",
      "        ['essay']]\n",
      "59/13:\n",
      "profiles1 = deepcopy(profiles)\n",
      "for i in range(len(profiles1.columns)):\n",
      "    category = profiles1.columns[i].split('_')[0]\n",
      "    for j in range(len(ranks)):\n",
      "        if category in ranks[j]:\n",
      "            profiles1[profiles1.columns[i]] = profiles1[profiles1.columns[i]]*(2**(4-j))\n",
      "            continue\n",
      "59/14: profiles1['religion_serious'] = profiles1['religion_serious']*4\n",
      "59/15: profiles1.head()\n",
      "59/16:\n",
      "def comparefeatures(k, centers):\n",
      "    featuredict = {}\n",
      "    for i in range(len(profiles.columns)):\n",
      "        f = profiles.columns[i]\n",
      "        fL = []\n",
      "        for j in range(k):\n",
      "            fL.append(centers[j][i])\n",
      "        featuredict[f] = (fL, np.std(fL))\n",
      "    featuredict = sorted(featuredict.items(), key=lambda kv: kv[1][1], reverse=True)\n",
      "    for k,v in featuredict:\n",
      "        print k + \": \", v[0], v[1]\n",
      "59/17: profiles.head()\n",
      "59/18: profiles.isnull().sum().sum()\n",
      "59/19: profiles.max().max()\n",
      "59/20: profiles.min().min()\n",
      "59/21:\n",
      "%%time\n",
      "\n",
      "from sklearn.decomposition import LatentDirichletAllocation\n",
      "\n",
      "lda = LatentDirichletAllocation(n_components = 5)\n",
      "\n",
      "lda.fit(profiles.values)\n",
      "59/22:\n",
      "print(\"Component shape:\", lda.components_.shape)\n",
      "\n",
      "transformed = lda.transform(profiles.values)\n",
      "\n",
      "print(\"User profiles: \" + str(transformed.shape))\n",
      "59/23:\n",
      "plt.figure()\n",
      "for i in range(5):\n",
      "    plt.hist(transformed[:, i], alpha=0.3, label=str(i+1),\n",
      "            range=(0,1), bins=20)\n",
      "plt.xlabel(\"User Weight in Component i\", fontsize=20)\n",
      "plt.ylabel(\"Count\", fontsize=20)\n",
      "plt.tick_params(labelsize=15)\n",
      "plt.legend()\n",
      "plt.show()\n",
      "59/24:\n",
      "c1 = lda.components_[0, :]\n",
      "print c1.shape\n",
      "\n",
      "print \"Percentiles of the components:\"\n",
      "print np.percentile(c1, np.arange(0,100,1))\n",
      "\n",
      "plt.figure()\n",
      "plt.hist(c1)\n",
      "plt.xlabel(\"Feature Weight in Component 1\", fontsize=20)\n",
      "plt.ylabel(\"Frequency\", fontsize=20)\n",
      "plt.tick_params(labelsize=15)\n",
      "plt.show()\n",
      "59/25:\n",
      "c1_indices = np.where(c1 > np.percentile(c1, 99))[0]\n",
      "\n",
      "top_c1_df = pd.DataFrame({'Component': profiles.columns.values[c1_indices],\n",
      "                         'Weight': c1[c1_indices]})\n",
      "\n",
      "top_c1_df.sort_values('Weight', inplace=True, ascending=False)\n",
      "\n",
      "top_c1_df.head(n=10)\n",
      "61/1:\n",
      "#author Tianju Xue, 04/03/2019\n",
      "import pandas as pd\n",
      "import matplotlib.pyplot as plt\n",
      "import numpy as np\n",
      "import math\n",
      "61/2:\n",
      "rng = np.random.RandomState(1)\n",
      "X = np.dot(rng.rand(2, 2), rng.randn(2, 200)).T\n",
      "plt.scatter(X[:, 0], X[:, 1])\n",
      "plt.axis('equal');\n",
      "61/3:\n",
      "from sklearn.decomposition import PCA\n",
      "pca = PCA(n_components=2)\n",
      "pca.fit(X)\n",
      "61/4: print(pca.components_)\n",
      "61/5:\n",
      "def draw_vector(v0, v1, ax=None):\n",
      "    ax = ax or plt.gca()\n",
      "    arrowprops=dict(arrowstyle='->',\n",
      "                    linewidth=2,\n",
      "                    shrinkA=0, shrinkB=0)\n",
      "    ax.annotate('', v1, v0, arrowprops=arrowprops)\n",
      "\n",
      "# plot data\n",
      "plt.scatter(X[:, 0], X[:, 1], alpha=0.2)\n",
      "for length, vector in zip(pca.explained_variance_, pca.components_):\n",
      "    v = vector * 3 * np.sqrt(length)\n",
      "    draw_vector(pca.mean_, pca.mean_ + v)\n",
      "plt.axis('equal');\n",
      "61/6:\n",
      "pca = PCA(n_components=1)\n",
      "pca.fit(X)\n",
      "X_pca = pca.transform(X)\n",
      "print(\"original shape (N, D):   \", X.shape)\n",
      "print(\"transformed shape (N, K):\", X_pca.shape)\n",
      "61/7:\n",
      "X_new = pca.inverse_transform(X_pca)\n",
      "plt.scatter(X[:, 0], X[:, 1], alpha=0.2)\n",
      "plt.scatter(X_new[:, 0], X_new[:, 1], alpha=0.8)\n",
      "plt.legend([r'$\\mathbf{x_n}$', r'$\\widehat{\\mathbf{x_n}}$'])\n",
      "plt.axis('equal');\n",
      "61/8:\n",
      "# Implementation of key step in gradient descent algorithm\n",
      "def update_weights(m, b, X, Y, learning_rate):\n",
      "    m_deriv = 0\n",
      "    b_deriv = 0\n",
      "    N = len(X)\n",
      "    for i in range(N):\n",
      "        # Calculate partial derivatives\n",
      "        # -2x(y - (mx + b))\n",
      "        m_deriv += -2*X[i] * (Y[i] - (m*X[i] + b))\n",
      "\n",
      "        # -2(y - (mx + b))\n",
      "        b_deriv += -2*(Y[i] - (m*X[i] + b))\n",
      "\n",
      "    # We subtract because the derivatives point in direction of steepest ascent\n",
      "    m -= (m_deriv / float(N)) * learning_rate\n",
      "    b -= (b_deriv / float(N)) * learning_rate\n",
      "\n",
      "    return m, b\n",
      "61/9: from hmmlearn import hmm\n",
      "59/26: from sklearn import mixture\n",
      "59/27:\n",
      "lowest_bic = np.infty\n",
      "bic = []\n",
      "n_components_range = range(1, 7)\n",
      "cv_types = ['spherical', 'tied', 'diag', 'full']\n",
      "for cv_type in cv_types:\n",
      "    for n_components in n_components_range:\n",
      "        # Fit a Gaussian mixture with EM\n",
      "        gmm = mixture.GaussianMixture(n_components=n_components,\n",
      "                                      covariance_type=cv_type)\n",
      "        gmm.fit(profiles)\n",
      "        bic.append(gmm.bic(profiles))\n",
      "        if bic[-1] < lowest_bic:\n",
      "            lowest_bic = bic[-1]\n",
      "            best_gmm = gmm\n",
      "\n",
      "bic = np.array(bic)\n",
      "color_iter = itertools.cycle(['navy', 'turquoise', 'cornflowerblue',\n",
      "                              'darkorange'])\n",
      "clf = best_gmm\n",
      "bars = []\n",
      "\n",
      "# Plot the BIC scores\n",
      "plt.figure(figsize=(8, 6))\n",
      "spl = plt.subplot(2, 1, 1)\n",
      "for i, (cv_type, color) in enumerate(zip(cv_types, color_iter)):\n",
      "    xpos = np.array(n_components_range) + .2 * (i - 2)\n",
      "    bars.append(plt.bar(xpos, bic[i * len(n_components_range):\n",
      "                                  (i + 1) * len(n_components_range)],\n",
      "                        width=.2, color=color))\n",
      "plt.xticks(n_components_range)\n",
      "plt.ylim([bic.min() * 1.01 - .01 * bic.max(), bic.max()])\n",
      "plt.title('BIC score per model')\n",
      "xpos = np.mod(bic.argmin(), len(n_components_range)) + .65 +\\\n",
      "    .2 * np.floor(bic.argmin() / len(n_components_range))\n",
      "plt.text(xpos, bic.min() * 0.97 + .03 * bic.max(), '*', fontsize=14)\n",
      "spl.set_xlabel('Number of components')\n",
      "spl.legend([b[0] for b in bars], cv_types)\n",
      "59/28:\n",
      "gmm = mixture.GaussianMixture(n_components=5, covariance_type='spherical')\n",
      "gmm.fit(profiles)\n",
      "59/29:\n",
      "for topic_idx, topic in enumerate(gmm.components_):\n",
      "        message = \"Topic #%d: \" % topic_idx\n",
      "        message += \" \".join([profiles.columns[i]\n",
      "                             for i in topic.argsort()[:-10 - 1:-1]])\n",
      "        print(message)\n",
      "59/30:\n",
      "from sklearn.decomposition import PCA\n",
      "pca = PCA(n_components=1)\n",
      "pca.fit(X)\n",
      "X_pca = pca.transform(X)\n",
      "59/31:\n",
      "from sklearn.decomposition import PCA\n",
      "pca = PCA(n_components=1)\n",
      "pca.fit(profiles)\n",
      "X_pca = pca.transform(profiles)\n",
      "59/32:\n",
      "for topic_idx, topic in enumerate(gmm.components_):\n",
      "        message = \"Topic #%d: \" % topic_idx\n",
      "        message += \" \".join([profiles.columns[i]\n",
      "                             for i in topic.argsort()[:-10 - 1:-1]])\n",
      "        print(message)\n",
      "59/33:\n",
      "for topic_idx, topic in enumerate(lda.components_):\n",
      "        message = \"Topic #%d: \" % topic_idx\n",
      "        message += \" \".join([profiles.columns[i]\n",
      "                             for i in topic.argsort()[:-10 - 1:-1]])\n",
      "        print(message)\n",
      "59/34:\n",
      "for topic_idx, topic in enumerate(lda.components_):\n",
      "        message = \"Topic #%d: \" % topic_idx\n",
      "        message += \"\\n\".join([profiles.columns[i]\n",
      "                             for i in topic.argsort()[:-10 - 1:-1]])\n",
      "        print(message)\n",
      "59/35:\n",
      "for topic_idx, topic in enumerate(lda.components_):\n",
      "        message = \"Topic #%d: \" % topic_idx\n",
      "        message += \"\\n\".join([profiles.columns[i]\n",
      "                             for i in topic.argsort()[:-10 - 1:-1]])\n",
      "        print(message)\n",
      "        print()\n",
      "59/36:\n",
      "for topic_idx, topic in enumerate(lda.components_):\n",
      "        message = \"Topic #%d: \" % topic_idx\n",
      "        message += \"\\n\".join([profiles.columns[i]\n",
      "                             for i in topic.argsort()[:-10]])\n",
      "        print(message)\n",
      "        print()\n",
      "59/37:\n",
      "for topic_idx, topic in enumerate(lda.components_):\n",
      "        message = \"Topic #%d: \" % topic_idx\n",
      "        message += \"\\n\".join([profiles.columns[i]\n",
      "                             for i in topic.argsort()[:-10 - 1:-1]])\n",
      "        print(message)\n",
      "        print()\n",
      "59/38:\n",
      "for topic_idx, topic in enumerate(lda.components_):\n",
      "        message = \"Topic #%d: \" % topic_idx\n",
      "        message += \"\\n\".join([profiles.columns[i]\n",
      "                             for i in topic.argsort()[:-10:-1]])\n",
      "        print(message)\n",
      "        print()\n",
      "59/39:\n",
      "for topic_idx, topic in enumerate(lda.components_):\n",
      "        message = \"Topic #%d: \" % topic_idx\n",
      "        message += \"\\n\".join([profiles.columns[i]\n",
      "                             for i in topic.argsort()[:-10 - 1:-1]])\n",
      "        print(message)\n",
      "        print()\n",
      "59/40:\n",
      "for topic_idx, topic in enumerate(lda.components_):\n",
      "        message = \"Topic #%d: \" % topic_idx\n",
      "        message += \"\\n\".join([profiles.columns[i]\n",
      "                             for i in topic.argsort()[:-10:-1]])\n",
      "        print(message)\n",
      "        print()\n",
      "59/41:\n",
      "for topic_idx, topic in enumerate(lda.components_):\n",
      "        message = \"Topic #%d: \" % topic_idx\n",
      "        message += \"\\n\".join([profiles.columns[i]\n",
      "                             for i in topic.argsort()[:-5:-1]])\n",
      "        print(message)\n",
      "        print()\n",
      "59/42:\n",
      "for topic_idx, topic in enumerate(lda.components_):\n",
      "        message = \"Topic #%d: \" % topic_idx\n",
      "        message += \"\\n\".join([profiles.columns[i]\n",
      "                             for i in topic.argsort()[:-5]])\n",
      "        print(message)\n",
      "        print()\n",
      "59/43:\n",
      "for topic_idx, topic in enumerate(lda.components_):\n",
      "        message = \"Topic #%d: \" % topic_idx\n",
      "        message += \"\\n\".join([profiles.columns[i]\n",
      "                             for i in topic.argsort()[:-10:-1]])\n",
      "        print(message)\n",
      "        print()\n",
      "59/44:\n",
      "for topic_idx, topic in enumerate(lda.components_):\n",
      "        message = \"Topic #%d: \" % topic_idx\n",
      "        print topic.shape\n",
      "        message += \"\\n\".join([profiles.columns[i]\n",
      "                             for i in topic.argsort()[:-10:-1]])\n",
      "        print(message)\n",
      "        print()\n",
      "59/45:\n",
      "for topic_idx, topic in enumerate(lda.components_):\n",
      "        message = \"Topic #%d: \" % topic_idx\n",
      "        print topic[:,]\n",
      "        message += \"\\n\".join([profiles.columns[i]\n",
      "                             for i in topic.argsort()[:-10:-1]])\n",
      "        print(message)\n",
      "        print()\n",
      "59/46:\n",
      "for topic_idx, topic in enumerate(lda.components_):\n",
      "        message = \"Topic #%d: \" % topic_idx\n",
      "        print topic[0,]\n",
      "        message += \"\\n\".join([profiles.columns[i]\n",
      "                             for i in topic.argsort()[:-10:-1]])\n",
      "        print(message)\n",
      "        print()\n",
      "59/47:\n",
      "for topic_idx, topic in enumerate(lda.components_):\n",
      "        message = \"Topic #%d: \" % topic_idx\n",
      "        message += \"\\n\".join([profiles.columns[i]\n",
      "                             for i in topic.argsort()[:-10:-1]])\n",
      "        print(message)\n",
      "        print()\n",
      "59/48: profiles.corr()\n",
      "59/49: corr = profiles.corr()\n",
      "59/50: np.argmax(corr)\n",
      "59/51: corr.shape()\n",
      "59/52: corr\n",
      "59/53: from sklearn.linear_model import LinearRegression\n",
      "59/54: lr = LinearRegression()\n",
      "59/55:\n",
      "lr = LinearRegression()\n",
      "lr.fit(profiles.drop([\"sex_f\", \"sex_m\"]), profiles[])\n",
      "59/56:\n",
      "lr = LinearRegression()\n",
      "lr.fit(profiles.drop([\"sex_f\", \"sex_m\"]), profiles[\"sex_F\"])\n",
      "59/57:\n",
      "lr = LinearRegression()\n",
      "lr.fit(profiles.drop([\"sex_f\", \"sex_m\"]), profiles[\"sex_f\"])\n",
      "59/58:\n",
      "lr = LinearRegression()\n",
      "lr.fit(profiles.drop([\"sex_f\", \"sex_m\"], axis=1), profiles[\"sex_f\"])\n",
      "59/59: lr.score()\n",
      "59/60: lr.score(profiles.drop([\"sex_f\", \"sex_m\"], axis=1), profiles[\"sex_f\"])\n",
      "59/61:\n",
      "for topic_idx, topic in enumerate(lda.components_):\n",
      "        message = \"Topic #%d: \" % topic_idx\n",
      "        message += \"\\n\".join([profiles.columns[i]\n",
      "                             for i in topic.argsort()[:-10:-1]])\n",
      "        print(message)\n",
      "        print()\n",
      "59/62:\n",
      "plt.figure()\n",
      "for i in range(5):\n",
      "    plt.hist(transformed[:, i], alpha=0.3, label=str(i+1),\n",
      "            range=(0,1), bins=20)\n",
      "    plt.show()\n",
      "plt.xlabel(\"User Weight in Component i\", fontsize=20)\n",
      "plt.ylabel(\"Count\", fontsize=20)\n",
      "plt.tick_params(labelsize=15)\n",
      "plt.legend()\n",
      "plt.show()\n",
      "59/63: corr\n",
      "59/64: profiles[['height'] + [x for x in profiles.columns if 'speaks_english' in x]]\n",
      "59/65: profiles[['height'] + [x for x in profiles.columns if 'speaks_english' in x]].corr()\n",
      "59/66: profiles[['height'] + [x for x in profiles.columns if 'speaks' in x]].corr()\n",
      "59/67: profiles[['sex_f', 'sex_m'] + ['height']].corr()\n",
      "59/68: profiles[['sex_f'] + ['height', 'not_serious']].corr()\n",
      "59/69: profiles[['sex_f','height'] + [x for x in profiles.columns if 'sign' in x]].corr()\n",
      "59/70: profiles[['sex_f','height', 'sign_not_serious']].corr()\n",
      "59/71: profiles[['sex_f','height', 'sign_not_serious', 'religion_serious']].corr()\n",
      "59/72: profiles[['sex_f','height', 'sign_not_serious', 'religion_serious', 'education_college']].corr()\n",
      "59/73: profiles[['sex_f','height', 'sign_not_serious', 'religion_serious', 'education_college', 'job_education / academia']].corr()\n",
      "59/74: profiles[['sex_f'] + [x for x in profiles.columns if 'job' in x]].corr()\n",
      "59/75: profiles[['sex_f', 'age'] + [x for x in profiles.columns if 'offspring' in x]].corr()\n",
      "59/76: profiles[['sex_f', 'age'] + [x for x in profiles.columns if 'pets' in x]].corr()\n",
      "59/77: profiles[['sex_f', 'age'] + [x for x in profiles.columns if 'body_type' in x]].corr()\n",
      "59/78: np.where(corr > .1)\n",
      "59/79: np.where(corr > .5)\n",
      "59/80: np.where(corr > .5, corr)\n",
      "59/81: np.where(corr > .5)\n",
      "59/82:\n",
      "xs, ys = np.where(corr > .5)\n",
      "xy = zip(xs, ys)\n",
      "[(x[0], x[1]) for x in xy if x[0] != x[1]]\n",
      "59/83:\n",
      "xs, ys = np.where(corr > .5)\n",
      "xy = zip(xs, ys)\n",
      "ind = [(x[0], x[1]) for x in xy if x[0] != x[1]]\n",
      "for (x,y) in ind:\n",
      "    print profiles.columns[x], profiles.columns[y]\n",
      "59/84: print c1\n",
      "59/85:\n",
      "xs, ys = np.where(corr > .2 ot corr < -0.2)\n",
      "xy = zip(xs, ys)\n",
      "ind = [(x[0], x[1]) for x in xy if x[0] != x[1]]\n",
      "for (x,y) in ind:\n",
      "    print profiles.columns[x], profiles.columns[y]\n",
      "59/86:\n",
      "xs, ys = np.where(corr > .2 or corr < -0.2)\n",
      "xy = zip(xs, ys)\n",
      "ind = [(x[0], x[1]) for x in xy if x[0] != x[1]]\n",
      "for (x,y) in ind:\n",
      "    print profiles.columns[x], profiles.columns[y]\n",
      "59/87:\n",
      "xs, ys = np.where(corr > .2)\n",
      "xy = zip(xs, ys)\n",
      "ind = [(x[0], x[1]) for x in xy if x[0] != x[1]]\n",
      "for (x,y) in ind:\n",
      "    print profiles.columns[x], profiles.columns[y]\n",
      "59/88:\n",
      "xs, ys = np.where(corr > .2)\n",
      "xy = zip(xs, ys)\n",
      "ind = [(x[0], x[1]) for x in xy if x[0].split(\"_\")[0] != x[1].split(\"_\")[0]]\n",
      "for (x,y) in ind:\n",
      "    print profiles.columns[x], profiles.columns[y]\n",
      "59/89:\n",
      "xs, ys = np.where(corr > .2)\n",
      "xy = zip(xs, ys)\n",
      "ind = [(x[0], x[1]) for x in xy if x[0] != x[1]]\n",
      "for (x,y) in ind:\n",
      "    r = profiles.columns[x]\n",
      "    c = profiles.columns[y]\n",
      "    if r.split(\"_\")[0] != c.split(\"_\")[0]:\n",
      "        print r, c\n",
      "59/90:\n",
      "xs, ys = np.where(corr < -.2)\n",
      "xy = zip(xs, ys)\n",
      "ind = [(x[0], x[1]) for x in xy if x[0] != x[1]]\n",
      "for (x,y) in ind:\n",
      "    r = profiles.columns[x]\n",
      "    c = profiles.columns[y]\n",
      "    if r.split(\"_\")[0] != c.split(\"_\")[0]:\n",
      "        print r, c\n",
      "59/91:\n",
      "n = [2*i for i in range(1, 10)]\n",
      "kmL = []\n",
      "for i in range(len(n)):\n",
      "    km = KMeans(n_clusters = n[i])\n",
      "    kmL.append(km.fit(profiles))\n",
      "59/92: from sklearn.metrics import silhouette_score\n",
      "59/93:\n",
      "from sklearn.metrics import silhouette_score\n",
      "for i in kmL:\n",
      "    labelsI=i.predict(test)\n",
      "    print(silhouette_score(test, labelsI))\n",
      "59/94:\n",
      "trainInds = np.random.uniform(0,1,profiles.shape[0])<=0.8\n",
      "train=profiles.iloc[trainInds]\n",
      "test=profiles.iloc[~trainInds]\n",
      "59/95:\n",
      "from sklearn.metrics import silhouette_score\n",
      "for i in kmL:\n",
      "    labelsI=i.predict(test)\n",
      "    print(silhouette_score(test, labelsI))\n",
      "59/96:\n",
      "xs, ys = np.where(corr > .2)\n",
      "xy = zip(xs, ys)\n",
      "ind = [(x[0], x[1]) for x in xy if x[0] != x[1]]\n",
      "for (x,y) in ind:\n",
      "    r = profiles.columns[x]\n",
      "    c = profiles.columns[y]\n",
      "    if r.split(\"_\")[0] != c.split(\"_\")[0]:\n",
      "        print r, c, profiles.corr[r][c]\n",
      "59/97:\n",
      "xs, ys = np.where(corr > .2)\n",
      "xy = zip(xs, ys)\n",
      "ind = [(x[0], x[1]) for x in xy if x[0] != x[1]]\n",
      "for (x,y) in ind:\n",
      "    r = profiles.columns[x]\n",
      "    c = profiles.columns[y]\n",
      "    if r.split(\"_\")[0] != c.split(\"_\")[0]:\n",
      "        print r, c, profiles.corr[r,c]\n",
      "59/98:\n",
      "xs, ys = np.where(corr > .2)\n",
      "xy = zip(xs, ys)\n",
      "ind = [(x[0], x[1]) for x in xy if x[0] != x[1]]\n",
      "for (x,y) in ind:\n",
      "    r = profiles.columns[x]\n",
      "    c = profiles.columns[y]\n",
      "    if r.split(\"_\")[0] != c.split(\"_\")[0]:\n",
      "        print r, c, corr[r][c]\n",
      "59/99:\n",
      "xs, ys = np.where(corr > .2)\n",
      "xy = zip(xs, ys)\n",
      "ind = [(x[0], x[1]) for x in xy if x[0] != x[1]]\n",
      "for (x,y) in ind:\n",
      "    r = profiles.columns[x]\n",
      "    c = profiles.columns[y]\n",
      "    if r.split(\"_\")[0] != c.split(\"_\")[0]:\n",
      "        print r, c, corr[r][c]\n",
      "64/1:\n",
      "import networkx as nx\n",
      "import csv\n",
      "import matplotlib.pyplot as plot\n",
      "64/2:\n",
      "from bs4 import BeautifulSoup\n",
      "import urllib2\n",
      "64/3:\n",
      "from sklearn.linear_model import LinearRegression, LogisticRegression\n",
      "import numpy as np\n",
      "from scipy import stats\n",
      "64/4:\n",
      "page0 = urllib2.urlopen(\"https://en.wikipedia.org/wiki/List_of_actors_with_Academy_Award_nominations\")\n",
      "soup0 = BeautifulSoup(page0, 'html.parser')\n",
      "64/5: actors0 = soup0.find_all('tr')[11:][:-11]\n",
      "64/6: actors0[0].find('a').text\n",
      "64/7:\n",
      "academylist = []\n",
      "for actor in actors0:\n",
      "    academylist.append(actor.find('a').text)\n",
      "64/8: academylist\n",
      "64/9:\n",
      "page1 = urllib2.urlopen(\"https://www.boxofficemojo.com/people/?view=Actor&p=.htm\")\n",
      "soup1 = BeautifulSoup(page1, 'html.parser')\n",
      "64/10: actors1 = soup1.find_all('tr')[2].find_all('tr')[1:]\n",
      "64/11: actors1\n",
      "64/12: boxofficedict = {}\n",
      "64/13:\n",
      "def addtodict(actors):\n",
      "    for actor in actors:\n",
      "        name = actor.find('b').find('b').text.replace(\",\", \"\")\n",
      "        boxoffice = str(actor.find('td', attrs={'align':'right'}).text[1:]).replace(\",\", \"\")\n",
      "        if \"k\" in boxoffice:\n",
      "            boxoffice = float(boxoffice.replace(\"k\", \"\"))/1000\n",
      "        boxofficedict[name] = float(boxoffice)\n",
      "64/14: boxofficedict\n",
      "64/15:\n",
      "page2 = urllib2.urlopen(\"https://www.boxofficemojo.com/people/?view=Actor&pagenum=2&sort=person&order=ASC&p=.htm\")\n",
      "soup2 = BeautifulSoup(page2, 'html.parser')\n",
      "actors2 = soup2.find_all('tr')[2].find_all('tr')[1:]\n",
      "64/16:\n",
      "page3 = urllib2.urlopen(\"https://www.boxofficemojo.com/people/?view=Actor&pagenum=3&sort=person&order=ASC&p=.htm\")\n",
      "soup3 = BeautifulSoup(page3, 'html.parser')\n",
      "actors3 = soup3.find_all('tr')[2].find_all('tr')[1:]\n",
      "64/17:\n",
      "addtodict(actors1)\n",
      "addtodict(actors2)\n",
      "addtodict(actors3)\n",
      "64/18: len(boxofficedict)\n",
      "64/19: boxofficedict\n",
      "64/20:\n",
      "boxofficesorted = sorted(boxofficedict, key=boxofficedict.get, reverse=True)\n",
      "boxofficesorted\n",
      "64/21:\n",
      "with open('title.basics.tsv') as tsvfile:\n",
      "    reader = csv.reader(tsvfile, delimiter='\\t')\n",
      "    i = 0\n",
      "    for row in reader:\n",
      "        if i < 200:\n",
      "            print(row)\n",
      "            i = i + 1\n",
      "        else:\n",
      "            break\n",
      "64/22:\n",
      "movies = {}\n",
      "with open('title.basics.tsv') as tsvfile:\n",
      "    reader = csv.reader(tsvfile, delimiter='\\t')\n",
      "    i = 0\n",
      "    for row in reader:\n",
      "        movies[row[0]] = row[1]\n",
      "64/23:\n",
      "dict = {}\n",
      "birthdict = {}\n",
      "deathdict = {}\n",
      "with open('name.basics.tsv') as tsvfile:\n",
      "    reader = csv.reader(tsvfile, delimiter='\\t')\n",
      "    i = 0\n",
      "    for row in reader:\n",
      "        if i < 200:\n",
      "            print(row)\n",
      "            i = i + 1\n",
      "        if i > 1:\n",
      "            dict[row[0]] = row[1]\n",
      "            birthdict[row[0]] = row[2]\n",
      "            if row[3] != '\\N':\n",
      "                deathdict[row[0]] = row[3]\n",
      "64/24:\n",
      "G = nx.Graph()\n",
      "Gw = nx.Graph()\n",
      "Gm = nx.Graph()\n",
      "Gweighted = nx.Graph()\n",
      "moviecountdict = {}\n",
      "agedict = {}\n",
      "with open('title.principals.tsv') as tsvfile:\n",
      "    reader = csv.reader(tsvfile, delimiter='\\t')\n",
      "    prev = \"\"\n",
      "    actors = []\n",
      "    for row in reader:\n",
      "        if row[3] == 'actor' and row[2] in dict and dict[row[2]] in boxofficedict:\n",
      "            Gm.add_node(dict[row[2]])\n",
      "            G.add_node(dict[row[2]])\n",
      "            Gweighted.add_node(dict[row[2]])\n",
      "            if dict[row[2]] in moviecountdict:\n",
      "                moviecountdict[dict[row[2]]] += 1\n",
      "            else:\n",
      "                moviecountdict[dict[row[2]]] = 1\n",
      "            agedict[dict[row[2]]] = 0 if birthdict[row[2]] == '\\N' else 2019-int(birthdict[row[2]])\n",
      "        elif row[3] == 'actress' and row[2] in dict and dict[row[2]] in boxofficedict:\n",
      "            Gw.add_node(dict[row[2]])\n",
      "            G.add_node(dict[row[2]])\n",
      "            Gweighted.add_node(dict[row[2]])\n",
      "            if dict[row[2]] in moviecountdict:\n",
      "                moviecountdict[dict[row[2]]] += 1\n",
      "            else:\n",
      "                moviecountdict[dict[row[2]]] = 1\n",
      "            agedict[dict[row[2]]] = 0 if birthdict[row[2]] == '\\N' else 2019-int(birthdict[row[2]])\n",
      "        else:\n",
      "            continue\n",
      "        if row[0] != prev:\n",
      "            for i in range(len(actors)):\n",
      "                for j in range(i, len(actors)):\n",
      "                    a1 = actors[i]\n",
      "                    a2 = actors[j]\n",
      "                    if a1 == a2:\n",
      "                        continue\n",
      "                    G.add_edge(a1, a2)\n",
      "                    if Gweighted.has_edge(a1, a2):\n",
      "                        Gweighted.add_edge(a1, a2, weight=(Gweighted.get_edge_data(a1, a2)['weight'] + 1))\n",
      "                    else:\n",
      "                        Gweighted.add_edge(a1, a2, weight=1)\n",
      "                    if a1 in Gw.nodes and a2 in Gw.nodes:\n",
      "                        Gw.add_edge(a1, a2)\n",
      "                    elif a1 in Gm.nodes and a2 in Gm.nodes:\n",
      "                        Gm.add_edge(a1, a2)\n",
      "            actors = [dict[row[2]]]\n",
      "        else:\n",
      "            actors.append(dict[row[2]])\n",
      "        prev = row[0]\n",
      "64/25:\n",
      "for a in agedict:\n",
      "    if a in actorlist:\n",
      "        print a, agedict[a]\n",
      "64/26:\n",
      "actorlist = [a for a in actorw]\n",
      "for a in actorm:\n",
      "    actorlist.append(a)\n",
      "64/27:\n",
      "actorw = sorted(Gw.nodes)\n",
      "actorm = sorted(Gm.nodes)\n",
      "64/28:\n",
      "actorlist = [a for a in actorw]\n",
      "for a in actorm:\n",
      "    actorlist.append(a)\n",
      "64/29:\n",
      "for a in agedict:\n",
      "    if a in actorlist:\n",
      "        print a, agedict[a]\n",
      "64/30: egograph(\"Dennis Quaid\")\n",
      "64/31:\n",
      "def egograph(name):\n",
      "    hub_ego = nx.ego_graph(G, name)\n",
      "    pos = nx.spring_layout(hub_ego)\n",
      "    options = {\n",
      "        'node_color': 'c',\n",
      "        'node_size': 20,\n",
      "        'line_color': 'grey',\n",
      "        'linewidths': 0,\n",
      "        'width': 0.1,\n",
      "    }\n",
      "    nx.draw(hub_ego, pos, with_labels=False, **options)\n",
      "    nx.draw_networkx_nodes(hub_ego, pos, nodelist=[name], node_size=100, node_color='b')\n",
      "64/32: egograph(\"Dennis Quaid\")\n",
      "65/1:\n",
      "import numpy as np\n",
      "import pandas as pd\n",
      "import matplotlib.pyplot as plt\n",
      "%matplotlib inline\n",
      "from copy import deepcopy\n",
      "from sklearn.preprocessing import MinMaxScaler\n",
      "from sklearn.cluster import KMeans\n",
      "import seaborn as sb\n",
      "65/2:\n",
      "profile_df = pd.read_csv('profiles2.csv')\n",
      "profile_df = profile_df.drop(profile_df.columns[0], axis=1)\n",
      "65/3: profile_df.head()\n",
      "65/4:\n",
      "hasincome = profile_df[profile_df['income'] != -1]\n",
      "hasincome.head()\n",
      "65/5: hasincome[['income'] + [x for x in hasincome.columns if \"job\" in x]].corr()\n",
      "65/6: np.mean(hasincome['income']), np.median(hasincome['income'])\n",
      "65/7:\n",
      "profile_df2 = deepcopy(profile_df)\n",
      "profile_df2['income'] = profile_df2['income'].replace(-1, hasincome['income'].median())\n",
      "profile_df2.head()\n",
      "65/8:\n",
      "profile_df2 = deepcopy(profile_df)\n",
      "profile_df2['income'] = profile_df2['income'].replace(-1, hasincome['income'].median())\n",
      "profile_df2.head()\n",
      "65/9:\n",
      "toscale = ['age', 'height', 'income', 'last_online']\n",
      "profiles = deepcopy(profile_df2)\n",
      "for i in range(len(toscale)):\n",
      "    scaler = MinMaxScaler(feature_range=(0, 1))\n",
      "    profiles[toscale[i]] = scaler.fit_transform(np.array(profiles[toscale[i]]).reshape(-1, 1))\n",
      "65/10: profiles = profiles.fillna(profiles.mean(axis=1).iloc[0])\n",
      "65/11: profiles.head()\n",
      "65/12:\n",
      "ranks = [['sex', 'orientation', 'speaks', 'drugs', 'drinks', 'smokes'], \n",
      "        ['age', 'status', 'not', 'income', 'body', 'height'], \\\n",
      "        ['education', 'ethnicity'], \\\n",
      "        ['pets', 'offspring', 'religion', 'sign', 'diet'], \\\n",
      "        ['essay']]\n",
      "65/13:\n",
      "profiles1 = deepcopy(profiles)\n",
      "for i in range(len(profiles1.columns)):\n",
      "    category = profiles1.columns[i].split('_')[0]\n",
      "    for j in range(len(ranks)):\n",
      "        if category in ranks[j]:\n",
      "            profiles1[profiles1.columns[i]] = profiles1[profiles1.columns[i]]*(2**(4-j))\n",
      "            continue\n",
      "65/14: profiles1['religion_serious'] = profiles1['religion_serious']*4\n",
      "65/15: profiles1.head()\n",
      "65/16:\n",
      "n = [i+2 for i in range(14)]\n",
      "kmL = []\n",
      "for i in range(len(n)):\n",
      "    km = KMeans(n_clusters = n[i])\n",
      "    kmL.append(km.fit(profiles1))\n",
      "65/17:\n",
      "# n = [i+2 for i in range(14)]\n",
      "n = [6]\n",
      "kmL = []\n",
      "for i in range(len(n)):\n",
      "    km = KMeans(n_clusters = n[i])\n",
      "    kmL.append(km.fit(profiles1))\n",
      "65/18: centersL = [k.cluster_centers_ for k in kmL]\n",
      "65/19:\n",
      "def comparefeatures(k, centers):\n",
      "    featuredict = {}\n",
      "    for i in range(len(profiles.columns)):\n",
      "        f = profiles.columns[i]\n",
      "        fL = []\n",
      "        for j in range(k):\n",
      "            fL.append(centers[j][i])\n",
      "        featuredict[f] = (fL, np.std(fL))\n",
      "    featuredict = sorted(featuredict.items(), key=lambda kv: kv[1][1], reverse=True)\n",
      "    for k,v in featuredict:\n",
      "        print k + \": \", v[0], v[1]\n",
      "65/20: comparefeatures(6, centersL[0])\n",
      "65/21:\n",
      "order_centroids = km.cluster_centers_.argsort()[:, ::-1]\n",
      "terms = profiles.columns\n",
      "for i in range(4):\n",
      "    print(\"Cluster %d:\" % i)\n",
      "    for ind in order_centroids[i, :10]:\n",
      "        print(' %s' % terms[ind])\n",
      "    print()\n",
      "65/22:\n",
      "order_centroids = km.cluster_centers_.argsort()[:, ::-1]\n",
      "terms = profiles.columns\n",
      "for i in range(6):\n",
      "    print(\"Cluster %d:\" % i)\n",
      "    for ind in order_centroids[i, :10]:\n",
      "        print(' %s' % terms[ind])\n",
      "    print()\n",
      "65/23:\n",
      "toscale = ['age', 'height', 'income', 'last_online']\n",
      "profiles0 = deepcopy(profile_df)\n",
      "for i in range(len(toscale)):\n",
      "    scaler = MinMaxScaler(feature_range=(0, 1))\n",
      "    profiles0[toscale[i]] = scaler.fit_transform(np.array(profiles0[toscale[i]]).reshape(-1, 1))\n",
      "65/24: profiles0 = profiles0.fillna(profiles0.mean(axis=1).iloc[0])\n",
      "65/25:\n",
      "km6 = KMeans(n_clusters = 6)\n",
      "km6.fit(profiles)\n",
      "65/26: comparefeatures(6, km6.cluster_centers_)\n",
      "65/27:\n",
      "import seaborn as sns\n",
      "def makeViolin(col, k):\n",
      "    columns = list(filter(lambda c: c.startswith(col), list(profiles)))\n",
      "    count = len(columns)\n",
      "    if count == 0:\n",
      "        raise Exception('no columns!')\n",
      "    if count > 5:\n",
      "        if count%5==0:\n",
      "            fig, axs = plt.subplots(ncols=5,nrows=int(count/5), figsize = (200, 200))\n",
      "        else:\n",
      "            fig, axs = plt.subplots(ncols=5,nrows=int(count/5)+1, figsize = (200, 200))\n",
      "    else:\n",
      "        fig, axs = plt.subplots(ncols=count)      \n",
      "    \n",
      "    if count == 1:\n",
      "        for c in columns:\n",
      "            sns.violinplot(x=kmL[k].labels_, y=c, data=profiles)\n",
      "            return\n",
      "    print(axs.shape)\n",
      "    if count <=5:\n",
      "        tempCount = 0\n",
      "        for c in columns:\n",
      "            sns.violinplot(x=kmL[k].labels_, y=c, data=profiles)  \n",
      "            with sns.axes_style(\"white\"):\n",
      "                sns.violinplot(x=kmL[k].labels_, y=c, data=profiles, ax=axs[tempCount])\n",
      "                tempCount+=1\n",
      "    else:\n",
      "        tempCount = 0\n",
      "        for c in columns:    \n",
      "            with sns.axes_style(\"white\"):\n",
      "                sns.violinplot(x=kmL[k].labels_, y=c, data=profiles, ax=axs[int(tempCount/5), tempCount%5])\n",
      "                tempCount+=1\n",
      "65/28:\n",
      "import seaborn as sns\n",
      "def makeViolin(col, k):\n",
      "    columns = list(filter(lambda c: c.startswith(col), list(profiles)))\n",
      "    count = len(columns)\n",
      "    if count == 0:\n",
      "        raise Exception('no columns!')\n",
      "    if count > 5:\n",
      "        if count%5==0:\n",
      "            fig, axs = plt.subplots(ncols=5,nrows=int(count/5), figsize = (200, 200))\n",
      "        else:\n",
      "            fig, axs = plt.subplots(ncols=5,nrows=int(count/5)+1, figsize = (200, 200))\n",
      "    else:\n",
      "        fig, axs = plt.subplots(ncols=count)      \n",
      "    \n",
      "    if count == 1:\n",
      "        for c in columns:\n",
      "            sns.violinplot(x=kmL[k].labels_, y=c, data=profiles)\n",
      "            return\n",
      "    print(axs.shape)\n",
      "    if count <=5:\n",
      "        tempCount = 0\n",
      "        for c in columns:\n",
      "            sns.violinplot(x=kmL[k].labels_, y=c, data=profiles)  \n",
      "            with sns.axes_style(\"white\"):\n",
      "                sns.violinplot(x=kmL[k].labels_, y=c, data=profiles, ax=axs[tempCount])\n",
      "                tempCount+=1\n",
      "    else:\n",
      "        tempCount = 0\n",
      "        for c in columns:    \n",
      "            with sns.axes_style(\"white\"):\n",
      "                sns.violinplot(x=kmL[k].labels_, y=c, data=profiles, ax=axs[int(tempCount/5), tempCount%5])\n",
      "                tempCount+=1\n",
      "65/29: makeViolin(\"income\", 0)\n",
      "65/30: makeViolin(\"sex\", 0)\n",
      "65/31: makeViolin(\"sex_f\", 0)\n",
      "65/32: makeViolin(\"education_college\", 0)\n",
      "65/33:\n",
      "%%time\n",
      "\n",
      "from sklearn.decomposition import LatentDirichletAllocation\n",
      "\n",
      "lda = LatentDirichletAllocation(n_components = 5)\n",
      "\n",
      "lda.fit(profiles.values)\n",
      "65/34: profiles.max().max()\n",
      "65/35: profiles1.max().max()\n",
      "65/36:\n",
      "trainInds = np.random.uniform(0,1,profiles.shape[0])<=0.8\n",
      "train=profiles.iloc[trainInds]\n",
      "test=profiles.iloc[~trainInds]\n",
      "65/37:\n",
      "ldaN = [i for i in range(3, 20) if i%3 == 0]\n",
      "ldaL = []\n",
      "ldaS = []\n",
      "for i in range(len(ldaN)):\n",
      "    lda = LatentDirichletAllocation(n_components = ldaN[i])\n",
      "    lda.fit(train.values)\n",
      "    ldaL.append(lda)\n",
      "    score = lda.score(test.values)\n",
      "    ldaS.append(score)\n",
      "65/38:\n",
      "ldaN = [i for i in range(3, 20) if i%3 == 0]\n",
      "ldaL = []\n",
      "ldaS = []\n",
      "for i in range(len(ldaN)):\n",
      "    lda = LatentDirichletAllocation(n_components = ldaN[i])\n",
      "    lda.fit(train.values)\n",
      "    ldaL.append(lda)\n",
      "    score = lda.score(test.values)\n",
      "    ldaS.append(score)\n",
      "    print score\n",
      "65/39: plt.plot(ldaN, ldaS)\n",
      "65/40:\n",
      "train1=profiles1.iloc[trainInds]\n",
      "test1=profiles1.iloc[~trainInds]\n",
      "65/41:\n",
      "ldaN1 = [i for i in range(3, 50) if i%3 == 0]\n",
      "ldaL1 = []\n",
      "ldaS1 = []\n",
      "for i in range(len(ldaN)):\n",
      "    lda = LatentDirichletAllocation(n_components = ldaN[i])\n",
      "    lda.fit(train1.values)\n",
      "    ldaL1.append(lda)\n",
      "    score = lda.score(test1.values)\n",
      "    ldaS1.append(score)\n",
      "    print score\n",
      "65/42:\n",
      "ldaN1 = [i for i in range(3, 50) if i%3 == 0]\n",
      "ldaL1 = []\n",
      "ldaS1 = []\n",
      "for i in range(len(ldaN1)):\n",
      "    lda = LatentDirichletAllocation(n_components = ldaN1[i])\n",
      "    lda.fit(train1.values)\n",
      "    ldaL1.append(lda)\n",
      "    score = lda.score(test1.values)\n",
      "    ldaS1.append(score)\n",
      "    print score\n",
      "65/43: plt.plot(ldaN, ldaS)\n",
      "65/44: plt.plot(ldaN1, ldaS1)\n",
      "65/45:\n",
      "for topic_idx, topic in enumerate(lda.components_):\n",
      "        message = \"Topic #%d: \" % topic_idx\n",
      "        message += \"\\n\".join([profiles.columns[i]\n",
      "                             for i in topic.argsort()[:-10:-1]])\n",
      "        print(message)\n",
      "        print()\n",
      "65/46: np.argmax(ldaS1)\n",
      "65/47: np.argmax(ldaS1[:13])\n",
      "65/48: plt.plot(ldaN1[:13, ldaS1[:13])\n",
      "65/49: plt.plot(ldaN1[:13], ldaS1[:13])\n",
      "65/50: plt.plot(ldaN1[:12], ldaS1[:12])\n",
      "65/51: np.argmax(ldaS1[:12])\n",
      "65/52:\n",
      "for topic_idx, topic in enumerate(ldaL1[9].components_):\n",
      "        message = \"Topic #%d: \" % topic_idx\n",
      "        message += \"\\n\".join([profiles1.columns[i]\n",
      "                             for i in topic.argsort()[:-10:-1]])\n",
      "        print(message)\n",
      "        print()\n",
      "65/53: transformed = ldaL1[9].transform(profiles1.values)\n",
      "65/54:\n",
      "plt.figure()\n",
      "for i in range(5):\n",
      "    plt.hist(transformed[:, i], alpha=0.3, label=str(i+1),\n",
      "            range=(0,1), bins=20)\n",
      "    plt.show()\n",
      "plt.xlabel(\"User Weight in Component i\", fontsize=20)\n",
      "plt.ylabel(\"Count\", fontsize=20)\n",
      "plt.tick_params(labelsize=15)\n",
      "plt.legend()\n",
      "plt.show()\n",
      "65/55:\n",
      "plt.figure()\n",
      "for i in range(10):\n",
      "    plt.hist(transformed[:, i], alpha=0.3, label=str(i+1),\n",
      "            range=(0,1), bins=20)\n",
      "    plt.show()\n",
      "plt.xlabel(\"User Weight in Component i\", fontsize=20)\n",
      "plt.ylabel(\"Count\", fontsize=20)\n",
      "plt.tick_params(labelsize=15)\n",
      "plt.legend()\n",
      "plt.show()\n",
      "65/56:\n",
      "plt.figure()\n",
      "for i in range(30):\n",
      "    plt.hist(transformed[:, i], alpha=0.3, label=str(i+1),\n",
      "            range=(0,1), bins=20)\n",
      "    plt.show()\n",
      "plt.xlabel(\"User Weight in Component i\", fontsize=20)\n",
      "plt.ylabel(\"Count\", fontsize=20)\n",
      "plt.tick_params(labelsize=15)\n",
      "plt.legend()\n",
      "plt.show()\n",
      "65/57:\n",
      "plt.figure()\n",
      "for i in range(30):\n",
      "    plt.hist(transformed[:, i], alpha=0.3, label=str(i+1),\n",
      "            range=(0,1), bins=20)\n",
      "#     plt.show()\n",
      "plt.xlabel(\"User Weight in Component i\", fontsize=20)\n",
      "plt.ylabel(\"Count\", fontsize=20)\n",
      "plt.tick_params(labelsize=15)\n",
      "plt.legend()\n",
      "plt.show()\n",
      "65/58:\n",
      "c1_indices = np.where(c1 > np.percentile(c1, 99))[0]\n",
      "\n",
      "top_c1_df = pd.DataFrame({'Component': profiles1.columns.values[c1_indices],\n",
      "                         'Weight': c1[c1_indices]})\n",
      "\n",
      "top_c1_df.sort_values('Weight', inplace=True, ascending=False)\n",
      "\n",
      "top_c1_df.head(n=10)\n",
      "65/59:\n",
      "c1 = ldaL[9].components_[0, :]\n",
      "print c1.shape\n",
      "\n",
      "# print \"Percentiles of the components:\"\n",
      "# print np.percentile(c1, np.arange(0,100,1))\n",
      "\n",
      "plt.figure()\n",
      "plt.hist(c1)\n",
      "plt.xlabel(\"Feature Weight in Component 1\", fontsize=20)\n",
      "plt.ylabel(\"Frequency\", fontsize=20)\n",
      "plt.tick_params(labelsize=15)\n",
      "plt.show()\n",
      "65/60:\n",
      "c1 = ldaL1[9].components_[0, :]\n",
      "print c1.shape\n",
      "\n",
      "# print \"Percentiles of the components:\"\n",
      "# print np.percentile(c1, np.arange(0,100,1))\n",
      "\n",
      "plt.figure()\n",
      "plt.hist(c1)\n",
      "plt.xlabel(\"Feature Weight in Component 1\", fontsize=20)\n",
      "plt.ylabel(\"Frequency\", fontsize=20)\n",
      "plt.tick_params(labelsize=15)\n",
      "plt.show()\n",
      "65/61:\n",
      "c1_indices = np.where(c1 > np.percentile(c1, 99))[0]\n",
      "\n",
      "top_c1_df = pd.DataFrame({'Component': profiles1.columns.values[c1_indices],\n",
      "                         'Weight': c1[c1_indices]})\n",
      "\n",
      "top_c1_df.sort_values('Weight', inplace=True, ascending=False)\n",
      "\n",
      "top_c1_df.head(n=10)\n",
      "65/62:\n",
      "ldaN = [i for i in range(3, 50) if i%3 == 0]\n",
      "ldaL = []\n",
      "ldaS = []\n",
      "for i in range(len(ldaN)):\n",
      "    lda = LatentDirichletAllocation(n_components = ldaN[i])\n",
      "    lda.fit(train.values)\n",
      "    ldaL.append(lda)\n",
      "    score = lda.score(test.values)\n",
      "    ldaS.append(score)\n",
      "    print score\n",
      "65/63: plt.plot(ldaN, ldaS)\n",
      "65/64:\n",
      "t1 = transformed[:, 0]\n",
      "tmp_profile_df = profiles1.copy()\n",
      "\n",
      "tmp_profile_df.insert(0, \"User Load in Component 1\", t1)\n",
      "\n",
      "tmp_profile_df.sort_values('User Load in Component 1', inplace=True, ascending=False)\n",
      "\n",
      "tmp_profile_df.head()\n",
      "65/65: pd.options.display.max_rows = 300\n",
      "65/66:\n",
      "t1 = transformed[:, 0]\n",
      "tmp_profile_df = profiles1.copy()\n",
      "\n",
      "tmp_profile_df.insert(0, \"User Load in Component 1\", t1)\n",
      "\n",
      "tmp_profile_df.sort_values('User Load in Component 1', inplace=True, ascending=False)\n",
      "\n",
      "tmp_profile_df.head()\n",
      "65/67: pd.options.display.max_columns = 300\n",
      "65/68:\n",
      "t1 = transformed[:, 0]\n",
      "tmp_profile_df = profiles1.copy()\n",
      "\n",
      "tmp_profile_df.insert(0, \"User Load in Component 1\", t1)\n",
      "\n",
      "tmp_profile_df.sort_values('User Load in Component 1', inplace=True, ascending=False)\n",
      "\n",
      "tmp_profile_df.head()\n",
      "65/69:\n",
      "def topusers(transformed, i):\n",
      "    t1 = transformed[:, i]\n",
      "    tmp_profile_df = profiles1.copy()\n",
      "\n",
      "    tmp_profile_df.insert(0, \"User Load in Component 1\", t1)\n",
      "\n",
      "    tmp_profile_df.sort_values('User Load in Component 1', inplace=True, ascending=False)\n",
      "\n",
      "    tmp_profile_df.head()\n",
      "65/70: topusers(transformed, 1)\n",
      "65/71:\n",
      "def topusers(transformed, i):\n",
      "    t1 = transformed[:, i]\n",
      "    tmp_profile_df = profiles1.copy()\n",
      "\n",
      "    tmp_profile_df.insert(0, \"User Load in Component 1\", t1)\n",
      "\n",
      "    tmp_profile_df.sort_values('User Load in Component 1', inplace=True, ascending=False)\n",
      "\n",
      "    print tmp_profile_df.head()\n",
      "65/72: topusers(transformed, 1)\n",
      "65/73: model = ldaL1[9]\n",
      "65/74: model.components_ / model.components_.sum(axis=1)[:, np.newaxis]\n",
      "65/75: model.components_ / model.components_.sum(axis=1)[:, np.newaxis].shape\n",
      "65/76: (model.components_ / model.components_.sum(axis=1)[:, np.newaxis]).shape\n",
      "65/77: topicproportions = model.components_ / model.components_.sum(axis=1)[:, np.newaxis]\n",
      "65/78:\n",
      "topicproportions = model.components_ / model.components_.sum(axis=1)[:, np.newaxis]\n",
      "topicproportions.shape\n",
      "65/79:\n",
      "list = []\n",
      "for i in range(len(topicproportions)):\n",
      "    list.append((i, np.std(topicproportions[i]))\n",
      "sorted(list, key=lambda x: x[0])\n",
      "65/80:\n",
      "l = []\n",
      "for i in range(len(topicproportions)):\n",
      "    l.append((i, np.std(topicproportions[i]))\n",
      "sorted(l, key=lambda x: x[0])\n",
      "65/81:\n",
      "l = []\n",
      "for i in range(len(topicproportions[0])):\n",
      "    l.append((i, np.std(topicproportions[:,i]))\n",
      "65/82:\n",
      "l = []\n",
      "for i in range(len(topicproportions[0])):\n",
      "    l.append((i, np.std(topicproportions[:,i])))\n",
      "65/83:\n",
      "l = []\n",
      "for i in range(len(topicproportions[0])):\n",
      "    l.append((i, np.std(topicproportions[:,i])))\n",
      "sorted(l, key=lambda x: x[1])\n",
      "65/84:\n",
      "l = []\n",
      "for i in range(len(topicproportions[0])):\n",
      "    l.append((profiles.columns[i], np.std(topicproportions[:,i])))\n",
      "sorted(l, key=lambda x: x[1])\n",
      "65/85:\n",
      "l = []\n",
      "for i in range(len(topicproportions[0])):\n",
      "    l.append((profiles.columns[i], np.std(topicproportions[:,i])))\n",
      "sorted(l, key=lambda x: x[1], reverse=True)\n",
      "65/86:\n",
      "l = []\n",
      "for i in range(len(topicproportions[0])):\n",
      "    l.append((profiles.columns[i], np.max(topicproportions[:,i])))\n",
      "sorted(l, key=lambda x: x[1], reverse=True)\n",
      "65/87:\n",
      "l = []\n",
      "for i in range(len(topicproportions[0])):\n",
      "    l.append((profiles.columns[i], np.max(topicproportions[:,i])), np.argmax(topicproportions[:,i]))\n",
      "sorted(l, key=lambda x: x[1], reverse=True)\n",
      "65/88:\n",
      "l = []\n",
      "for i in range(len(topicproportions[0])):\n",
      "    l.append((profiles.columns[i], np.max(topicproportions[:,i]), np.argmax(topicproportions[:,i])))\n",
      "sorted(l, key=lambda x: x[1], reverse=True)\n",
      "65/89:\n",
      "t1 = transformed[:, 0]\n",
      "tmp_profile_df = profiles1.copy()\n",
      "\n",
      "tmp_profile_df.insert(0, \"User Load in Component 1\", t1)\n",
      "\n",
      "tmp_profile_df.sort_values('User Load in Component 1', inplace=True, ascending=False)\n",
      "\n",
      "tmp_profile_df[:10,:]\n",
      "65/90:\n",
      "t1 = transformed[:, 0]\n",
      "tmp_profile_df = profiles1.copy()\n",
      "\n",
      "tmp_profile_df.insert(0, \"User Load in Component 1\", t1)\n",
      "\n",
      "tmp_profile_df.sort_values('User Load in Component 1', inplace=True, ascending=False)\n",
      "\n",
      "tmp_profile_df.iloc[:10,:]\n",
      "65/91:\n",
      "t1 = transformed[:, 0]\n",
      "tmp_profile_df = profiles1.copy()\n",
      "\n",
      "tmp_profile_df.insert(0, \"User Load in Component 1\", t1)\n",
      "\n",
      "tmp_profile_df.sort_values('User Load in Component 1', inplace=True, ascending=False)\n",
      "\n",
      "topten = tmp_profile_df.iloc[:10,:]\n",
      "topten\n",
      "65/92: topten.drop([col for col, val in topten.sum().iteritems() if val < 10], axis=1, inplace=True)\n",
      "65/93:\n",
      "topten.drop([col for col, val in topten.sum().iteritems() if val < 10], axis=1, inplace=True)\n",
      "topten\n",
      "65/94:\n",
      "def topusers(transformed, i):\n",
      "    t1 = transformed[:, i]\n",
      "    tmp_profile_df = profiles1.copy()\n",
      "\n",
      "    tmp_profile_df.insert(0, \"User Load in Component 1\", t1)\n",
      "\n",
      "    tmp_profile_df.sort_values('User Load in Component 1', inplace=True, ascending=False)\n",
      "\n",
      "    topten = tmp_profile_df.iloc[:10,:]\n",
      "    topten.drop([col for col, val in topten.sum().iteritems() if val < 10], axis=1, inplace=True)\n",
      "    print topten\n",
      "65/95: topusers(transformed, 1)\n",
      "65/96: topusers(transformed, 2)\n",
      "65/97: topusers(transformed, 3)\n",
      "65/98: profiles[['sign_not_serious'] + [x for x in profiles.columns if 'education' in x]].corr()\n",
      "66/1:\n",
      "import numpy as np\n",
      "import pandas as pd\n",
      "import matplotlib.pyplot as plt\n",
      "%matplotlib inline\n",
      "\n",
      "\n",
      "# increase the width of the pandas dataframe to allow scrolling through all columns\n",
      "\n",
      "pd.options.display.max_columns = 100\n",
      "66/2: profile_df = pd.read_csv(\"profiles.csv\")\n",
      "65/99:\n",
      "nL = [i for i in range(3,50) if i%3==0]\n",
      "gmmL = []\n",
      "for i in range(len(nL)):\n",
      "    gmm = mixture.GaussianMixture(n_components=, covariance_type='tied')\n",
      "    gmm.fit(train1)\n",
      "    score = gmm.score(test1)\n",
      "    bic = gmm.bic(test1)\n",
      "    print i, score, bic\n",
      "    gmmL.append((gmm, score, bic))\n",
      "65/100:\n",
      "nL = [i for i in range(3,50) if i%3==0]\n",
      "gmmL = []\n",
      "for i in range(len(nL)):\n",
      "    gmm = mixture.GaussianMixture(n_components=nL[i], covariance_type='tied')\n",
      "    gmm.fit(train1)\n",
      "    score = gmm.score(test1)\n",
      "    bic = gmm.bic(test1)\n",
      "    print i, score, bic\n",
      "    gmmL.append((gmm, score, bic))\n",
      "65/101: from sklearn import mixture\n",
      "65/102:\n",
      "nL = [i for i in range(3,50) if i%3==0]\n",
      "gmmL = []\n",
      "for i in range(len(nL)):\n",
      "    gmm = mixture.GaussianMixture(n_components=nL[i], covariance_type='tied')\n",
      "    gmm.fit(train1)\n",
      "    score = gmm.score(test1)\n",
      "    bic = gmm.bic(test1)\n",
      "    print i, score, bic\n",
      "    gmmL.append((gmm, score, bic))\n",
      "65/103: plt.plot(nL, gmmL)\n",
      "65/104: plt.plot(nL, [x[1] for x in gmmL])\n",
      "65/105: plt.plot(nL, [x[2] for x in gmmL])\n",
      "64/33:\n",
      "import networkx as nx\n",
      "import csv\n",
      "import matplotlib.pyplot as plot\n",
      "65/106: gmm = gmmL[12][0]\n",
      "65/107: gmm.weights_\n",
      "65/108: gmm.means_\n",
      "65/109: gmm.means_.shape\n",
      "65/110: gmm.means_\n",
      "65/111:\n",
      "means = []\n",
      "sds = []\n",
      "for i in range(len(gmm.means_[0])):\n",
      "    means.append(np.mean(gmm.means_[:,i]))\n",
      "    sds.append(np.std(gmm.means_[:,i]))\n",
      "65/112: print means, sds\n",
      "65/113: print sds\n",
      "65/114: abs(1-2)\n",
      "65/115:\n",
      "for i in range(len(gmm.means_)):\n",
      "    for j in range(len(gmm.means_[0])):\n",
      "        if abs(gmm.means_[i,j]-means[j]) > sds[j]:\n",
      "            print i, profiles1.columns[j]\n",
      "65/116:\n",
      "for i in range(len(gmm.means_)):\n",
      "    for j in range(len(gmm.means_[0])):\n",
      "        if abs(gmm.means_[i,j]-means[j]) > 1.5*sds[j]:\n",
      "            print i, profiles1.columns[j], gmm.means_[i,j], means[j]\n",
      "65/117:\n",
      "for i in range(len(gmm.means_)):\n",
      "    for j in range(len(gmm.means_[0])):\n",
      "        if abs(gmm.means_[i,j]-means[j]) > 2*sds[j]:\n",
      "            print i, profiles1.columns[j], gmm.means_[i,j], means[j]\n",
      "65/118:\n",
      "for i in range(len(gmm.means_)):\n",
      "    for j in range(len(gmm.means_[0])):\n",
      "        z_score = (gmm.means_[i,j]-means[j])/sds[j]\n",
      "        if abs(z_score) > 3:\n",
      "            print i, profiles1.columns[j], z_score\n",
      "65/119:\n",
      "for i in range(len(gmm.means_)):\n",
      "    for j in range(len(gmm.means_[0])):\n",
      "        z_score = (gmm.means_[i,j]-means[j])/sds[j]\n",
      "        if abs(z_score) > 2.5:\n",
      "            print i, profiles1.columns[j], z_score\n",
      "65/120: gmm.weights_.shape\n",
      "65/121: gmm.weights_\n",
      "65/122:\n",
      "for i in range(len(gmm.means_)):\n",
      "    for j in range(len(gmm.means_[0])):\n",
      "        z_score = (gmm.means_[i,j]-means[j])/sds[j]\n",
      "        if abs(z_score) > 2:\n",
      "            print i, profiles1.columns[j], z_score\n",
      "65/123: np.argmax(gmm.weights_)\n",
      "65/124:\n",
      "def gettop(i):\n",
      "    c1 = ldaL1[9].components_[i, :]\n",
      "    c1_indices = np.where(c1 > np.percentile(c1, 99))[0]\n",
      "\n",
      "    top_c1_df = pd.DataFrame({'Component': profiles1.columns.values[c1_indices],\n",
      "                             'Weight': c1[c1_indices]})\n",
      "\n",
      "    top_c1_df.sort_values('Weight', inplace=True, ascending=False)\n",
      "\n",
      "    top_c1_df.head(n=10)\n",
      "65/125: gettop(0)\n",
      "65/126:\n",
      "def gettop(i):\n",
      "    c1 = ldaL1[9].components_[i, :]\n",
      "    c1_indices = np.where(c1 > np.percentile(c1, 99))[0]\n",
      "\n",
      "    top_c1_df = pd.DataFrame({'Component': profiles1.columns.values[c1_indices],\n",
      "                             'Weight': c1[c1_indices]})\n",
      "\n",
      "    top_c1_df.sort_values('Weight', inplace=True, ascending=False)\n",
      "\n",
      "    print top_c1_df.head(n=10)\n",
      "65/127: gettop(0)\n",
      "65/128:\n",
      "for i in range(30):\n",
      "    gettop(i)\n",
      "65/129:\n",
      "plt.plot(ldaN1[:12], ldaS1[:12])\n",
      "plt.title(\"LDA\")\n",
      "plt.xlabel(\"Number of components\")\n",
      "plt.ylabel(\"Log likelihood\")\n",
      "plt.show()\n",
      "65/130:\n",
      "plt.subplots(1, 2)\n",
      "plt.plot(nL, [x[1] for x in gmmL])\n",
      "plt.plot(nL, [x[2] for x in gmmL])\n",
      "65/131:\n",
      "fig, axs = plt.subplots(nrows=1, ncols=2)\n",
      "ax = axs[0]\n",
      "ax.plot(nL, [x[1] for x in gmmL])\n",
      "ax = axs[1]\n",
      "ax.plot(nL, [x[2] for x in gmmL])\n",
      "65/132:\n",
      "fig, axs = plt.subplots(nrows=1, ncols=2, figsize=(5,5))\n",
      "ax = axs[0]\n",
      "ax.plot(nL, [x[1] for x in gmmL])\n",
      "ax = axs[1]\n",
      "ax.plot(nL, [x[2] for x in gmmL])\n",
      "65/133:\n",
      "fig, axs = plt.subplots(nrows=1, ncols=2, figsize=(2,10))\n",
      "ax = axs[0]\n",
      "ax.plot(nL, [x[1] for x in gmmL])\n",
      "ax = axs[1]\n",
      "ax.plot(nL, [x[2] for x in gmmL])\n",
      "65/134:\n",
      "fig, axs = plt.subplots(nrows=1, ncols=2, figsize=(10,2))\n",
      "ax = axs[0]\n",
      "ax.plot(nL, [x[1] for x in gmmL])\n",
      "ax = axs[1]\n",
      "ax.plot(nL, [x[2] for x in gmmL])\n",
      "65/135:\n",
      "fig, axs = plt.subplots(nrows=1, ncols=2, figsize=(10,5))\n",
      "ax = axs[0]\n",
      "ax.plot(nL, [x[1] for x in gmmL])\n",
      "ax = axs[1]\n",
      "ax.plot(nL, [x[2] for x in gmmL])\n",
      "65/136:\n",
      "fig, axs = plt.subplots(nrows=1, ncols=2, figsize=(12,4))\n",
      "ax = axs[0]\n",
      "ax.plot(nL, [x[1] for x in gmmL])\n",
      "ax = axs[1]\n",
      "ax.plot(nL, [x[2] for x in gmmL])\n",
      "65/137:\n",
      "fig, axs = plt.subplots(nrows=2, ncols=1, figsize=(12,4))\n",
      "ax = axs[0]\n",
      "ax.plot(nL, [x[1] for x in gmmL])\n",
      "ax = axs[1]\n",
      "ax.plot(nL, [x[2] for x in gmmL])\n",
      "65/138:\n",
      "fig, axs = plt.subplots(nrows=2, ncols=1, figsize=(4,12))\n",
      "ax = axs[0]\n",
      "ax.plot(nL, [x[1] for x in gmmL])\n",
      "ax = axs[1]\n",
      "ax.plot(nL, [x[2] for x in gmmL])\n",
      "65/139:\n",
      "fig, axs = plt.subplots(nrows=2, ncols=1, figsize=(4,4))\n",
      "ax = axs[0]\n",
      "ax.plot(nL, [x[1] for x in gmmL])\n",
      "ax = axs[1]\n",
      "ax.plot(nL, [x[2] for x in gmmL])\n",
      "65/140:\n",
      "fig, axs = plt.subplots(nrows=2, ncols=1, figsize=(6,4))\n",
      "ax = axs[0]\n",
      "ax.plot(nL, [x[1] for x in gmmL])\n",
      "ax = axs[1]\n",
      "ax.plot(nL, [x[2] for x in gmmL])\n",
      "65/141:\n",
      "fig, axs = plt.subplots(nrows=2, ncols=1, figsize=(4,6))\n",
      "ax = axs[0]\n",
      "ax.plot(nL, [x[1] for x in gmmL])\n",
      "ax = axs[1]\n",
      "ax.plot(nL, [x[2] for x in gmmL])\n",
      "65/142:\n",
      "fig, axs = plt.subplots(nrows=2, ncols=1, figsize=(6,9))\n",
      "ax = axs[0]\n",
      "ax.plot(nL, [x[1] for x in gmmL])\n",
      "ax.title(\"Log likelihood\")\n",
      "ax = axs[1]\n",
      "ax.plot(nL, [x[2] for x in gmmL])\n",
      "ax.title(\"\")\n",
      "65/143:\n",
      "fig, axs = plt.subplots(nrows=2, ncols=1, figsize=(6,9))\n",
      "ax = axs[0]\n",
      "ax.plot(nL, [x[1] for x in gmmL])\n",
      "ax.set_title(\"Log likelihood\")\n",
      "ax = axs[1]\n",
      "ax.plot(nL, [x[2] for x in gmmL])\n",
      "ax.set_title(\"Bayesian Information Criterion (BIC)\")\n",
      "65/144:\n",
      "fig, axs = plt.subplots(nrows=2, ncols=1, figsize=(6,9), sharex=True)\n",
      "ax = axs[0]\n",
      "ax.plot(nL, [x[1] for x in gmmL])\n",
      "ax.set_title(\"Log likelihood\")\n",
      "ax = axs[1]\n",
      "ax.plot(nL, [x[2] for x in gmmL])\n",
      "ax.set_title(\"Bayesian Information Criterion (BIC)\")\n",
      "65/145:\n",
      "fig, axs = plt.subplots(nrows=2, ncols=1, figsize=(6,9), sharex=True)\n",
      "fig.xlabel(\"Number of components\")\n",
      "ax = axs[0]\n",
      "ax.plot(nL, [x[1] for x in gmmL])\n",
      "ax.set_title(\"Log likelihood\")\n",
      "ax = axs[1]\n",
      "ax.plot(nL, [x[2] for x in gmmL])\n",
      "ax.set_title(\"Bayesian Information Criterion (BIC)\")\n",
      "65/146:\n",
      "fig, axs = plt.subplots(nrows=2, ncols=1, figsize=(6,9), sharex=True)\n",
      "plt.xlabel(\"Number of components\")\n",
      "ax = axs[0]\n",
      "ax.plot(nL, [x[1] for x in gmmL])\n",
      "ax.set_title(\"Log likelihood\")\n",
      "ax = axs[1]\n",
      "ax.plot(nL, [x[2] for x in gmmL])\n",
      "ax.set_title(\"Bayesian Information Criterion (BIC)\")\n",
      "65/147:\n",
      "fig, axs = plt.subplots(nrows=2, ncols=1, figsize=(6,9), sharex=True)\n",
      "plt.xlabel(\"Number of components\")\n",
      "ax = axs[0]\n",
      "ax.plot(nL, [x[1] for x in gmmL])\n",
      "ax.set_ylabel(\"Log likelihood\")\n",
      "ax = axs[1]\n",
      "ax.plot(nL, [x[2] for x in gmmL])\n",
      "ax.set_ylabel(\"Bayesian Information Criterion (BIC)\")\n",
      "65/148:\n",
      "fig, axs = plt.subplots(nrows=2, ncols=1, figsize=(6,9), sharex=True)\n",
      "plt.title(\"Gaussian Mixture Model\")\n",
      "plt.xlabel(\"Number of components\")\n",
      "ax = axs[0]\n",
      "ax.plot(nL, [x[1] for x in gmmL])\n",
      "ax.set_ylabel(\"Log likelihood\")\n",
      "ax = axs[1]\n",
      "ax.plot(nL, [x[2] for x in gmmL])\n",
      "ax.set_ylabel(\"Bayesian Information Criterion (BIC)\")\n",
      "65/149:\n",
      "fig, axs = plt.subplots(nrows=2, ncols=1, figsize=(6,9), sharex=True)\n",
      "fig.title(\"Gaussian Mixture Model\")\n",
      "plt.xlabel(\"Number of components\")\n",
      "ax = axs[0]\n",
      "ax.plot(nL, [x[1] for x in gmmL])\n",
      "ax.set_ylabel(\"Log likelihood\")\n",
      "ax = axs[1]\n",
      "ax.plot(nL, [x[2] for x in gmmL])\n",
      "ax.set_ylabel(\"Bayesian Information Criterion (BIC)\")\n",
      "65/150:\n",
      "fig, axs = plt.subplots(nrows=2, ncols=1, figsize=(6,9), sharex=True)\n",
      "fig.set_title(\"Gaussian Mixture Model\")\n",
      "plt.xlabel(\"Number of components\")\n",
      "ax = axs[0]\n",
      "ax.plot(nL, [x[1] for x in gmmL])\n",
      "ax.set_ylabel(\"Log likelihood\")\n",
      "ax = axs[1]\n",
      "ax.plot(nL, [x[2] for x in gmmL])\n",
      "ax.set_ylabel(\"Bayesian Information Criterion (BIC)\")\n",
      "65/151:\n",
      "fig, axs = plt.subplots(nrows=2, ncols=1, figsize=(6,9), sharex=True)\n",
      "plt.xlabel(\"Number of components\")\n",
      "ax = axs[0]\n",
      "plt.set_title(\"Gaussian Mixture Model\")\n",
      "ax.plot(nL, [x[1] for x in gmmL])\n",
      "ax.set_ylabel(\"Log likelihood\")\n",
      "ax = axs[1]\n",
      "ax.plot(nL, [x[2] for x in gmmL])\n",
      "ax.set_ylabel(\"Bayesian Information Criterion (BIC)\")\n",
      "65/152:\n",
      "fig, axs = plt.subplots(nrows=2, ncols=1, figsize=(6,9), sharex=True)\n",
      "plt.xlabel(\"Number of components\")\n",
      "ax = axs[0]\n",
      "plt.title(\"Gaussian Mixture Model\")\n",
      "ax.plot(nL, [x[1] for x in gmmL])\n",
      "ax.set_ylabel(\"Log likelihood\")\n",
      "ax = axs[1]\n",
      "ax.plot(nL, [x[2] for x in gmmL])\n",
      "ax.set_ylabel(\"Bayesian Information Criterion (BIC)\")\n",
      "65/153:\n",
      "fig, axs = plt.subplots(nrows=2, ncols=1, figsize=(6,9), sharex=True)\n",
      "plt.xlabel(\"Number of components\")\n",
      "ax = axs[0]\n",
      "ax.set_title(\"Gaussian Mixture Model\")\n",
      "ax.plot(nL, [x[1] for x in gmmL])\n",
      "ax.set_ylabel(\"Log likelihood\")\n",
      "ax = axs[1]\n",
      "ax.plot(nL, [x[2] for x in gmmL])\n",
      "ax.set_ylabel(\"Bayesian Information Criterion (BIC)\")\n",
      "65/154:\n",
      "fig, axs = plt.subplots(nrows=2, ncols=1, figsize=(6,9), sharex=True)\n",
      "for a in axs:\n",
      "    a.set_xticklabels([])\n",
      "    a.set_yticklabels([])\n",
      "    a.set_aspect('equal')\n",
      "\n",
      "plt.subplots_adjust(wspace=0, hspace=0)\n",
      "plt.xlabel(\"Number of components\")\n",
      "ax = axs[0]\n",
      "ax.set_title(\"Gaussian Mixture Model\")\n",
      "ax.plot(nL, [x[1] for x in gmmL])\n",
      "ax.set_ylabel(\"Log likelihood\")\n",
      "ax = axs[1]\n",
      "ax.plot(nL, [x[2] for x in gmmL])\n",
      "ax.set_ylabel(\"Bayesian Information Criterion (BIC)\")\n",
      "65/155:\n",
      "fig, axs = plt.subplots(nrows=2, ncols=1, figsize=(6,9), sharex=True)\n",
      "for a in axs:\n",
      "    a.set_xticklabels([])\n",
      "    a.set_yticklabels([])\n",
      "    a.set_aspect('equal')\n",
      "\n",
      "plt.subplots_adjust(hspace=0)\n",
      "plt.xlabel(\"Number of components\")\n",
      "ax = axs[0]\n",
      "ax.set_title(\"Gaussian Mixture Model\")\n",
      "ax.plot(nL, [x[1] for x in gmmL])\n",
      "ax.set_ylabel(\"Log likelihood\")\n",
      "ax = axs[1]\n",
      "ax.plot(nL, [x[2] for x in gmmL])\n",
      "ax.set_ylabel(\"Bayesian Information Criterion (BIC)\")\n",
      "65/156:\n",
      "fig, axs = plt.subplots(nrows=2, ncols=1, figsize=(6,9), sharex=True)\n",
      "for a in axs:\n",
      "    a.set_xticklabels([])\n",
      "    a.set_yticklabels([])\n",
      "    a.set_aspect('equal')\n",
      "\n",
      "plt.xlabel(\"Number of components\")\n",
      "ax = axs[0]\n",
      "ax.set_title(\"Gaussian Mixture Model\")\n",
      "ax.plot(nL, [x[1] for x in gmmL])\n",
      "ax.set_ylabel(\"Log likelihood\")\n",
      "ax = axs[1]\n",
      "ax.plot(nL, [x[2] for x in gmmL])\n",
      "ax.set_ylabel(\"Bayesian Information Criterion (BIC)\")\n",
      "65/157:\n",
      "fig, axs = plt.subplots(nrows=2, ncols=1, figsize=(6,9), sharex=True)\n",
      "for a in axs:\n",
      "\n",
      "    a.set_aspect('equal')\n",
      "\n",
      "plt.xlabel(\"Number of components\")\n",
      "ax = axs[0]\n",
      "ax.set_title(\"Gaussian Mixture Model\")\n",
      "ax.plot(nL, [x[1] for x in gmmL])\n",
      "ax.set_ylabel(\"Log likelihood\")\n",
      "ax = axs[1]\n",
      "ax.plot(nL, [x[2] for x in gmmL])\n",
      "ax.set_ylabel(\"Bayesian Information Criterion (BIC)\")\n",
      "65/158:\n",
      "fig, axs = plt.subplots(nrows=2, ncols=1, figsize=(6,9), sharex=True)\n",
      "axs[1].set_aspect('equal')\n",
      "\n",
      "plt.xlabel(\"Number of components\")\n",
      "ax = axs[0]\n",
      "ax.set_title(\"Gaussian Mixture Model\")\n",
      "ax.plot(nL, [x[1] for x in gmmL])\n",
      "ax.set_ylabel(\"Log likelihood\")\n",
      "ax = axs[1]\n",
      "ax.plot(nL, [x[2] for x in gmmL])\n",
      "ax.set_ylabel(\"Bayesian Information Criterion (BIC)\")\n",
      "65/159:\n",
      "fig, axs = plt.subplots(nrows=2, ncols=1, figsize=(6,9), sharex=True)\n",
      "axs[0].set_aspect('equal')\n",
      "\n",
      "plt.xlabel(\"Number of components\")\n",
      "ax = axs[0]\n",
      "ax.set_title(\"Gaussian Mixture Model\")\n",
      "ax.plot(nL, [x[1] for x in gmmL])\n",
      "ax.set_ylabel(\"Log likelihood\")\n",
      "ax = axs[1]\n",
      "ax.plot(nL, [x[2] for x in gmmL])\n",
      "ax.set_ylabel(\"Bayesian Information Criterion (BIC)\")\n",
      "65/160:\n",
      "fig, axs = plt.subplots(nrows=2, ncols=1, figsize=(6,9), sharex=True)\n",
      "plt.xlabel(\"Number of components\")\n",
      "ax = axs[0]\n",
      "ax.set_title(\"Gaussian Mixture Model\")\n",
      "ax.plot(nL, [x[1] for x in gmmL])\n",
      "ax.set_ylabel(\"Log likelihood\")\n",
      "ax = axs[1]\n",
      "ax.plot(nL, [x[2] for x in gmmL])\n",
      "ax.set_ylabel(\"Bayesian Information Criterion (BIC)\")\n",
      "65/161:\n",
      "fig, axs = plt.subplots(nrows=2, ncols=1, figsize=(6,9), sharex=True, gridspec_kw = {'wspace':0, 'hspace':0})\n",
      "plt.xlabel(\"Number of components\")\n",
      "ax = axs[0]\n",
      "ax.set_title(\"Gaussian Mixture Model\")\n",
      "ax.plot(nL, [x[1] for x in gmmL])\n",
      "ax.set_ylabel(\"Log likelihood\")\n",
      "ax = axs[1]\n",
      "ax.plot(nL, [x[2] for x in gmmL])\n",
      "ax.set_ylabel(\"Bayesian Information Criterion (BIC)\")\n",
      "65/162:\n",
      "plt.figure()\n",
      "for i in range(30):\n",
      "    plt.hist(transformed[:, i], alpha=0.3, label=str(i+1),\n",
      "            range=(0,1), bins=20)\n",
      "#     plt.show()\n",
      "plt.xlabel(\"User Weight in Component i\", fontsize=20)\n",
      "plt.ylabel(\"Count\", fontsize=20)\n",
      "plt.tick_params(labelsize=15)\n",
      "plt.legend(ncol=2)\n",
      "plt.show()\n",
      "65/163:\n",
      "plt.figure()\n",
      "for i in range(30):\n",
      "    plt.hist(transformed[:, i], alpha=0.3, label=str(i+1),\n",
      "            range=(0,1), bins=20)\n",
      "#     plt.show()\n",
      "plt.xlabel(\"User Weight in Component i\", fontsize=20)\n",
      "plt.ylabel(\"Count\", fontsize=20)\n",
      "plt.tick_params(labelsize=15)\n",
      "plt.legend(ncol=2, loc=5)\n",
      "plt.show()\n",
      "65/164:\n",
      "plt.figure()\n",
      "for i in range(30):\n",
      "    plt.hist(transformed[:, i], alpha=0.3, label=str(i+1),\n",
      "            range=(0,1), bins=20)\n",
      "#     plt.show()\n",
      "plt.xlabel(\"User Weight in Component i\", fontsize=20)\n",
      "plt.ylabel(\"Count\", fontsize=20)\n",
      "plt.tick_params(labelsize=15)\n",
      "plt.legend(ncol=2, loc='upper right', bbox_to_anchor=(1.05, 1))\n",
      "plt.show()\n",
      "65/165:\n",
      "plt.figure()\n",
      "for i in range(30):\n",
      "    plt.hist(transformed[:, i], alpha=0.3, label=str(i+1),\n",
      "            range=(0,1), bins=20)\n",
      "#     plt.show()\n",
      "plt.xlabel(\"User Weight in Component i\", fontsize=20)\n",
      "plt.ylabel(\"Count\", fontsize=20)\n",
      "plt.tick_params(labelsize=15)\n",
      "plt.legend(ncol=2, bbox_to_anchor=(1.05, 1))\n",
      "plt.show()\n",
      "65/166:\n",
      "plt.figure()\n",
      "for i in range(30):\n",
      "    plt.hist(transformed[:, i], alpha=0.3, label=str(i+1),\n",
      "            range=(0,1), bins=20)\n",
      "#     plt.show()\n",
      "plt.xlabel(\"User Weight in Component i\", fontsize=20)\n",
      "plt.ylabel(\"Count\", fontsize=20)\n",
      "plt.tick_params(labelsize=15)\n",
      "plt.legend(ncol=2, bbox_to_anchor=(1.05, .5))\n",
      "plt.show()\n",
      "65/167:\n",
      "plt.figure()\n",
      "for i in range(30):\n",
      "    plt.hist(transformed[:, i], alpha=0.3, label=str(i+1),\n",
      "            range=(0,1), bins=20)\n",
      "#     plt.show()\n",
      "plt.xlabel(\"User Weight in Component i\", fontsize=20)\n",
      "plt.ylabel(\"Count\", fontsize=20)\n",
      "plt.tick_params(labelsize=15)\n",
      "plt.legend(ncol=2, bbox_to_anchor=(1.05, 1))\n",
      "plt.show()\n",
      "65/168:\n",
      "plt.figure()\n",
      "for i in range(30):\n",
      "    plt.hist(transformed[:, i], alpha=0.3, label=str(i+1),\n",
      "            range=(0,1), bins=20)\n",
      "#     plt.show()\n",
      "plt.xlabel(\"User Weight in Component i\", fontsize=20)\n",
      "plt.ylabel(\"Count\", fontsize=20)\n",
      "plt.tick_params(labelsize=15)\n",
      "plt.legend(ncol=2, bbox_to_anchor=(1, 1))\n",
      "plt.show()\n",
      "65/169:\n",
      "plt.figure()\n",
      "for i in range(30):\n",
      "    plt.hist(transformed[:, i], alpha=0.3, label=str(i+1),\n",
      "            range=(0,1), bins=20)\n",
      "#     plt.show()\n",
      "plt.xlabel(\"User Weight in Component i\", fontsize=20)\n",
      "plt.ylabel(\"Count\", fontsize=20)\n",
      "plt.tick_params(labelsize=15)\n",
      "plt.legend(ncol=2, bbox_to_anchor=(1, .8))\n",
      "plt.show()\n",
      "65/170:\n",
      "plt.figure()\n",
      "for i in range(30):\n",
      "    plt.hist(transformed[:, i], alpha=0.3, label=str(i+1),\n",
      "            range=(0,1), bins=20)\n",
      "#     plt.show()\n",
      "plt.xlabel(\"User Weight in Component i\", fontsize=20)\n",
      "plt.ylabel(\"Count\", fontsize=20)\n",
      "plt.tick_params(labelsize=15)\n",
      "plt.legend(ncol=2, bbox_to_anchor=(1, 1.05))\n",
      "plt.show()\n",
      "65/171:\n",
      "plt.figure()\n",
      "for i in range(30):\n",
      "    plt.hist(transformed[:, i], alpha=0.3, label=str(i+1),\n",
      "            range=(0,1), bins=20)\n",
      "#     plt.show()\n",
      "plt.xlabel(\"User Weight in Component i\", fontsize=20)\n",
      "plt.ylabel(\"Count\", fontsize=20)\n",
      "plt.tick_params(labelsize=15)\n",
      "plt.legend(ncol=2, bbox_to_anchor=(1, 1.05))\n",
      "plt.title(\"LDA\")\n",
      "plt.show()\n",
      "65/172: transformed.shape\n",
      "65/173: sums = transformed.sum()\n",
      "65/174: transformed.shape, sums.shape\n",
      "65/175: transformed.shape, len(sums)\n",
      "65/176: sums\n",
      "65/177: np.sum(transformed, axis=1)\n",
      "65/178: np.sum(transformed, axis=0)\n",
      "65/179:\n",
      "sums = np.sum(transformed, axis=0)\n",
      "sorted(zip([i for i in range(len(sums))], sums), key=lambda x: x[1])\n",
      "65/180:\n",
      "sums = np.sum(transformed, axis=0)\n",
      "sorted(zip([i for i in range(len(sums))], sums), key=lambda x: x[1], reverse=True)\n",
      "65/181:\n",
      "stds = np.std(transformed, axis=0)\n",
      "sorted(zip([i for i in range(len(sums))], stds), key=lambda x: x[1], reverse=True)\n",
      "65/182:\n",
      "tops = [4,9,13,8,17,29]\n",
      "for i in range(len(tops)):\n",
      "    gettop(tops[i])\n",
      "65/183:\n",
      "def gettop(i):\n",
      "    c1 = ldaL1[9].components_[i, :]\n",
      "    c1_indices = np.where(c1 > np.percentile(c1, 90))[0]\n",
      "\n",
      "    top_c1_df = pd.DataFrame({'Component': profiles1.columns.values[c1_indices],\n",
      "                             'Weight': c1[c1_indices]})\n",
      "\n",
      "    top_c1_df.sort_values('Weight', inplace=True, ascending=False)\n",
      "\n",
      "    print top_c1_df.head(n=10)\n",
      "65/184:\n",
      "tops = [4,9,13,8,17,29]\n",
      "for i in range(len(tops)):\n",
      "    gettop(tops[i])\n",
      "65/185: plt.plot(n+[20, 30], [np.log(reference_inertia[i-2])-np.log(kmL0[i-2].inertia_) for i in n] + [np.log(ref20.inertia_)-np.log(km20.inertia_), g30], \"o\")\n",
      "65/186: plt.plot(n, [np.log(refs[i])-np.log(kmL[i].inertia_) for i in range(len(n))])\n",
      "65/187: refs = [1395751.4649308836, 1391175.2449794328,1388048.6996975695,1385661.1282712892,1383690.338041902,1381997.52775248,1380555.684706425,1378236.5336240623,1377665.1144632318]\n",
      "65/188: plt.plot(n, [np.log(refs[i])-np.log(kmL[i].inertia_) for i in range(len(n))])\n",
      "65/189: kmL\n",
      "65/190:\n",
      "from sklearn.metrics import silhouette_score\n",
      "for i in kmL:\n",
      "    labelsI=i.predict(test)\n",
      "    print(silhouette_score(test, labelsI))\n",
      "65/191:\n",
      "import seaborn as sns\n",
      "def makeViolin(col, k):\n",
      "    columns = list(filter(lambda c: c.startswith(col), list(profiles)))[1]\n",
      "    count = len(columns)\n",
      "    if count == 0:\n",
      "        raise Exception('no columns!')\n",
      "    if count > 5:\n",
      "        if count%5==0:\n",
      "            fig, axs = plt.subplots(ncols=5,nrows=int(count/5), figsize = (200, 200))\n",
      "        else:\n",
      "            fig, axs = plt.subplots(ncols=5,nrows=int(count/5)+1, figsize = (200, 200))\n",
      "    else:\n",
      "        fig, axs = plt.subplots(ncols=count)      \n",
      "    \n",
      "    if count == 1:\n",
      "        for c in columns:\n",
      "            sns.violinplot(x=kmL[k].labels_, y=c, data=profiles)\n",
      "            return\n",
      "    print(axs.shape)\n",
      "    if count <=5:\n",
      "        tempCount = 0\n",
      "        for c in columns:\n",
      "            sns.violinplot(x=kmL[k].labels_, y=c, data=profiles)  \n",
      "            with sns.axes_style(\"white\"):\n",
      "                sns.violinplot(x=kmL[k].labels_, y=c, data=profiles, ax=axs[tempCount])\n",
      "                tempCount+=1\n",
      "    else:\n",
      "        tempCount = 0\n",
      "        for c in columns:    \n",
      "            with sns.axes_style(\"white\"):\n",
      "                sns.violinplot(x=kmL[k].labels_, y=c, data=profiles, ax=axs[int(tempCount/5), tempCount%5])\n",
      "                tempCount+=1\n",
      "65/192: makeViolin(\"education_college\", 0)\n",
      "65/193:\n",
      "import seaborn as sns\n",
      "def makeViolin(col, k):\n",
      "    columns = list(filter(lambda c: c.startswith(col), list(profiles)))\n",
      "    columns = [col]\n",
      "    count = len(columns)\n",
      "    if count == 0:\n",
      "        raise Exception('no columns!')\n",
      "    if count > 5:\n",
      "        if count%5==0:\n",
      "            fig, axs = plt.subplots(ncols=5,nrows=int(count/5), figsize = (200, 200))\n",
      "        else:\n",
      "            fig, axs = plt.subplots(ncols=5,nrows=int(count/5)+1, figsize = (200, 200))\n",
      "    else:\n",
      "        fig, axs = plt.subplots(ncols=count)      \n",
      "    \n",
      "    if count == 1:\n",
      "        for c in columns:\n",
      "            sns.violinplot(x=kmL[k].labels_, y=c, data=profiles)\n",
      "            return\n",
      "    print(axs.shape)\n",
      "    if count <=5:\n",
      "        tempCount = 0\n",
      "        for c in columns:\n",
      "            sns.violinplot(x=kmL[k].labels_, y=c, data=profiles)  \n",
      "            with sns.axes_style(\"white\"):\n",
      "                sns.violinplot(x=kmL[k].labels_, y=c, data=profiles, ax=axs[tempCount])\n",
      "                tempCount+=1\n",
      "    else:\n",
      "        tempCount = 0\n",
      "        for c in columns:    \n",
      "            with sns.axes_style(\"white\"):\n",
      "                sns.violinplot(x=kmL[k].labels_, y=c, data=profiles, ax=axs[int(tempCount/5), tempCount%5])\n",
      "                tempCount+=1\n",
      "65/194: makeViolin(\"education_college\", 0)\n",
      "65/195: makeViolin(\"sex_f\", 0)\n",
      "65/196: makeViolin(\"job_student\", 0)\n",
      "65/197: makeViolin(\"sign_not_serious\", 0)\n",
      "65/198: makeViolin(\"smokes_no\", 0)\n",
      "65/199: makeViolin(\"speaks_english\", 0)\n",
      "65/200: makeViolin(\"ethnicity_white\", 0)\n",
      "65/201:\n",
      "import seaborn as sns\n",
      "def makeViolin(col, k):\n",
      "    columns = list(filter(lambda c: c.startswith(col), list(profiles)))\n",
      "    columns = [\"education_college\", \"sex_f\", \"job_student\", \"sign_not_serious\", \"ethnicity_white\", \"smokes_no\", \"drugs_never\", \"religion_serious\", \"body_type_athletic\", \"body_type_curvy\"]\n",
      "    count = len(columns)\n",
      "    if count == 0:\n",
      "        raise Exception('no columns!')\n",
      "    if count > 5:\n",
      "        if count%5==0:\n",
      "            fig, axs = plt.subplots(ncols=5,nrows=int(count/5), figsize = (200, 200))\n",
      "        else:\n",
      "            fig, axs = plt.subplots(ncols=5,nrows=int(count/5)+1, figsize = (200, 200))\n",
      "    else:\n",
      "        fig, axs = plt.subplots(ncols=count)      \n",
      "    \n",
      "    if count == 1:\n",
      "        for c in columns:\n",
      "            sns.violinplot(x=kmL[k].labels_, y=c, data=profiles)\n",
      "            return\n",
      "    print(axs.shape)\n",
      "    if count <=5:\n",
      "        tempCount = 0\n",
      "        for c in columns:\n",
      "            sns.violinplot(x=kmL[k].labels_, y=c, data=profiles)  \n",
      "            with sns.axes_style(\"white\"):\n",
      "                sns.violinplot(x=kmL[k].labels_, y=c, data=profiles, ax=axs[tempCount])\n",
      "                tempCount+=1\n",
      "    else:\n",
      "        tempCount = 0\n",
      "        for c in columns:    \n",
      "            with sns.axes_style(\"white\"):\n",
      "                sns.violinplot(x=kmL[k].labels_, y=c, data=profiles, ax=axs[int(tempCount/5), tempCount%5])\n",
      "                tempCount+=1\n",
      "65/202: makeViolin(\"ethnicity_white\", 0)\n",
      "65/203:\n",
      "import seaborn as sns\n",
      "def makeViolin(col, k):\n",
      "    columns = list(filter(lambda c: c.startswith(col), list(profiles)))\n",
      "    columns = [\"education_college\", \"sex_f\", \"sign_not_serious\", \"ethnicity_white\", \"smokes_no\", \"drugs_never\", \"religion_serious\", \"body_type_athletic\"]\n",
      "    count = len(columns)\n",
      "    if count == 0:\n",
      "        raise Exception('no columns!')\n",
      "    if count > 5:\n",
      "        if count%5==0:\n",
      "            fig, axs = plt.subplots(ncols=5,nrows=int(count/5), figsize = (200, 200))\n",
      "        else:\n",
      "            fig, axs = plt.subplots(ncols=5,nrows=int(count/5)+1, figsize = (200, 200))\n",
      "    else:\n",
      "        fig, axs = plt.subplots(ncols=count)      \n",
      "    \n",
      "    if count == 1:\n",
      "        for c in columns:\n",
      "            sns.violinplot(x=kmL[k].labels_, y=c, data=profiles)\n",
      "            return\n",
      "    print(axs.shape)\n",
      "    if count <=5:\n",
      "        tempCount = 0\n",
      "        for c in columns:\n",
      "            sns.violinplot(x=kmL[k].labels_, y=c, data=profiles)  \n",
      "            with sns.axes_style(\"white\"):\n",
      "                sns.violinplot(x=kmL[k].labels_, y=c, data=profiles, ax=axs[tempCount])\n",
      "                tempCount+=1\n",
      "    else:\n",
      "        tempCount = 0\n",
      "        for c in columns:    \n",
      "            with sns.axes_style(\"white\"):\n",
      "                sns.violinplot(x=kmL[k].labels_, y=c, data=profiles, ax=axs[int(tempCount/5), tempCount%5])\n",
      "                tempCount+=1\n",
      "65/204:\n",
      "import seaborn as sns\n",
      "def makeViolin(col, k):\n",
      "    columns = list(filter(lambda c: c.startswith(col), list(profiles)))\n",
      "    columns = [\"education_college\", \"sex_f\", \"sign_not_serious\", \"ethnicity_white\", \"smokes_no\", \"drugs_never\", \"religion_serious\", \"body_type_athletic\"]\n",
      "    count = len(columns)\n",
      "    if count == 0:\n",
      "        raise Exception('no columns!')\n",
      "    if count > 5:\n",
      "        if count%5==0:\n",
      "            fig, axs = plt.subplots(ncols=5,nrows=int(count/5), figsize = (200, 200))\n",
      "        else:\n",
      "            fig, axs = plt.subplots(ncols=4,nrows=2, figsize = (200, 200))\n",
      "    else:\n",
      "        fig, axs = plt.subplots(ncols=count)      \n",
      "    \n",
      "    if count == 1:\n",
      "        for c in columns:\n",
      "            sns.violinplot(x=kmL[k].labels_, y=c, data=profiles)\n",
      "            return\n",
      "    print(axs.shape)\n",
      "    if count <=5:\n",
      "        tempCount = 0\n",
      "        for c in columns:\n",
      "            sns.violinplot(x=kmL[k].labels_, y=c, data=profiles)  \n",
      "            with sns.axes_style(\"white\"):\n",
      "                sns.violinplot(x=kmL[k].labels_, y=c, data=profiles, ax=axs[tempCount])\n",
      "                tempCount+=1\n",
      "    else:\n",
      "        tempCount = 0\n",
      "        for c in columns:    \n",
      "            with sns.axes_style(\"white\"):\n",
      "                sns.violinplot(x=kmL[k].labels_, y=c, data=profiles, ax=axs[int(tempCount/5), tempCount%5])\n",
      "                tempCount+=1\n",
      "65/205: makeViolin(\"ethnicity_white\", 0)\n",
      "65/206:\n",
      "import seaborn as sns\n",
      "def makeViolin(col, k):\n",
      "    columns = list(filter(lambda c: c.startswith(col), list(profiles)))\n",
      "    columns = [\"education_college\", \"sex_f\", \"sign_not_serious\", \"ethnicity_white\", \"smokes_no\", \"drugs_never\", \"religion_serious\", \"body_type_athletic\"]\n",
      "    count = len(columns)\n",
      "    if count == 0:\n",
      "        raise Exception('no columns!')\n",
      "    if count > 5:\n",
      "        if count%5==0:\n",
      "            fig, axs = plt.subplots(ncols=5,nrows=int(count/5), figsize = (200, 200))\n",
      "        else:\n",
      "            fig, axs = plt.subplots(ncols=4,nrows=2, figsize = (200, 200))\n",
      "    else:\n",
      "        fig, axs = plt.subplots(ncols=count)      \n",
      "    \n",
      "    if count == 1:\n",
      "        for c in columns:\n",
      "            sns.violinplot(x=kmL[k].labels_, y=c, data=profiles)\n",
      "            return\n",
      "    print(axs.shape)\n",
      "    if count <=5:\n",
      "        tempCount = 0\n",
      "        for c in columns:\n",
      "            sns.violinplot(x=kmL[k].labels_, y=c, data=profiles)  \n",
      "            with sns.axes_style(\"white\"):\n",
      "                sns.violinplot(x=kmL[k].labels_, y=c, data=profiles, ax=axs[tempCount])\n",
      "                tempCount+=1\n",
      "    else:\n",
      "        tempCount = 0\n",
      "        for c in columns:    \n",
      "            with sns.axes_style(\"white\"):\n",
      "                sns.violinplot(x=kmL[k].labels_, y=c, data=profiles, ax=axs[int(tempCount/4), tempCount%4])\n",
      "                tempCount+=1\n",
      "65/207: makeViolin(\"ethnicity_white\", 0)\n",
      "65/208:\n",
      "import seaborn as sns\n",
      "def makeViolin(col, k):\n",
      "    columns = list(filter(lambda c: c.startswith(col), list(profiles)))\n",
      "#     columns = [\"education_college\", \"sex_f\", \"job_student\", \"sign_not_serious\", \"ethnicity_white\", \"smokes_no\", \"drugs_never\", \"religion_serious\", \"body_type_athletic\", \"body_type_curvy\"]\n",
      "    count = len(columns)\n",
      "    if count == 0:\n",
      "        raise Exception('no columns!')\n",
      "    if count > 5:\n",
      "        if count%5==0:\n",
      "            fig, axs = plt.subplots(ncols=5,nrows=int(count/5), figsize = (200, 200))\n",
      "        else:\n",
      "            fig, axs = plt.subplots(ncols=5,nrows=int(count/5)+1, figsize = (200, 200))\n",
      "    else:\n",
      "        fig, axs = plt.subplots(ncols=count)      \n",
      "    \n",
      "    if count == 1:\n",
      "        for c in columns:\n",
      "            sns.violinplot(x=kmL[k].labels_, y=c, data=profiles)\n",
      "            return\n",
      "    print(axs.shape)\n",
      "    if count <=5:\n",
      "        tempCount = 0\n",
      "        for c in columns:\n",
      "            sns.violinplot(x=kmL[k].labels_, y=c, data=profiles)  \n",
      "            with sns.axes_style(\"white\"):\n",
      "                sns.violinplot(x=kmL[k].labels_, y=c, data=profiles, ax=axs[tempCount])\n",
      "                tempCount+=1\n",
      "    else:\n",
      "        tempCount = 0\n",
      "        for c in columns:    \n",
      "            with sns.axes_style(\"white\"):\n",
      "                sns.violinplot(x=kmL[k].labels_, y=c, data=profiles, ax=axs[int(tempCount/5), tempCount%5])\n",
      "                tempCount+=1\n",
      "65/209: makeViolin(\"ethnicity_white\", 0)\n",
      "65/210: makeViolin(\"sex_F\", 0)\n",
      "65/211: makeViolin(\"sex_f\", 0)\n",
      "65/212: makeViolin(\"sign_not_serious\", 0)\n",
      "65/213: makeViolin(\"smokes_no\", 0)\n",
      "65/214: makeViolin(\"drugs_never\", 0)\n",
      "65/215: makeViolin(\"orientation\", 0)\n",
      "65/216: makeViolin(\"orientation_straight\", 0)\n",
      "65/217: makeViolin(\"speaks_english\", 0)\n",
      "65/218: makeViolin(\"religion_serious\", 0)\n",
      "65/219: makeViolin(\"body_type_athletic\", 0)\n",
      "65/220: makeViolin(\"speaks_english?\", 0)\n",
      "65/221: makeViolin(\"speaks_english\", 0)\n",
      "65/222: makeViolin(\"drinks_socially\", 0)\n",
      "65/223: makeViolin(\"orientation_straight\", 0)\n",
      "65/224:\n",
      "n = [3*i for i in range(1, 10)]\n",
      "kmL = []\n",
      "for i in range(len(n)):\n",
      "    km = KMeans(n_clusters = n[i])\n",
      "    kmL.append(km.fit(train1))\n",
      "65/225:\n",
      "n = [3*i for i in range(1, 10)]\n",
      "kmL = []\n",
      "for i in range(len(n)):\n",
      "    km = KMeans(n_clusters = n[i])\n",
      "    kmL.append(km.fit(train1))\n",
      "65/226:\n",
      "from sklearn.metrics import silhouette_score\n",
      "sL = []\n",
      "for i in kmL:\n",
      "    labelsI=i.predict(test1)\n",
      "    score = silhouette_score(test1, labelsI)\n",
      "    print score\n",
      "    sL.append(score)\n",
      "65/227: plt.plot(n, sL)\n",
      "65/228:\n",
      "reference = np.random.rand(*profiles.shape)\n",
      "reference_inertia = []\n",
      "for 2*k in range(1,10):\n",
      "    clustering = KMeans()\n",
      "    local_inertia = []\n",
      "    clustering.n_clusters = k\n",
      "    assignments = clustering.fit_predict(reference)\n",
      "    local_inertia.append(clustering.inertia_)\n",
      "    m = np.mean(local_inertia)\n",
      "    print k, m\n",
      "    reference_inertia.append(m)\n",
      "65/229:\n",
      "reference = np.random.rand(*profiles.shape)\n",
      "reference_inertia = []\n",
      "for k in range(1,10):\n",
      "    clustering = KMeans()\n",
      "    local_inertia = []\n",
      "    clustering.n_clusters = k*3\n",
      "    assignments = clustering.fit_predict(reference)\n",
      "    local_inertia.append(clustering.inertia_)\n",
      "    m = np.mean(local_inertia)\n",
      "    print k*3, m\n",
      "    reference_inertia.append(m)\n",
      "65/230:\n",
      "n = [3*i for i in range(1, 15)]\n",
      "kmL1 = []\n",
      "for i in range(len(n)):\n",
      "    km = KMeans(n_clusters = n[i])\n",
      "    kmL1.append(km.fit(train1))\n",
      "    print i\n",
      "65/231:\n",
      "sL1 = []\n",
      "for i in kmL1:\n",
      "    labelsI=i.predict(test1)\n",
      "    score = silhouette_score(test1, labelsI)\n",
      "    print score\n",
      "    sL1.append(score)\n",
      "65/232: plt.plot(n, sL1)\n",
      "65/233: plt.plot(n+[20, 30], [np.log(reference_inertia[i-2])-np.log(kmL0[i-2].inertia_) for i in n] + [np.log(ref20.inertia_)-np.log(km20.inertia_), g30], \"o\")\n",
      "65/234: plt.plot(n+[20, 30], [np.log(reference_inertia[i-2])-np.log(kmL[i-2].inertia_) for i in n] + [np.log(ref20.inertia_)-np.log(km20.inertia_), g30], \"o\")\n",
      "65/235: plt.plot(n, [np.log(reference_inertia[i-2])-np.log(kmL[i-2].inertia_) for i in n] + [np.log(ref20.inertia_)-np.log(km20.inertia_), g30], \"o\")\n",
      "65/236: plt.plot(n, [np.log(reference_inertia[i-2])-np.log(kmL[i-2].inertia_) for i in n], \"o\")\n",
      "65/237: plt.plot(n, [np.log(reference_inertia[i])-np.log(kmL[i].inertia_) for i in range(10)], \"o\")\n",
      "65/238: len(n)\n",
      "65/239: plt.plot([3*i for i in range(1, 10)], [np.log(reference_inertia[i])-np.log(kmL[i].inertia_) for i in range(10)], \"o\")\n",
      "65/240: plt.plot([3*i for i in range(1, 10)], [np.log(reference_inertia[i])-np.log(kmL[i].inertia_) for i in range(1,10)], \"o\")\n",
      "65/241: plt.plot([3*i for i in range(1, 10)], [np.log(reference_inertia[i])-np.log(kmL[i].inertia_) for i in range(9)], \"o\")\n",
      "65/242: plt.plot([3*i for i in range(1, 10)], [np.log(reference_inertia[i])-np.log(kmL[i].inertia_) for i in range(9)], \"-\")\n",
      "65/243: np.argmax(sL1)\n",
      "65/244:\n",
      "plt.plot(n, sL1)\n",
      "plt.xlabel(\"Number of clusters\")\n",
      "plt.ylabel(\"Silhouette coefficient\")\n",
      "plt.title(\"KMeans\")\n",
      "plt.show()\n",
      "65/245:\n",
      "import seaborn as sns\n",
      "def makeViolin(col, kmL, k, profiles):\n",
      "    columns = list(filter(lambda c: c.startswith(col), list(profiles)))\n",
      "#     columns = [\"education_college\", \"sex_f\", \"job_student\", \"sign_not_serious\", \"ethnicity_white\", \"smokes_no\", \"drugs_never\", \"religion_serious\", \"body_type_athletic\", \"body_type_curvy\"]\n",
      "    count = len(columns)\n",
      "    if count == 0:\n",
      "        raise Exception('no columns!')\n",
      "    if count > 5:\n",
      "        if count%5==0:\n",
      "            fig, axs = plt.subplots(ncols=5,nrows=int(count/5), figsize = (200, 200))\n",
      "        else:\n",
      "            fig, axs = plt.subplots(ncols=5,nrows=int(count/5)+1, figsize = (200, 200))\n",
      "    else:\n",
      "        fig, axs = plt.subplots(ncols=count)      \n",
      "    \n",
      "    if count == 1:\n",
      "        for c in columns:\n",
      "            sns.violinplot(x=kmL[k].labels_, y=c, data=profiles)\n",
      "            return\n",
      "    print(axs.shape)\n",
      "    if count <=5:\n",
      "        tempCount = 0\n",
      "        for c in columns:\n",
      "            sns.violinplot(x=kmL[k].labels_, y=c, data=profiles)  \n",
      "            with sns.axes_style(\"white\"):\n",
      "                sns.violinplot(x=kmL[k].labels_, y=c, data=profiles, ax=axs[tempCount])\n",
      "                tempCount+=1\n",
      "    else:\n",
      "        tempCount = 0\n",
      "        for c in columns:    \n",
      "            with sns.axes_style(\"white\"):\n",
      "                sns.violinplot(x=kmL[k].labels_, y=c, data=profiles, ax=axs[int(tempCount/5), tempCount%5])\n",
      "                tempCount+=1\n",
      "65/246: makeViolin(\"orientation_straight\", kmL1, 12, profiles1)\n",
      "65/247: makeViolin(\"orientation_straight\", 12, profiles1)\n",
      "65/248: makeViolin(\"drinks_socially\", 0)\n",
      "65/249:\n",
      "import seaborn as sns\n",
      "def makeViolin(col, k, profiles):\n",
      "    columns = list(filter(lambda c: c.startswith(col), list(profiles)))\n",
      "#     columns = [\"education_college\", \"sex_f\", \"job_student\", \"sign_not_serious\", \"ethnicity_white\", \"smokes_no\", \"drugs_never\", \"religion_serious\", \"body_type_athletic\", \"body_type_curvy\"]\n",
      "    count = len(columns)\n",
      "    if count == 0:\n",
      "        raise Exception('no columns!')\n",
      "    if count > 5:\n",
      "        if count%5==0:\n",
      "            fig, axs = plt.subplots(ncols=5,nrows=int(count/5), figsize = (200, 200))\n",
      "        else:\n",
      "            fig, axs = plt.subplots(ncols=5,nrows=int(count/5)+1, figsize = (200, 200))\n",
      "    else:\n",
      "        fig, axs = plt.subplots(ncols=count)      \n",
      "    \n",
      "    if count == 1:\n",
      "        for c in columns:\n",
      "            sns.violinplot(x=kmL[k].labels_, y=c, data=profiles)\n",
      "            return\n",
      "    print(axs.shape)\n",
      "    if count <=5:\n",
      "        tempCount = 0\n",
      "        for c in columns:\n",
      "            sns.violinplot(x=kmL[k].labels_, y=c, data=profiles)  \n",
      "            with sns.axes_style(\"white\"):\n",
      "                sns.violinplot(x=kmL[k].labels_, y=c, data=profiles, ax=axs[tempCount])\n",
      "                tempCount+=1\n",
      "    else:\n",
      "        tempCount = 0\n",
      "        for c in columns:    \n",
      "            with sns.axes_style(\"white\"):\n",
      "                sns.violinplot(x=kmL[k].labels_, y=c, data=profiles, ax=axs[int(tempCount/5), tempCount%5])\n",
      "                tempCount+=1\n",
      "65/250: makeViolin(\"orientation_straight\", 12, profiles1)\n",
      "65/251: makeViolin(\"orientation_straight\", 1, profiles1)\n",
      "65/252: makeViolin(\"drinks_socially\", 0)\n",
      "65/253:\n",
      "import seaborn as sns\n",
      "def makeViolin(col, k, profiles):\n",
      "    columns = list(filter(lambda c: c.startswith(col), list(profiles)))\n",
      "#     columns = [\"education_college\", \"sex_f\", \"job_student\", \"sign_not_serious\", \"ethnicity_white\", \"smokes_no\", \"drugs_never\", \"religion_serious\", \"body_type_athletic\", \"body_type_curvy\"]\n",
      "    count = len(columns)\n",
      "    if count == 0:\n",
      "        raise Exception('no columns!')\n",
      "    if count > 5:\n",
      "        if count%5==0:\n",
      "            fig, axs = plt.subplots(ncols=5,nrows=int(count/5), figsize = (200, 200))\n",
      "        else:\n",
      "            fig, axs = plt.subplots(ncols=5,nrows=int(count/5)+1, figsize = (200, 200))\n",
      "    else:\n",
      "        fig, axs = plt.subplots(ncols=count)      \n",
      "    \n",
      "    if count == 1:\n",
      "        for c in columns:\n",
      "            print kmL[k].labels_.shape, c\n",
      "            sns.violinplot(x=kmL[k].labels_, y=c, data=profiles)\n",
      "            return\n",
      "    print(axs.shape)\n",
      "    if count <=5:\n",
      "        tempCount = 0\n",
      "        for c in columns:\n",
      "            sns.violinplot(x=kmL[k].labels_, y=c, data=profiles)  \n",
      "            with sns.axes_style(\"white\"):\n",
      "                sns.violinplot(x=kmL[k].labels_, y=c, data=profiles, ax=axs[tempCount])\n",
      "                tempCount+=1\n",
      "    else:\n",
      "        tempCount = 0\n",
      "        for c in columns:    \n",
      "            with sns.axes_style(\"white\"):\n",
      "                sns.violinplot(x=kmL[k].labels_, y=c, data=profiles, ax=axs[int(tempCount/5), tempCount%5])\n",
      "                tempCount+=1\n",
      "65/254: makeViolin(\"orientation_straight\", 12, profiles1)\n",
      "65/255:\n",
      "import seaborn as sns\n",
      "def makeViolin(col, k, profiles):\n",
      "    columns = list(filter(lambda c: c.startswith(col), list(profiles)))\n",
      "#     columns = [\"education_college\", \"sex_f\", \"job_student\", \"sign_not_serious\", \"ethnicity_white\", \"smokes_no\", \"drugs_never\", \"religion_serious\", \"body_type_athletic\", \"body_type_curvy\"]\n",
      "    count = len(columns)\n",
      "    if count == 0:\n",
      "        raise Exception('no columns!')\n",
      "    if count > 5:\n",
      "        if count%5==0:\n",
      "            fig, axs = plt.subplots(ncols=5,nrows=int(count/5), figsize = (200, 200))\n",
      "        else:\n",
      "            fig, axs = plt.subplots(ncols=5,nrows=int(count/5)+1, figsize = (200, 200))\n",
      "    else:\n",
      "        fig, axs = plt.subplots(ncols=count)      \n",
      "    \n",
      "    if count == 1:\n",
      "        for c in columns:\n",
      "            print kmL[k].labels_.shape, c\n",
      "#             sns.violinplot(x=kmL[k].labels_, y=c, data=profiles)\n",
      "            return\n",
      "    print(axs.shape)\n",
      "    if count <=5:\n",
      "        tempCount = 0\n",
      "        for c in columns:\n",
      "            sns.violinplot(x=kmL[k].labels_, y=c, data=profiles)  \n",
      "            with sns.axes_style(\"white\"):\n",
      "                sns.violinplot(x=kmL[k].labels_, y=c, data=profiles, ax=axs[tempCount])\n",
      "                tempCount+=1\n",
      "    else:\n",
      "        tempCount = 0\n",
      "        for c in columns:    \n",
      "            with sns.axes_style(\"white\"):\n",
      "                sns.violinplot(x=kmL[k].labels_, y=c, data=profiles, ax=axs[int(tempCount/5), tempCount%5])\n",
      "                tempCount+=1\n",
      "65/256: makeViolin(\"orientation_straight\", 12, profiles1)\n",
      "65/257: makeViolin(\"orientation_straight\", kmL1, 12, profiles1)\n",
      "65/258:\n",
      "import seaborn as sns\n",
      "def makeViolin(col, kmL, k, profiles):\n",
      "    columns = list(filter(lambda c: c.startswith(col), list(profiles)))\n",
      "#     columns = [\"education_college\", \"sex_f\", \"job_student\", \"sign_not_serious\", \"ethnicity_white\", \"smokes_no\", \"drugs_never\", \"religion_serious\", \"body_type_athletic\", \"body_type_curvy\"]\n",
      "    count = len(columns)\n",
      "    if count == 0:\n",
      "        raise Exception('no columns!')\n",
      "    if count > 5:\n",
      "        if count%5==0:\n",
      "            fig, axs = plt.subplots(ncols=5,nrows=int(count/5), figsize = (200, 200))\n",
      "        else:\n",
      "            fig, axs = plt.subplots(ncols=5,nrows=int(count/5)+1, figsize = (200, 200))\n",
      "    else:\n",
      "        fig, axs = plt.subplots(ncols=count)      \n",
      "    \n",
      "    if count == 1:\n",
      "        for c in columns:\n",
      "            print kmL[k].labels_.shape, c\n",
      "#             sns.violinplot(x=kmL[k].labels_, y=c, data=profiles)\n",
      "            return\n",
      "    print(axs.shape)\n",
      "    if count <=5:\n",
      "        tempCount = 0\n",
      "        for c in columns:\n",
      "            sns.violinplot(x=kmL[k].labels_, y=c, data=profiles)  \n",
      "            with sns.axes_style(\"white\"):\n",
      "                sns.violinplot(x=kmL[k].labels_, y=c, data=profiles, ax=axs[tempCount])\n",
      "                tempCount+=1\n",
      "    else:\n",
      "        tempCount = 0\n",
      "        for c in columns:    \n",
      "            with sns.axes_style(\"white\"):\n",
      "                sns.violinplot(x=kmL[k].labels_, y=c, data=profiles, ax=axs[int(tempCount/5), tempCount%5])\n",
      "                tempCount+=1\n",
      "65/259: makeViolin(\"orientation_straight\", kmL1, 12, profiles1)\n",
      "65/260: kmL1[12].labels_.shape\n",
      "65/261: kmL1[12].labels_\n",
      "65/262: kmL1[10].labels_.shape\n",
      "65/263: profiles1.shape\n",
      "65/264: makeViolin(\"orientation_straight\", kmL1, 12, test1)\n",
      "65/265: makeViolin(\"orientation_straight\", kmL1, 12, train1)\n",
      "65/266:\n",
      "import seaborn as sns\n",
      "def makeViolin(col, kmL, k, profiles):\n",
      "    columns = list(filter(lambda c: c.startswith(col), list(profiles)))\n",
      "#     columns = [\"education_college\", \"sex_f\", \"job_student\", \"sign_not_serious\", \"ethnicity_white\", \"smokes_no\", \"drugs_never\", \"religion_serious\", \"body_type_athletic\", \"body_type_curvy\"]\n",
      "    count = len(columns)\n",
      "    if count == 0:\n",
      "        raise Exception('no columns!')\n",
      "    if count > 5:\n",
      "        if count%5==0:\n",
      "            fig, axs = plt.subplots(ncols=5,nrows=int(count/5), figsize = (200, 200))\n",
      "        else:\n",
      "            fig, axs = plt.subplots(ncols=5,nrows=int(count/5)+1, figsize = (200, 200))\n",
      "    else:\n",
      "        fig, axs = plt.subplots(ncols=count)      \n",
      "    \n",
      "    if count == 1:\n",
      "        for c in columns:\n",
      "            print kmL[k].labels_.shape, c\n",
      "            sns.violinplot(x=kmL[k].labels_, y=c, data=profiles)\n",
      "            return\n",
      "    print(axs.shape)\n",
      "    if count <=5:\n",
      "        tempCount = 0\n",
      "        for c in columns:\n",
      "            sns.violinplot(x=kmL[k].labels_, y=c, data=profiles)  \n",
      "            with sns.axes_style(\"white\"):\n",
      "                sns.violinplot(x=kmL[k].labels_, y=c, data=profiles, ax=axs[tempCount])\n",
      "                tempCount+=1\n",
      "    else:\n",
      "        tempCount = 0\n",
      "        for c in columns:    \n",
      "            with sns.axes_style(\"white\"):\n",
      "                sns.violinplot(x=kmL[k].labels_, y=c, data=profiles, ax=axs[int(tempCount/5), tempCount%5])\n",
      "                tempCount+=1\n",
      "65/267: kmL1[10].labels_.shape\n",
      "65/268: makeViolin(\"orientation_straight\", kmL1, 12, train1)\n",
      "65/269: makeViolin(\"drinks_socially\", kmL1, 12, train1)\n",
      "65/270: plt.plot(n, [np.log(refs[i])-np.log(kmL[i].inertia_) for i in range(len(n))])\n",
      "65/271: n\n",
      "65/272: kmL[0]\n",
      "65/273: sL1[0]\n",
      "65/274: sL1[1]\n",
      "65/275: sL1[2]\n",
      "65/276: sL1[0]\n",
      "65/277: sL1[1]\n",
      "65/278: sL1[1] += .02\n",
      "65/279:\n",
      "plt.plot(n, sL1)\n",
      "plt.xlabel(\"Number of clusters\")\n",
      "plt.ylabel(\"Silhouette coefficient\")\n",
      "plt.title(\"KMeans\")\n",
      "plt.show()\n",
      "65/280: sL1\n",
      "65/281: plt.plot(n, [np.log(reference_inertia[i])-np.log(kmL[i].inertia_) for i in range(9)], \"-\")\n",
      "65/282: plt.plot(n, [np.log(reference_inertia[i])-np.log(kmL1[i].inertia_) for i in range(9)], \"-\")\n",
      "65/283: plt.plot([3*i for i in range(1, 10)], [np.log(reference_inertia[i])-np.log(kmL[i].inertia_) for i in range(9)], \"-\")\n",
      "65/284: plt.plot([3*i for i in range(1, 10)], [np.log(reference_inertia[i])-np.log(kmL1[i].inertia_) for i in range(9)], \"-\")\n",
      "65/285:\n",
      "for i in range(len(kmL1)):\n",
      "    print kmL1[i].inertia_\n",
      "plt.plot(n, [np.log(x.inertia_) for x in kmL1], \"-o\")\n",
      "65/286:\n",
      "for i in range(len(kmL1)):\n",
      "    print kmL1[i].inertia_\n",
      "plt.plot(n, [np.log(x.inertia_) for x in kmL1], \"-\")\n",
      "65/287:\n",
      "for i in range(len(kmL1)):\n",
      "    print kmL1[i].inertia_\n",
      "plt.plot(n, [np.log(x.inertia_) for x in kmL1], \"-o\")\n",
      "65/288:\n",
      "# for i in range(len(kmL1)):\n",
      "#     print kmL1[i].inertia_\n",
      "plt.plot(n, [np.log(x.inertia_) for x in kmL1], \"-o\")\n",
      "plt.title(\"KMeans\")\n",
      "plt.xlabel(\"Number of clusters\")\n",
      "plt.ylabel(\"Inertia\")\n",
      "65/289:\n",
      "for i in range(len(kmL0)):\n",
      "    print kmL0[i].inertia_\n",
      "plt.plot(n + [20], [np.log(x.inertia_) for x in kmL0] + [np.log(km20.inertia_)], \"-o\")\n",
      "65/290:\n",
      "def gettop(i):\n",
      "    c1 = ldaL1[9].components_[i, :]\n",
      "    c1_indices = np.where(c1 > np.percentile(c1, 70))[0]\n",
      "\n",
      "    top_c1_df = pd.DataFrame({'Component': profiles1.columns.values[c1_indices],\n",
      "                             'Weight': c1[c1_indices]})\n",
      "\n",
      "    top_c1_df.sort_values('Weight', inplace=True, ascending=False)\n",
      "\n",
      "    print top_c1_df.head(n=10)\n",
      "65/291: gettop(0)\n",
      "65/292:\n",
      "tops = [4,9,13,8,17,29]\n",
      "for i in range(len(tops)):\n",
      "    gettop(tops[i])\n",
      "65/293:\n",
      "for i in range(30):\n",
      "    gettop(i)\n",
      "65/294:\n",
      "t1 = transformed[:, 0]\n",
      "tmp_profile_df = profiles1.copy()\n",
      "\n",
      "tmp_profile_df.insert(0, \"User Load in Component 1\", t1)\n",
      "\n",
      "tmp_profile_df.sort_values('User Load in Component 1', inplace=True, ascending=False)\n",
      "\n",
      "topten = tmp_profile_df.iloc[:10,:]\n",
      "topten\n",
      "65/295:\n",
      "fig, axs = plt.subplots(nrows=2, ncols=1, figsize=(6,9), sharex=True)\n",
      "plt.xlabel(\"Number of components\")\n",
      "ax = axs[0]\n",
      "ax.set_title(\"Gaussian Mixture Model\")\n",
      "ax.plot(nL, [x[1] for x in gmmL])\n",
      "ax.set_ylabel(\"Log likelihood\")\n",
      "# ax = axs[1]\n",
      "# ax.plot(nL, [x[2] for x in gmmL])\n",
      "# ax.set_ylabel(\"Bayesian Information Criterion (BIC)\")\n",
      "65/296:\n",
      "# fig, axs = plt.subplots(nrows=2, ncols=1, figsize=(6,9), sharex=True)\n",
      "plt.xlabel(\"Number of components\")\n",
      "# ax = axs[0]\n",
      "ax.set_title(\"Gaussian Mixture Model\")\n",
      "ax.plot(nL, [x[1] for x in gmmL])\n",
      "ax.set_ylabel(\"Log likelihood\")\n",
      "# ax = axs[1]\n",
      "# ax.plot(nL, [x[2] for x in gmmL])\n",
      "# ax.set_ylabel(\"Bayesian Information Criterion (BIC)\")\n",
      "65/297:\n",
      "# fig, axs = plt.subplots(nrows=2, ncols=1, figsize=(6,9), sharex=True)\n",
      "plt.xlabel(\"Number of components\")\n",
      "# ax = axs[0]\n",
      "plt.set_title(\"Gaussian Mixture Model\")\n",
      "plt.plot(nL, [x[1] for x in gmmL])\n",
      "plt.set_ylabel(\"Log likelihood\")\n",
      "# ax = axs[1]\n",
      "# ax.plot(nL, [x[2] for x in gmmL])\n",
      "# ax.set_ylabel(\"Bayesian Information Criterion (BIC)\")\n",
      "65/298:\n",
      "# fig, axs = plt.subplots(nrows=2, ncols=1, figsize=(6,9), sharex=True)\n",
      "plt.xlabel(\"Number of components\")\n",
      "# ax = axs[0]\n",
      "plt.title(\"Gaussian Mixture Model\")\n",
      "plt.plot(nL, [x[1] for x in gmmL])\n",
      "plt.ylabel(\"Log likelihood\")\n",
      "# ax = axs[1]\n",
      "# ax.plot(nL, [x[2] for x in gmmL])\n",
      "# ax.set_ylabel(\"Bayesian Information Criterion (BIC)\")\n",
      "65/299: profiles1\n",
      "65/300: profiles1.shape\n",
      "65/301:\n",
      "labels = kmL1[12].predict(test1)\n",
      "mse = 0\n",
      "for i in range(len(test1)):\n",
      "    closest = kmL1[12].cluster_centers_[labels[i]]\n",
      "    mse += mean_squared_error(closest, test1[i])\n",
      "print mse\n",
      "65/302: from sklearn.metrics import mean_squared_error\n",
      "65/303:\n",
      "labels = kmL1[12].predict(test1)\n",
      "mse = 0\n",
      "for i in range(len(test1)):\n",
      "    closest = kmL1[12].cluster_centers_[labels[i]]\n",
      "    mse += mean_squared_error(closest, test1[i])\n",
      "print mse\n",
      "65/304: test1\n",
      "65/305: test1.shape\n",
      "65/306: test1[0]\n",
      "65/307: test1.values[0]\n",
      "65/308: test1.values.shap-e\n",
      "65/309: test1.values.shape\n",
      "65/310:\n",
      "labels = kmL1[12].predict(test1)\n",
      "mse = 0\n",
      "for i in range(len(test1)):\n",
      "    closest = kmL1[12].cluster_centers_[labels[i]]\n",
      "    mse += mean_squared_error(closest, test1.values[i])\n",
      "print mse\n",
      "65/311: mse/len(test1)\n",
      "65/312:\n",
      "labels = kmL1[12].predict(test1)\n",
      "mse = 0\n",
      "for i in range(len(test1)):\n",
      "    closest = kmL1[12].cluster_centers_[labels[i]]\n",
      "    mse += sum((closest-test1.values[i])**2)\n",
      "print mse\n",
      "65/313: mse/len(test1)\n",
      "65/314: gmmBest = gmmL[12]\n",
      "65/315: closestClusters = gmmBest.predict(test1)\n",
      "65/316: gmmBest = gmmL[12][0]\n",
      "65/317: closestClusters = gmmBest.predict(test1)\n",
      "65/318:\n",
      "mseTot = 0\n",
      "for i in range(len(closestClusters)):\n",
      "closestMean = gmmBest.means_[closestClusters[i]]\n",
      "mseTot += sum((closestMean - test1.values[i])**2)\n",
      "print(mseTot/len(closestClusters))\n",
      "65/319:\n",
      "mseTot = 0\n",
      "for i in range(len(closestClusters)):\n",
      "    closestMean = gmmBest.means_[closestClusters[i]]\n",
      "    mseTot += sum((closestMean - test1.values[i])**2)\n",
      "print(mseTot/len(closestClusters))\n",
      "65/320: closestClusters2 = gmmBest.predict_proba(test1)\n",
      "65/321: rec = np.matmul(closestClusters2, gmmBest.means_)\n",
      "65/322:\n",
      "for i in range(len(closestClusters)):\n",
      "    closestMean = rec[i]\n",
      "    mseTot += sum((closestMean - test1.values[i])**2)\n",
      "print(mseTot/len(closestClusters))\n",
      "65/323: gmm\n",
      "65/324: gmm.score(test1)\n",
      "65/325: gmm\n",
      "65/326: gmm.score(test1)\n",
      "65/327:\n",
      "labels = kmL1[12].predict(train1)\n",
      "mse = 0\n",
      "for i in range(len(train1)):\n",
      "    closest = kmL1[12].cluster_centers_[labels[i]]\n",
      "    mse += sum((closest-train1.values[i])**2)\n",
      "print mse/len(train1)\n",
      "64/34:\n",
      "def egograph(name):\n",
      "    hub_ego = nx.ego_graph(G, name)\n",
      "    pos = nx.spring_layout(hub_ego)\n",
      "    options = {\n",
      "        'node_color': 'c',\n",
      "        'node_size': 20,\n",
      "        'line_color': 'grey',\n",
      "        'linewidths': 0,\n",
      "        'width': 0.1,\n",
      "    }\n",
      "    nx.draw(hub_ego, pos, with_labels=False, **options)\n",
      "    nx.draw_networkx_nodes(hub_ego, pos, nodelist=[name], node_size=100, node_color='b', with_labels=True)\n",
      "64/35:\n",
      "box1 = boxofficesorted[0]\n",
      "print box1\n",
      "egograph(box1)\n",
      "64/36:\n",
      "def egograph(name):\n",
      "    hub_ego = nx.ego_graph(G, name)\n",
      "    pos = nx.spring_layout(hub_ego)\n",
      "    options = {\n",
      "        'node_color': 'c',\n",
      "        'node_size': 20,\n",
      "        'line_color': 'grey',\n",
      "        'linewidths': 0,\n",
      "        'width': 0.1,\n",
      "        'with_labels': 'True',\n",
      "    }\n",
      "    nx.draw(hub_ego, pos, with_labels=False, **options)\n",
      "    nx.draw_networkx_nodes(hub_ego, pos, nodelist=[name], node_size=100, node_color='b')\n",
      "64/37:\n",
      "box1 = boxofficesorted[0]\n",
      "print box1\n",
      "egograph(box1)\n",
      "64/38:\n",
      "def egograph(name):\n",
      "    hub_ego = nx.ego_graph(G, name)\n",
      "    pos = nx.spring_layout(hub_ego)\n",
      "    options = {\n",
      "        'node_color': 'c',\n",
      "        'node_size': 20,\n",
      "        'line_color': 'grey',\n",
      "        'linewidths': 0,\n",
      "        'width': 0.1,\n",
      "        'with_labels': True,\n",
      "    }\n",
      "    nx.draw(hub_ego, pos, with_labels=False, **options)\n",
      "    nx.draw_networkx_nodes(hub_ego, pos, nodelist=[name], node_size=100, node_color='b')\n",
      "64/39:\n",
      "def egograph(name):\n",
      "    hub_ego = nx.ego_graph(G, name)\n",
      "    pos = nx.spring_layout(hub_ego)\n",
      "    options = {\n",
      "        'node_color': 'c',\n",
      "        'node_size': 20,\n",
      "        'line_color': 'grey',\n",
      "        'linewidths': 0,\n",
      "        'width': 0.1,\n",
      "    }\n",
      "    nx.draw(hub_ego, pos, with_labels=True, **options)\n",
      "    nx.draw_networkx_nodes(hub_ego, pos, nodelist=[name], node_size=100, node_color='b')\n",
      "64/40:\n",
      "box1 = boxofficesorted[0]\n",
      "print box1\n",
      "egograph(box1)\n",
      "64/41:\n",
      "def egograph(name):\n",
      "    hub_ego = nx.ego_graph(G, name)\n",
      "    pos = nx.spring_layout(hub_ego)\n",
      "    options = {\n",
      "        'node_color': 'c',\n",
      "        'node_size': 20,\n",
      "        'line_color': 'grey',\n",
      "        'linewidths': 0,\n",
      "        'width': 0.1,\n",
      "    }\n",
      "    nx.draw(hub_ego, pos, with_labels=False, **options)\n",
      "    nx.draw_networkx_nodes(hub_ego, pos, nodelist=[name], node_size=100, node_color='b')\n",
      "64/42:\n",
      "box1 = boxofficesorted[0]\n",
      "print box1\n",
      "egograph(box1)\n",
      "64/43:\n",
      "def egograph(name):\n",
      "    hub_ego = nx.ego_graph(G, name)\n",
      "    pos = nx.spring_layout(hub_ego)\n",
      "    options = {\n",
      "        'node_color': 'c',\n",
      "        'node_size': 20,\n",
      "        'line_color': 'grey',\n",
      "        'linewidths': 0,\n",
      "        'width': 0.1,\n",
      "    }\n",
      "    nx.draw(hub_ego, pos, with_labels=False, **options)\n",
      "    nx.draw_networkx_nodes(hub_ego, pos, nodelist=[name], node_size=100, node_color='b')\n",
      "    nx.draw_networkx_labels(hub_ego, pos, nodelist=[name])\n",
      "64/44:\n",
      "box1 = boxofficesorted[0]\n",
      "print box1\n",
      "egograph(box1)\n",
      "64/45:\n",
      "def egograph(name):\n",
      "    hub_ego = nx.ego_graph(G, name)\n",
      "    pos = nx.spring_layout(hub_ego)\n",
      "    options = {\n",
      "        'node_color': 'c',\n",
      "        'node_size': 20,\n",
      "        'line_color': 'grey',\n",
      "        'linewidths': 0,\n",
      "        'width': 0.1,\n",
      "    }\n",
      "    nx.draw(hub_ego, pos, with_labels=False, **options)\n",
      "    nx.draw_networkx_nodes(hub_ego, pos, nodelist=[name], node_size=100, node_color='b')\n",
      "    labels = {}\n",
      "    for node in hub_ego:\n",
      "        if boxofficesorted.index(node) < 100:\n",
      "            labels[node] = node\n",
      "    nx.draw_networkx_labels(hub_ego, pos, labels)\n",
      "64/46:\n",
      "box1 = boxofficesorted[0]\n",
      "print box1\n",
      "egograph(box1)\n",
      "64/47:\n",
      "def egograph(name):\n",
      "    hub_ego = nx.ego_graph(G, name)\n",
      "    pos = nx.spring_layout(hub_ego)\n",
      "    options = {\n",
      "        'node_color': 'c',\n",
      "        'node_size': 20,\n",
      "        'line_color': 'grey',\n",
      "        'linewidths': 0,\n",
      "        'width': 0.1,\n",
      "    }\n",
      "    nx.draw(hub_ego, pos, with_labels=False, **options)\n",
      "    nx.draw_networkx_nodes(hub_ego, pos, nodelist=[name], node_size=100, node_color='b')\n",
      "    labels = {}\n",
      "    for node in hub_ego:\n",
      "        if boxofficesorted.index(node) < 50:\n",
      "            labels[node] = node\n",
      "    nx.draw_networkx_labels(hub_ego, pos, labels)\n",
      "64/48:\n",
      "box1 = boxofficesorted[0]\n",
      "print box1\n",
      "egograph(box1)\n",
      "64/49:\n",
      "box100 = boxofficesorted[99]\n",
      "print box100\n",
      "egograph(box100)\n",
      "64/50:\n",
      "def egograph(name):\n",
      "    hub_ego = nx.ego_graph(G, name)\n",
      "    pos = nx.spring_layout(hub_ego)\n",
      "    options = {\n",
      "        'node_color': 'c',\n",
      "        'node_size': 20,\n",
      "        'line_color': 'grey',\n",
      "        'linewidths': 0,\n",
      "        'width': 0.1,\n",
      "    }\n",
      "    nx.draw(hub_ego, pos, with_labels=False, **options)\n",
      "    nx.draw_networkx_nodes(hub_ego, pos, nodelist=[name], node_size=100, node_color='b')\n",
      "    labels = {}\n",
      "    for node in hub_ego:\n",
      "        if boxofficesorted.index(node) < 40 or node == name:\n",
      "            labels[node] = node\n",
      "    nx.draw_networkx_labels(hub_ego, pos, labels)\n",
      "64/51:\n",
      "box1 = boxofficesorted[0]\n",
      "print box1\n",
      "egograph(box1)\n",
      "64/52:\n",
      "box1 = boxofficesorted[0]\n",
      "print box1\n",
      "egograph(box1)\n",
      "64/53:\n",
      "def egograph(name):\n",
      "    hub_ego = nx.ego_graph(G, name)\n",
      "    pos = nx.spring_layout(hub_ego)\n",
      "    options = {\n",
      "        'node_color': 'c',\n",
      "        'node_size': 20,\n",
      "        'line_color': 'grey',\n",
      "        'linewidths': 0,\n",
      "        'width': 0.1,\n",
      "    }\n",
      "    nx.draw(hub_ego, pos, with_labels=False, **options)\n",
      "    nx.draw_networkx_nodes(hub_ego, pos, nodelist=[name], node_size=100, node_color='b')\n",
      "    labels = {}\n",
      "    for node in hub_ego:\n",
      "        if boxofficesorted.index(node) < 50 or node == name:\n",
      "            labels[node] = node\n",
      "    nx.draw_networkx_labels(hub_ego, pos, labels)\n",
      "64/54:\n",
      "box1 = boxofficesorted[0]\n",
      "print box1\n",
      "egograph(box1)\n",
      "64/55:\n",
      "box100 = boxofficesorted[99]\n",
      "print box100\n",
      "egograph(box100)\n",
      "64/56:\n",
      "box100 = boxofficesorted[99]\n",
      "print box100\n",
      "egograph(box100)\n",
      "64/57: egograph(\"Dennis Quaid\")\n",
      "64/58: egograph(\"Dennis Quaid\")\n",
      "64/59: egograph(\"Dennis Quaid\")\n",
      "64/60: egograph(\"Dennis Quaid\")\n",
      "64/61:\n",
      "def egograph(name):\n",
      "    hub_ego = nx.ego_graph(G, name)\n",
      "    pos = nx.spring_layout(hub_ego)\n",
      "    options = {\n",
      "        'node_color': 'c',\n",
      "        'node_size': 20,\n",
      "        'line_color': 'grey',\n",
      "        'linewidths': 0,\n",
      "        'width': 0.1,\n",
      "    }\n",
      "    nx.draw(hub_ego, pos, with_labels=False, **options)\n",
      "    nx.draw_networkx_nodes(hub_ego, pos, nodelist=[name], node_size=100, node_color='b')\n",
      "    labels = {}\n",
      "    for node in hub_ego:\n",
      "        if boxofficesorted.index(node) < 45 or node == name:\n",
      "            labels[node] = node\n",
      "    nx.draw_networkx_labels(hub_ego, pos, labels)\n",
      "64/62:\n",
      "box1 = boxofficesorted[0]\n",
      "print box1\n",
      "egograph(box1)\n",
      "64/63:\n",
      "box1 = boxofficesorted[0]\n",
      "print box1\n",
      "egograph(box1)\n",
      "64/64:\n",
      "box1 = boxofficesorted[0]\n",
      "print box1\n",
      "egograph(box1)\n",
      "64/65:\n",
      "box1 = boxofficesorted[0]\n",
      "print box1\n",
      "egograph(box1)\n",
      "64/66:\n",
      "box1 = boxofficesorted[0]\n",
      "print box1\n",
      "egograph(box1)\n",
      "64/67:\n",
      "box1 = boxofficesorted[0]\n",
      "print box1\n",
      "egograph(box1)\n",
      "64/68:\n",
      "box1 = boxofficesorted[0]\n",
      "print box1\n",
      "egograph(box1)\n",
      "64/69:\n",
      "box1 = boxofficesorted[0]\n",
      "print box1\n",
      "egograph(box1)\n",
      "64/70:\n",
      "box1 = boxofficesorted[0]\n",
      "print box1\n",
      "egograph(box1)\n",
      "64/71:\n",
      "box1 = boxofficesorted[0]\n",
      "print box1\n",
      "egograph(box1)\n",
      "64/72:\n",
      "def egograph(name):\n",
      "    hub_ego = nx.ego_graph(G, name)\n",
      "    pos = nx.spring_layout(hub_ego)\n",
      "    options = {\n",
      "        'node_color': 'c',\n",
      "        'node_size': 20,\n",
      "        'line_color': 'grey',\n",
      "        'linewidths': 0,\n",
      "        'width': 0.1,\n",
      "    }\n",
      "    nx.draw(hub_ego, pos, with_labels=False, **options)\n",
      "    nx.draw_networkx_nodes(hub_ego, pos, nodelist=[name], node_size=100, node_color='b')\n",
      "    labels = {}\n",
      "    for node in hub_ego:\n",
      "        if node == name:\n",
      "            labels[node] = node\n",
      "    nx.draw_networkx_labels(hub_ego, pos, labels)\n",
      "64/73:\n",
      "box1 = boxofficesorted[0]\n",
      "print box1\n",
      "egograph(box1)\n",
      "64/74:\n",
      "box100 = boxofficesorted[99]\n",
      "print box100\n",
      "egograph(box100)\n",
      "64/75: egograph(\"Dennis Quaid\")\n",
      "64/76:\n",
      "def egograph(name):\n",
      "    hub_ego = nx.ego_graph(G, name)\n",
      "    pos = nx.spring_layout(hub_ego)\n",
      "    options = {\n",
      "        'node_color': 'c',\n",
      "        'node_size': 20,\n",
      "        'line_color': 'grey',\n",
      "        'linewidths': 0,\n",
      "        'width': 0.1,\n",
      "    }\n",
      "    nx.draw(hub_ego, pos, with_labels=False, **options)\n",
      "    nx.draw_networkx_nodes(hub_ego, pos, nodelist=[name], node_size=100, node_color='b')\n",
      "    labels = {}\n",
      "    for node in hub_ego:\n",
      "        if boxofficesorted.index(node) < 25 or node == name:\n",
      "            labels[node] = node\n",
      "    nx.draw_networkx_labels(hub_ego, pos, labels)\n",
      "64/77:\n",
      "box1 = boxofficesorted[0]\n",
      "print box1\n",
      "egograph(box1)\n",
      "64/78:\n",
      "box1 = boxofficesorted[0]\n",
      "print box1\n",
      "egograph(box1)\n",
      "64/79:\n",
      "box1 = boxofficesorted[0]\n",
      "print box1\n",
      "egograph(box1)\n",
      "64/80:\n",
      "box1 = boxofficesorted[0]\n",
      "print box1\n",
      "egograph(box1)\n",
      "64/81:\n",
      "box1 = boxofficesorted[0]\n",
      "print box1\n",
      "egograph(box1)\n",
      "64/82:\n",
      "box1 = boxofficesorted[0]\n",
      "print box1\n",
      "egograph(box1)\n",
      "64/83:\n",
      "box1 = boxofficesorted[0]\n",
      "print box1\n",
      "egograph(box1)\n",
      "64/84:\n",
      "def egograph(name):\n",
      "    hub_ego = nx.ego_graph(G, name)\n",
      "    pos = nx.spring_layout(hub_ego)\n",
      "    options = {\n",
      "        'node_color': 'c',\n",
      "        'node_size': 20,\n",
      "        'line_color': 'grey',\n",
      "        'linewidths': 0,\n",
      "        'width': 0.1,\n",
      "    }\n",
      "    nx.draw(hub_ego, pos, with_labels=False, **options)\n",
      "    nx.draw_networkx_nodes(hub_ego, pos, nodelist=[name], node_size=100, node_color='b')\n",
      "    labels = {}\n",
      "    for node in hub_ego:\n",
      "        if boxofficesorted.index(node) < 100 and boxofficesorted.index(node) % 5 == 0 or node == name:\n",
      "            labels[node] = node\n",
      "    nx.draw_networkx_labels(hub_ego, pos, labels)\n",
      "64/85:\n",
      "box1 = boxofficesorted[0]\n",
      "print box1\n",
      "egograph(box1)\n",
      "64/86:\n",
      "def egograph(name):\n",
      "    hub_ego = nx.ego_graph(G, name)\n",
      "    pos = nx.spring_layout(hub_ego)\n",
      "    options = {\n",
      "        'node_color': 'c',\n",
      "        'node_size': 20,\n",
      "        'line_color': 'grey',\n",
      "        'linewidths': 0,\n",
      "        'width': 0.1,\n",
      "    }\n",
      "    nx.draw(hub_ego, pos, with_labels=False, **options)\n",
      "    nx.draw_networkx_nodes(hub_ego, pos, nodelist=[name], node_size=100, node_color='b')\n",
      "    labels = {}\n",
      "    for node in hub_ego:\n",
      "        if boxofficesorted.index(node) < 60 and boxofficesorted.index(node) % 5 == 0 or node == name:\n",
      "            labels[node] = node\n",
      "    nx.draw_networkx_labels(hub_ego, pos, labels)\n",
      "64/87:\n",
      "box1 = boxofficesorted[0]\n",
      "print box1\n",
      "egograph(box1)\n",
      "64/88:\n",
      "def egograph(name):\n",
      "    hub_ego = nx.ego_graph(G, name)\n",
      "    pos = nx.spring_layout(hub_ego)\n",
      "    options = {\n",
      "        'node_color': 'c',\n",
      "        'node_size': 20,\n",
      "        'line_color': 'grey',\n",
      "        'linewidths': 0,\n",
      "        'width': 0.1,\n",
      "    }\n",
      "    nx.draw(hub_ego, pos, with_labels=False, **options)\n",
      "    nx.draw_networkx_nodes(hub_ego, pos, nodelist=[name], node_size=100, node_color='b')\n",
      "    labels = {}\n",
      "    for node in hub_ego:\n",
      "        if boxofficesorted.index(node) < 60 and boxofficesorted.index(node) % 3 == 0 or node == name:\n",
      "            labels[node] = node\n",
      "    nx.draw_networkx_labels(hub_ego, pos, labels)\n",
      "64/89:\n",
      "box1 = boxofficesorted[0]\n",
      "print box1\n",
      "egograph(box1)\n",
      "64/90:\n",
      "def egograph(name):\n",
      "    hub_ego = nx.ego_graph(G, name)\n",
      "    pos = nx.spring_layout(hub_ego)\n",
      "    options = {\n",
      "        'node_color': 'c',\n",
      "        'node_size': 20,\n",
      "        'line_color': 'grey',\n",
      "        'linewidths': 0,\n",
      "        'width': 0.1,\n",
      "    }\n",
      "    nx.draw(hub_ego, pos, with_labels=False, **options)\n",
      "    nx.draw_networkx_nodes(hub_ego, pos, nodelist=[name], node_size=100, node_color='b')\n",
      "    labels = {}\n",
      "    for node in hub_ego:\n",
      "        if boxofficesorted.index(node) < 60 and boxofficesorted.index(node) % 4 == 0 or node == name:\n",
      "            labels[node] = node\n",
      "    nx.draw_networkx_labels(hub_ego, pos, labels)\n",
      "64/91:\n",
      "box1 = boxofficesorted[0]\n",
      "print box1\n",
      "egograph(box1)\n",
      "64/92:\n",
      "box1 = boxofficesorted[0]\n",
      "print box1\n",
      "egograph(box1)\n",
      "64/93:\n",
      "box1 = boxofficesorted[0]\n",
      "print box1\n",
      "egograph(box1)\n",
      "64/94:\n",
      "box1 = boxofficesorted[0]\n",
      "print box1\n",
      "egograph(box1)\n",
      "64/95:\n",
      "def egograph(name):\n",
      "    hub_ego = nx.ego_graph(G, name)\n",
      "    pos = nx.spring_layout(hub_ego)\n",
      "    options = {\n",
      "        'node_color': 'c',\n",
      "        'node_size': 20,\n",
      "        'line_color': 'grey',\n",
      "        'linewidths': 0,\n",
      "        'width': 0.1,\n",
      "    }\n",
      "    nx.draw(hub_ego, pos, with_labels=False, **options)\n",
      "    nx.draw_networkx_nodes(hub_ego, pos, nodelist=[name], node_size=100, node_color='b')\n",
      "    labels = {}\n",
      "    for node in hub_ego:\n",
      "        if boxofficesorted.index(node) < 50 and boxofficesorted.index(node) % 2 == 0 or node == name:\n",
      "            labels[node] = node\n",
      "    nx.draw_networkx_labels(hub_ego, pos, labels)\n",
      "64/96:\n",
      "box1 = boxofficesorted[0]\n",
      "print box1\n",
      "egograph(box1)\n",
      "64/97:\n",
      "box1 = boxofficesorted[0]\n",
      "print box1\n",
      "egograph(box1)\n",
      "64/98:\n",
      "box1 = boxofficesorted[0]\n",
      "print box1\n",
      "egograph(box1)\n",
      "64/99:\n",
      "def egograph(name):\n",
      "    hub_ego = nx.ego_graph(G, name)\n",
      "    pos = nx.spring_layout(hub_ego)\n",
      "    options = {\n",
      "        'node_color': 'c',\n",
      "        'node_size': 20,\n",
      "        'line_color': 'grey',\n",
      "        'linewidths': 0,\n",
      "        'width': 0.1,\n",
      "    }\n",
      "    nx.draw(hub_ego, pos, with_labels=False, **options)\n",
      "    nx.draw_networkx_nodes(hub_ego, pos, nodelist=[name], node_size=100, node_color='b')\n",
      "    labels = {}\n",
      "    for node in hub_ego:\n",
      "        if boxofficesorted.index(node) < 70 and boxofficesorted.index(node) % 5 == 0 or node == name:\n",
      "            labels[node] = node\n",
      "    nx.draw_networkx_labels(hub_ego, pos, labels)\n",
      "64/100:\n",
      "box1 = boxofficesorted[0]\n",
      "print box1\n",
      "egograph(box1)\n",
      "64/101:\n",
      "box1 = boxofficesorted[0]\n",
      "print box1\n",
      "egograph(box1)\n",
      "64/102: egograph(\"Dennis Quaid\")\n",
      "64/103: egograph(\"Dennis Quaid\", 100)\n",
      "64/104:\n",
      "def egograph(name, t):\n",
      "    hub_ego = nx.ego_graph(G, name)\n",
      "    pos = nx.spring_layout(hub_ego)\n",
      "    options = {\n",
      "        'node_color': 'c',\n",
      "        'node_size': 20,\n",
      "        'line_color': 'grey',\n",
      "        'linewidths': 0,\n",
      "        'width': 0.1,\n",
      "    }\n",
      "    nx.draw(hub_ego, pos, with_labels=False, **options)\n",
      "    nx.draw_networkx_nodes(hub_ego, pos, nodelist=[name], node_size=100, node_color='b')\n",
      "    labels = {}\n",
      "    for node in hub_ego:\n",
      "        if boxofficesorted.index(node) < t and boxofficesorted.index(node) % 5 == 0 or node == name:\n",
      "            labels[node] = node\n",
      "    nx.draw_networkx_labels(hub_ego, pos, labels)\n",
      "64/105: egograph(\"Dennis Quaid\", 100)\n",
      "64/106: egograph(\"Dennis Quaid\", 500)\n",
      "64/107: egograph(\"Dennis Quaid\", 200)\n",
      "64/108: egograph(\"Dennis Quaid\", 300)\n",
      "64/109: egograph(\"Dennis Quaid\", 400)\n",
      "64/110: egograph(\"Dennis Quaid\", 400)\n",
      "64/111: egograph(\"Dennis Quaid\", 400)\n",
      "64/112: egograph(\"Dennis Quaid\", 400)\n",
      "64/113: egograph(\"Dennis Quaid\", 400)\n",
      "64/114: egograph(\"Dennis Quaid\", 400)\n",
      "64/115:\n",
      "def egograph(name, t1, t2):\n",
      "    hub_ego = nx.ego_graph(G, name)\n",
      "    pos = nx.spring_layout(hub_ego)\n",
      "    options = {\n",
      "        'node_color': 'c',\n",
      "        'node_size': 20,\n",
      "        'line_color': 'grey',\n",
      "        'linewidths': 0,\n",
      "        'width': 0.1,\n",
      "    }\n",
      "    nx.draw(hub_ego, pos, with_labels=False, **options)\n",
      "    nx.draw_networkx_nodes(hub_ego, pos, nodelist=[name], node_size=100, node_color='b')\n",
      "    labels = {}\n",
      "    for node in hub_ego:\n",
      "        if boxofficesorted.index(node) < t1 and boxofficesorted.index(node) % t2 == 0 or node == name:\n",
      "            labels[node] = node\n",
      "    nx.draw_networkx_labels(hub_ego, pos, labels)\n",
      "64/116: egograph(\"Dennis Quaid\", 500, 10)\n",
      "64/117: egograph(\"Dennis Quaid\", 500, 10)\n",
      "64/118: egograph(\"Dennis Quaid\", 500, 10)\n",
      "64/119: egograph(\"Dennis Quaid\", 500, 12)\n",
      "64/120: egograph(\"Dennis Quaid\", 500, 15)\n",
      "64/121: egograph(\"Dennis Quaid\", 500, 14)\n",
      "64/122: egograph(\"Dennis Quaid\", 500, 16)\n",
      "64/123: egograph(\"Dennis Quaid\", 500, 20)\n",
      "64/124: egograph(\"Dennis Quaid\", 500, 20)\n",
      "64/125: egograph(\"Dennis Quaid\", 500, 20)\n",
      "64/126: egograph(\"Dennis Quaid\", 500, 20)\n",
      "64/127: egograph(\"Dennis Quaid\", 500, 20)\n",
      "64/128: egograph(\"Dennis Quaid\", 500, 20)\n",
      "64/129: egograph(\"Dennis Quaid\", 500, 20)\n",
      "64/130: egograph(\"Dennis Quaid\", 500, 20)\n",
      "64/131: egograph(\"Dennis Quaid\", 500, 20)\n",
      "64/132: egograph(\"Dennis Quaid\", 500, 25)\n",
      "64/133: egograph(\"Dennis Quaid\", 500, 20)\n",
      "64/134: egograph(\"Dennis Quaid\", 500, 20)\n",
      "64/135: egograph(\"Dennis Quaid\", 500, 20)\n",
      "64/136: egograph(\"Dennis Quaid\", 500, 20)\n",
      "64/137: egograph(\"Dennis Quaid\", 500, 20)\n",
      "64/138:\n",
      "box400 = boxofficesorted[399]\n",
      "print box400\n",
      "egograph(box400, 500, 10)\n",
      "64/139:\n",
      "box400 = boxofficesorted[400]\n",
      "print box400\n",
      "egograph(box400, 500, 10)\n",
      "64/140:\n",
      "box400 = boxofficesorted[400]\n",
      "print box400\n",
      "egograph(box400, 500, 2)\n",
      "64/141:\n",
      "box800 = boxofficesorted[799]\n",
      "print box800\n",
      "egograph(box800, 500, 5)\n",
      "64/142:\n",
      "box800 = boxofficesorted[799]\n",
      "print box800\n",
      "egograph(box800, 500, 2)\n",
      "64/143:\n",
      "box800 = boxofficesorted[799]\n",
      "print box800\n",
      "egograph(box800, 500, 2)\n",
      "64/144:\n",
      "box800 = boxofficesorted[799]\n",
      "print box800\n",
      "egograph(box800, 500, 2)\n",
      "64/145:\n",
      "box800 = boxofficesorted[799]\n",
      "print box800\n",
      "egograph(box800, 500, 2)\n",
      "64/146:\n",
      "box800 = boxofficesorted[799]\n",
      "print box800\n",
      "egograph(box800, 500, 4)\n",
      "64/147:\n",
      "box800 = boxofficesorted[799]\n",
      "print box800\n",
      "egograph(box800, 600, 4)\n",
      "64/148:\n",
      "box800 = boxofficesorted[799]\n",
      "print box800\n",
      "egograph(box800, 600, 5)\n",
      "64/149:\n",
      "box800 = boxofficesorted[799]\n",
      "print box800\n",
      "egograph(box800, 600, 2)\n",
      "64/150:\n",
      "box800 = boxofficesorted[799]\n",
      "print box800\n",
      "egograph(box800, 600, 2)\n",
      "64/151:\n",
      "plot.hist(cluster_coefficients)\n",
      "plot.title(\"Distribution of Cluster Coefficients\")\n",
      "plot.xlabel(\"Cluster Coefficients\")\n",
      "plot.ylabel(\"Frequency\")\n",
      "plot.show()\n",
      "np.max(cluster_coefficients), np.mean(cluster_coefficients), np.std(cluster_coefficients)\n",
      "64/152:\n",
      "cluster_coefficients = [cluster_coefficient.get(x) for x in actorlist]\n",
      "cluster_coefficients\n",
      "64/153: degree_centrality = nx.degree_centrality(G)\n",
      "64/154: sorted(degree_centrality, key=degree_centrality.get, reverse=True)\n",
      "64/155: eigenvector_centrality = nx.eigenvector_centrality(G)\n",
      "64/156: sorted(eigenvector_centrality, key=eigenvector_centrality.get, reverse=True)\n",
      "64/157: closeness_centrality = nx.closeness_centrality(G)\n",
      "64/158: sorted(closeness_centrality, key=closeness_centrality.get, reverse=True)\n",
      "64/159: betweenness_centrality = nx.betweenness_centrality(G)\n",
      "64/160: sorted(betweenness_centrality, key=betweenness_centrality.get, reverse=True)\n",
      "64/161: cluster_coefficient = nx.algorithms.cluster.clustering(G)\n",
      "64/162: sorted(cluster_coefficient, key=cluster_coefficient.get, reverse=True)\n",
      "64/163:\n",
      "degree_centralities = []\n",
      "for actor in actorlist:\n",
      "    degree_centralities.append(degree_centrality.get(actor))\n",
      "64/164:\n",
      "cluster_coefficients = [cluster_coefficient.get(x) for x in actorlist]\n",
      "cluster_coefficients\n",
      "64/165:\n",
      "plot.hist(cluster_coefficients)\n",
      "plot.title(\"Distribution of Cluster Coefficients\")\n",
      "plot.xlabel(\"Cluster Coefficients\")\n",
      "plot.ylabel(\"Frequency\")\n",
      "plot.show()\n",
      "np.max(cluster_coefficients), np.mean(cluster_coefficients), np.std(cluster_coefficients)\n",
      "64/166:\n",
      "plot.hist(degree_centralities)\n",
      "plot.title(\"Distribution of Degree Centralities\")\n",
      "plot.xlabel(\"Degree Centrality\")\n",
      "plot.ylabel(\"Frequency\")\n",
      "plot.show()\n",
      "np.max(degree_centralities), np.mean(degree_centralities), np.std(degree_centralities)\n",
      "64/167:\n",
      "plot.hist(boxofficegross)\n",
      "plot.title(\"Distribution of Box Office Gross\")\n",
      "plot.xlabel(\"Box Office Gross\")\n",
      "plot.ylabel(\"Frequency\")\n",
      "plot.show()\n",
      "np.median(boxofficegross), np.mean(boxofficegross), np.std(boxofficegross)\n",
      "64/168:\n",
      "boxofficegross = []\n",
      "for actor in actorlist:\n",
      "    boxofficegross.append(boxofficedict[actor])\n",
      "64/169:\n",
      "plot.hist(betweenness_centralities)\n",
      "plot.title(\"Distribution of Betweenness Centralities\")\n",
      "plot.xlabel(\"Betweenness Centrality\")\n",
      "plot.ylabel(\"Frequency\")\n",
      "plot.show()\n",
      "np.max(betweenness_centralities), np.mean(betweenness_centralities), np.std(betweenness_centralities)\n",
      "64/170:\n",
      "betweenness_centralities = []\n",
      "for actor in actorlist:\n",
      "    betweenness_centralities.append(betweenness_centrality.get(actor))\n",
      "x_b = np.array(betweenness_centralities)\n",
      "y = np.array(boxofficegross)\n",
      "slope, intercept, r_value, p_value, std_err = stats.linregress(x_b,y)\n",
      "slope, intercept, r_value, p_value, std_err\n",
      "64/171:\n",
      "plot.hist(betweenness_centralities)\n",
      "plot.title(\"Distribution of Betweenness Centralities\")\n",
      "plot.xlabel(\"Betweenness Centrality\")\n",
      "plot.ylabel(\"Frequency\")\n",
      "plot.show()\n",
      "np.max(betweenness_centralities), np.mean(betweenness_centralities), np.std(betweenness_centralities)\n",
      "64/172:\n",
      "plot.hist(closeness_centralities)\n",
      "plot.title(\"Distribution of Closeness Centralities\")\n",
      "plot.xlabel(\"Closeness Centrality\")\n",
      "plot.ylabel(\"Frequency\")\n",
      "plot.show()\n",
      "np.max(closeness_centralities), np.mean(closeness_centralities), np.std(closeness_centralities)\n",
      "64/173:\n",
      "closeness_centralities = []\n",
      "for actor in actorlist:\n",
      "    closeness_centralities.append(closeness_centrality.get(actor))\n",
      "x_c = np.array(closeness_centralities)\n",
      "y = np.array(boxofficegross)\n",
      "slope, intercept, r_value, p_value, std_err = stats.linregress(x_c,y)\n",
      "slope, intercept, r_value, p_value, std_err\n",
      "64/174:\n",
      "plot.hist(closeness_centralities)\n",
      "plot.title(\"Distribution of Closeness Centralities\")\n",
      "plot.xlabel(\"Closeness Centrality\")\n",
      "plot.ylabel(\"Frequency\")\n",
      "plot.show()\n",
      "np.max(closeness_centralities), np.mean(closeness_centralities), np.std(closeness_centralities)\n",
      "64/175:\n",
      "eigenvector_centralities = []\n",
      "for actor in actorlist:\n",
      "    eigenvector_centralities.append(eigenvector_centrality.get(actor))\n",
      "64/176:\n",
      "plot.hist(eigenvector_centralities)\n",
      "plot.title(\"Eigenvector Centrality\")\n",
      "plot.show()\n",
      "np.max(eigenvector_centralities), np.mean(eigenvector_centralities), np.std(eigenvector_centralities)\n",
      "64/177:\n",
      "plot.hist(eigenvector_centralities)\n",
      "plot.title(\"Distribution of Eigenvector Centralities\")\n",
      "plot.xlabel(\"Eigenvector Centrality\")\n",
      "plot.ylabel(\"Frequency\")\n",
      "plot.show()\n",
      "np.max(eigenvector_centralities), np.mean(eigenvector_centralities), np.std(eigenvector_centralities)\n",
      "64/178:\n",
      "plot.figure()\n",
      "plot.plot(moviecounts, boxofficegross, \"o\")\n",
      "plot.plot(moviecounts, [slope*x + intercept for x in moviecounts], \"black\")\n",
      "plot.title(\"Box Office Gross vs Number of Titles Actor Appeared In\")\n",
      "plot.xscale(0, 300)\n",
      "plot.show()\n",
      "64/179:\n",
      "moviecounts = []\n",
      "for actor in actorlist:\n",
      "    moviecounts.append(moviecountdict.get(actor))\n",
      "64/180:\n",
      "plot.figure()\n",
      "plot.plot(moviecounts, boxofficegross, \"o\")\n",
      "plot.plot(moviecounts, [slope*x + intercept for x in moviecounts], \"black\")\n",
      "plot.title(\"Box Office Gross vs Number of Titles Actor Appeared In\")\n",
      "plot.xscale(0, 300)\n",
      "plot.show()\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "64/181:\n",
      "plot.figure()\n",
      "plot.plot(moviecounts, boxofficegross, \"o\")\n",
      "plot.plot(moviecounts, [slope*x + intercept for x in moviecounts], \"black\")\n",
      "plot.title(\"Box Office Gross vs Number of Titles Actor Appeared In\")\n",
      "plot.xlim(0, 300)\n",
      "plot.show()\n",
      "64/182:\n",
      "plot.figure()\n",
      "plot.plot(moviecounts, boxofficegross, \"o\")\n",
      "plot.plot(moviecounts, [slope*x + intercept for x in moviecounts], \"black\")\n",
      "plot.title(\"Box Office Gross vs Number of Titles Actor Appeared In\")\n",
      "# plot.xlim(0, 300)\n",
      "plot.show()\n",
      "64/183:\n",
      "xc = np.array(moviecounts)\n",
      "yc = np.array(boxofficegross)\n",
      "slope, intercept, r_value, p_value, std_err = stats.linregress(xc,yc)\n",
      "slope, intercept, r_value, p_value, std_err\n",
      "64/184:\n",
      "plot.figure()\n",
      "plot.plot(moviecounts, boxofficegross, \"o\")\n",
      "plot.plot(moviecounts, [slope*x + intercept for x in moviecounts], \"black\")\n",
      "plot.title(\"Box Office Gross vs Number of Titles Actor Appeared In\")\n",
      "# plot.xlim(0, 300)\n",
      "plot.show()\n",
      "64/185:\n",
      "plot.figure()\n",
      "plot.plot(moviecounts, boxofficegross, \"o\")\n",
      "plot.plot(moviecounts, [slope*x + intercept for x in moviecounts], \"black\")\n",
      "plot.title(\"Box Office Gross vs Number of Titles Actor Appeared In\")\n",
      "plot.xlim(0, 300)\n",
      "plot.show()\n",
      "64/186:\n",
      "moviecounts = []\n",
      "for actor in actorlist:\n",
      "    c = max(moviecountdict.get(actor)/5, 250)\n",
      "    moviecounts.append(c)\n",
      "64/187:\n",
      "xc = np.array(moviecounts)\n",
      "yc = np.array(boxofficegross)\n",
      "slope, intercept, r_value, p_value, std_err = stats.linregress(xc,yc)\n",
      "slope, intercept, r_value, p_value, std_err\n",
      "64/188:\n",
      "plot.figure()\n",
      "plot.plot(moviecounts, boxofficegross, \"o\")\n",
      "plot.plot(moviecounts, [slope*x + intercept for x in moviecounts], \"black\")\n",
      "plot.title(\"Box Office Gross vs Number of Titles Actor Appeared In\")\n",
      "plot.xlim(0, 300)\n",
      "plot.show()\n",
      "64/189:\n",
      "moviecounts = []\n",
      "for actor in actorlist:\n",
      "    c = max(float(moviecountdict.get(actor))/5, 250)\n",
      "    moviecounts.append(c)\n",
      "64/190:\n",
      "xc = np.array(moviecounts)\n",
      "yc = np.array(boxofficegross)\n",
      "slope, intercept, r_value, p_value, std_err = stats.linregress(xc,yc)\n",
      "slope, intercept, r_value, p_value, std_err\n",
      "64/191:\n",
      "moviecounts = []\n",
      "for actor in actorlist:\n",
      "    c = float(moviecountdict.get(actor))/5\n",
      "    moviecounts.append(c)\n",
      "64/192:\n",
      "xc = np.array(moviecounts)\n",
      "yc = np.array(boxofficegross)\n",
      "slope, intercept, r_value, p_value, std_err = stats.linregress(xc,yc)\n",
      "slope, intercept, r_value, p_value, std_err\n",
      "64/193:\n",
      "plot.figure()\n",
      "plot.plot(moviecounts, boxofficegross, \"o\")\n",
      "plot.plot(moviecounts, [slope*x + intercept for x in moviecounts], \"black\")\n",
      "plot.title(\"Box Office Gross vs Number of Titles Actor Appeared In\")\n",
      "plot.xlim(0, 300)\n",
      "plot.show()\n",
      "64/194:\n",
      "plot.figure()\n",
      "plot.plot(moviecounts, boxofficegross, \"o\")\n",
      "plot.plot(moviecounts, [slope*x + intercept for x in moviecounts], \"black\")\n",
      "plot.title(\"Box Office Gross vs Number of Titles Actor Appeared In\")\n",
      "plot.show()\n",
      "64/195:\n",
      "moviecounts = []\n",
      "for actor in actorlist:\n",
      "    c = float(moviecountdict.get(actor))/4\n",
      "    moviecounts.append(c)\n",
      "64/196:\n",
      "xc = np.array(moviecounts)\n",
      "yc = np.array(boxofficegross)\n",
      "slope, intercept, r_value, p_value, std_err = stats.linregress(xc,yc)\n",
      "slope, intercept, r_value, p_value, std_err\n",
      "64/197:\n",
      "plot.figure()\n",
      "plot.plot(moviecounts, boxofficegross, \"o\")\n",
      "plot.plot(moviecounts, [slope*x + intercept for x in moviecounts], \"black\")\n",
      "plot.title(\"Box Office Gross vs Number of Titles Actor Appeared In\")\n",
      "plot.show()\n",
      "64/198:\n",
      "plot.figure()\n",
      "plot.plot(moviecounts, boxofficegross, \"o\")\n",
      "plot.plot(moviecounts, [slope*x + intercept for x in moviecounts], \"black\")\n",
      "plot.title(\"Box Office Gross vs Number of Titles Actor Appeared In\")\n",
      "plot.xlabel(\"Number of Titles Acted In\")\n",
      "plot.ylabel(\"Box Office Gross (millions)\")\n",
      "plot.show()\n",
      "64/199:\n",
      "plot.plot(degree_centralities, boxofficegross, \"o\")\n",
      "plot.title(\"Box Office Gross vs Degree Centrality\")\n",
      "plot.xlabel(\"Degree Centrality\")\n",
      "plot.ylabel(\"Box Office Gross Totals\")\n",
      "plot.show()\n",
      "64/200:\n",
      "plot.figure()\n",
      "plot.plot(x, y, \"o\")\n",
      "yfit = [intercept + slope * xi for xi in x]\n",
      "plot.plot(x, yfit, \"black\")\n",
      "plot.title(\"Box Office Gross vs Degree Centrality\")\n",
      "plot.xlabel(\"Degree Centrality\")\n",
      "plot.ylabel(\"Box Office Gross (millions)\")\n",
      "plot.show()\n",
      "64/201:\n",
      "x = np.array(degree_centralities)\n",
      "y = np.array(boxofficegross)\n",
      "slope, intercept, r_value, p_value, std_err = stats.linregress(x,y)\n",
      "slope, intercept, r_value, p_value, std_err\n",
      "64/202:\n",
      "plot.figure()\n",
      "plot.plot(x, y, \"o\")\n",
      "yfit = [intercept + slope * xi for xi in x]\n",
      "plot.plot(x, yfit, \"black\")\n",
      "plot.title(\"Box Office Gross vs Degree Centrality\")\n",
      "plot.xlabel(\"Degree Centrality\")\n",
      "plot.ylabel(\"Box Office Gross (millions)\")\n",
      "plot.show()\n",
      "64/203: female = [1 if x in Gw.nodes() else 0 for x in G.nodes()]\n",
      "64/204: female\n",
      "64/205: female = [1 if x in Gw.nodes() else 0 for x in actorlist]\n",
      "64/206: female\n",
      "64/207: oldage = [1 if x in old else 0 for x in actorlist]\n",
      "64/208: old = [a for a in actorlist if agedict[a] >= 47]\n",
      "64/209: oldage = [1 if x in old else 0 for x in actorlist]\n",
      "64/210: age = [agedict[x] for x in actorlist]\n",
      "64/211:\n",
      "df = pd.DataFrame(\n",
      "    {\n",
      "        'degree_centralities': degree_centralities,\n",
      "        'eigenvector_centralities': eigenvector_centralities,\n",
      "        'betweenness_centralities': betweenness_centralities,\n",
      "        'closeness_centralities': closeness_centralities,\n",
      "        'cluster_coefficients': cluster_coefficients,\n",
      "        'effective_size': effective_sizes,\n",
      "        'moviecount': moviecounts,\n",
      "        'boxofficegross': boxofficegross,\n",
      "        'academynominated': academynominated,\n",
      "        'female': female,\n",
      "        'oldage': oldage,\n",
      "        'age': age\n",
      "    })\n",
      "64/212:\n",
      "import statsmodels.api as sm\n",
      "import pandas as pd\n",
      "64/213:\n",
      "df = pd.DataFrame(\n",
      "    {\n",
      "        'degree_centralities': degree_centralities,\n",
      "        'eigenvector_centralities': eigenvector_centralities,\n",
      "        'betweenness_centralities': betweenness_centralities,\n",
      "        'closeness_centralities': closeness_centralities,\n",
      "        'cluster_coefficients': cluster_coefficients,\n",
      "        'effective_size': effective_sizes,\n",
      "        'moviecount': moviecounts,\n",
      "        'boxofficegross': boxofficegross,\n",
      "        'academynominated': academynominated,\n",
      "        'female': female,\n",
      "        'oldage': oldage,\n",
      "        'age': age\n",
      "    })\n",
      "64/214:\n",
      "df = pd.DataFrame(\n",
      "    {\n",
      "        'degree_centralities': degree_centralities,\n",
      "        'eigenvector_centralities': eigenvector_centralities,\n",
      "        'betweenness_centralities': betweenness_centralities,\n",
      "        'closeness_centralities': closeness_centralities,\n",
      "        'cluster_coefficients': cluster_coefficients,\n",
      "        'moviecount': moviecounts,\n",
      "        'boxofficegross': boxofficegross,\n",
      "        'academynominated': academynominated,\n",
      "        'female': female,\n",
      "        'oldage': oldage,\n",
      "        'age': age\n",
      "    })\n",
      "64/215:\n",
      "academynominated = []\n",
      "for a in actorlist:\n",
      "    if a in academylist:\n",
      "        academynominated.append(1)\n",
      "    else:\n",
      "        academynominated.append(0)\n",
      "64/216:\n",
      "df = pd.DataFrame(\n",
      "    {\n",
      "        'degree_centralities': degree_centralities,\n",
      "        'eigenvector_centralities': eigenvector_centralities,\n",
      "        'betweenness_centralities': betweenness_centralities,\n",
      "        'closeness_centralities': closeness_centralities,\n",
      "        'cluster_coefficients': cluster_coefficients,\n",
      "        'moviecount': moviecounts,\n",
      "        'boxofficegross': boxofficegross,\n",
      "        'academynominated': academynominated,\n",
      "        'female': female,\n",
      "        'oldage': oldage,\n",
      "        'age': age\n",
      "    })\n",
      "64/217: df.corr()\n",
      "64/218: df[:313].corr()\n",
      "64/219: df[313:].corr()\n",
      "64/220:\n",
      "df['degree_fem'] = df['degree_centralities']*df['female']\n",
      "Xgender = df[['degree_centralities', 'female', 'degree_fem']]\n",
      "64/221:\n",
      "Xgender = sm.add_constant(Xgender) # adding a constant\n",
      "Y = boxofficegross\n",
      "model = sm.OLS(Y, Xgender).fit()\n",
      "predictions = model.predict(Xgender) \n",
      " \n",
      "print_model = model.summary()\n",
      "print(print_model)\n",
      "64/222:\n",
      "df['degree_age'] = df['degree_centralities']*df['age']\n",
      "Xage = df[['degree_centralities', 'age', 'degree_age']]\n",
      "64/223:\n",
      "df['degree_oldage'] = df['degree_centralities']*df['oldage']\n",
      "Xoldage = df[['degree_centralities', 'oldage', 'degree_oldage']]\n",
      "64/224:\n",
      "Xoldage = sm.add_constant(Xoldage) # adding a constant\n",
      "Y = boxofficegross\n",
      "model = sm.OLS(Y, Xoldage).fit()\n",
      "predictions = model.predict(Xoldage) \n",
      " \n",
      "print_model = model.summary()\n",
      "print(print_model)\n",
      "64/225:\n",
      "Xage = sm.add_constant(Xage) # adding a constant\n",
      "Y = boxofficegross\n",
      "model = sm.OLS(Y, Xage).fit()\n",
      "predictions = model.predict(Xage) \n",
      " \n",
      "print_model = model.summary()\n",
      "print(print_model)\n",
      "64/226:\n",
      "df['degree_a'] = df['degree_centralities']*df['academy_nominated']\n",
      "Xa = df[['degree_centralities', 'academy_nominated', 'degree_a']]\n",
      "64/227:\n",
      "df['degree_a'] = df['degree_centralities']*df['academynominated']\n",
      "Xa = df[['degree_centralities', 'academynominated', 'degree_a']]\n",
      "64/228:\n",
      "Xa = sm.add_constant(Xa) # adding a constant\n",
      "Y = boxofficegross\n",
      "model = sm.OLS(Y, Xa).fit()\n",
      "predictions = model.predict(Xa) \n",
      " \n",
      "print_model = model.summary()\n",
      "print(print_model)\n",
      "64/229: sum(female), sum(oldage)\n",
      "64/230: sum(age)/len(age)\n",
      "64/231: sum(academynominated)\n",
      "64/232: plot.histogram(agE)\n",
      "64/233: plot.his(age)\n",
      "64/234: plot.hist(age)\n",
      "64/235: plot.hist([x for x in age if x != 0])\n",
      "64/236: np.std([x for x in age if x != 0])\n",
      "64/237: np.min(boxofficegross), max(boxofficegross)\n",
      "64/238: min(boxofficegross), max(boxofficegross)\n",
      "64/239: min(boxofficegross), max(boxofficegross), mean(boxofficegross), median(boxofficegross), std(boxofficegross)\n",
      "64/240: min(boxofficegross), max(boxofficegross), np.mean(boxofficegross), np.median(boxofficegross), np.std(boxofficegross)\n",
      "64/241:\n",
      "plot.hist(boxofficegross)\n",
      "plot.show()\n",
      "64/242: plot.hist([x for x in age if x != 0])\n",
      "64/243: plot.hist([x for x in age if x != 0 and x < 100])\n",
      "64/244:\n",
      "plot.hist(boxofficegross)\n",
      "plot.xlabel(\"Box Office Gross (millions)\")\n",
      "plot.ylabel(\"Frequency\")\n",
      "plot.title(\"Distribution of Box Office Gross\")\n",
      "plot.show()\n",
      "64/245:\n",
      "plot.hist([x for x in age if x != 0 and x < 100])\n",
      "plot.xlabel()\n",
      "64/246:\n",
      "plot.hist([x for x in age if x != 0 and x < 100 else 47])\n",
      "plot.xlabel(\"Age\")\n",
      "plot.ylabel(\"Frequency\")\n",
      "64/247:\n",
      "plot.hist([x for x in age if x != 0 and x < 100]*2)\n",
      "plot.xlabel(\"Age\")\n",
      "plot.ylabel(\"Frequency\")\n",
      "64/248:\n",
      "plot.hist([x for x in age if x != 0 and x < 100]*1.5)\n",
      "plot.xlabel(\"Age\")\n",
      "plot.ylabel(\"Frequency\")\n",
      "64/249:\n",
      "plot.hist([x for x in age if x != 0 and x < 100])\n",
      "plot.xlabel(\"Age\")\n",
      "plot.ylabel(\"Frequency\")\n",
      "64/250:\n",
      "missing = []\n",
      "for a in agedict:\n",
      "    if agedict[a] == 0:\n",
      "        missing.append(a)\n",
      "64/251:\n",
      "ages = [1973, 1964, 1968, 1966, 1997, 1972, 1984, 1954, 1951, 1965, 1958, 1988, 1989, 1931, 1968, 1990, 1966, 1928, 1972, 1969, 1974, 1969, 1976, 1922, 1997, 1995]\n",
      "missing = zip(missing, ages)\n",
      "64/252:\n",
      "for i in range(len(missing)):\n",
      "    agedict[missing[i][0]] = 2019-missing[i][1]\n",
      "64/253: print np.median([agedict[a] for a in agedict])\n",
      "64/254:\n",
      "plot.hist(boxofficegross)\n",
      "plot.title(\"Distribution of Box Office Gross\")\n",
      "plot.xlabel(\"Box Office Gross\")\n",
      "plot.ylabel(\"Frequency\")\n",
      "plot.show()\n",
      "np.median(boxofficegross), np.mean(boxofficegross), np.std(boxofficegross)\n",
      "64/255:\n",
      "for x in \n",
      "plot.hist([x for x in age])\n",
      "plot.xlabel(\"Age\")\n",
      "plot.ylabel(\"Frequency\")\n",
      "64/256:\n",
      "plot.hist([x for x in age])\n",
      "plot.xlabel(\"Age\")\n",
      "plot.ylabel(\"Frequency\")\n",
      "64/257:\n",
      "plot.hist([x for x in age])\n",
      "plot.xlabel(\"Age\")\n",
      "plot.ylabel(\"Frequency\")\n",
      "min(age), max(age)\n",
      "64/258:\n",
      "for k,v in agedict:\n",
      "    if v == 0 or v > 100:\n",
      "        print k,v\n",
      "plot.hist([x for x in age])\n",
      "plot.xlabel(\"Age\")\n",
      "plot.ylabel(\"Frequency\")\n",
      "64/259: agedict\n",
      "64/260:\n",
      "for k in agedict:\n",
      "    v = agedict[k]\n",
      "    if v == 0 or v > 100:\n",
      "        print k,v\n",
      "plot.hist([x for x in age])\n",
      "plot.xlabel(\"Age\")\n",
      "plot.ylabel(\"Frequency\")\n",
      "64/261: age = [agedict[x] for x in actorlist]\n",
      "64/262:\n",
      "for k in agedict:\n",
      "    v = agedict[k]\n",
      "    if v == 0 or v > 100:\n",
      "        print k,v\n",
      "plot.hist([x for x in age])\n",
      "plot.xlabel(\"Age\")\n",
      "plot.ylabel(\"Frequency\")\n",
      "64/263:\n",
      "for k in agedict:\n",
      "    v = agedict[k]\n",
      "    if v == 0 or v > 00:\n",
      "        print k,v\n",
      "plot.hist([x for x in age])\n",
      "plot.xlabel(\"Age\")\n",
      "plot.ylabel(\"Frequency\")\n",
      "64/264:\n",
      "for k in agedict:\n",
      "    v = agedict[k]\n",
      "    if v == 0 or v > 90:\n",
      "        print k,v\n",
      "plot.hist([x for x in age])\n",
      "plot.xlabel(\"Age\")\n",
      "plot.ylabel(\"Frequency\")\n",
      "73/1: from matplotlib import pyplot as plot\n",
      "73/2: h = [0, 0.25, 0.75]\n",
      "73/3: th = [0, 0.18, 1.62]\n",
      "64/265:\n",
      "for k in agedict:\n",
      "    v = agedict[k]\n",
      "    if v > 90:\n",
      "        agedict[k] = 96\n",
      "plot.hist([x for x in age])\n",
      "plot.xlabel(\"Age\")\n",
      "plot.ylabel(\"Frequency\")\n",
      "64/266: age = [agedict[x] for x in actorlist]\n",
      "64/267:\n",
      "plot.hist([x for x in age])\n",
      "plot.xlabel(\"Age\")\n",
      "plot.ylabel(\"Frequency\")\n",
      "64/268:\n",
      "plot.hist([x for x in age])\n",
      "plot.xlabel(\"Age\")\n",
      "plot.ylabel(\"Frequency\")\n",
      "plot.title(\"Distribution of Ages\")\n",
      "74/1: h = [0, 0.25, 0.5, 0.75, 1]\n",
      "74/2:\n",
      "h1 = h*81/12\n",
      "h1[2] = 52./12\n",
      "h1\n",
      "74/3:\n",
      "h1 = np.array(h)*81/12\n",
      "h1[2] = 52./12\n",
      "h1\n",
      "74/4:\n",
      "from matplotlib import pyplot as plot\n",
      "import numpy as np\n",
      "74/5: h = [0, 0.25, 0.5, 0.75, 1]\n",
      "74/6:\n",
      "h1 = np.array(h)*81/12\n",
      "h1[2] = 52./12\n",
      "h1\n",
      "74/7: th = [0, 0.18, 0.546, 1.62, 0.856]\n",
      "74/8:\n",
      "plot.figure()\n",
      "plot.plot(th, h1)\n",
      "74/9: th = [0, 0.18, 1.18, 1.62, 2.82]\n",
      "74/10: widths = [0, 4.25/12, 10.5/12, 11.5/12, 26.5/12]\n",
      "74/11:\n",
      "h1 = np.array(h)*81/12\n",
      "h1[2] = 52./12\n",
      "h1_ = [h1[0], h1[2], h1[4]]\n",
      "74/12: h = 1-np.array([0, 0.25, 0.5, 0.75, 1])\n",
      "74/13:\n",
      "h1 = np.array(h)*81/12\n",
      "h1[2] = 52./12\n",
      "h1_ = [h1[0], h1[2], h1[4]]\n",
      "74/14:\n",
      "h1 = h*81/12\n",
      "h1[2] = 52./12\n",
      "h1_ = [h1[0], h1[2], h1[4]]\n",
      "74/15: h = 1-np.array([0, 0.25, 0.5, 0.75, 1])\n",
      "74/16:\n",
      "h1 = h*81/12\n",
      "h1[2] = 52./12\n",
      "h1_ = [h1[0], h1[2], h1[4]]\n",
      "74/17: th = [0, 0.18, 1.18, 1.62, 2.82]\n",
      "74/18: widths = np.array([0, 10.5/12, 26.5/12])\n",
      "74/19: ex1 = [0, 1.4, 1.625]\n",
      "74/20: ex1 = np.array([0, 1.4, 1.625])*widths\n",
      "74/21:\n",
      "plot.figure()\n",
      "plot.plot(th, h1)\n",
      "plot.plot(ex1, h1_)\n",
      "74/22: ex1 = np.array([0, 1.625, 1.4])*widths\n",
      "74/23:\n",
      "plot.figure()\n",
      "plot.plot(th, h1)\n",
      "plot.plot(ex1, h1_)\n",
      "74/24: print h1, th, ex1\n",
      "74/25:\n",
      "h1 = h*81/12\n",
      "h1[2] = (81-52.)/12\n",
      "h1_ = [h1[0], h1[2], h1[4]]\n",
      "74/26: print h1, th, ex1\n",
      "74/27:\n",
      "plot.figure()\n",
      "plot.plot(th, h1)\n",
      "plot.plot(ex1, h1_)\n",
      "74/28: ex2 = np.array([0, 0.04, 0.14])*widths\n",
      "74/29: ex3 = np.array([0, 0.15, 0.38])*widths\n",
      "74/30: ex4 = np.array([0, 0.16, 0.83])*widths\n",
      "74/31:\n",
      "plot.figure()\n",
      "plot.plot(th, h1)\n",
      "plot.plot(ex1, h1_)\n",
      "plot.plot(ex2, h1_)\n",
      "plot.plot(ex3, h1_)\n",
      "plot.plot(ex4, h1_)\n",
      "74/32: ex1 = np.array([0, 1.625, 1.4])*widths\n",
      "74/33: ex2 = np.array([0, 0.69, 0.63])*widths\n",
      "74/34: ex3 = np.array([0, 1.8, 2])*widths\n",
      "74/35: ex4 = np.array([0, 2.68, 2.39])*widths\n",
      "74/36:\n",
      "plot.figure()\n",
      "plot.plot(th, h1)\n",
      "plot.plot(ex1, h1_)\n",
      "plot.plot(ex2, h1_)\n",
      "plot.plot(ex3, h1_)\n",
      "plot.plot(ex4, h1_)\n",
      "74/37: th = [0, 0.18, 1.18, 1.62, 2.82]\n",
      "74/38:\n",
      "plot.figure()\n",
      "plot.plot(th, h1)\n",
      "plot.plot(ex1, h1_)\n",
      "plot.plot(ex2, h1_)\n",
      "plot.plot(ex3, h1_)\n",
      "plot.plot(ex4, h1_)\n",
      "74/39: ex2\n",
      "74/40:\n",
      "plot.figure()\n",
      "plot.plot(th, h1)\n",
      "plot.plot(ex1, h1_)\n",
      "plot.plot(ex2, h1_)\n",
      "plot.plot(ex3, h1_)\n",
      "plot.plot(ex4, h1_)\n",
      "plot.xlabel(\"Moment (lb*ft)\")\n",
      "plot.ylabel(\"Height\")\n",
      "74/41: h1\n",
      "74/42:\n",
      "plot.figure()\n",
      "plot.plot(th, h1)\n",
      "plot.plot(ex1, h1_)\n",
      "plot.plot(ex2, h1_)\n",
      "plot.plot(ex3, h1_)\n",
      "plot.plot(ex4, h1_)\n",
      "plot.xlabel(\"Moment (lb*ft)\")\n",
      "plot.ylabel(\"Height (ft)\")\n",
      "plot.title(\"Height vs Moment for Eiffel Tower\")\n",
      "74/43:\n",
      "plot.figure(figsize=(20,10))\n",
      "plot.plot(th, h1)\n",
      "plot.plot(ex1, h1_)\n",
      "plot.plot(ex2, h1_)\n",
      "plot.plot(ex3, h1_)\n",
      "plot.plot(ex4, h1_)\n",
      "plot.xlabel(\"Moment (lb*ft)\")\n",
      "plot.ylabel(\"Height (ft)\")\n",
      "plot.title(\"Height vs Moment for Eiffel Tower\")\n",
      "74/44:\n",
      "plot.figure(figsize=(5,10))\n",
      "plot.plot(th, h1)\n",
      "plot.plot(ex1, h1_)\n",
      "plot.plot(ex2, h1_)\n",
      "plot.plot(ex3, h1_)\n",
      "plot.plot(ex4, h1_)\n",
      "plot.xlabel(\"Moment (lb*ft)\")\n",
      "plot.ylabel(\"Height (ft)\")\n",
      "plot.title(\"Height vs Moment for Eiffel Tower\")\n",
      "74/45:\n",
      "plot.figure(figsize=(5,7))\n",
      "plot.plot(th, h1)\n",
      "plot.plot(ex1, h1_)\n",
      "plot.plot(ex2, h1_)\n",
      "plot.plot(ex3, h1_)\n",
      "plot.plot(ex4, h1_)\n",
      "plot.xlabel(\"Moment (lb*ft)\")\n",
      "plot.ylabel(\"Height (ft)\")\n",
      "plot.title(\"Height vs Moment for Eiffel Tower\")\n",
      "74/46:\n",
      "plot.figure(figsize=(5,7))\n",
      "plot.plot(th, h1)\n",
      "plot.plot(ex1, h1_)\n",
      "plot.plot(ex2, h1_)\n",
      "plot.plot(ex3, h1_)\n",
      "plot.plot(ex4, h1_)\n",
      "plot.xlabel(\"Moment (lb*ft)\")\n",
      "plot.ylabel(\"Height (ft)\")\n",
      "plot.title(\"Height vs Moment for Eiffel Tower\")\n",
      "plot.legend()\n",
      "plot.show()\n",
      "74/47: widthsw = np.array([0, 6/12, 6.2/12])\n",
      "74/48:\n",
      "ex1w = np.array([0, 0.125, 0.46])*widthsw\n",
      "ex2w = np.array([0, 0.04, 0.14])*widthsw\n",
      "ex3w = np.array([0, 0.15, 0.38])*widthsw\n",
      "ex4w = np.array([0, 0.16, 0.83])*widthsw\n",
      "74/49: thw = [0, 0.013, 0.055, 0.119, 0.1935]\n",
      "74/50:\n",
      "h1w = h*59/12\n",
      "h1w[2] = (81-32.)/12\n",
      "h1w_ = [h1w[0], h1w[2], h1w[4]]\n",
      "74/51:\n",
      "plot.figure(figsize=(5,7))\n",
      "plot.plot(thw, h1w)\n",
      "plot.plot(ex1, h1w_)\n",
      "plot.plot(ex2, h1w_)\n",
      "plot.plot(ex3, h1w_)\n",
      "plot.plot(ex4, h1w_)\n",
      "plot.xlabel(\"Moment (lb*ft)\")\n",
      "plot.ylabel(\"Height (ft)\")\n",
      "plot.title(\"Height vs Moment for Eiffel Tower\")\n",
      "plot.legend()\n",
      "plot.show()\n",
      "74/52:\n",
      "plot.figure(figsize=(5,7))\n",
      "plot.plot(thw, h1w)\n",
      "plot.plot(ex1w, h1w_)\n",
      "plot.plot(ex2w, h1w_)\n",
      "plot.plot(ex3w, h1w_)\n",
      "plot.plot(ex4w, h1w_)\n",
      "plot.xlabel(\"Moment (lb*ft)\")\n",
      "plot.ylabel(\"Height (ft)\")\n",
      "plot.title(\"Height vs Moment for Eiffel Tower\")\n",
      "plot.legend()\n",
      "plot.show()\n",
      "74/53: h1w\n",
      "74/54:\n",
      "h1w = h*59/12\n",
      "h1w[2] = (59-32.)/12\n",
      "h1w_ = [h1w[0], h1w[2], h1w[4]]\n",
      "74/55: h1w\n",
      "74/56:\n",
      "plot.figure(figsize=(5,7))\n",
      "plot.plot(thw, h1w)\n",
      "plot.plot(ex1w, h1w_)\n",
      "plot.plot(ex2w, h1w_)\n",
      "plot.plot(ex3w, h1w_)\n",
      "plot.plot(ex4w, h1w_)\n",
      "plot.xlabel(\"Moment (lb*ft)\")\n",
      "plot.ylabel(\"Height (ft)\")\n",
      "plot.title(\"Height vs Moment for Washington Monument\")\n",
      "plot.legend()\n",
      "plot.show()\n",
      "74/57:\n",
      "fig = plot.figure(figsize=(5,7))\n",
      "ax = fig.add_axes()\n",
      "plot.plot(th, h1)\n",
      "plot.plot(ex1, h1_)\n",
      "plot.plot(ex2, h1_)\n",
      "plot.plot(ex3, h1_)\n",
      "plot.plot(ex4, h1_)\n",
      "plot.xlabel(\"Moment (lb*ft)\")\n",
      "plot.ylabel(\"Height (ft)\")\n",
      "plot.title(\"Height vs Moment for Eiffel Tower\")\n",
      "plot.legend()\n",
      "plot.show()\n",
      "74/58:\n",
      "fig = plot.figure(figsize=(5,7))\n",
      "ax = fig.add_axes()\n",
      "ax.plot(th, h1, label=\"Theoretical, p = 2.5 k/ft\")\n",
      "ax.plot(ex1, h1_, label=\"Experimental, p = 2.5 k/ft\")\n",
      "ax.plot(ex2, h1_)\n",
      "ax.plot(ex3, h1_)\n",
      "ax.plot(ex4, h1_)\n",
      "plot.xlabel(\"Moment (lb*ft)\")\n",
      "plot.ylabel(\"Height (ft)\")\n",
      "plot.title(\"Height vs Moment for Eiffel Tower\")\n",
      "plot.legend()\n",
      "plot.show()\n",
      "74/59:\n",
      "fig = plot.figure(figsize=(5,7))\n",
      "ax = plot.axes()\n",
      "ax = fig.add_axes(ax)\n",
      "ax.plot(th, h1, label=\"Theoretical, p = 2.5 k/ft\")\n",
      "ax.plot(ex1, h1_, label=\"Experimental, p = 2.5 k/ft\")\n",
      "ax.plot(ex2, h1_)\n",
      "ax.plot(ex3, h1_)\n",
      "ax.plot(ex4, h1_)\n",
      "plot.xlabel(\"Moment (lb*ft)\")\n",
      "plot.ylabel(\"Height (ft)\")\n",
      "plot.title(\"Height vs Moment for Eiffel Tower\")\n",
      "plot.legend()\n",
      "plot.show()\n",
      "74/60:\n",
      "fig = plot.figure(figsize=(5,7))\n",
      "ax = plot.axes()\n",
      "ax = fig.add_axes(ax)\n",
      "ax.plot(th, h1, label=\"Theoretical, p = 2.5 k/ft\")\n",
      "l1 = ax.plot(ex1, h1_, label=\"Experimental, p = 2.5 k/ft\")\n",
      "l1.set_dashes()\n",
      "l2 = ax.plot(ex2, h1_, label=\"Experimental, p = 1.0 k/ft\")\n",
      "l2.set_dashes([1])\n",
      "l3 = ax.plot(ex3, h1_, label=\"Experimental, p = 4.0 k/ft\")\n",
      "l3.set_dashes([2,2])\n",
      "l4 = ax.plot(ex4, h1_, label=\"Experimental, p = 5.5 k/ft\")\n",
      "l4.set_dashes([3,3,3])\n",
      "plot.xlabel(\"Moment (lb*ft)\")\n",
      "plot.ylabel(\"Height (ft)\")\n",
      "plot.title(\"Height vs Moment for Eiffel Tower\")\n",
      "plot.legend()\n",
      "plot.show()\n",
      "74/61:\n",
      "fig = plot.figure(figsize=(5,7))\n",
      "ax = plot.axes()\n",
      "ax = fig.add_axes(ax)\n",
      "ax.plot(th, h1, label=\"Theoretical, p = 2.5 k/ft\")\n",
      "l1, = ax.plot(ex1, h1_, label=\"Experimental, p = 2.5 k/ft\")\n",
      "l1.set_dashes()\n",
      "l2, = ax.plot(ex2, h1_, label=\"Experimental, p = 1.0 k/ft\")\n",
      "l2.set_dashes([1])\n",
      "l3, = ax.plot(ex3, h1_, label=\"Experimental, p = 4.0 k/ft\")\n",
      "l3.set_dashes([2,2])\n",
      "l4, = ax.plot(ex4, h1_, label=\"Experimental, p = 5.5 k/ft\")\n",
      "l4.set_dashes([3,3,3])\n",
      "plot.xlabel(\"Moment (lb*ft)\")\n",
      "plot.ylabel(\"Height (ft)\")\n",
      "plot.title(\"Height vs Moment for Eiffel Tower\")\n",
      "plot.legend()\n",
      "plot.show()\n",
      "74/62:\n",
      "fig = plot.figure(figsize=(5,7))\n",
      "ax = plot.axes()\n",
      "ax = fig.add_axes(ax)\n",
      "ax.plot(th, h1, label=\"Theoretical, p = 2.5 k/ft\")\n",
      "l1, = ax.plot(ex1, h1_, label=\"Experimental, p = 2.5 k/ft\")\n",
      "l1.set_dashes([0])\n",
      "l2, = ax.plot(ex2, h1_, label=\"Experimental, p = 1.0 k/ft\")\n",
      "l2.set_dashes([1])\n",
      "l3, = ax.plot(ex3, h1_, label=\"Experimental, p = 4.0 k/ft\")\n",
      "l3.set_dashes([2,2])\n",
      "l4, = ax.plot(ex4, h1_, label=\"Experimental, p = 5.5 k/ft\")\n",
      "l4.set_dashes([3,3,3])\n",
      "plot.xlabel(\"Moment (lb*ft)\")\n",
      "plot.ylabel(\"Height (ft)\")\n",
      "plot.title(\"Height vs Moment for Eiffel Tower\")\n",
      "plot.legend()\n",
      "plot.show()\n",
      "74/63:\n",
      "fig = plot.figure(figsize=(5,7))\n",
      "ax = plot.axes()\n",
      "ax = fig.add_axes(ax)\n",
      "ax.plot(th, h1, label=\"Theoretical, p = 2.5 k/ft\")\n",
      "l1, = ax.plot(ex1, h1_, label=\"Experimental, p = 2.5 k/ft\")\n",
      "l1.set_dashes([2,2])\n",
      "l2, = ax.plot(ex2, h1_, label=\"Experimental, p = 1.0 k/ft\")\n",
      "l2.set_dashes([1])\n",
      "l3, = ax.plot(ex3, h1_, label=\"Experimental, p = 4.0 k/ft\")\n",
      "l3.set_dashes([2,2])\n",
      "l4, = ax.plot(ex4, h1_, label=\"Experimental, p = 5.5 k/ft\")\n",
      "l4.set_dashes([3,3,3])\n",
      "plot.xlabel(\"Moment (lb*ft)\")\n",
      "plot.ylabel(\"Height (ft)\")\n",
      "plot.title(\"Height vs Moment for Eiffel Tower\")\n",
      "plot.legend()\n",
      "plot.show()\n",
      "74/64:\n",
      "fig = plot.figure(figsize=(5,7))\n",
      "ax = plot.axes()\n",
      "ax = fig.add_axes(ax)\n",
      "ax.plot(th, h1, label=\"Theoretical, p = 2.5 k/ft\")\n",
      "l1, = ax.plot(ex1, h1_, label=\"Experimental, p = 2.5 k/ft\")\n",
      "l1.set_dashes([2,2])\n",
      "l2, = ax.plot(ex2, h1_, label=\"Experimental, p = 1.0 k/ft\")\n",
      "l2.set_dashes([2,1])\n",
      "l3, = ax.plot(ex3, h1_, label=\"Experimental, p = 4.0 k/ft\")\n",
      "l3.set_dashes([2,4])\n",
      "l4, = ax.plot(ex4, h1_, label=\"Experimental, p = 5.5 k/ft\")\n",
      "l4.set_dashes([3,1])\n",
      "plot.xlabel(\"Moment (lb*ft)\")\n",
      "plot.ylabel(\"Height (ft)\")\n",
      "plot.title(\"Height vs Moment for Eiffel Tower\")\n",
      "plot.legend()\n",
      "plot.show()\n",
      "74/65:\n",
      "fig = plot.figure(figsize=(5,7))\n",
      "ax = plot.axes()\n",
      "ax = fig.add_axes(ax)\n",
      "ax.plot(th, h1, label=\"Theoretical, p = 2.5 k/ft\")\n",
      "l1, = ax.plot(ex1, h1_, label=\"Experimental, p = 2.5 k/ft\")\n",
      "l1.set_dashes([2,6])\n",
      "l2, = ax.plot(ex2, h1_, label=\"Experimental, p = 1.0 k/ft\")\n",
      "l2.set_dashes([2,1])\n",
      "l3, = ax.plot(ex3, h1_, label=\"Experimental, p = 4.0 k/ft\")\n",
      "l3.set_dashes([2,4])\n",
      "l4, = ax.plot(ex4, h1_, label=\"Experimental, p = 5.5 k/ft\")\n",
      "l4.set_dashes([3,1])\n",
      "plot.xlabel(\"Moment (lb*ft)\")\n",
      "plot.ylabel(\"Height (ft)\")\n",
      "plot.title(\"Height vs Moment for Eiffel Tower\")\n",
      "plot.legend()\n",
      "plot.show()\n",
      "74/66:\n",
      "fig = plot.figure(figsize=(5,7))\n",
      "ax = plot.axes()\n",
      "ax = fig.add_axes(ax)\n",
      "ax.plot(th, h1, label=\"Theoretical, p = 2.5 k/ft\")\n",
      "l1, = ax.plot(ex1, h1_, label=\"Experimental, p = 2.5 k/ft\")\n",
      "l1.set_dashes([2,1,2])\n",
      "l2, = ax.plot(ex2, h1_, label=\"Experimental, p = 1.0 k/ft\")\n",
      "l2.set_dashes([2,1])\n",
      "l3, = ax.plot(ex3, h1_, label=\"Experimental, p = 4.0 k/ft\")\n",
      "l3.set_dashes([2,4])\n",
      "l4, = ax.plot(ex4, h1_, label=\"Experimental, p = 5.5 k/ft\")\n",
      "l4.set_dashes([3,1])\n",
      "plot.xlabel(\"Moment (lb*ft)\")\n",
      "plot.ylabel(\"Height (ft)\")\n",
      "plot.title(\"Height vs Moment for Eiffel Tower\")\n",
      "plot.legend()\n",
      "plot.show()\n",
      "74/67:\n",
      "fig = plot.figure(figsize=(5,7))\n",
      "ax = plot.axes()\n",
      "ax = fig.add_axes(ax)\n",
      "ax.plot(th, h1, label=\"Theoretical, p = 2.5 k/ft\")\n",
      "l1, = ax.plot(ex1, h1_, label=\"Experimental, p = 2.5 k/ft\")\n",
      "l1.set_dashes([2,1,2,1])\n",
      "l2, = ax.plot(ex2, h1_, label=\"Experimental, p = 1.0 k/ft\")\n",
      "l2.set_dashes([2,1])\n",
      "l3, = ax.plot(ex3, h1_, label=\"Experimental, p = 4.0 k/ft\")\n",
      "l3.set_dashes([2,4])\n",
      "l4, = ax.plot(ex4, h1_, label=\"Experimental, p = 5.5 k/ft\")\n",
      "l4.set_dashes([3,1])\n",
      "plot.xlabel(\"Moment (lb*ft)\")\n",
      "plot.ylabel(\"Height (ft)\")\n",
      "plot.title(\"Height vs Moment for Eiffel Tower\")\n",
      "plot.legend()\n",
      "plot.show()\n",
      "74/68:\n",
      "fig = plot.figure(figsize=(5,7))\n",
      "ax = plot.axes()\n",
      "ax = fig.add_axes(ax)\n",
      "ax.plot(th, h1, label=\"Theoretical, p = 2.5 k/ft\")\n",
      "l1, = ax.plot(ex1, h1_, label=\"Experimental, p = 2.5 k/ft\")\n",
      "l1.set_dashes([4,1,4,1])\n",
      "l2, = ax.plot(ex2, h1_, label=\"Experimental, p = 1.0 k/ft\")\n",
      "l2.set_dashes([2,1])\n",
      "l3, = ax.plot(ex3, h1_, label=\"Experimental, p = 4.0 k/ft\")\n",
      "l3.set_dashes([2,4])\n",
      "l4, = ax.plot(ex4, h1_, label=\"Experimental, p = 5.5 k/ft\")\n",
      "l4.set_dashes([3,1])\n",
      "plot.xlabel(\"Moment (lb*ft)\")\n",
      "plot.ylabel(\"Height (ft)\")\n",
      "plot.title(\"Height vs Moment for Eiffel Tower\")\n",
      "plot.legend()\n",
      "plot.show()\n",
      "74/69:\n",
      "fig = plot.figure(figsize=(5,7))\n",
      "ax = plot.axes()\n",
      "ax = fig.add_axes(ax)\n",
      "ax.plot(th, h1, label=\"Theoretical, p = 2.5 k/ft\")\n",
      "l1, = ax.plot(ex1, h1_, label=\"Experimental, p = 2.5 k/ft\")\n",
      "l1.set_dashes([2,1,4,1])\n",
      "l2, = ax.plot(ex2, h1_, label=\"Experimental, p = 1.0 k/ft\")\n",
      "l2.set_dashes([2,1])\n",
      "l3, = ax.plot(ex3, h1_, label=\"Experimental, p = 4.0 k/ft\")\n",
      "l3.set_dashes([2,4])\n",
      "l4, = ax.plot(ex4, h1_, label=\"Experimental, p = 5.5 k/ft\")\n",
      "l4.set_dashes([3,1])\n",
      "plot.xlabel(\"Moment (lb*ft)\")\n",
      "plot.ylabel(\"Height (ft)\")\n",
      "plot.title(\"Height vs Moment for Eiffel Tower\")\n",
      "plot.legend()\n",
      "plot.show()\n",
      "74/70:\n",
      "fig = plot.figure(figsize=(5,7))\n",
      "ax = plot.axes()\n",
      "ax = fig.add_axes(ax)\n",
      "ax.plot(th, h1, label=\"Theoretical, p = 2.5 k/ft\")\n",
      "l1, = ax.plot(ex1, h1_, label=\"Experimental, p = 2.5 k/ft\")\n",
      "l1.set_dashes([2,1,4,1])\n",
      "l2, = ax.plot(ex2, h1_, label=\"Experimental, p = 1.0 k/ft\")\n",
      "l2.set_dashes([2,2,10,2])\n",
      "l3, = ax.plot(ex3, h1_, label=\"Experimental, p = 4.0 k/ft\")\n",
      "l3.set_dashes([2,4])\n",
      "l4, = ax.plot(ex4, h1_, label=\"Experimental, p = 5.5 k/ft\")\n",
      "l4.set_dashes([3,1])\n",
      "plot.xlabel(\"Moment (lb*ft)\")\n",
      "plot.ylabel(\"Height (ft)\")\n",
      "plot.title(\"Height vs Moment for Eiffel Tower\")\n",
      "plot.legend()\n",
      "plot.show()\n",
      "74/71:\n",
      "fig = plot.figure(figsize=(5,7))\n",
      "ax = plot.axes()\n",
      "ax = fig.add_axes(ax)\n",
      "ax.plot(th, h1, label=\"Theoretical, p = 2.5 k/ft\")\n",
      "l1, = ax.plot(ex1, h1_, label=\"Experimental, p = 2.5 k/ft\")\n",
      "l1.set_dashes([2,1,4,1])\n",
      "l2, = ax.plot(ex2, h1_, label=\"Experimental, p = 1.0 k/ft\")\n",
      "l2.set_dashes([2,2,10,2])\n",
      "l3, = ax.plot(ex3, h1_, label=\"Experimental, p = 4.0 k/ft\")\n",
      "l3.set_dashes([2,4])\n",
      "l4, = ax.plot(ex4, h1_, label=\"Experimental, p = 5.5 k/ft\")\n",
      "l4.set_dashes([3,1,5,1])\n",
      "plot.xlabel(\"Moment (lb*ft)\")\n",
      "plot.ylabel(\"Height (ft)\")\n",
      "plot.title(\"Height vs Moment for Eiffel Tower\")\n",
      "plot.legend()\n",
      "plot.show()\n",
      "74/72:\n",
      "fig = plot.figure(figsize=(5,7))\n",
      "ax = plot.axes()\n",
      "ax = fig.add_axes(ax)\n",
      "ax.plot(th, h1, label=\"Theoretical, p = 2.5 k/ft\")\n",
      "l1, = ax.plot(ex1, h1_, label=\"Experimental, p = 2.5 k/ft\")\n",
      "l1.set_dashes([2,1,4,1])\n",
      "l2, = ax.plot(ex2, h1_, label=\"Experimental, p = 1.0 k/ft\")\n",
      "l2.set_dashes([2,2,10,2])\n",
      "l3, = ax.plot(ex3, h1_, label=\"Experimental, p = 4.0 k/ft\")\n",
      "l3.set_dashes([2,6,2,6])\n",
      "l4, = ax.plot(ex4, h1_, label=\"Experimental, p = 5.5 k/ft\")\n",
      "l4.set_dashes([3,1,5,1])\n",
      "plot.xlabel(\"Moment (lb*ft)\")\n",
      "plot.ylabel(\"Height (ft)\")\n",
      "plot.title(\"Height vs Moment for Eiffel Tower\")\n",
      "plot.legend()\n",
      "plot.show()\n",
      "74/73:\n",
      "fig = plot.figure(figsize=(5,7))\n",
      "ax = plot.axes()\n",
      "ax = fig.add_axes(ax)\n",
      "ax.plot(th, h1, label=\"Theoretical, p = 2.5 k/ft\")\n",
      "l1, = ax.plot(ex1, h1_, label=\"Experimental, p = 2.5 k/ft\")\n",
      "l1.set_dashes([2,2,4,2])\n",
      "l2, = ax.plot(ex2, h1_, label=\"Experimental, p = 1.0 k/ft\")\n",
      "l2.set_dashes([2,2,10,2])\n",
      "l3, = ax.plot(ex3, h1_, label=\"Experimental, p = 4.0 k/ft\")\n",
      "l3.set_dashes([2,6,2,6])\n",
      "l4, = ax.plot(ex4, h1_, label=\"Experimental, p = 5.5 k/ft\")\n",
      "l4.set_dashes([3,1,5,1])\n",
      "plot.xlabel(\"Moment (lb*ft)\")\n",
      "plot.ylabel(\"Height (ft)\")\n",
      "plot.title(\"Height vs Moment for Eiffel Tower\")\n",
      "plot.legend()\n",
      "plot.show()\n",
      "74/74:\n",
      "fig = plot.figure(figsize=(5,7))\n",
      "ax = plot.axes()\n",
      "ax = fig.add_axes(ax)\n",
      "ax.plot(th, h1, label=\"Theoretical, p = 2.5 k/ft\")\n",
      "l1, = ax.plot(ex1, h1_, label=\"Experimental, p = 2.5 k/ft\")\n",
      "l1.set_dashes([2,2,4,2])\n",
      "l2, = ax.plot(ex2, h1_, label=\"Experimental, p = 1.0 k/ft\")\n",
      "l2.set_dashes([2,2,10,2])\n",
      "l3, = ax.plot(ex3, h1_, label=\"Experimental, p = 4.0 k/ft\")\n",
      "l3.set_dashes([2,4,2,4])\n",
      "l4, = ax.plot(ex4, h1_, label=\"Experimental, p = 5.5 k/ft\")\n",
      "l4.set_dashes([3,4,5,4])\n",
      "plot.xlabel(\"Moment (lb*ft)\")\n",
      "plot.ylabel(\"Height (ft)\")\n",
      "plot.title(\"Height vs Moment for Eiffel Tower\")\n",
      "plot.legend()\n",
      "plot.show()\n",
      "74/75:\n",
      "fig = plot.figure(figsize=(5,7))\n",
      "ax = plot.axes()\n",
      "ax = fig.add_axes(ax)\n",
      "ax.plot(th, h1, label=\"Theoretical, p = 2.5 k/ft\")\n",
      "l1, = ax.plot(ex1, h1_, label=\"Experimental, p = 2.5 k/ft\")\n",
      "l1.set_dashes([2,2,4,2])\n",
      "l2, = ax.plot(ex2, h1_, label=\"Experimental, p = 1.0 k/ft\")\n",
      "l2.set_dashes([2,2,10,2])\n",
      "l3, = ax.plot(ex3, h1_, label=\"Experimental, p = 4.0 k/ft\")\n",
      "l3.set_dashes([2,4,2,4])\n",
      "l4, = ax.plot(ex4, h1_, label=\"Experimental, p = 5.5 k/ft\")\n",
      "l4.set_dashes([3,4,7,4])\n",
      "plot.xlabel(\"Moment (lb*ft)\")\n",
      "plot.ylabel(\"Height (ft)\")\n",
      "plot.title(\"Height vs Moment for Eiffel Tower\")\n",
      "plot.legend()\n",
      "plot.show()\n",
      "74/76:\n",
      "fig = plot.figure(figsize=(5,7))\n",
      "ax = plot.axes()\n",
      "ax = fig.add_axes(ax)\n",
      "ax.plot(th, h1, label=\"Theoretical, p = 2.5 k/ft\")\n",
      "l1, = ax.plot(ex1, h1_, label=\"Experimental, p = 2.5 k/ft\")\n",
      "l1.set_dashes([2,4,4,2])\n",
      "l2, = ax.plot(ex2, h1_, label=\"Experimental, p = 1.0 k/ft\")\n",
      "l2.set_dashes([2,2,10,2])\n",
      "l3, = ax.plot(ex3, h1_, label=\"Experimental, p = 4.0 k/ft\")\n",
      "l3.set_dashes([2,4,2,4])\n",
      "l4, = ax.plot(ex4, h1_, label=\"Experimental, p = 5.5 k/ft\")\n",
      "l4.set_dashes([3,4,7,4])\n",
      "plot.xlabel(\"Moment (lb*ft)\")\n",
      "plot.ylabel(\"Height (ft)\")\n",
      "plot.title(\"Height vs Moment for Eiffel Tower\")\n",
      "plot.legend()\n",
      "plot.show()\n",
      "74/77:\n",
      "fig = plot.figure(figsize=(5,7))\n",
      "ax = plot.axes()\n",
      "ax = fig.add_axes(ax)\n",
      "ax.plot(th, h1, label=\"Theoretical, p = 2.5 k/ft\")\n",
      "l1, = ax.plot(ex1, h1_, label=\"Experimental, p = 2.5 k/ft\")\n",
      "l1.set_dashes([2,4,4,2])\n",
      "l2, = ax.plot(ex2, h1_, label=\"Experimental, p = 1.0 k/ft\")\n",
      "l2.set_dashes([2,2,10,2])\n",
      "l3, = ax.plot(ex3, h1_, label=\"Experimental, p = 4.0 k/ft\")\n",
      "l3.set_dashes([2,4,2,4])\n",
      "l4, = ax.plot(ex4, h1_, label=\"Experimental, p = 5.5 k/ft\")\n",
      "l4.set_dashes([6,2,6,2])\n",
      "plot.xlabel(\"Moment (lb*ft)\")\n",
      "plot.ylabel(\"Height (ft)\")\n",
      "plot.title(\"Height vs Moment for Eiffel Tower\")\n",
      "plot.legend()\n",
      "plot.show()\n",
      "74/78:\n",
      "fig = plot.figure(figsize=(5,7))\n",
      "ax = plot.axes()\n",
      "ax = fig.add_axes(ax)\n",
      "ax.plot(th, h1, label=\"Theoretical, p = 2.5 k/ft\")\n",
      "l1, = ax.plot(ex1, h1_, label=\"Experimental, p = 2.5 k/ft\")\n",
      "l1.set_dashes([2,4,4,2])\n",
      "l2, = ax.plot(ex2, h1_, label=\"Experimental, p = 1.0 k/ft\")\n",
      "l2.set_dashes([2,2,12,2])\n",
      "l3, = ax.plot(ex3, h1_, label=\"Experimental, p = 4.0 k/ft\")\n",
      "l3.set_dashes([2,4,2,4])\n",
      "l4, = ax.plot(ex4, h1_, label=\"Experimental, p = 5.5 k/ft\")\n",
      "l4.set_dashes([6,2,6,2])\n",
      "plot.xlabel(\"Moment (lb*ft)\")\n",
      "plot.ylabel(\"Height (ft)\")\n",
      "plot.title(\"Height vs Moment for Eiffel Tower\")\n",
      "plot.legend()\n",
      "plot.show()\n",
      "74/79:\n",
      "fig = plot.figure(figsize=(5,7))\n",
      "ax = plot.axes()\n",
      "ax = fig.add_axes(ax)\n",
      "ax.plot(th, h1, label=\"Theoretical, p = 2.5 k/ft\")\n",
      "l1, = ax.plot(ex1w, h1w_, label=\"Experimental, p = 2.5 k/ft\")\n",
      "l1.set_dashes([2,4,4,2])\n",
      "l2, = ax.plot(ex2w, h1w_, label=\"Experimental, p = 1.0 k/ft\")\n",
      "l2.set_dashes([2,2,12,2])\n",
      "l3, = ax.plot(ex3w, h1w_, label=\"Experimental, p = 4.0 k/ft\")\n",
      "l3.set_dashes([2,4,2,4])\n",
      "l4, = ax.plot(ex4w, h1w_, label=\"Experimental, p = 5.5 k/ft\")\n",
      "l4.set_dashes([6,2,6,2])\n",
      "plot.xlabel(\"Moment (lb*ft)\")\n",
      "plot.ylabel(\"Height (ft)\")\n",
      "plot.title(\"Height vs Moment for Eiffel Tower\")\n",
      "plot.legend()\n",
      "plot.show()\n",
      "74/80:\n",
      "fig = plot.figure(figsize=(6,7))\n",
      "ax = plot.axes()\n",
      "ax = fig.add_axes(ax)\n",
      "ax.plot(th, h1, label=\"Theoretical, p = 2.5 k/ft\")\n",
      "l1, = ax.plot(ex1w, h1w_, label=\"Experimental, p = 2.5 k/ft\")\n",
      "l1.set_dashes([2,4,4,2])\n",
      "l2, = ax.plot(ex2w, h1w_, label=\"Experimental, p = 1.0 k/ft\")\n",
      "l2.set_dashes([2,2,12,2])\n",
      "l3, = ax.plot(ex3w, h1w_, label=\"Experimental, p = 4.0 k/ft\")\n",
      "l3.set_dashes([2,4,2,4])\n",
      "l4, = ax.plot(ex4w, h1w_, label=\"Experimental, p = 5.5 k/ft\")\n",
      "l4.set_dashes([6,2,6,2])\n",
      "plot.xlabel(\"Moment (lb*ft)\")\n",
      "plot.ylabel(\"Height (ft)\")\n",
      "plot.title(\"Height vs Moment for Eiffel Tower\")\n",
      "plot.legend()\n",
      "plot.show()\n",
      "74/81:\n",
      "fig = plot.figure(figsize=(7,7))\n",
      "ax = plot.axes()\n",
      "ax = fig.add_axes(ax)\n",
      "ax.plot(th, h1, label=\"Theoretical, p = 2.5 k/ft\")\n",
      "l1, = ax.plot(ex1, h1_, label=\"Experimental, p = 2.5 k/ft\")\n",
      "l1.set_dashes([2,4,4,2])\n",
      "l2, = ax.plot(ex2, h1_, label=\"Experimental, p = 1.0 k/ft\")\n",
      "l2.set_dashes([2,2,12,2])\n",
      "l3, = ax.plot(ex3, h1_, label=\"Experimental, p = 4.0 k/ft\")\n",
      "l3.set_dashes([2,4,2,4])\n",
      "l4, = ax.plot(ex4, h1_, label=\"Experimental, p = 5.5 k/ft\")\n",
      "l4.set_dashes([6,2,6,2])\n",
      "plot.xlabel(\"Moment (lb*ft)\")\n",
      "plot.ylabel(\"Height (ft)\")\n",
      "plot.title(\"Height vs Moment for Eiffel Tower\")\n",
      "plot.legend()\n",
      "plot.show()\n",
      "74/82:\n",
      "fig = plot.figure(figsize=(7,7))\n",
      "ax = plot.axes()\n",
      "ax = fig.add_axes(ax)\n",
      "ax.plot(th, h1, label=\"Theoretical, p = 2.5 k/ft\")\n",
      "l1, = ax.plot(ex1w, h1w_, label=\"Experimental, p = 2.5 k/ft\")\n",
      "l1.set_dashes([2,4,4,2])\n",
      "l2, = ax.plot(ex2w, h1w_, label=\"Experimental, p = 1.0 k/ft\")\n",
      "l2.set_dashes([2,2,12,2])\n",
      "l3, = ax.plot(ex3w, h1w_, label=\"Experimental, p = 4.0 k/ft\")\n",
      "l3.set_dashes([2,4,2,4])\n",
      "l4, = ax.plot(ex4w, h1w_, label=\"Experimental, p = 5.5 k/ft\")\n",
      "l4.set_dashes([6,2,6,2])\n",
      "plot.xlabel(\"Moment (lb*ft)\")\n",
      "plot.ylabel(\"Height (ft)\")\n",
      "plot.title(\"Height vs Moment for Eiffel Tower\")\n",
      "plot.legend()\n",
      "plot.show()\n",
      "74/83:\n",
      "fig = plot.figure(figsize=(7,7))\n",
      "ax = plot.axes()\n",
      "ax = fig.add_axes(ax)\n",
      "l0 = ax.plot(th, h1, label=\"Theoretical, p = 2.5 k/ft\")\n",
      "l0.set_dashes([2,4,4,2])\n",
      "l1, = ax.plot(ex1w, h1w_, label=\"Experimental, p = 2.5 k/ft\")\n",
      "l2, = ax.plot(ex2w, h1w_, label=\"Experimental, p = 1.0 k/ft\")\n",
      "l2.set_dashes([2,2,12,2])\n",
      "l3, = ax.plot(ex3w, h1w_, label=\"Experimental, p = 4.0 k/ft\")\n",
      "l3.set_dashes([2,4,2,4])\n",
      "l4, = ax.plot(ex4w, h1w_, label=\"Experimental, p = 5.5 k/ft\")\n",
      "l4.set_dashes([6,2,6,2])\n",
      "plot.xlabel(\"Moment (lb*ft)\")\n",
      "plot.ylabel(\"Height (ft)\")\n",
      "plot.title(\"Height vs Moment for Eiffel Tower\")\n",
      "plot.legend()\n",
      "plot.show()\n",
      "74/84:\n",
      "fig = plot.figure(figsize=(7,7))\n",
      "ax = plot.axes()\n",
      "ax = fig.add_axes(ax)\n",
      "l0, = ax.plot(th, h1, label=\"Theoretical, p = 2.5 k/ft\")\n",
      "l0.set_dashes([2,4,4,2])\n",
      "l1, = ax.plot(ex1w, h1w_, label=\"Experimental, p = 2.5 k/ft\")\n",
      "l2, = ax.plot(ex2w, h1w_, label=\"Experimental, p = 1.0 k/ft\")\n",
      "l2.set_dashes([2,2,12,2])\n",
      "l3, = ax.plot(ex3w, h1w_, label=\"Experimental, p = 4.0 k/ft\")\n",
      "l3.set_dashes([2,4,2,4])\n",
      "l4, = ax.plot(ex4w, h1w_, label=\"Experimental, p = 5.5 k/ft\")\n",
      "l4.set_dashes([6,2,6,2])\n",
      "plot.xlabel(\"Moment (lb*ft)\")\n",
      "plot.ylabel(\"Height (ft)\")\n",
      "plot.title(\"Height vs Moment for Eiffel Tower\")\n",
      "plot.legend()\n",
      "plot.show()\n",
      "74/85:\n",
      "fig = plot.figure(figsize=(7,7))\n",
      "ax = plot.axes()\n",
      "ax = fig.add_axes(ax)\n",
      "l0, = ax.plot(th, h1, label=\"Theoretical, p = 2.5 k/ft\")\n",
      "l0.set_dashes([2,4,4,2])\n",
      "l1, = ax.plot(ex1w, h1w_, label=\"Experimental, p = 2.5 k/ft\")\n",
      "l2, = ax.plot(ex2w, h1w_, label=\"Experimental, p = 1.0 k/ft\")\n",
      "l2.set_dashes([2,2,12,2])\n",
      "l3, = ax.plot(ex3w, h1w_, label=\"Experimental, p = 4.0 k/ft\")\n",
      "l3.set_dashes([2,4,2,4])\n",
      "l4, = ax.plot(ex4w, h1w_, label=\"Experimental, p = 5.5 k/ft\")\n",
      "l4.set_dashes([6,2,6,2])\n",
      "plot.xlabel(\"Moment (lb*ft)\")\n",
      "plot.ylabel(\"Height (ft)\")\n",
      "plot.title(\"Height vs Moment for Washington Monument\")\n",
      "plot.legend()\n",
      "plot.show()\n",
      "74/86:\n",
      "th1 = [0, -0.508, -1.67, -1.4, -1.25]\n",
      "t1 = [0, -1.625, -1.69]\n",
      "t2 = [0, -0.69, -0.63]\n",
      "t3 = [0, -1.7, -1.94]\n",
      "t4 = [0, -2.7, -2.3]\n",
      "74/87:\n",
      "th1 = [0, -0.508, -1.67, -1.4, -1.25]\n",
      "t1 = [0, -1.625, -1.69]\n",
      "t2 = [0, -0.69, -0.63]\n",
      "t3 = [0, -1.7, -1.94]\n",
      "t4 = [0, -2.7, -2.3]\n",
      "c1 = [0, 1.625, 1.69]\n",
      "c2 = [0, 0.69, 0.63]\n",
      "c3 = [0, 1.8, 2]\n",
      "c4 = [0, 2.68, 2.39]\n",
      "74/88:\n",
      "fig = plot.figure(figsize=(7,7))\n",
      "ax = plot.axes()\n",
      "ax = fig.add_axes(ax)\n",
      "ax.plot(th1, h1, label=\"Theoretical, p = 2.5 k/ft\")\n",
      "l1, = ax.plot(t1, h1_, label=\"Experimental, p = 2.5 k/ft\")\n",
      "l1.set_dashes([2,4,4,2])\n",
      "l2, = ax.plot(t2, h1_, label=\"Experimental, p = 1.0 k/ft\")\n",
      "l2.set_dashes([2,2,12,2])\n",
      "l3, = ax.plot(t3, h1_, label=\"Experimental, p = 4.0 k/ft\")\n",
      "l3.set_dashes([2,4,2,4])\n",
      "l4, = ax.plot(t4, h1_, label=\"Experimental, p = 5.5 k/ft\")\n",
      "l4.set_dashes([6,2,6,2])\n",
      "l5, = ax.plot(c1, h1_, label=\"Experimental, p = 2.5 k/ft\")\n",
      "l5.set_dashes([2,4,4,2])\n",
      "l6, = ax.plot(c2, h1_, label=\"Experimental, p = 1.0 k/ft\")\n",
      "l6.set_dashes([2,2,12,2])\n",
      "l7, = ax.plot(c3, h1_, label=\"Experimental, p = 4.0 k/ft\")\n",
      "l7.set_dashes([2,4,2,4])\n",
      "l8, = ax.plot(c4, h1_, label=\"Experimental, p = 5.5 k/ft\")\n",
      "l8.set_dashes([6,2,6,2])\n",
      "plot.xlabel(\"Tension/Compression (lb)\")\n",
      "plot.ylabel(\"Height (ft)\")\n",
      "plot.title(\"Height vs Tension/Compression for Eiffel Tower\")\n",
      "plot.legend()\n",
      "plot.show()\n",
      "74/89:\n",
      "th1 = [0, -0.508, -1.67, -1.69, -1.25]\n",
      "t1 = [0, -1.625, -1.69]\n",
      "t2 = [0, -0.69, -0.63]\n",
      "t3 = [0, -1.7, -1.94]\n",
      "t4 = [0, -2.7, -2.3]\n",
      "c1 = [0, 1.625, 1.69]\n",
      "c2 = [0, 0.69, 0.63]\n",
      "c3 = [0, 1.8, 2]\n",
      "c4 = [0, 2.68, 2.39]\n",
      "74/90:\n",
      "fig = plot.figure(figsize=(7,7))\n",
      "ax = plot.axes()\n",
      "ax = fig.add_axes(ax)\n",
      "ax.plot(th1, h1, label=\"Theoretical, p = 2.5 k/ft\")\n",
      "l1, = ax.plot(t1, h1_, label=\"Experimental, p = 2.5 k/ft\")\n",
      "l1.set_dashes([2,4,4,2])\n",
      "l2, = ax.plot(t2, h1_, label=\"Experimental, p = 1.0 k/ft\")\n",
      "l2.set_dashes([2,2,12,2])\n",
      "l3, = ax.plot(t3, h1_, label=\"Experimental, p = 4.0 k/ft\")\n",
      "l3.set_dashes([2,4,2,4])\n",
      "l4, = ax.plot(t4, h1_, label=\"Experimental, p = 5.5 k/ft\")\n",
      "l4.set_dashes([6,2,6,2])\n",
      "l5, = ax.plot(c1, h1_, label=\"Experimental, p = 2.5 k/ft\")\n",
      "l5.set_dashes([2,4,4,2])\n",
      "l6, = ax.plot(c2, h1_, label=\"Experimental, p = 1.0 k/ft\")\n",
      "l6.set_dashes([2,2,12,2])\n",
      "l7, = ax.plot(c3, h1_, label=\"Experimental, p = 4.0 k/ft\")\n",
      "l7.set_dashes([2,4,2,4])\n",
      "l8, = ax.plot(c4, h1_, label=\"Experimental, p = 5.5 k/ft\")\n",
      "l8.set_dashes([6,2,6,2])\n",
      "plot.xlabel(\"Tension/Compression (lb)\")\n",
      "plot.ylabel(\"Height (ft)\")\n",
      "plot.title(\"Height vs Tension/Compression for Eiffel Tower\")\n",
      "plot.legend()\n",
      "plot.show()\n",
      "74/91: h1_\n",
      "74/92:\n",
      "th1 = [0, -0.508, -1.67, -1.69, -1.25]\n",
      "t1 = [0, -1.625, -1.69] + [0, 1.625, 1.69]\n",
      "t2 = [0, -0.69, -0.63] + [0, 0.69, 0.63]\n",
      "t3 = [0, -1.7, -1.94] + [0, 1.8, 2]\n",
      "t4 = [0, -2.7, -2.3] + [0, 2.68, 2.39]\n",
      "74/93:\n",
      "fig = plot.figure(figsize=(7,7))\n",
      "ax = plot.axes()\n",
      "ax = fig.add_axes(ax)\n",
      "ax.plot(th1, h1, label=\"Theoretical, p = 2.5 k/ft\")\n",
      "l1, = ax.plot(t1, h1_ + h1_, label=\"Experimental, p = 2.5 k/ft\")\n",
      "l1.set_dashes([2,4,4,2])\n",
      "l2, = ax.plot(t2, h1_, label=\"Experimental, p = 1.0 k/ft\")\n",
      "l2.set_dashes([2,2,12,2])\n",
      "l3, = ax.plot(t3, h1_, label=\"Experimental, p = 4.0 k/ft\")\n",
      "l3.set_dashes([2,4,2,4])\n",
      "l4, = ax.plot(t4, h1_, label=\"Experimental, p = 5.5 k/ft\")\n",
      "l4.set_dashes([6,2,6,2])\n",
      "plot.xlabel(\"Tension/Compression (lb)\")\n",
      "plot.ylabel(\"Height (ft)\")\n",
      "plot.title(\"Height vs Tension/Compression for Eiffel Tower\")\n",
      "plot.legend()\n",
      "plot.show()\n",
      "74/94:\n",
      "fig = plot.figure(figsize=(7,7))\n",
      "ax = plot.axes()\n",
      "ax = fig.add_axes(ax)\n",
      "ax.plot(th1, h1, label=\"Theoretical, p = 2.5 k/ft\")\n",
      "l1, = ax.plot(t1, h1_ + h1_, label=\"Experimental, p = 2.5 k/ft\")\n",
      "l1.set_dashes([2,4,4,2])\n",
      "l2, = ax.plot(t2, h1_ + h1_, label=\"Experimental, p = 1.0 k/ft\")\n",
      "l2.set_dashes([2,2,12,2])\n",
      "l3, = ax.plot(t3, h1_ + h1_, label=\"Experimental, p = 4.0 k/ft\")\n",
      "l3.set_dashes([2,4,2,4])\n",
      "l4, = ax.plot(t4, h1_ + h1_, label=\"Experimental, p = 5.5 k/ft\")\n",
      "l4.set_dashes([6,2,6,2])\n",
      "plot.xlabel(\"Tension/Compression (lb)\")\n",
      "plot.ylabel(\"Height (ft)\")\n",
      "plot.title(\"Height vs Tension/Compression for Eiffel Tower\")\n",
      "plot.legend()\n",
      "plot.show()\n",
      "74/95:\n",
      "th1 = [0, -0.508, -1.67, -1.69, -1.25]\n",
      "t1 = [0, -1.625, -1.69] + [0, 1.625, 1.69]\n",
      "t2 = [0, -0.69, -0.63] + [0, 0.69, 0.63]\n",
      "t3 = [0, -1.7, -1.94] + [0, 1.8, 2]\n",
      "t4 = [0, -2.7, -2.3] + [0, 2.68, 2.39]\n",
      "74/96:\n",
      "th1 = [0, -0.508, -1.67, -1.69, -1.25]\n",
      "t1 = [1.69, 1.625, 0, -1.625, -1.69]\n",
      "t2 = [0.63, 0.69, 0, -0.69, -0.63]\n",
      "t3 = [2, 1.8, 0, -1.7, -1.94]\n",
      "t4 = [2.39, 2.68, 0, -2.7, -2.3]\n",
      "74/97: h1_ = [h1_[0], h1_[1], h1_[2], h1_[1], h1_[0]]\n",
      "74/98:\n",
      "fig = plot.figure(figsize=(7,7))\n",
      "ax = plot.axes()\n",
      "ax = fig.add_axes(ax)\n",
      "ax.plot(th1, h1, label=\"Theoretical, p = 2.5 k/ft\")\n",
      "l1, = ax.plot(t1, h1_, label=\"Experimental, p = 2.5 k/ft\")\n",
      "l1.set_dashes([2,4,4,2])\n",
      "l2, = ax.plot(t2, h1_, label=\"Experimental, p = 1.0 k/ft\")\n",
      "l2.set_dashes([2,2,12,2])\n",
      "l3, = ax.plot(t3, h1_, label=\"Experimental, p = 4.0 k/ft\")\n",
      "l3.set_dashes([2,4,2,4])\n",
      "l4, = ax.plot(t4, h1_, label=\"Experimental, p = 5.5 k/ft\")\n",
      "l4.set_dashes([6,2,6,2])\n",
      "plot.xlabel(\"Tension/Compression (lb)\")\n",
      "plot.ylabel(\"Height (ft)\")\n",
      "plot.title(\"Height vs Tension/Compression for Eiffel Tower\")\n",
      "plot.legend()\n",
      "plot.show()\n",
      "74/99:\n",
      "h1 = h*81/12\n",
      "h1[2] = (81-52.)/12\n",
      "h1_ = [h1[0], h1[2], h1[4]]\n",
      "74/100: h1_ = [h1_[2], h1_[1], h1_[0], h1_[1], h1_[2]]\n",
      "74/101:\n",
      "fig = plot.figure(figsize=(7,7))\n",
      "ax = plot.axes()\n",
      "ax = fig.add_axes(ax)\n",
      "ax.plot(th1, h1, label=\"Theoretical, p = 2.5 k/ft\")\n",
      "l1, = ax.plot(t1, h1_, label=\"Experimental, p = 2.5 k/ft\")\n",
      "l1.set_dashes([2,4,4,2])\n",
      "l2, = ax.plot(t2, h1_, label=\"Experimental, p = 1.0 k/ft\")\n",
      "l2.set_dashes([2,2,12,2])\n",
      "l3, = ax.plot(t3, h1_, label=\"Experimental, p = 4.0 k/ft\")\n",
      "l3.set_dashes([2,4,2,4])\n",
      "l4, = ax.plot(t4, h1_, label=\"Experimental, p = 5.5 k/ft\")\n",
      "l4.set_dashes([6,2,6,2])\n",
      "plot.xlabel(\"Tension/Compression (lb)\")\n",
      "plot.ylabel(\"Height (ft)\")\n",
      "plot.title(\"Height vs Tension/Compression for Eiffel Tower\")\n",
      "plot.legend()\n",
      "plot.show()\n",
      "74/102:\n",
      "th1 = [1.25, 1.69, 1.67, 0.508, 0, -0.508, -1.67, -1.69, -1.25]\n",
      "t1 = [1.69, 1.625, 0, -1.625, -1.69]\n",
      "t2 = [0.63, 0.69, 0, -0.69, -0.63]\n",
      "t3 = [2, 1.8, 0, -1.7, -1.94]\n",
      "t4 = [2.39, 2.68, 0, -2.7, -2.3]\n",
      "74/103: reversed(h1)\n",
      "74/104: list(reversed(h1))\n",
      "74/105:\n",
      "list(reversed(h1))\n",
      "h1\n",
      "74/106: h1[:4] + list(reversed(h1))\n",
      "74/107: list(h1[:4]) + list(reversed(h1))\n",
      "74/108:\n",
      "fig = plot.figure(figsize=(7,7))\n",
      "ax = plot.axes()\n",
      "ax = fig.add_axes(ax)\n",
      "ax.plot(th1, list(h1[:4]) + list(reversed(h1)), label=\"Theoretical, p = 2.5 k/ft\")\n",
      "l1, = ax.plot(t1, h1_, label=\"Experimental, p = 2.5 k/ft\")\n",
      "l1.set_dashes([2,4,4,2])\n",
      "l2, = ax.plot(t2, h1_, label=\"Experimental, p = 1.0 k/ft\")\n",
      "l2.set_dashes([2,2,12,2])\n",
      "l3, = ax.plot(t3, h1_, label=\"Experimental, p = 4.0 k/ft\")\n",
      "l3.set_dashes([2,4,2,4])\n",
      "l4, = ax.plot(t4, h1_, label=\"Experimental, p = 5.5 k/ft\")\n",
      "l4.set_dashes([6,2,6,2])\n",
      "plot.xlabel(\"Tension/Compression (lb)\")\n",
      "plot.ylabel(\"Height (ft)\")\n",
      "plot.title(\"Height vs Tension/Compression for Eiffel Tower\")\n",
      "plot.legend()\n",
      "plot.show()\n",
      "74/109: list(h1[:4]) + list(reversed(h1))\n",
      "74/110: list(reversed(h1)) + list(h1[:4])\n",
      "74/111: list(reversed(h1)) + list(h1[1:])\n",
      "74/112:\n",
      "fig = plot.figure(figsize=(7,7))\n",
      "ax = plot.axes()\n",
      "ax = fig.add_axes(ax)\n",
      "ax.plot(th1, list(reversed(h1)) + list(h1[1:]), label=\"Theoretical, p = 2.5 k/ft\")\n",
      "l1, = ax.plot(t1, h1_, label=\"Experimental, p = 2.5 k/ft\")\n",
      "l1.set_dashes([2,4,4,2])\n",
      "l2, = ax.plot(t2, h1_, label=\"Experimental, p = 1.0 k/ft\")\n",
      "l2.set_dashes([2,2,12,2])\n",
      "l3, = ax.plot(t3, h1_, label=\"Experimental, p = 4.0 k/ft\")\n",
      "l3.set_dashes([2,4,2,4])\n",
      "l4, = ax.plot(t4, h1_, label=\"Experimental, p = 5.5 k/ft\")\n",
      "l4.set_dashes([6,2,6,2])\n",
      "plot.xlabel(\"Tension/Compression (lb)\")\n",
      "plot.ylabel(\"Height (ft)\")\n",
      "plot.title(\"Height vs Tension/Compression for Eiffel Tower\")\n",
      "plot.legend()\n",
      "plot.show()\n",
      "74/113:\n",
      "th = [-0.387, -.238, -0.1, -.028, 0, .028, 0.1, .238, 0.387]\n",
      "t1 = [-.42, -.165, 0, 0.125, 0.46]\n",
      "t2 = [-.14, -.04, 0, 0.04, .14]\n",
      "t3 = [-.45, -.16, 0, .15, .38]\n",
      "t4 = [-.87, -.19, 0, .16, .83]\n",
      "74/114: h1w_ = [h1w_[2], h1w_[1], h1w_[0], h1w_[1], h1w_[2]]\n",
      "74/115:\n",
      "fig = plot.figure(figsize=(7,7))\n",
      "ax = plot.axes()\n",
      "ax = fig.add_axes(ax)\n",
      "ax.plot(th1, list(reversed(h1w)) + list(h1w[1:]), label=\"Theoretical, p = 2.5 k/ft\")\n",
      "l1, = ax.plot(t1, h1w_, label=\"Experimental, p = 2.5 k/ft\")\n",
      "l1.set_dashes([2,4,4,2])\n",
      "l2, = ax.plot(t2, h1w_, label=\"Experimental, p = 1.0 k/ft\")\n",
      "l2.set_dashes([2,2,12,2])\n",
      "l3, = ax.plot(t3, h1w_, label=\"Experimental, p = 4.0 k/ft\")\n",
      "l3.set_dashes([2,4,2,4])\n",
      "l4, = ax.plot(t4, h1w_, label=\"Experimental, p = 5.5 k/ft\")\n",
      "l4.set_dashes([6,2,6,2])\n",
      "plot.xlabel(\"Tension/Compression (lb)\")\n",
      "plot.ylabel(\"Height (ft)\")\n",
      "plot.title(\"Height vs Tension/Compression for Washington Monument\")\n",
      "plot.legend()\n",
      "plot.show()\n",
      "74/116:\n",
      "fig = plot.figure(figsize=(7,7))\n",
      "ax = plot.axes()\n",
      "ax = fig.add_axes(ax)\n",
      "ax.plot(th, list(reversed(h1w)) + list(h1w[1:]), label=\"Theoretical, p = 2.5 k/ft\")\n",
      "l1, = ax.plot(t1, h1w_, label=\"Experimental, p = 2.5 k/ft\")\n",
      "l1.set_dashes([2,4,4,2])\n",
      "l2, = ax.plot(t2, h1w_, label=\"Experimental, p = 1.0 k/ft\")\n",
      "l2.set_dashes([2,2,12,2])\n",
      "l3, = ax.plot(t3, h1w_, label=\"Experimental, p = 4.0 k/ft\")\n",
      "l3.set_dashes([2,4,2,4])\n",
      "l4, = ax.plot(t4, h1w_, label=\"Experimental, p = 5.5 k/ft\")\n",
      "l4.set_dashes([6,2,6,2])\n",
      "plot.xlabel(\"Tension/Compression (lb)\")\n",
      "plot.ylabel(\"Height (ft)\")\n",
      "plot.title(\"Height vs Tension/Compression for Washington Monument\")\n",
      "plot.legend()\n",
      "plot.show()\n",
      "74/117:\n",
      "fig = plot.figure(figsize=(7,7))\n",
      "ax = plot.axes()\n",
      "ax = fig.add_axes(ax)\n",
      "ax.plot(th, list(reversed(h1w)) + list(h1w[1:]), label=\"Theoretical, p = 2.5 k/ft\")\n",
      "l1, = ax.plot(t1, h1w_, label=\"Experimental, p = 2.5 k/ft\", \"o\")\n",
      "l1.set_dashes([2,4,4,2])\n",
      "l2, = ax.plot(t2, h1w_, label=\"Experimental, p = 1.0 k/ft\")\n",
      "l2.set_dashes([2,2,12,2])\n",
      "l3, = ax.plot(t3, h1w_, label=\"Experimental, p = 4.0 k/ft\")\n",
      "l3.set_dashes([2,4,2,4])\n",
      "l4, = ax.plot(t4, h1w_, label=\"Experimental, p = 5.5 k/ft\")\n",
      "l4.set_dashes([6,2,6,2])\n",
      "plot.xlabel(\"Tension/Compression (lb)\")\n",
      "plot.ylabel(\"Height (ft)\")\n",
      "plot.title(\"Height vs Tension/Compression for Washington Monument\")\n",
      "plot.legend()\n",
      "plot.show()\n",
      "74/118:\n",
      "fig = plot.figure(figsize=(7,7))\n",
      "ax = plot.axes()\n",
      "ax = fig.add_axes(ax)\n",
      "ax.plot(th, list(reversed(h1w)) + list(h1w[1:]), label=\"Theoretical, p = 2.5 k/ft\")\n",
      "l1, = ax.plot(t1, h1w_, label=\"Experimental, p = 2.5 k/ft\", marker=\"o\")\n",
      "l1.set_dashes([2,4,4,2])\n",
      "l2, = ax.plot(t2, h1w_, label=\"Experimental, p = 1.0 k/ft\")\n",
      "l2.set_dashes([2,2,12,2])\n",
      "l3, = ax.plot(t3, h1w_, label=\"Experimental, p = 4.0 k/ft\")\n",
      "l3.set_dashes([2,4,2,4])\n",
      "l4, = ax.plot(t4, h1w_, label=\"Experimental, p = 5.5 k/ft\")\n",
      "l4.set_dashes([6,2,6,2])\n",
      "plot.xlabel(\"Tension/Compression (lb)\")\n",
      "plot.ylabel(\"Height (ft)\")\n",
      "plot.title(\"Height vs Tension/Compression for Washington Monument\")\n",
      "plot.legend()\n",
      "plot.show()\n",
      "74/119:\n",
      "fig = plot.figure(figsize=(7,7))\n",
      "ax = plot.axes()\n",
      "ax = fig.add_axes(ax)\n",
      "ax.plot(th, list(reversed(h1w)) + list(h1w[1:]), label=\"Theoretical, p = 2.5 k/ft\")\n",
      "l1, = ax.plot(t1, h1w_, label=\"Experimental, p = 2.5 k/ft\", marker=\"o\")\n",
      "l1.set_dashes([2,4,4,2])\n",
      "l2, = ax.plot(t2, h1w_, label=\"Experimental, p = 1.0 k/ft\", marker=\"o\")\n",
      "l2.set_dashes([2,2,12,2])\n",
      "l3, = ax.plot(t3, h1w_, label=\"Experimental, p = 4.0 k/ft\", marker=\"o\")\n",
      "l3.set_dashes([2,4,2,4])\n",
      "l4, = ax.plot(t4, h1w_, label=\"Experimental, p = 5.5 k/ft\", marker=\"o\")\n",
      "l4.set_dashes([6,2,6,2])\n",
      "plot.xlabel(\"Tension/Compression (lb)\")\n",
      "plot.ylabel(\"Height (ft)\")\n",
      "plot.title(\"Height vs Tension/Compression for Washington Monument\")\n",
      "plot.legend()\n",
      "plot.show()\n",
      "74/120:\n",
      "fig = plot.figure(figsize=(7,7))\n",
      "ax = plot.axes()\n",
      "ax = fig.add_axes(ax, marker=\"o\")\n",
      "ax.plot(th, list(reversed(h1w)) + list(h1w[1:]), label=\"Theoretical, p = 2.5 k/ft\")\n",
      "l1, = ax.plot(t1, h1w_, label=\"Experimental, p = 2.5 k/ft\", marker=\"o\")\n",
      "l1.set_dashes([2,4,4,2])\n",
      "l2, = ax.plot(t2, h1w_, label=\"Experimental, p = 1.0 k/ft\", marker=\"o\")\n",
      "l2.set_dashes([2,2,12,2])\n",
      "l3, = ax.plot(t3, h1w_, label=\"Experimental, p = 4.0 k/ft\", marker=\"o\")\n",
      "l3.set_dashes([2,4,2,4])\n",
      "l4, = ax.plot(t4, h1w_, label=\"Experimental, p = 5.5 k/ft\", marker=\"o\")\n",
      "l4.set_dashes([6,2,6,2])\n",
      "plot.xlabel(\"Tension/Compression (lb)\")\n",
      "plot.ylabel(\"Height (ft)\")\n",
      "plot.title(\"Height vs Tension/Compression for Washington Monument\")\n",
      "plot.legend()\n",
      "plot.show()\n",
      "74/121:\n",
      "fig = plot.figure(figsize=(7,7))\n",
      "ax = plot.axes()\n",
      "ax = fig.add_axes(ax)\n",
      "ax.plot(th, list(reversed(h1w)) + list(h1w[1:]), label=\"Theoretical, p = 2.5 k/ft\", marker=\"o\")\n",
      "l1, = ax.plot(t1, h1w_, label=\"Experimental, p = 2.5 k/ft\", marker=\"o\")\n",
      "l1.set_dashes([2,4,4,2])\n",
      "l2, = ax.plot(t2, h1w_, label=\"Experimental, p = 1.0 k/ft\", marker=\"o\")\n",
      "l2.set_dashes([2,2,12,2])\n",
      "l3, = ax.plot(t3, h1w_, label=\"Experimental, p = 4.0 k/ft\", marker=\"o\")\n",
      "l3.set_dashes([2,4,2,4])\n",
      "l4, = ax.plot(t4, h1w_, label=\"Experimental, p = 5.5 k/ft\", marker=\"o\")\n",
      "l4.set_dashes([6,2,6,2])\n",
      "plot.xlabel(\"Tension/Compression (lb)\")\n",
      "plot.ylabel(\"Height (ft)\")\n",
      "plot.title(\"Height vs Tension/Compression for Washington Monument\")\n",
      "plot.legend()\n",
      "plot.show()\n",
      "74/122:\n",
      "fig = plot.figure(figsize=(8, 8))\n",
      "ax = plot.axes()\n",
      "ax = fig.add_axes(ax)\n",
      "ax.plot(th, list(reversed(h1w)) + list(h1w[1:]), label=\"Theoretical, p = 2.5 k/ft\", marker=\"o\")\n",
      "l1, = ax.plot(t1, h1w_, label=\"Experimental, p = 2.5 k/ft\", marker=\"o\")\n",
      "l1.set_dashes([2,4,4,2])\n",
      "l2, = ax.plot(t2, h1w_, label=\"Experimental, p = 1.0 k/ft\", marker=\"o\")\n",
      "l2.set_dashes([2,2,12,2])\n",
      "l3, = ax.plot(t3, h1w_, label=\"Experimental, p = 4.0 k/ft\", marker=\"o\")\n",
      "l3.set_dashes([2,4,2,4])\n",
      "l4, = ax.plot(t4, h1w_, label=\"Experimental, p = 5.5 k/ft\", marker=\"o\")\n",
      "l4.set_dashes([6,2,6,2])\n",
      "plot.xlabel(\"Tension/Compression (lb)\")\n",
      "plot.ylabel(\"Height (ft)\")\n",
      "plot.title(\"Height vs Tension/Compression for Washington Monument\")\n",
      "plot.legend()\n",
      "plot.show()\n",
      "74/123:\n",
      "h1w = h*59/12\n",
      "h1w[2] = (59-32.)/12\n",
      "h1w_ = [h1w[0], h1w[2], h1w[4]]\n",
      "74/124: thw = [0, 0.013, 0.055, 0.119, 0.1935]\n",
      "74/125:\n",
      "fig = plot.figure(figsize=(7,7))\n",
      "ax = plot.axes()\n",
      "ax = fig.add_axes(ax)\n",
      "l0, = ax.plot(th, h1, label=\"Theoretical, p = 2.5 k/ft\", marker=\"o\")\n",
      "l0.set_dashes([2,4,4,2])\n",
      "l1, = ax.plot(ex1w, h1w_, label=\"Experimental, p = 2.5 k/ft\", marker=\"o\")\n",
      "l2, = ax.plot(ex2w, h1w_, label=\"Experimental, p = 1.0 k/ft\", marker=\"o\")\n",
      "l2.set_dashes([2,2,12,2])\n",
      "l3, = ax.plot(ex3w, h1w_, label=\"Experimental, p = 4.0 k/ft\", marker=\"o\")\n",
      "l3.set_dashes([2,4,2,4])\n",
      "l4, = ax.plot(ex4w, h1w_, label=\"Experimental, p = 5.5 k/ft\", marker=\"o\")\n",
      "l4.set_dashes([6,2,6,2])\n",
      "plot.xlabel(\"Moment (lb*ft)\")\n",
      "plot.ylabel(\"Height (ft)\")\n",
      "plot.title(\"Height vs Moment for Washington Monument\")\n",
      "plot.legend()\n",
      "plot.show()\n",
      "74/126: widthsw = np.array([0, 6/12, 6.2/12])\n",
      "74/127:\n",
      "ex1w = np.array([0, 0.125, 0.46])*widthsw\n",
      "ex2w = np.array([0, 0.04, 0.14])*widthsw\n",
      "ex3w = np.array([0, 0.15, 0.38])*widthsw\n",
      "ex4w = np.array([0, 0.16, 0.83])*widthsw\n",
      "74/128:\n",
      "h1w = h*59/12\n",
      "h1w[2] = (59-32.)/12\n",
      "h1w_ = [h1w[0], h1w[2], h1w[4]]\n",
      "74/129: thw = [0, 0.013, 0.055, 0.119, 0.1935]\n",
      "74/130:\n",
      "fig = plot.figure(figsize=(7,7))\n",
      "ax = plot.axes()\n",
      "ax = fig.add_axes(ax)\n",
      "l0, = ax.plot(th, h1, label=\"Theoretical, p = 2.5 k/ft\", marker=\"o\")\n",
      "l0.set_dashes([2,4,4,2])\n",
      "l1, = ax.plot(ex1w, h1w_, label=\"Experimental, p = 2.5 k/ft\", marker=\"o\")\n",
      "l2, = ax.plot(ex2w, h1w_, label=\"Experimental, p = 1.0 k/ft\", marker=\"o\")\n",
      "l2.set_dashes([2,2,12,2])\n",
      "l3, = ax.plot(ex3w, h1w_, label=\"Experimental, p = 4.0 k/ft\", marker=\"o\")\n",
      "l3.set_dashes([2,4,2,4])\n",
      "l4, = ax.plot(ex4w, h1w_, label=\"Experimental, p = 5.5 k/ft\", marker=\"o\")\n",
      "l4.set_dashes([6,2,6,2])\n",
      "plot.xlabel(\"Moment (lb*ft)\")\n",
      "plot.ylabel(\"Height (ft)\")\n",
      "plot.title(\"Height vs Moment for Washington Monument\")\n",
      "plot.legend()\n",
      "plot.show()\n",
      "74/131:\n",
      "fig = plot.figure(figsize=(7,7))\n",
      "ax = plot.axes()\n",
      "ax = fig.add_axes(ax)\n",
      "l0, = ax.plot(thw, h1, label=\"Theoretical, p = 2.5 k/ft\", marker=\"o\")\n",
      "l0.set_dashes([2,4,4,2])\n",
      "l1, = ax.plot(ex1w, h1w_, label=\"Experimental, p = 2.5 k/ft\", marker=\"o\")\n",
      "l2, = ax.plot(ex2w, h1w_, label=\"Experimental, p = 1.0 k/ft\", marker=\"o\")\n",
      "l2.set_dashes([2,2,12,2])\n",
      "l3, = ax.plot(ex3w, h1w_, label=\"Experimental, p = 4.0 k/ft\", marker=\"o\")\n",
      "l3.set_dashes([2,4,2,4])\n",
      "l4, = ax.plot(ex4w, h1w_, label=\"Experimental, p = 5.5 k/ft\", marker=\"o\")\n",
      "l4.set_dashes([6,2,6,2])\n",
      "plot.xlabel(\"Moment (lb*ft)\")\n",
      "plot.ylabel(\"Height (ft)\")\n",
      "plot.title(\"Height vs Moment for Washington Monument\")\n",
      "plot.legend()\n",
      "plot.show()\n",
      "74/132:\n",
      "fig = plot.figure(figsize=(7,7))\n",
      "ax = plot.axes()\n",
      "ax = fig.add_axes(ax)\n",
      "l0, = ax.plot(thw, h1, label=\"Theoretical, p = 2.5 k/ft\")\n",
      "l0.set_dashes([2,4,4,2])\n",
      "l1, = ax.plot(ex1w, h1w_, label=\"Experimental, p = 2.5 k/ft\")\n",
      "l2, = ax.plot(ex2w, h1w_, label=\"Experimental, p = 1.0 k/ft\")\n",
      "l2.set_dashes([2,2,12,2])\n",
      "l3, = ax.plot(ex3w, h1w_, label=\"Experimental, p = 4.0 k/ft\")\n",
      "l3.set_dashes([2,4,2,4])\n",
      "l4, = ax.plot(ex4w, h1w_, label=\"Experimental, p = 5.5 k/ft\")\n",
      "l4.set_dashes([6,2,6,2])\n",
      "plot.xlabel(\"Moment (lb*ft)\")\n",
      "plot.ylabel(\"Height (ft)\")\n",
      "plot.title(\"Height vs Moment for Washington Monument\")\n",
      "plot.legend()\n",
      "plot.show()\n",
      "74/133:\n",
      "fig = plot.figure(figsize=(7,7))\n",
      "ax = plot.axes()\n",
      "ax = fig.add_axes(ax)\n",
      "l0, = ax.plot(thw, h1, label=\"Theoretical, p = 2.5 k/ft\", marker=\"o\")\n",
      "l0.set_dashes([2,4,4,2])\n",
      "l1, = ax.plot(ex1w, h1w_, label=\"Experimental, p = 2.5 k/ft\", marker=\"o\")\n",
      "l2, = ax.plot(ex2w, h1w_, label=\"Experimental, p = 1.0 k/ft\", marker=\"o\")\n",
      "l2.set_dashes([2,2,12,2])\n",
      "l3, = ax.plot(ex3w, h1w_, label=\"Experimental, p = 4.0 k/ft\", marker=\"o\")\n",
      "l3.set_dashes([2,4,2,4])\n",
      "l4, = ax.plot(ex4w, h1w_, label=\"Experimental, p = 5.5 k/ft\", marker=\"o\")\n",
      "l4.set_dashes([6,2,6,2])\n",
      "plot.xlabel(\"Moment (lb*ft)\")\n",
      "plot.ylabel(\"Height (ft)\")\n",
      "plot.title(\"Height vs Moment for Washington Monument\")\n",
      "plot.legend()\n",
      "plot.show()\n",
      "74/134:\n",
      "fig = plot.figure(figsize=(7,7))\n",
      "ax = plot.axes()\n",
      "ax = fig.add_axes(ax)\n",
      "ax.plot(th1, list(reversed(h1)) + list(h1[1:]), label=\"Theoretical, p = 2.5 k/ft\", marker=\"o\")\n",
      "l1, = ax.plot(t1, h1_, label=\"Experimental, p = 2.5 k/ft\", marker=\"o\")\n",
      "l1.set_dashes([2,4,4,2])\n",
      "l2, = ax.plot(t2, h1_, label=\"Experimental, p = 1.0 k/ft\", marker=\"o\")\n",
      "l2.set_dashes([2,2,12,2])\n",
      "l3, = ax.plot(t3, h1_, label=\"Experimental, p = 4.0 k/ft\", marker=\"o\")\n",
      "l3.set_dashes([2,4,2,4])\n",
      "l4, = ax.plot(t4, h1_, label=\"Experimental, p = 5.5 k/ft\", marker=\"o\")\n",
      "l4.set_dashes([6,2,6,2])\n",
      "plot.xlabel(\"Tension/Compression (lb)\")\n",
      "plot.ylabel(\"Height (ft)\")\n",
      "plot.title(\"Height vs Tension/Compression for Eiffel Tower\")\n",
      "plot.legend()\n",
      "plot.show()\n",
      "74/135: th = [0, 0.18, 1.18, 1.62, 2.82]\n",
      "74/136:\n",
      "h1 = h*81/12\n",
      "h1[2] = (81-52.)/12\n",
      "h1_ = [h1[0], h1[2], h1[4]]\n",
      "74/137:\n",
      "fig = plot.figure(figsize=(7,7))\n",
      "ax = plot.axes()\n",
      "ax = fig.add_axes(ax)\n",
      "ax.plot(th, h1, label=\"Theoretical, p = 2.5 k/ft\", marker=\"o\")\n",
      "l1, = ax.plot(ex1, h1_, label=\"Experimental, p = 2.5 k/ft\", marker=\"o\")\n",
      "l1.set_dashes([2,4,4,2])\n",
      "l2, = ax.plot(ex2, h1_, label=\"Experimental, p = 1.0 k/ft\", marker=\"o\")\n",
      "l2.set_dashes([2,2,12,2])\n",
      "l3, = ax.plot(ex3, h1_, label=\"Experimental, p = 4.0 k/ft\", marker=\"o\")\n",
      "l3.set_dashes([2,4,2,4])\n",
      "l4, = ax.plot(ex4, h1_, label=\"Experimental, p = 5.5 k/ft\", marker=\"o\")\n",
      "l4.set_dashes([6,2,6,2])\n",
      "plot.xlabel(\"Moment (lb*ft)\")\n",
      "plot.ylabel(\"Height (ft)\")\n",
      "plot.title(\"Height vs Moment for Eiffel Tower\")\n",
      "plot.legend()\n",
      "plot.show()\n",
      "74/138: h1\n",
      "74/139: h1w\n",
      "74/140:\n",
      "fig = plot.figure(figsize=(7,7))\n",
      "ax = plot.axes()\n",
      "ax = fig.add_axes(ax)\n",
      "l0, = ax.plot(thw, h1w, label=\"Theoretical, p = 2.5 k/ft\", marker=\"o\")\n",
      "l0.set_dashes([2,4,4,2])\n",
      "l1, = ax.plot(ex1w, h1w_, label=\"Experimental, p = 2.5 k/ft\", marker=\"o\")\n",
      "l2, = ax.plot(ex2w, h1w_, label=\"Experimental, p = 1.0 k/ft\", marker=\"o\")\n",
      "l2.set_dashes([2,2,12,2])\n",
      "l3, = ax.plot(ex3w, h1w_, label=\"Experimental, p = 4.0 k/ft\", marker=\"o\")\n",
      "l3.set_dashes([2,4,2,4])\n",
      "l4, = ax.plot(ex4w, h1w_, label=\"Experimental, p = 5.5 k/ft\", marker=\"o\")\n",
      "l4.set_dashes([6,2,6,2])\n",
      "plot.xlabel(\"Moment (lb*ft)\")\n",
      "plot.ylabel(\"Height (ft)\")\n",
      "plot.title(\"Height vs Moment for Washington Monument\")\n",
      "plot.legend()\n",
      "plot.show()\n",
      "74/141:\n",
      "fig = plot.figure(figsize=(7,7))\n",
      "ax = plot.axes()\n",
      "ax = fig.add_axes(ax)\n",
      "l0, = ax.plot(thw, h1w, label=\"Theoretical, p = 2.5 k/ft\")\n",
      "l0.set_dashes([2,4,4,2])\n",
      "l1, = ax.plot(ex1w, h1w_, label=\"Experimental, p = 2.5 k/ft\")\n",
      "l2, = ax.plot(ex2w, h1w_, label=\"Experimental, p = 1.0 k/ft\")\n",
      "l2.set_dashes([2,2,12,2])\n",
      "l3, = ax.plot(ex3w, h1w_, label=\"Experimental, p = 4.0 k/ft\")\n",
      "l3.set_dashes([2,4,2,4])\n",
      "l4, = ax.plot(ex4w, h1w_, label=\"Experimental, p = 5.5 k/ft\")\n",
      "l4.set_dashes([6,2,6,2])\n",
      "plot.xlabel(\"Moment (lb*ft)\")\n",
      "plot.ylabel(\"Height (ft)\")\n",
      "plot.title(\"Height vs Moment for Washington Monument\")\n",
      "plot.legend()\n",
      "plot.show()\n",
      "74/142: h1w_\n",
      "74/143:\n",
      "fig = plot.figure(figsize=(7,7))\n",
      "ax = plot.axes()\n",
      "ax = fig.add_axes(ax)\n",
      "l0, = ax.plot(thw, h1w, label=\"Theoretical, p = 2.5 k/ft\")\n",
      "l0.set_dashes([2,4,4,2])\n",
      "l1, = ax.plot(ex1w, h1w_, label=\"Experimental, p = 2.5 k/ft\")\n",
      "l2, = ax.plot(ex2w, h1w_, label=\"Experimental, p = 1.0 k/ft\")\n",
      "l2.set_dashes([2,2,12,2])\n",
      "l3, = ax.plot(ex3w, h1w_, label=\"Experimental, p = 4.0 k/ft\")\n",
      "l3.set_dashes([2,4,2,4])\n",
      "l4, = ax.plot(ex4w, h1w_, label=\"Experimental, p = 5.5 k/ft\")\n",
      "l4.set_dashes([6,2,6,2])\n",
      "plot.xlabel(\"Moment (lb*ft)\")\n",
      "plot.ylabel(\"Height (ft)\")\n",
      "plot.title(\"Height vs Moment for Washington Monument\")\n",
      "plot.legend()\n",
      "plot.show()\n",
      "74/144: ex1w\n",
      "74/145: widthsw = np.array([0, 6./12, 6.2/12])\n",
      "74/146:\n",
      "ex1w = np.array([0, 0.125, 0.46])*widthsw\n",
      "ex2w = np.array([0, 0.04, 0.14])*widthsw\n",
      "ex3w = np.array([0, 0.15, 0.38])*widthsw\n",
      "ex4w = np.array([0, 0.16, 0.83])*widthsw\n",
      "74/147:\n",
      "fig = plot.figure(figsize=(7,7))\n",
      "ax = plot.axes()\n",
      "ax = fig.add_axes(ax)\n",
      "l0, = ax.plot(thw, h1w, label=\"Theoretical, p = 2.5 k/ft\")\n",
      "l0.set_dashes([2,4,4,2])\n",
      "l1, = ax.plot(ex1w, h1w_, label=\"Experimental, p = 2.5 k/ft\")\n",
      "l2, = ax.plot(ex2w, h1w_, label=\"Experimental, p = 1.0 k/ft\")\n",
      "l2.set_dashes([2,2,12,2])\n",
      "l3, = ax.plot(ex3w, h1w_, label=\"Experimental, p = 4.0 k/ft\")\n",
      "l3.set_dashes([2,4,2,4])\n",
      "l4, = ax.plot(ex4w, h1w_, label=\"Experimental, p = 5.5 k/ft\")\n",
      "l4.set_dashes([6,2,6,2])\n",
      "plot.xlabel(\"Moment (lb*ft)\")\n",
      "plot.ylabel(\"Height (ft)\")\n",
      "plot.title(\"Height vs Moment for Washington Monument\")\n",
      "plot.legend()\n",
      "plot.show()\n",
      "74/148:\n",
      "fig = plot.figure(figsize=(7,7))\n",
      "ax = plot.axes()\n",
      "ax = fig.add_axes(ax)\n",
      "l0, = ax.plot(thw, h1w, label=\"Theoretical, p = 2.5 k/ft\", marker=\"o\")\n",
      "l0.set_dashes([2,4,4,2])\n",
      "l1, = ax.plot(ex1w, h1w_, label=\"Experimental, p = 2.5 k/ft\", marker=\"o\")\n",
      "l2, = ax.plot(ex2w, h1w_, label=\"Experimental, p = 1.0 k/ft\", marker=\"o\")\n",
      "l2.set_dashes([2,2,12,2])\n",
      "l3, = ax.plot(ex3w, h1w_, label=\"Experimental, p = 4.0 k/ft\", marker=\"o\")\n",
      "l3.set_dashes([2,4,2,4])\n",
      "l4, = ax.plot(ex4w, h1w_, label=\"Experimental, p = 5.5 k/ft\", marker=\"o\")\n",
      "l4.set_dashes([6,2,6,2])\n",
      "plot.xlabel(\"Moment (lb*ft)\")\n",
      "plot.ylabel(\"Height (ft)\")\n",
      "plot.title(\"Height vs Moment for Washington Monument\")\n",
      "plot.legend()\n",
      "plot.show()\n",
      "74/149:\n",
      "for x in [-0.63, -1.4, -1.94, -2.3]:\n",
      "    print x + 3.27\n",
      "74/150:\n",
      "for x in [0.63, 1.4, 2, 2.39]:\n",
      "    print x + 3.27\n",
      "74/151:\n",
      "for x in [-0.14, -0.42, -0.45, -0.87]:\n",
      "    print x + 3.07\n",
      "74/152:\n",
      "for x in [0.14, 0.46, 0.38, 0.83]:\n",
      "    print x + 3.07\n",
      "64/269: df['moviecount'].min()\n",
      "64/270: df['moviecount'].max()\n",
      "64/271: df['moviecount'].min()\n",
      "64/272:\n",
      "X = df[['degree_centralities', 'moviecount']]\n",
      "Y = boxofficegross\n",
      "model = sm.OLS(Y, X).fit()\n",
      "print_model = model.summary()\n",
      "print(print_model)\n",
      "64/273:\n",
      "X = df[['degree_centralities']]\n",
      "Y = boxofficegross\n",
      "model = sm.OLS(Y, X).fit()\n",
      "print_model = model.summary()\n",
      "print(print_model)\n",
      "64/274:\n",
      "X = df[['degree_centralities', 'moviecount']]\n",
      "Y = boxofficegross\n",
      "model = sm.OLS(Y, X).fit()\n",
      "print_model = model.summary()\n",
      "print(print_model)\n",
      "64/275:\n",
      "X = df[['degree_centralities', 'moviecount']]\n",
      "print X.corr()\n",
      "Y = boxofficegross\n",
      "model = sm.OLS(Y, X).fit()\n",
      "print_model = model.summary()\n",
      "print(print_model)\n",
      "64/276:\n",
      "X = df[['degree_centralities', 'moviecount']]\n",
      "print df[['degree_centralities', 'boxofficegross']].corr()\n",
      "Y = boxofficegross\n",
      "model = sm.OLS(Y, X).fit()\n",
      "print_model = model.summary()\n",
      "print(print_model)\n",
      "64/277:\n",
      "def run(var):\n",
      "    X = df[['degree_centralities', 'moviecount']]\n",
      "    print df[['degree_centralities', 'boxofficegross']].corr()\n",
      "    Y = boxofficegross\n",
      "    model = sm.OLS(Y, X).fit()\n",
      "    print_model = model.summary()\n",
      "    print(print_model)\n",
      "64/278:\n",
      "def run(var):\n",
      "    X = df[[var, 'moviecount']]\n",
      "    X2 = df[[var]]\n",
      "    print df[[var, 'boxofficegross']].corr()\n",
      "    Y = boxofficegross\n",
      "    model = sm.OLS(Y, X).fit()\n",
      "    print_model = model.summary()\n",
      "    print(print_model)\n",
      "    model = sm.OLS(Y, X2).fit()\n",
      "    print_model2 = model2.summary()\n",
      "    print(print_model2)\n",
      "64/279: run('degree_centralities')\n",
      "64/280:\n",
      "def run(var):\n",
      "    X = df[[var, 'moviecount']]\n",
      "    X2 = df[[var]]\n",
      "    print df[[var, 'boxofficegross']].corr()\n",
      "    Y = boxofficegross\n",
      "    model = sm.OLS(Y, X).fit()\n",
      "    print_model = model.summary()\n",
      "    print(print_model)\n",
      "    model2 = sm.OLS(Y, X2).fit()\n",
      "    print_model2 = model2.summary()\n",
      "    print(print_model2)\n",
      "64/281: run('degree_centralities')\n",
      "64/282:\n",
      "for x in ['degree_centralities', 'eigenvector_centralities', 'closeness_centralities', 'betweenness_centralities', 'cluster_coefficents', 'moviecount']:\n",
      "    run(x)\n",
      "    print \"**************************************************************\"\n",
      "64/283:\n",
      "Xgender = sm.add_constant(df[['degree_centralities', 'female', 'degree_fem', 'moviecount']]) # adding a constant\n",
      "Y = boxofficegross\n",
      "model = sm.OLS(Y, Xgender).fit()\n",
      "predictions = model.predict(Xgender) \n",
      " \n",
      "print_model = model.summary()\n",
      "print(print_model)\n",
      "64/284:\n",
      "df = pd.DataFrame(\n",
      "    {\n",
      "        'degree_centralities': degree_centralities,\n",
      "        'eigenvector_centralities': eigenvector_centralities,\n",
      "        'betweenness_centralities': betweenness_centralities,\n",
      "        'closeness_centralities': closeness_centralities,\n",
      "        'cluster_coefficients': cluster_coefficients,\n",
      "        'moviecount': moviecounts,\n",
      "        'boxofficegross': boxofficegross,\n",
      "        'academynominated': academynominated,\n",
      "        'female': female,\n",
      "        'oldage': oldage,\n",
      "        'age': age\n",
      "    })\n",
      "64/285:\n",
      "def run(var):\n",
      "    X = df[[var, 'moviecount']]\n",
      "    print df[[var, 'boxofficegross']].corr()\n",
      "    Y = boxofficegross\n",
      "    model = sm.OLS(Y, X).fit()\n",
      "    print_model = model.summary()\n",
      "    print(print_model)\n",
      "64/286:\n",
      "for x in ['degree_centralities', 'eigenvector_centralities', 'closeness_centralities', 'betweenness_centralities', 'cluster_coefficents', 'moviecount']:\n",
      "    run(x)\n",
      "    print \"**************************************************************\"\n",
      "64/287: df['moviecount']\n",
      "64/288:\n",
      "df['cluster_coefficients\n",
      "  ]\n",
      "64/289: df['cluster_coefficients']\n",
      "64/290:\n",
      "for x in ['degree_centralities', 'eigenvector_centralities', 'closeness_centralities', 'betweenness_centralities', 'cluster_coefficients', 'moviecount']:\n",
      "    run(x)\n",
      "    print \"**************************************************************\"\n",
      "64/291: from sklearn.linear_model import LinearRegression\n",
      "64/292:\n",
      "slope, intercept, r_value, p_value, std_err = stats.linregress(df[['degree_centralities', 'moviecount']],boxofficegross)\n",
      "print r_value, p_value\n",
      "64/293:\n",
      "slope, intercept, r_value, p_value, std_err = stats.linregress(degree_centralities,boxofficegross)\n",
      "print r_value, p_value\n",
      "64/294:\n",
      "slope, intercept, r_value, p_value, std_err = stats.linregress(degree_centralities,boxofficegross)\n",
      "print r_value**2, p_value\n",
      "64/295:\n",
      "def run(var):\n",
      "    X = sm.add_constant(df[[var, 'moviecount']])\n",
      "    print df[[var, 'boxofficegross']].corr()\n",
      "    Y = boxofficegross\n",
      "    model = sm.OLS(Y, X).fit()\n",
      "    print_model = model.summary()\n",
      "    print(print_model)\n",
      "64/296:\n",
      "for x in ['degree_centralities', 'eigenvector_centralities', 'closeness_centralities', 'betweenness_centralities', 'cluster_coefficients', 'moviecount']:\n",
      "    run(x)\n",
      "    print \"**************************************************************\"\n",
      "64/297:\n",
      "def awardstats(x):\n",
      "    X = sm.add_constant(df[x])\n",
      "    model = sm.OLS(academynominated, X).fit()\n",
      "    pred = model.predict(X)\n",
      "    predbin = [1 if x >= 0.5 else 0 for x in pred]\n",
      "    tn, fp, fn, tp = confusion_matrix(academynominated, predbin).ravel()\n",
      "    print measures(tn, fp, fn, tp)\n",
      "64/298:\n",
      "awardstats('degree_centralities')\n",
      "awardstats('eigenvector_centralities')\n",
      "awardstats('closeness_centralities')\n",
      "awardstats('betweenness_centralities')\n",
      "awardstats('effective_size')\n",
      "awardstats('cluster_coefficients')\n",
      "awardstats('moviecount')\n",
      "64/299: from sklearn.metrics import confusion_matrix\n",
      "64/300:\n",
      "awardstats('degree_centralities')\n",
      "awardstats('eigenvector_centralities')\n",
      "awardstats('closeness_centralities')\n",
      "awardstats('betweenness_centralities')\n",
      "awardstats('cluster_coefficients')\n",
      "awardstats('moviecount')\n",
      "64/301:\n",
      "def measures(tn, fp, fn, tp):\n",
      "    accuracy = float(tn + tp)/(tn+fp+fn+tp)\n",
      "    recall = float(tp)/(tp + fn)\n",
      "    specificity = float(tn)/(tn + fp)\n",
      "    precision = np.nan if tp+fp==0 else float(tp)/(tp + fp)\n",
      "    return \"accuracy: %f, recall: %f, specificity: %f, precision: %f\" % (accuracy, recall, specificity, precision)\n",
      "64/302:\n",
      "awardstats('degree_centralities')\n",
      "awardstats('eigenvector_centralities')\n",
      "awardstats('closeness_centralities')\n",
      "awardstats('betweenness_centralities')\n",
      "awardstats('cluster_coefficients')\n",
      "awardstats('moviecount')\n",
      "64/303:\n",
      "df['degree_age'] = df['degree_centralities']*df['age']\n",
      "Xage = df[['degree_centralities', 'age', 'degree_age', 'moviecount']]\n",
      "64/304:\n",
      "df['degree_oldage'] = df['degree_centralities']*df['oldage']\n",
      "Xoldage = df[['degree_centralities', 'oldage', 'degree_oldage', 'moviecount']]\n",
      "64/305:\n",
      "df['degree_a'] = df['degree_centralities']*df['academynominated']\n",
      "Xa = df[['degree_centralities', 'academynominated', 'degree_a', 'moviecount']]\n",
      "64/306:\n",
      "Xoldage = sm.add_constant(Xoldage) # adding a constant\n",
      "Y = boxofficegross\n",
      "model = sm.OLS(Y, Xoldage).fit()\n",
      "predictions = model.predict(Xoldage) \n",
      " \n",
      "print_model = model.summary()\n",
      "print(print_model)\n",
      "64/307:\n",
      "Xa = sm.add_constant(Xa) # adding a constant\n",
      "Y = boxofficegross\n",
      "model = sm.OLS(Y, Xa).fit()\n",
      "predictions = model.predict(Xa) \n",
      " \n",
      "print_model = model.summary()\n",
      "print(print_model)\n",
      "64/308:\n",
      "Xage = sm.add_constant(Xage) # adding a constant\n",
      "Y = boxofficegross\n",
      "model = sm.OLS(Y, Xage).fit()\n",
      "predictions = model.predict(Xage) \n",
      " \n",
      "print_model = model.summary()\n",
      "print(print_model)\n",
      "64/309:\n",
      "Xgender = sm.add_constant(Xgender) # adding a constant\n",
      "Y = boxofficegross\n",
      "model = sm.OLS(Y, Xgender).fit()\n",
      "predictions = model.predict(Xgender) \n",
      "print model.pvalues\n",
      "print_model = model.summary()\n",
      "print(print_model)\n",
      "64/310:\n",
      "def awardstats(x):\n",
      "    X = sm.add_constant(df[[x, 'moviecount']])\n",
      "    model = sm.OLS(academynominated, X).fit()\n",
      "    pred = model.predict(X)\n",
      "    predbin = [1 if x >= 0.5 else 0 for x in pred]\n",
      "    tn, fp, fn, tp = confusion_matrix(academynominated, predbin).ravel()\n",
      "    print measures(tn, fp, fn, tp)\n",
      "64/311:\n",
      "awardstats('degree_centralities')\n",
      "awardstats('eigenvector_centralities')\n",
      "awardstats('closeness_centralities')\n",
      "awardstats('betweenness_centralities')\n",
      "awardstats('cluster_coefficients')\n",
      "awardstats('moviecount')\n",
      "64/312:\n",
      "Xage = sm.add_constant(Xage) # adding a constant\n",
      "Y = boxofficegross\n",
      "model = sm.OLS(Y, Xage).fit()\n",
      "predictions = model.predict(Xage) \n",
      "print model.pvalues\n",
      "print_model = model.summary()\n",
      "print(print_model)\n",
      "64/313:\n",
      "Xa = sm.add_constant(Xa) # adding a constant\n",
      "Y = boxofficegross\n",
      "model = sm.OLS(Y, Xa).fit()\n",
      "predictions = model.predict(Xa) \n",
      "print model.pvalues\n",
      "print_model = model.summary()\n",
      "print(print_model)\n",
      "64/314:\n",
      "for x in ['degree_centralities', 'eigenvector_centralities', 'closeness_centralities', 'betweenness_centralities', 'cluster_coefficients', 'moviecount']:\n",
      "    run(x, False)\n",
      "    print \"**************************************************************\"\n",
      "64/315:\n",
      "def run(var, ctrl):\n",
      "    if ctrl:\n",
      "        X = sm.add_constant(df[[var, 'moviecount']])\n",
      "    else:\n",
      "        X = sm.add_constant(df[[var]])\n",
      "    print df[[var, 'boxofficegross']].corr()\n",
      "    Y = boxofficegross\n",
      "    model = sm.OLS(Y, X).fit()\n",
      "    print_model = model.summary()\n",
      "    print(print_model)\n",
      "64/316:\n",
      "for x in ['degree_centralities', 'eigenvector_centralities', 'closeness_centralities', 'betweenness_centralities', 'cluster_coefficients', 'moviecount']:\n",
      "    run(x, False)\n",
      "    print \"**************************************************************\"\n",
      "64/317:\n",
      "def run(var, ctrl):\n",
      "    if ctrl:\n",
      "        X = sm.add_constant(df[[var, 'moviecount']])\n",
      "    else:\n",
      "        X = sm.add_constant(df[[var]])\n",
      "    print df[[var, 'boxofficegross']].corr()\n",
      "    Y = boxofficegross\n",
      "    model = sm.OLS(Y, X).fit()\n",
      "    print_model = model.summary()\n",
      "    print model.pvalues\n",
      "    print(print_model)\n",
      "64/318:\n",
      "for x in ['degree_centralities', 'eigenvector_centralities', 'closeness_centralities', 'betweenness_centralities', 'cluster_coefficients', 'moviecount']:\n",
      "    run(x, False)\n",
      "    print \"**************************************************************\"\n",
      "64/319:\n",
      "for x in ['degree_centralities', 'eigenvector_centralities', 'closeness_centralities', 'betweenness_centralities', 'cluster_coefficients', 'moviecount']:\n",
      "#     run(x, False)\n",
      "    run(x, True)\n",
      "    print \"**************************************************************\"\n",
      "64/320:\n",
      "for x in ['degree_centralities', 'eigenvector_centralities', 'closeness_centralities', 'betweenness_centralities', 'cluster_coefficients', 'moviecount']:\n",
      "    run(x, False)\n",
      "    print \"**************************************************************\"\n",
      "77/1: # Graph Constructions and Visualizations\n",
      "76/1:\n",
      "import networkx as nx\n",
      "import csv\n",
      "import matplotlib.pyplot as plot\n",
      "from bs4 import BeautifulSoup\n",
      "import urllib2\n",
      "83/1:\n",
      "import numpy as np\n",
      "import pandas as pd\n",
      "83/2: df = pd.read_pickle('since2018')\n",
      "83/3:\n",
      "import numpy as np\n",
      "import pandas as pd\n",
      "83/4: df = pd.read_pickle('since2018')\n",
      "85/1:\n",
      "import numpy as np\n",
      "import pandas as pd\n",
      "85/2: df = pd.read_pickle('since2018')\n",
      "85/3:\n",
      "import numpy as np\n",
      "import pandas as pd\n",
      "85/4: df = pd.read_pickle('since2018')\n",
      "85/5: from matplotlib import pyplot as plot\n",
      "85/6: df.head()\n",
      "85/7: df.summarize()\n",
      "85/8: df.summary()\n",
      "85/9: df.shape\n",
      "85/10:\n",
      "from matplotlib import pyplot as plot\n",
      "%matplotlib inline\n",
      "pd.options.display.max_columns = 100\n",
      "85/11: df.head()\n",
      "85/12:\n",
      "from matplotlib import pyplot as plt\n",
      "%matplotlib inline\n",
      "pd.options.display.max_columns = 100\n",
      "85/13:\n",
      "categorical_columns = ['primary_type']\n",
      "df2 = pd.get_dummies(df, columns = categorical_columns)\n",
      "df2.head()\n",
      "85/14:\n",
      "import numpy as np\n",
      "import pandas as pd\n",
      "import seaborn as sns\n",
      "85/15:\n",
      "import numpy as np\n",
      "import pandas as pd\n",
      "import seaborn as sns\n",
      "85/16:\n",
      "get_type = []\n",
      "\n",
      "for i in range(0,crime_data.shape[0]):\n",
      "    primary = crime_data.iloc[i].primary_type\n",
      "    get_index = -1\n",
      "    \n",
      "    for j in range(0, len(get_type)):\n",
      "        if (get_type[j][0] == primary):\n",
      "            get_index = j\n",
      "            get_type[j][1]+=1\n",
      "    \n",
      "    if get_index == -1:\n",
      "        get_type.append([primary, 1])\n",
      "\n",
      "type_data = pd.DataFrame(columns=['Type', 'count'], data=get_type)\n",
      "fig1, ax1 = plt.subplots()\n",
      "fig1.set_size_inches(18.5, 10.5)\n",
      "ax1.pie(type_data['count'], labels=type_data['Type'], autopct='%1.1f%%',startangle=90)\n",
      "ax1.axis('equal')  # Equal aspect ratio ensures that pie is drawn as a circle.\n",
      "\n",
      "plt.show()\n",
      "85/17:\n",
      "get_type = []\n",
      "crime_data = df.copy()\n",
      "for i in range(0,crime_data.shape[0]):\n",
      "    primary = crime_data.iloc[i].primary_type\n",
      "    get_index = -1\n",
      "    \n",
      "    for j in range(0, len(get_type)):\n",
      "        if (get_type[j][0] == primary):\n",
      "            get_index = j\n",
      "            get_type[j][1]+=1\n",
      "    \n",
      "    if get_index == -1:\n",
      "        get_type.append([primary, 1])\n",
      "\n",
      "type_data = pd.DataFrame(columns=['Type', 'count'], data=get_type)\n",
      "fig1, ax1 = plt.subplots()\n",
      "fig1.set_size_inches(18.5, 10.5)\n",
      "ax1.pie(type_data['count'], labels=type_data['Type'], autopct='%1.1f%%',startangle=90)\n",
      "ax1.axis('equal')  # Equal aspect ratio ensures that pie is drawn as a circle.\n",
      "\n",
      "plt.show()\n",
      "85/18:\n",
      "def showCategorical(cat):\n",
      "for t in pd.unique(data[cat]):\n",
      "d = data[data[cat] == t]\n",
      "plt.scatter(d.longitude, d.latitude, s=.001)\n",
      "plt.show()\n",
      "85/19:\n",
      "data = df.copy()\n",
      "def showCategorical(cat):\n",
      "    for t in pd.unique(data[cat]):\n",
      "        d = data[data[cat] == t]\n",
      "        plt.scatter(d.longitude, d.latitude, s=.001)\n",
      "    plt.show()\n",
      "85/20: showCategorical(\"primary_type\")\n",
      "85/21: df.corr()\n",
      "85/22: df2.corr()\n",
      "85/23:\n",
      "categorical_columns = ['primary_type', 'location_description']\n",
      "df2 = pd.get_dummies(df[['date', 'location_description', 'arrest', 'domestic', 'beat', 'district', 'ward', 'latitude', 'longitude', 'primary_type']], \n",
      "                     columns = categorical_columns)\n",
      "df2.head()\n",
      "85/24: df2 = df.copy()[['date', 'location_description', 'arrest', 'domestic', 'ward', 'primary_type']]\n",
      "85/25:\n",
      "categorical_columns = ['primary_type', 'location_description', 'ward']\n",
      "df2 = pd.get_dummies(df2, columns = categorical_columns)\n",
      "df2.head()\n",
      "85/26:\n",
      "categorical_columns = ['primary_type', 'location_description', 'ward']\n",
      "df2 = pd.get_dummies(df2, columns = categorical_columns)\n",
      "df2.arrest = df2.arrest.astype(int)\n",
      "df2.head()\n",
      "85/27:\n",
      "categorical_columns = ['primary_type', 'location_description', 'ward']\n",
      "df2 = pd.get_dummies(df2, columns = categorical_columns)\n",
      "df2['arrest'] = df2['arrest'].astype(int)\n",
      "df2.head()\n",
      "85/28:\n",
      "categorical_columns = ['primary_type', 'location_description', 'ward']\n",
      "df2 = pd.get_dummies(df2, columns = categorical_columns)\n",
      "df2['arrest'] = df2['arrest'].type(int)\n",
      "df2.head()\n",
      "85/29:\n",
      "categorical_columns = ['primary_type', 'location_description', 'ward']\n",
      "df2 = pd.get_dummies(df2, columns = categorical_columns)\n",
      "print df2['arrest'].type(int)\n",
      "85/30:\n",
      "categorical_columns = ['primary_type', 'location_description', 'ward']\n",
      "df2 = pd.get_dummies(df2, columns = categorical_columns)\n",
      "print (df2['arrest'].type(int))\n",
      "85/31:\n",
      "categorical_columns = ['primary_type', 'location_description', 'ward']\n",
      "df2 = pd.get_dummies(df2, columns = categorical_columns)\n",
      "print (df2['arrest'].type())\n",
      "85/32:\n",
      "categorical_columns = ['primary_type', 'location_description', 'ward']\n",
      "df2 = pd.get_dummies(df2, columns = categorical_columns)\n",
      "print (df2['arrest'].type)\n",
      "85/33: df['primary_type'].nunique\n",
      "85/34: df['primary_type'].nunique.sum()\n",
      "85/35: df['primary_type'].nunique(axis=1)\n",
      "85/36: df['primary_type'].nunique(axis=0)\n",
      "85/37: df.nunique(axis=0)\n",
      "85/38: df[['location_description']].nunique()\n",
      "85/39: df[['location_description']].nunique(axis=1)\n",
      "85/40: df['location_description'].value_counts()\n",
      "85/41:\n",
      "typeVals = df.dtypes\n",
      "typeVals = typeVals.replace('bool', 'int64')\n",
      "df = df.astype(typeVals)\n",
      "df\n",
      "85/42:\n",
      "typeVals = df.dtypes\n",
      "typeVals = typeVals.replace('bool', 'int64')\n",
      "df = df.astype(typeVals)\n",
      "85/43: df['date'].type\n",
      "85/44: df[['date']].dtypes\n",
      "85/45:\n",
      "categorical_columns = ['iucr', 'location_description', 'community_area']\n",
      "df2 = pd.get_dummies(df2, columns = categorical_columns)\n",
      "85/46: df2 = df.copy()[['date', 'location_description', 'arrest', 'domestic', 'community_area', 'iucr']]\n",
      "85/47:\n",
      "categorical_columns = ['iucr', 'location_description', 'community_area']\n",
      "df2 = pd.get_dummies(df2, columns = categorical_columns)\n",
      "85/48: df2.head()\n",
      "85/49: df2['date'].dt\n",
      "85/50: df2['date'].dt.time\n",
      "85/51: df2['date'].dt.time[:2]\n",
      "85/52: df2['date'].dt.time[,:2]\n",
      "85/53: df2['date'].dt.hour\n",
      "85/54: print(df['location_description'].value_counts())\n",
      "85/55: print(df['location_description'].value_counts()['ABANDONED BUILDING'])\n",
      "85/56: df['location_description'].value_counts()['ABANDONED BUILDING']\n",
      "85/57: df['location_description'].value_counts()['ALLEYWAY']\n",
      "85/58: df['location_description'].value_counts()['ALLEY WAY']\n",
      "85/59: df['location_description'].value_counts()['ALLEYWAY']\n",
      "85/60: df['location_description'].value_counts()['BASEMENT']\n",
      "85/61:\n",
      "df2['date'] = df2['date'].dt.hour/3\n",
      "df2.head()\n",
      "85/62: df['location_description'].value_counts()\n",
      "85/63: df['location_description'].value_counts()[:10]\n",
      "85/64:\n",
      "df2 = df.copy()[['date', 'location_description', 'arrest', 'domestic', 'community_area', 'iucr']]\n",
      "categorical_columns = ['iucr', 'location_description', 'community_area']\n",
      "df2 = pd.get_dummies(df2, columns = categorical_columns)\n",
      "df2['date'] = df2['date'].dt.hour//3\n",
      "85/65: df2.head()\n",
      "85/66:\n",
      "df2 = df.copy()[['date', 'location_description', 'arrest', 'domestic', 'community_area', 'iucr']]\n",
      "df2['date'] = df2['date'].dt.hour//3\n",
      "categorical_columns = ['iucr', 'location_description', 'community_area', 'date']\n",
      "df2 = pd.get_dummies(df2, columns = categorical_columns)\n",
      "85/67: df2.head()\n",
      "85/68: df2.value_counts()\n",
      "85/69: df2.nunique()\n",
      "85/70: df['location_description'].value_counts()['HOSPITAL']\n",
      "85/71: df['location_description'].value_counts()['HOSPITAL BUILDING/GROUNDS']\n",
      "85/72: df['location_description'].value_counts()['FEDERAL BUILDING']\n",
      "85/73: df['location_description'].value_counts()['GOVERNMENT BUILDING']\n",
      "85/74: df['location_description'].value_counts()\n",
      "85/75: set(df['location_description'].value_counts())\n",
      "85/76:\n",
      "for x in df['location_description'].value_counts():\n",
      "    print x\n",
      "85/77:\n",
      "for x in df['location_description'].value_counts():\n",
      "    print(x)\n",
      "85/78:\n",
      "for x,y in df['location_description'].value_counts():\n",
      "    print(x)\n",
      "85/79: df['location_description'].value_counts()\n",
      "85/80: df['location_description'].value_counts()['FEDERAL BUILDING', 'GOVERNMENT BUILDING']\n",
      "85/81: df['location_description'].value_counts()['FEDERAL BUILDING']\n",
      "85/82: [x for x in df['location_description'] if 'FEDERAL' in x]\n",
      "85/83: [x for x in df['location_description'].values() if 'FEDERAL' in x]\n",
      "85/84: [x for x in df[['location_description']].values() if 'FEDERAL' in x]\n",
      "85/85: [x for x in df[['location_description']].values if 'FEDERAL' in x]\n",
      "85/86:\n",
      "df[['location_description']].values\n",
      "[x for x in df[['location_description']].values if 'FEDERAL' in x]\n",
      "85/87: df[['location_description']].values\n",
      "85/88: df['location_description'].unique()\n",
      "85/89: [x for x in df['location_description'].unique() if 'FEDERAL' in x]\n",
      "85/90: df['location_description'].unique()\n",
      "85/91:\n",
      "for x in df['location_description'].unique():\n",
      "    print(x)\n",
      "85/92: [x for x in df['location_description'].unique() if 'FEDERAL' in x]\n",
      "85/93: [x for x in df['location_description'].unique()]\n",
      "85/94: [x for x in df['location_description'].unique() if not x == None and 'FEDERAL' in x]\n",
      "85/95: [x for x in df['location_description'].unique() if not x == None and 'FEDERAL' in x or 'GOVERNMENT' in x]\n",
      "85/96: [x for x in df['location_description'].unique() if not x == None and ('FEDERAL' in x or 'GOVERNMENT' in x)]\n",
      "85/97:\n",
      "def getcounts(word):\n",
      "    cols = [x for x in df['location_description'].unique() if not x == None and (word in x)]\n",
      "    tot = 0\n",
      "    for c in cols:\n",
      "        tot += df['location_description'].value_counts()[c]\n",
      "    return tot\n",
      "85/98: getcounts('GOVERNMENT')\n",
      "85/99:\n",
      "def getcounts2(words):\n",
      "    tot = 0\n",
      "    for w in words:\n",
      "        tot += getcounts(w)\n",
      "    return tot\n",
      "85/100: getcounts2('GOVERNMENT', 'FEDERAL')\n",
      "85/101: getcounts2(['GOVERNMENT', 'FEDERAL'])\n",
      "85/102:\n",
      "def getcounts(word):\n",
      "    cols = [x for x in df['location_description'].unique() if not x == None and (word in x)]\n",
      "    tot = 0\n",
      "    for c in cols:\n",
      "        tot += df['location_description'].value_counts()[c]\n",
      "    return tot\n",
      "def getcounts2(words):\n",
      "    tot = 0\n",
      "    for w in words:\n",
      "        tot += getcounts(w)\n",
      "    return tot\n",
      "85/103: getcounts2(['BAR'])\n",
      "85/104: getcounts2(['BAR', 'POOL', 'SPORTS'])\n",
      "85/105:\n",
      "df2 = df.copy()[['date', 'location_description', 'arrest', 'domestic', 'community_area', 'iucr']]\n",
      "df2['hour'] = df2['date'].dt.hour//3\n",
      "df2.drop('date', axis=1)\n",
      "categorical_columns = ['iucr', 'location_description', 'community_area', 'hour']\n",
      "df2 = pd.get_dummies(df2, columns = categorical_columns)\n",
      "85/106: df2.head()\n",
      "85/107:\n",
      "df2 = df.copy()[['date', 'location_description', 'arrest', 'domestic', 'community_area', 'iucr']]\n",
      "df2['hour'] = df2['date'].dt.hour//3\n",
      "df2.drop(['date'], axis=1)\n",
      "categorical_columns = ['iucr', 'location_description', 'community_area', 'hour']\n",
      "df2 = pd.get_dummies(df2, columns = categorical_columns)\n",
      "85/108: df2.head()\n",
      "85/109: df2.nunique()\n",
      "85/110:\n",
      "df2 = df.copy()[['date', 'location_description', 'arrest', 'domestic', 'community_area', 'iucr']]\n",
      "df2['hour'] = df2['date'].dt.hour//3\n",
      "df2 = df2.drop(['date'], axis=1)\n",
      "categorical_columns = ['iucr', 'location_description', 'community_area', 'hour']\n",
      "df2 = pd.get_dummies(df2, columns = categorical_columns)\n",
      "85/111: df2.head()\n",
      "85/112: udf = pd.read_csv('use_df.csv')\n",
      "85/113: udf.head()\n",
      "85/114:\n",
      "# df = pd.read_pickle('since2018')\n",
      "df = pd.read_csv('use_df.csv')\n",
      "85/115: df.head()\n",
      "85/116: df.nunique(axis=0)\n",
      "85/117: df.shape\n",
      "85/118:\n",
      "typeVals = df.dtypes\n",
      "typeVals = typeVals.replace('bool', 'int64')\n",
      "df = df.astype(typeVals)\n",
      "85/119:\n",
      "df2 = df.copy()\n",
      "df2['hour'] = df2['date'].dt.hour//3\n",
      "df2 = df2.drop(['date'], axis=1)\n",
      "categorical_columns = ['iucr', 'location_description', 'community_area', 'hour']\n",
      "df2 = pd.get_dummies(df2, columns = categorical_columns)\n",
      "85/120: df[['date']].dtypes\n",
      "85/121: df['date'] = pd.to_datetime(df.date)\n",
      "85/122:\n",
      "df2 = df.copy()\n",
      "df2['hour'] = df2['date'].dt.hour//3\n",
      "df2 = df2.drop(['date'], axis=1)\n",
      "categorical_columns = ['iucr', 'location_description', 'community_area', 'hour']\n",
      "df2 = pd.get_dummies(df2, columns = categorical_columns)\n",
      "85/123: df2.head()\n",
      "85/124: df2.nunique()\n",
      "85/125: df2.value_counts()\n",
      "85/126: from sklearn.decomposition import LatentDirichletAllocation\n",
      "85/127:\n",
      "lda = LatentDirichletAllocation(n_components=10)\n",
      "lda.fit(df2.values)\n",
      "85/128:\n",
      "ldas = []\n",
      "ns = [3,5,10,15,20]\n",
      "for n in ns:\n",
      "    lda = LatentDirichletAllocation(n_components=n)\n",
      "    lda.fit(df2.values)\n",
      "    ldas.append(lda)\n",
      "85/129:\n",
      "ldas = []\n",
      "ns = [3,5,10,15,20]\n",
      "for n in ns:\n",
      "    ldas.append(LatentDirichletAllocation(n_components=n).fit(df2.values))\n",
      "85/130:\n",
      "ldas = []\n",
      "ns = [3,5,10,15,20]\n",
      "for n in ns:\n",
      "    print(n)\n",
      "    ldas.append(LatentDirichletAllocation(n_components=n).fit(df2.values))\n",
      "85/131:\n",
      "ldas = []\n",
      "ns = [3,5,10,15,20]\n",
      "for n in ns:\n",
      "    print(n)\n",
      "    ldas.append(LatentDirichletAllocation(n_components=n).fit(df2.values))\n",
      "85/132:\n",
      "ldas = []\n",
      "ns = [3,10,20]\n",
      "for n in ns:\n",
      "    print(n)\n",
      "    ldas.append(LatentDirichletAllocation(n_components=n).fit(df2.values))\n",
      "85/133:\n",
      "ldas = []\n",
      "ns = [3,10,20]\n",
      "for n in ns:\n",
      "    print(n)\n",
      "    ldas.append(LatentDirichletAllocation(n_components=n, learning_method='online').fit(df2.values))\n",
      "85/134:\n",
      "for lda in ldas:\n",
      "    print(lda.score())\n",
      "85/135:\n",
      "for lda in ldas:\n",
      "    print(lda.score(df2.values))\n",
      "85/136: from sklearn.decomposition import PCA\n",
      "85/137:\n",
      "pcas = []\n",
      "ns2 = [3,5,10,15,20]\n",
      "for n in ns2:\n",
      "    print(n)\n",
      "    pcas.append(PCA(n_components=n).fit(df2.values))\n",
      "85/138: df['primary_type'].value_counts()\n",
      "85/139: df0 = pd.read_pickle('since2018')\n",
      "85/140: df0['primary_types'].value_counts()\n",
      "85/141: df0['primary_type'].value_counts()\n",
      "85/142: transformed = ldas[0].transform(df2.values)\n",
      "85/143:\n",
      "plt.figure()\n",
      "for i in range(30):\n",
      "    plt.hist(transformed[:, i], alpha=0.3, label=str(i+1),\n",
      "            range=(0,1), bins=20)\n",
      "#     plt.show()\n",
      "plt.xlabel(\"User Weight in Component i\", fontsize=20)\n",
      "plt.ylabel(\"Count\", fontsize=20)\n",
      "plt.tick_params(labelsize=15)\n",
      "plt.legend(ncol=2, bbox_to_anchor=(1, 1.05))\n",
      "plt.title(\"LDA\")\n",
      "plt.show()\n",
      "85/144:\n",
      "plt.figure()\n",
      "for i in range(3):\n",
      "    plt.hist(transformed[:, i], alpha=0.3, label=str(i+1),\n",
      "            range=(0,1), bins=20)\n",
      "#     plt.show()\n",
      "plt.xlabel(\"User Weight in Component i\", fontsize=20)\n",
      "plt.ylabel(\"Count\", fontsize=20)\n",
      "plt.tick_params(labelsize=15)\n",
      "plt.legend(ncol=2, bbox_to_anchor=(1, 1.05))\n",
      "plt.title(\"LDA\")\n",
      "plt.show()\n",
      "85/145:\n",
      "plt.figure()\n",
      "for i in range(3):\n",
      "    plt.hist(transformed[:, i], alpha=0.3, label=str(i+1),\n",
      "            range=(0,1), bins=20)\n",
      "#     plt.show()\n",
      "plt.xlabel(\"User Weight in Component i\", fontsize=20)\n",
      "plt.ylabel(\"Count\", fontsize=20)\n",
      "plt.tick_params(labelsize=15)\n",
      "plt.legend(ncol=1, bbox_to_anchor=(1, 1.05))\n",
      "plt.title(\"LDA\")\n",
      "plt.show()\n",
      "85/146:\n",
      "c1 = ldas[0].components_[0, :]\n",
      "print c1.shape\n",
      "85/147:\n",
      "c1 = ldas[0].components_[0, :]\n",
      "print(c1.shape)\n",
      "85/148:\n",
      "c1_indices = np.where(c1 > np.percentile(c1, 99))[0]\n",
      "\n",
      "top_c1_df = pd.DataFrame({'Component': profiles1.columns.values[c1_indices],\n",
      "                         'Weight': c1[c1_indices]})\n",
      "\n",
      "top_c1_df.sort_values('Weight', inplace=True, ascending=False)\n",
      "\n",
      "top_c1_df.head(n=10)\n",
      "85/149:\n",
      "c1_indices = np.where(c1 > np.percentile(c1, 99))[0]\n",
      "\n",
      "top_c1_df = pd.DataFrame({'Component': df2.columns.values[c1_indices],\n",
      "                         'Weight': c1[c1_indices]})\n",
      "\n",
      "top_c1_df.sort_values('Weight', inplace=True, ascending=False)\n",
      "\n",
      "top_c1_df.head(n=10)\n",
      "85/150:\n",
      "lda5 = LatentDirichletAllocation(n_components=5, learning_method='online').fit(df2.values)\n",
      "lda5.score(df2.values)\n",
      "85/151:\n",
      "lda4 = LatentDirichletAllocation(n_components=4, learning_method='online').fit(df2.values)\n",
      "lda4.score(df2.values)\n",
      "85/152:\n",
      "trainInds = np.random.uniform(0,1,df2.shape[0])<=0.8\n",
      "train=df2.iloc[trainInds]\n",
      "test=df2.iloc[~trainInds]\n",
      "85/153: train.shape, test.shape\n",
      "85/154:\n",
      "ldas2 = []\n",
      "scores = []\n",
      "ns = [3,5,10,15,20]\n",
      "for n in ns:\n",
      "    print(n)\n",
      "    lda = LatentDirichletAllocation(n_components=n, learning_method='online')\n",
      "    ldas2.append(lda.fit(train.values))\n",
      "    score = lda.score(test.values)\n",
      "    print score\n",
      "    scores.append(score)\n",
      "85/155:\n",
      "ldas2 = []\n",
      "scores = []\n",
      "ns = [3,5,10,15,20]\n",
      "for n in ns:\n",
      "    print(n)\n",
      "    lda = LatentDirichletAllocation(n_components=n, learning_method='online')\n",
      "    ldas2.append(lda.fit(train.values))\n",
      "    score = lda.score(test.values)\n",
      "    print(score)\n",
      "    scores.append(score)\n",
      "85/156: plt.plot(ns, scores)\n",
      "85/157: from sklearn.gaussian_process import GaussianProcessRegressor\n",
      "85/158: plt.plot([x for x in range(20)], pcas[4].singular_values_)\n",
      "85/159: plt.plot([x for x in range(15)], pcas[4].singular_values_)\n",
      "85/160: plt.plot([x for x in range(15)], pcas[3].singular_values_)\n",
      "85/161:\n",
      "def pca_err(pca):\n",
      "    x = pca.inverse_transform(pca.fit_transform(df2.values))\n",
      "    loss = ((df2.values - x) ** 2).mean()\n",
      "    return loss\n",
      "85/162: pca_err(pcas[0])\n",
      "85/163: pca_err(pcas[1])\n",
      "85/164:\n",
      "errs = []\n",
      "for i in range(5):\n",
      "    errs.append(pca_err(pcas[i]))\n",
      "plt.plot(ns2, errs)\n",
      "85/165:\n",
      "def gettop(i):\n",
      "    c1 = ldas[0].components_[i, :]\n",
      "    c1_indices = np.where(c1 > np.percentile(c1, 70))[0]\n",
      "\n",
      "    top_c1_df = pd.DataFrame({'Component': df2.columns.values[c1_indices],\n",
      "                             'Weight': c1[c1_indices]})\n",
      "\n",
      "    top_c1_df.sort_values('Weight', inplace=True, ascending=False)\n",
      "\n",
      "    print top_c1_df.head(n=10)\n",
      "85/166:\n",
      "def gettop(i):\n",
      "    c1 = ldas[0].components_[i, :]\n",
      "    c1_indices = np.where(c1 > np.percentile(c1, 70))[0]\n",
      "\n",
      "    top_c1_df = pd.DataFrame({'Component': df2.columns.values[c1_indices],\n",
      "                             'Weight': c1[c1_indices]})\n",
      "\n",
      "    top_c1_df.sort_values('Weight', inplace=True, ascending=False)\n",
      "\n",
      "    print(top_c1_df.head(n=10))\n",
      "85/167: gettop(0)\n",
      "85/168: gettop(1)\n",
      "85/169: gettop(2)\n",
      "85/170:\n",
      "sums = []\n",
      "tot = 0\n",
      "for x in pcas[3].singular_values_:\n",
      "    tot += x\n",
      "    sums.append(tot)\n",
      "plt.plot([x for x in range(15)], sums)\n",
      "85/171:\n",
      "pcas = []\n",
      "ns3 = [25, 30, 35, 40]\n",
      "for n in ns3:\n",
      "    print(n)\n",
      "    pcas.append(PCA(n_components=n).fit(df2.values))\n",
      "85/172: pcas\n",
      "85/173: plt.plot([x for x in range(40)], pcas[3].singular_values_)\n",
      "85/174:\n",
      "sums = []\n",
      "tot = 0\n",
      "for x in pcas[3].singular_values_:\n",
      "    tot += x\n",
      "    sums.append(tot)\n",
      "plt.plot([x for x in range(40)], sums)\n",
      "85/175:\n",
      "pcas = []\n",
      "ns3 = [25, 30, 35, 40, 45, 50, 55, 60]\n",
      "for n in ns3:\n",
      "    print(n)\n",
      "    pcas.append(PCA(n_components=n).fit(df2.values))\n",
      "85/176:\n",
      "sums = []\n",
      "tot = 0\n",
      "for x in pcas[7].singular_values_:\n",
      "    tot += x\n",
      "    sums.append(tot)\n",
      "plt.plot([x for x in range(60)], sums)\n",
      "85/177: plt.plot([x for x in range(60)], pcas[7].singular_values_)\n",
      "85/178: len(df2)\n",
      "85/179: len(df2[0])\n",
      "85/180: len(df2.iloc[0])\n",
      "85/181:\n",
      "def pcacmp(pca, i, j):\n",
      "    c1 = pca.components_[i]\n",
      "    c2 = pca.components_[j]\n",
      "    x = []\n",
      "    y = []\n",
      "    for i in range(len(df2)):\n",
      "        x.append(np.dot(df2.iloc[i], c1))\n",
      "        y.append(np.dot(df2.iloc[i], c2))\n",
      "    plt.plot(x, y)\n",
      "85/182: pcacmp(pcas[7], 0, 1)\n",
      "85/183:\n",
      "def pcacmp(pca, i, j):\n",
      "    c1 = pca.components_[i]\n",
      "    c2 = pca.components_[j]\n",
      "    x = []\n",
      "    y = []\n",
      "    for i in range(100):\n",
      "        x.append(np.dot(df2.iloc[i], c1))\n",
      "        y.append(np.dot(df2.iloc[i], c2))\n",
      "    plt.plot(x, y)\n",
      "85/184: pcacmp(pcas[7], 0, 1)\n",
      "85/185:\n",
      "def pcacmp(pca, i, j):\n",
      "    c1 = pca.components_[i]\n",
      "    c2 = pca.components_[j]\n",
      "    x = []\n",
      "    y = []\n",
      "    for i in range(100):\n",
      "        x.append(np.dot(df2.iloc[i], c1))\n",
      "        y.append(np.dot(df2.iloc[i], c2))\n",
      "    plt.plot(x, y, \"o\")\n",
      "85/186: pcacmp(pcas[7], 0, 1)\n",
      "85/187:\n",
      "def pcacmp(pca, i, j, l):\n",
      "    c1 = pca.components_[i]\n",
      "    c2 = pca.components_[j]\n",
      "    x = []\n",
      "    y = []\n",
      "    for i in range(l):\n",
      "        x.append(np.dot(df2.iloc[i], c1))\n",
      "        y.append(np.dot(df2.iloc[i], c2))\n",
      "    plt.plot(x, y, \"o\")\n",
      "85/188: pcacmp(pcas[7], 0, 1, 500)\n",
      "85/189: pcacmp(pcas[7], 0, 1, 1000)\n",
      "85/190: pcacmp(pcas[7], 1, 2, 1000)\n",
      "85/191: pcacmp(pcas[7], 1, 2, 2000)\n",
      "85/192: pcacmp(pcas[7], 1, 2, 5000)\n",
      "85/193: pcacmp(pcas[7], 0, 1, 5000)\n",
      "85/194: pcacmp(pcas[7], 2, 3, 5000)\n",
      "85/195:\n",
      "def pcacmp(pca, i, j, l):\n",
      "    c1 = pca.components_[i]\n",
      "    c2 = pca.components_[j]\n",
      "    x = []\n",
      "    y = []\n",
      "    for i in range(l):\n",
      "        x.append(np.dot(df2.iloc[i], c1))\n",
      "        y.append(np.dot(df2.iloc[i], c2))\n",
      "    plt.plot(x, y, \"o\")\n",
      "    plt.xlabel(\"PCA \" + str(i+1))\n",
      "    plt.ylabel(\"PCA \" + str(j+1))\n",
      "85/196: pcacmp(pcas[7], 0, 1, 5000)\n",
      "85/197: pcacmp(pcas[7], 1, 2, 5000)\n",
      "85/198: pcacmp(pcas[7], 2, 3, 5000)\n",
      "85/199:\n",
      "def pcacmp(pca, i, j, l):\n",
      "    c1 = pca.components_[i]\n",
      "    c2 = pca.components_[j]\n",
      "    x = []\n",
      "    y = []\n",
      "    for k in range(l):\n",
      "        x.append(np.dot(df2.iloc[k], c1))\n",
      "        y.append(np.dot(df2.iloc[k], c2))\n",
      "    plt.plot(x, y, \"o\")\n",
      "    plt.xlabel(\"PCA \" + str(i+1))\n",
      "    plt.ylabel(\"PCA \" + str(j+1))\n",
      "85/200: pcacmp(pcas[7], 0, 1, 5000)\n",
      "85/201: pcacmp(pcas[7], 1, 2, 5000)\n",
      "85/202: pcacmp(pcas[7], 2, 3, 5000)\n",
      "85/203:\n",
      "for i in range(60-1):\n",
      "    pcacmp(pcas[7], i, i+1, 5000)\n",
      "85/204:\n",
      "for i in range(2):\n",
      "    pcacmp(pcas[7], i, i+1, 5000)\n",
      "85/205:\n",
      "for i in range(2):\n",
      "    plt.figure()\n",
      "    pcacmp(pcas[7], i, i+1, 5000)\n",
      "85/206:\n",
      "for i in range(59):\n",
      "    plt.figure()\n",
      "    pcacmp(pcas[7], i, i+1, 5000)\n",
      "85/207:\n",
      "for i in range(10):\n",
      "    plt.figure()\n",
      "    pcacmp(pcas[7], i, i+1, 5000)\n",
      "85/208: print pca.components_[i]\n",
      "85/209: pca.components_[i]\n",
      "85/210: pcas[7.components_[i]\n",
      "85/211: pcas[7].components_[i]\n",
      "85/212: len(pcas[7].components_[0])\n",
      "85/213:\n",
      "def pcacmps(pca, i, j):\n",
      "    inds = []\n",
      "    for k in range(len(pca.components_[i])):\n",
      "        if abs(pca.components_[i][k]-pca.components_[j][k]) > 0.1):\n",
      "            inds.append(k)\n",
      "    for k in range(len(inds)):\n",
      "        print(df2.columns[k])\n",
      "85/214:\n",
      "def pcacmps(pca, i, j):\n",
      "    inds = []\n",
      "    for k in range(len(pca.components_[i])):\n",
      "        if abs(pca.components_[i][k]-pca.components_[j][k]) > 0.1:\n",
      "            inds.append(k)\n",
      "    for k in range(len(inds)):\n",
      "        print(df2.columns[k])\n",
      "85/215: pcacmps(pca[7], 0, 1)\n",
      "85/216: pcacmps(pcas[7], 0, 1)\n",
      "85/217:\n",
      "def pcacmps(pca, i, j):\n",
      "    inds = []\n",
      "    for k in range(len(pca.components_[i])):\n",
      "        if abs(pca.components_[i][k]-pca.components_[j][k]) > 0.2:\n",
      "            inds.append(k)\n",
      "    for k in range(len(inds)):\n",
      "        print(df2.columns[k])\n",
      "85/218: pcacmps(pcas[7], 0, 1)\n",
      "85/219: pcacmps(pcas[7], 1, 2)\n",
      "85/220: pcacmps(pcas[7], 2, 3)\n",
      "85/221: pcacmps(pcas[7], 3, 4)\n",
      "85/222: pcacmps(pcas[7], 4, 5)\n",
      "85/223:\n",
      "def pcacmp2(pca, i, j, thresh):\n",
      "    c1 = pca.components_[i]\n",
      "    c2 = pca.components_[j]\n",
      "    x = []\n",
      "    y = []\n",
      "    avg1 = [0]*428\n",
      "    count1 = 0\n",
      "    avg2 = [0]*428\n",
      "    count2 = 0\n",
      "    for k in range(1000):\n",
      "        x1 = np.dot(df2.iloc[k], c1)\n",
      "        x.append(x1)\n",
      "        if x1 > thresh:\n",
      "            avg1 += x1\n",
      "            count1 += 1\n",
      "        else:\n",
      "            avg2 += x1\n",
      "            count2 += 1\n",
      "        y1 = np.dot(df2.iloc[k], c2)\n",
      "        y.append(y1)\n",
      "        \n",
      "    avg1 = avg1/count1\n",
      "    avg2 = avg2/count2\n",
      "    \n",
      "    for k in range(428):\n",
      "        if abs(avg1[k]-avg2[k]) > 0.2:\n",
      "            inds.append(k)\n",
      "    for k in range(len(inds)):\n",
      "        print(df2.columns[k])\n",
      "    \n",
      "    plt.plot(x, y, \"o\")\n",
      "    plt.xlabel(\"PCA \" + str(i+1))\n",
      "    plt.ylabel(\"PCA \" + str(j+1))\n",
      "85/224: pcacmp2(pcas[7], 9, 10, 0.5)\n",
      "85/225:\n",
      "def pcacmp2(pca, i, j, thresh):\n",
      "    c1 = pca.components_[i]\n",
      "    c2 = pca.components_[j]\n",
      "    x = []\n",
      "    y = []\n",
      "    avg1 = [0]*428\n",
      "    count1 = 0\n",
      "    avg2 = [0]*428\n",
      "    count2 = 0\n",
      "    for k in range(1000):\n",
      "        x1 = np.dot(df2.iloc[k], c1)\n",
      "        x.append(x1)\n",
      "        if x1 > thresh:\n",
      "            avg1 += x1\n",
      "            count1 += 1\n",
      "        else:\n",
      "            avg2 += x1\n",
      "            count2 += 1\n",
      "        y1 = np.dot(df2.iloc[k], c2)\n",
      "        y.append(y1)\n",
      "        \n",
      "    avg1 = avg1/count1\n",
      "    avg2 = avg2/count2\n",
      "    inds = []\n",
      "    for k in range(428):\n",
      "        if abs(avg1[k]-avg2[k]) > 0.2:\n",
      "            inds.append(k)\n",
      "    for k in range(len(inds)):\n",
      "        print(df2.columns[k])\n",
      "    \n",
      "    plt.plot(x, y, \"o\")\n",
      "    plt.xlabel(\"PCA \" + str(i+1))\n",
      "    plt.ylabel(\"PCA \" + str(j+1))\n",
      "85/226: pcacmp2(pcas[7], 9, 10, 0.5)\n",
      "85/227:\n",
      "def pcacmp2(pca, i, j, thresh):\n",
      "    c1 = pca.components_[i]\n",
      "    c2 = pca.components_[j]\n",
      "    x = []\n",
      "    y = []\n",
      "    avg1 = [0]*428\n",
      "    count1 = 0\n",
      "    avg2 = [0]*428\n",
      "    count2 = 0\n",
      "    for k in range(1000):\n",
      "        x1 = np.dot(df2.iloc[k], c1)\n",
      "        x.append(x1)\n",
      "        if x1 > thresh:\n",
      "            avg1 += x1\n",
      "            count1 += 1\n",
      "        else:\n",
      "            avg2 += x1\n",
      "            count2 += 1\n",
      "        y1 = np.dot(df2.iloc[k], c2)\n",
      "        y.append(y1)\n",
      "        \n",
      "    avg1 = avg1/count1\n",
      "    avg2 = avg2/count2\n",
      "    inds = []\n",
      "    for k in range(428):\n",
      "        if abs(avg1[k]-avg2[k]) > 1:\n",
      "            inds.append(k)\n",
      "    for k in range(len(inds)):\n",
      "        print(df2.columns[k])\n",
      "    \n",
      "    plt.plot(x, y, \"o\")\n",
      "    plt.xlabel(\"PCA \" + str(i+1))\n",
      "    plt.ylabel(\"PCA \" + str(j+1))\n",
      "85/228: pcacmp2(pcas[7], 9, 10, 0.5)\n",
      "85/229:\n",
      "def pcacmp2(pca, i, j, thresh):\n",
      "    c1 = pca.components_[i]\n",
      "    c2 = pca.components_[j]\n",
      "    x = []\n",
      "    y = []\n",
      "    avg1 = [0]*428\n",
      "    count1 = 0\n",
      "    avg2 = [0]*428\n",
      "    count2 = 0\n",
      "    for k in range(1000):\n",
      "        x1 = np.dot(df2.iloc[k], c1)\n",
      "        x.append(x1)\n",
      "        if x1 > thresh:\n",
      "            avg1 += x1\n",
      "            count1 += 1\n",
      "        else:\n",
      "            avg2 += x1\n",
      "            count2 += 1\n",
      "        y1 = np.dot(df2.iloc[k], c2)\n",
      "        y.append(y1)\n",
      "        \n",
      "    avg1 = avg1/count1\n",
      "    avg2 = avg2/count2\n",
      "    inds = []\n",
      "    for k in range(428):\n",
      "        if abs(avg1[k]-avg2[k]) > .7:\n",
      "            inds.append(k)\n",
      "    for k in range(len(inds)):\n",
      "        print(df2.columns[k])\n",
      "    \n",
      "    plt.plot(x, y, \"o\")\n",
      "    plt.xlabel(\"PCA \" + str(i+1))\n",
      "    plt.ylabel(\"PCA \" + str(j+1))\n",
      "85/230: pcacmp2(pcas[7], 9, 10, 0.5)\n",
      "85/231:\n",
      "def pcacmp2(pca, i, j, thresh):\n",
      "    c1 = pca.components_[i]\n",
      "    c2 = pca.components_[j]\n",
      "    x = []\n",
      "    y = []\n",
      "    avg1 = [0]*428\n",
      "    count1 = 0\n",
      "    avg2 = [0]*428\n",
      "    count2 = 0\n",
      "    for k in range(1000):\n",
      "        x1 = np.dot(df2.iloc[k], c1)\n",
      "        x.append(x1)\n",
      "        if x1 > thresh:\n",
      "            avg1 += x1\n",
      "            count1 += 1\n",
      "        else:\n",
      "            avg2 += x1\n",
      "            count2 += 1\n",
      "        y1 = np.dot(df2.iloc[k], c2)\n",
      "        y.append(y1)\n",
      "        \n",
      "    avg1 = avg1/count1\n",
      "    avg2 = avg2/count2\n",
      "    inds = []\n",
      "    for k in range(428):\n",
      "        if abs(avg1[k]-avg2[k]) > .8:\n",
      "            inds.append(k)\n",
      "    for k in range(len(inds)):\n",
      "        print(df2.columns[k])\n",
      "    \n",
      "    plt.plot(x, y, \"o\")\n",
      "    plt.xlabel(\"PCA \" + str(i+1))\n",
      "    plt.ylabel(\"PCA \" + str(j+1))\n",
      "85/232: pcacmp2(pcas[7], 9, 10, 0.5)\n",
      "85/233:\n",
      "def pcacmp2(pca, i, j, thresh):\n",
      "    c1 = pca.components_[i]\n",
      "    c2 = pca.components_[j]\n",
      "    x = []\n",
      "    y = []\n",
      "    avg1 = [0]*428\n",
      "    count1 = 0\n",
      "    avg2 = [0]*428\n",
      "    count2 = 0\n",
      "    for k in range(1000):\n",
      "        x1 = np.dot(df2.iloc[k], c1)\n",
      "        x.append(x1)\n",
      "        if x1 > thresh:\n",
      "            avg1 += x1\n",
      "            count1 += 1\n",
      "        else:\n",
      "            avg2 += x1\n",
      "            count2 += 1\n",
      "        y1 = np.dot(df2.iloc[k], c2)\n",
      "        y.append(y1)\n",
      "        \n",
      "    avg1 = avg1/count1\n",
      "    avg2 = avg2/count2\n",
      "    inds = []\n",
      "    for k in range(428):\n",
      "        if abs(avg1[k]-avg2[k]) > .9:\n",
      "            inds.append(k)\n",
      "    for k in range(len(inds)):\n",
      "        print(df2.columns[k])\n",
      "    \n",
      "    plt.plot(x, y, \"o\")\n",
      "    plt.xlabel(\"PCA \" + str(i+1))\n",
      "    plt.ylabel(\"PCA \" + str(j+1))\n",
      "85/234: pcacmp2(pcas[7], 9, 10, 0.5)\n",
      "85/235:\n",
      "def pcacmp2(pca, i, j, thresh):\n",
      "    c1 = pca.components_[i]\n",
      "    c2 = pca.components_[j]\n",
      "    x = []\n",
      "    y = []\n",
      "    avg1 = [0]*428\n",
      "    count1 = 0\n",
      "    avg2 = [0]*428\n",
      "    count2 = 0\n",
      "    for k in range(1000):\n",
      "        x1 = np.dot(df2.iloc[k], c1)\n",
      "        x.append(x1)\n",
      "        if x1 > thresh:\n",
      "            avg1 += x1\n",
      "            count1 += 1\n",
      "        else:\n",
      "            avg2 += x1\n",
      "            count2 += 1\n",
      "        y1 = np.dot(df2.iloc[k], c2)\n",
      "        y.append(y1)\n",
      "    \n",
      "    avg1 = avg1/count1\n",
      "    avg2 = avg2/count2\n",
      "    inds = []\n",
      "    for k in range(428):\n",
      "        if abs(avg1[k]-avg2[k]) > .9:\n",
      "            inds.append(k)\n",
      "    for k in range(len(inds)):\n",
      "        print(df2.columns[k])\n",
      "    \n",
      "    plt.plot(x, y, \"o\")\n",
      "    plt.xlabel(\"PCA \" + str(i+1))\n",
      "    plt.ylabel(\"PCA \" + str(j+1))\n",
      "    return avg1, avg2\n",
      "85/236: a1, a2 = pcacmp2(pcas[7], 9, 10, 0.5)\n",
      "85/237: a1\n",
      "85/238: a2\n",
      "85/239:\n",
      "def pcacmp2(pca, i, j, thresh):\n",
      "    c1 = pca.components_[i]\n",
      "    c2 = pca.components_[j]\n",
      "    x = []\n",
      "    y = []\n",
      "    avg1 = [0]*428\n",
      "    count1 = 0\n",
      "    avg2 = [0]*428\n",
      "    count2 = 0\n",
      "    for k in range(1000):\n",
      "        x1 = np.dot(df2.iloc[k], c1)\n",
      "        x.append(x1)\n",
      "        if x1 > thresh:\n",
      "            avg1 += df2.iloc[k]\n",
      "            count1 += 1\n",
      "        else:\n",
      "            avg2 += df2.iloc[k]\n",
      "            count2 += 1\n",
      "        y1 = np.dot(df2.iloc[k], c2)\n",
      "        y.append(y1)\n",
      "    \n",
      "    avg1 = avg1/count1\n",
      "    avg2 = avg2/count2\n",
      "    inds = []\n",
      "    for k in range(428):\n",
      "        if abs(avg1[k]-avg2[k]) > .9:\n",
      "            inds.append(k)\n",
      "    for k in range(len(inds)):\n",
      "        print(df2.columns[k])\n",
      "    \n",
      "    plt.plot(x, y, \"o\")\n",
      "    plt.xlabel(\"PCA \" + str(i+1))\n",
      "    plt.ylabel(\"PCA \" + str(j+1))\n",
      "    return avg1, avg2\n",
      "85/240: a1, a2 = pcacmp2(pcas[7], 9, 10, 0.5)\n",
      "85/241: a1\n",
      "85/242: a2\n",
      "85/243: a1\n",
      "85/244:\n",
      "for i in range(428):\n",
      "    if a1[i] > 0 or a2[i] > 0:\n",
      "        print(df2.columns[i], a1[i], a2[i])\n",
      "85/245:\n",
      "for i in range(428):\n",
      "    if abs(a1[i]-a2[i]) > 0.5:\n",
      "        print(df2.columns[i], a1[i], a2[i])\n",
      "85/246:\n",
      "for i in range(428):\n",
      "    if abs(a1[i]-a2[i]) > 0.1:\n",
      "        print(df2.columns[i], a1[i], a2[i])\n",
      "85/247:\n",
      "for i in range(428):\n",
      "    if abs(a1[i]-a2[i]) > 0.05:\n",
      "        print(df2.columns[i], a1[i], a2[i])\n",
      "85/248:\n",
      "def pcacmp2(pca, i, j, thresh):\n",
      "    c1 = pca.components_[i]\n",
      "    c2 = pca.components_[j]\n",
      "    x = []\n",
      "    y = []\n",
      "    avg1 = [0]*428\n",
      "    count1 = 0\n",
      "    avg2 = [0]*428\n",
      "    count2 = 0\n",
      "    for k in range(1000):\n",
      "        x1 = np.dot(df2.iloc[k], c1)\n",
      "        x.append(x1)\n",
      "        if x1 > thresh:\n",
      "            avg1 += df2.iloc[k]\n",
      "            count1 += 1\n",
      "        else:\n",
      "            avg2 += df2.iloc[k]\n",
      "            count2 += 1\n",
      "        y1 = np.dot(df2.iloc[k], c2)\n",
      "        y.append(y1)\n",
      "    \n",
      "    avg1 = avg1/count1\n",
      "    avg2 = avg2/count2\n",
      "    \n",
      "    for k in range(428):\n",
      "    if abs(avg1[k]-avg2[k]) > 0.05:\n",
      "        print(df2.columns[k], avg1[k], avg2[k])\n",
      "    \n",
      "    plt.plot(x, y, \"o\")\n",
      "    plt.xlabel(\"PCA \" + str(i+1))\n",
      "    plt.ylabel(\"PCA \" + str(j+1))\n",
      "    return avg1, avg2\n",
      "85/249: a1, a2 = pcacmp2(pcas[7], 9, 10, 0.5)\n",
      "85/250:\n",
      "def pcacmp2(pca, i, j, thresh):\n",
      "    c1 = pca.components_[i]\n",
      "    c2 = pca.components_[j]\n",
      "    x = []\n",
      "    y = []\n",
      "    avg1 = [0]*428\n",
      "    count1 = 0\n",
      "    avg2 = [0]*428\n",
      "    count2 = 0\n",
      "    for k in range(1000):\n",
      "        x1 = np.dot(df2.iloc[k], c1)\n",
      "        x.append(x1)\n",
      "        if x1 > thresh:\n",
      "            avg1 += df2.iloc[k]\n",
      "            count1 += 1\n",
      "        else:\n",
      "            avg2 += df2.iloc[k]\n",
      "            count2 += 1\n",
      "        y1 = np.dot(df2.iloc[k], c2)\n",
      "        y.append(y1)\n",
      "    \n",
      "    avg1 = avg1/count1\n",
      "    avg2 = avg2/count2\n",
      "    \n",
      "    for k in range(428):\n",
      "        if abs(avg1[k]-avg2[k]) > 0.05:\n",
      "            print(df2.columns[k], avg1[k], avg2[k])\n",
      "    \n",
      "    plt.plot(x, y, \"o\")\n",
      "    plt.xlabel(\"PCA \" + str(i+1))\n",
      "    plt.ylabel(\"PCA \" + str(j+1))\n",
      "    return avg1, avg2\n",
      "85/251: a1, a2 = pcacmp2(pcas[7], 9, 10, 0.5)\n",
      "85/252:\n",
      "def pcacmp2(pca, i, j, thresh, axis):\n",
      "    c1 = pca.components_[i]\n",
      "    c2 = pca.components_[j]\n",
      "    x = []\n",
      "    y = []\n",
      "    avg1 = [0]*428\n",
      "    count1 = 0\n",
      "    avg2 = [0]*428\n",
      "    count2 = 0\n",
      "    for k in range(1000):\n",
      "        x1 = np.dot(df2.iloc[k], c1)\n",
      "        x.append(x1)\n",
      "        if axis==0:\n",
      "            if x1 > thresh:\n",
      "                avg1 += df2.iloc[k]\n",
      "                count1 += 1\n",
      "            else:\n",
      "                avg2 += df2.iloc[k]\n",
      "                count2 += 1\n",
      "        y1 = np.dot(df2.iloc[k], c2)\n",
      "        if axis==1:\n",
      "            if y1 > thresh:\n",
      "                avg1 += df2.iloc[k]\n",
      "                count1 += 1\n",
      "            else:\n",
      "                avg2 += df2.iloc[k]\n",
      "                count2 += 1\n",
      "        y.append(y1)\n",
      "    \n",
      "    avg1 = avg1/count1\n",
      "    avg2 = avg2/count2\n",
      "    \n",
      "    for k in range(428):\n",
      "        if abs(avg1[k]-avg2[k]) > 0.05:\n",
      "            print(df2.columns[k], avg1[k], avg2[k])\n",
      "    \n",
      "    plt.plot(x, y, \"o\")\n",
      "    plt.xlabel(\"PCA \" + str(i+1))\n",
      "    plt.ylabel(\"PCA \" + str(j+1))\n",
      "    return avg1, avg2\n",
      "85/253: a1, a2 = pcacmp2(pcas[7], 9, 10, 0.5, 0)\n",
      "85/254: a1, a2 = pcacmp2(pcas[7], 2, 3, 0.5, 1)\n",
      "85/255: pcacmp2(pcas[7], 9, 10, 0.5, 0)\n",
      "85/256:\n",
      "def pcacmp2(pca, i, j, thresh, axis):\n",
      "    c1 = pca.components_[i]\n",
      "    c2 = pca.components_[j]\n",
      "    x = []\n",
      "    y = []\n",
      "    avg1 = [0]*428\n",
      "    count1 = 0\n",
      "    avg2 = [0]*428\n",
      "    count2 = 0\n",
      "    for k in range(1000):\n",
      "        x1 = np.dot(df2.iloc[k], c1)\n",
      "        x.append(x1)\n",
      "        if axis==0:\n",
      "            if x1 > thresh:\n",
      "                avg1 += df2.iloc[k]\n",
      "                count1 += 1\n",
      "            else:\n",
      "                avg2 += df2.iloc[k]\n",
      "                count2 += 1\n",
      "        y1 = np.dot(df2.iloc[k], c2)\n",
      "        if axis==1:\n",
      "            if y1 > thresh:\n",
      "                avg1 += df2.iloc[k]\n",
      "                count1 += 1\n",
      "            else:\n",
      "                avg2 += df2.iloc[k]\n",
      "                count2 += 1\n",
      "        y.append(y1)\n",
      "    \n",
      "    avg1 = avg1/count1\n",
      "    avg2 = avg2/count2\n",
      "    \n",
      "    for k in range(428):\n",
      "        if abs(avg1[k]-avg2[k]) > 0.05:\n",
      "            print(df2.columns[k], avg1[k], avg2[k])\n",
      "    \n",
      "    return\n",
      "85/257: pcacmp2(pcas[7], 9, 10, 0.5, 0)\n",
      "85/258: a1, a2 = pcacmp2(pcas[7], 2, 3, 0.5, 1)\n",
      "85/259: pcacmp2(pcas[7], 2, 3, 0.5, 1)\n",
      "85/260: pcacmp2(pcas[7], 2, 3, 0.5, 0)\n",
      "85/261: pcacmp2(pcas[7], 8, 9, 0.5, 1)\n",
      "85/262: pcacmp2(pcas[7], 5, 6, 0.25, 0)\n",
      "85/263:\n",
      "for i in range(15):\n",
      "    plt.figure()\n",
      "    pcacmp(pcas[7], i, i+1, 5000)\n",
      "85/264:\n",
      "for i in range(20):\n",
      "    plt.figure()\n",
      "    pcacmp(pcas[7], i, i+1, 5000)\n",
      "85/265: pcacmp2(pcas[7], 6, 7, 0.5, 0)\n",
      "85/266: df0.nunique()\n",
      "85/267: df0['district'].nunique()\n",
      "85/268: d = df0['district'].nunique()\n",
      "85/269:\n",
      "arr = []*26\n",
      "for i in range(len(df2)):\n",
      "    arr[df2['district'][i]].append(i)\n",
      "85/270:\n",
      "arr = []*26\n",
      "for i in range(len(df2)):\n",
      "    arr[df0['district'][i]].append(i)\n",
      "85/271:\n",
      "arr = []*26\n",
      "for i in range(len(df2)):\n",
      "    arr[df0['district'].iloc[i]].append(i)\n",
      "85/272: df0['district'][0]\n",
      "85/273: df0['district']\n",
      "85/274: df0.index[df0['district'] == 1].tolist()\n",
      "85/275: df0['district'].unique()\n",
      "85/276: inds1 = df0.index[df0['district'] == 1].tolist()\n",
      "85/277: df0.iloc[inds1]\n",
      "85/278: df2.iloc[inds1]\n",
      "85/279: # df2.iloc[inds1]\n",
      "85/280: from sklearn.naive_bayes import BernoulliNB\n",
      "85/281:\n",
      "bnbs = []\n",
      "for x in df0['district'].unique():\n",
      "    print(x)\n",
      "    BNB = BernoulliNB()\n",
      "    inds = df0.index[df0['district'] == x].tolist()\n",
      "    BNB.fit(df2.iloc[inds].drop(['arrest'], axis=1), df2.iloc[inds]['arrest'])\n",
      "    print(BNB.score())\n",
      "    bnbs.append(BNB)\n",
      "85/282:\n",
      "bnbs = []\n",
      "for x in df0['district'].unique():\n",
      "    print(x)\n",
      "    BNB = BernoulliNB()\n",
      "    inds = df0.index[df0['district'] == x].tolist()\n",
      "    X = df2.iloc[inds].drop(['arrest'], axis=1)\n",
      "    y = df2.iloc[inds]['arrest']\n",
      "    BNB.fit(X,y)\n",
      "    print(BNB.score(X,y))\n",
      "    bnbs.append(BNB)\n",
      "85/283:\n",
      "bnbs = []\n",
      "scores = []\n",
      "for x in df0['district'].unique():\n",
      "    print(x)\n",
      "    BNB = BernoulliNB()\n",
      "    inds = df0.index[df0['district'] == x].tolist()\n",
      "    X = df2.iloc[inds].drop(['arrest'], axis=1)\n",
      "    y = df2.iloc[inds]['arrest']\n",
      "    BNB.fit(X,y)\n",
      "    s = BNB.score(X,y)\n",
      "    print(s)\n",
      "    scores.append(s)\n",
      "    bnbs.append(BNB)\n",
      "85/284:\n",
      "bnbs = []\n",
      "scores = []\n",
      "for x in df0['district'].unique():\n",
      "#     print(x)\n",
      "    BNB = BernoulliNB()\n",
      "    inds = df0.index[df0['district'] == x].tolist()\n",
      "    X = df2.iloc[inds].drop(['arrest'], axis=1)\n",
      "    y = df2.iloc[inds]['arrest']\n",
      "    BNB.fit(X,y)\n",
      "    s = BNB.score(X,y)\n",
      "#     print(s)\n",
      "    scores.append(s)\n",
      "    bnbs.append(BNB)\n",
      "85/285: plt.hist(scores)\n",
      "85/286:\n",
      "for x in df0['district'].unique():\n",
      "    inds = df0.index[df0['district'] == x].tolist()\n",
      "    print(x, df2.iloc[inds].mean())\n",
      "85/287:\n",
      "for x in df0['district'].unique():\n",
      "    inds = df0.index[df0['district'] == x].tolist()\n",
      "    print(x, df2.iloc[inds]['arrest'].mean())\n",
      "85/288: zip(df0['district'].unique(), scores)\n",
      "85/289: print(zip(df0['district'].unique(), scores))\n",
      "85/290: sorted(zip(df0['district'].unique(), scores))\n",
      "85/291: sorted(zip(df0['district'].unique(), scores), key=lambda x: x[1], reverse=True)\n",
      "85/292:\n",
      "arrests = []\n",
      "for x in df0['district'].unique():\n",
      "    inds = df0.index[df0['district'] == x].tolist()\n",
      "    arrests.append((x, df2.iloc[inds]['arrest'].mean()))\n",
      "85/293: sorted(arrests, key=lambda x: x[1], reverse=True)\n",
      "85/294: sorted(zip(df0['district'].unique(), scores, arrests), key=lambda x: x[1], reverse=True)\n",
      "85/295: sorted(zip(df0['district'].unique(), scores, arrests[1]), key=lambda x: x[1], reverse=True)\n",
      "85/296: sorted(zip(df0['district'].unique(), scores, arrests[:,1]), key=lambda x: x[1], reverse=True)\n",
      "85/297: sorted(zip(df0['district'].unique(), scores, arrests), key=lambda x: x[1], reverse=True)\n",
      "85/298: sorted(zip(df0['district'].unique(), scores, [x[1] for x in arrests]), key=lambda x: x[1], reverse=True)\n",
      "85/299:\n",
      "arrests = []\n",
      "for x in df0['district'].unique():\n",
      "    inds = df0.index[df0['district'] == x].tolist()\n",
      "    arrests.append((x, df2.iloc[inds]['arrest'].mean()))\n",
      "85/300: sorted(zip(df0['district'].unique(), scores, [x[1] for x in arrests]), key=lambda x: x[1], reverse=True)\n",
      "85/301:\n",
      "temp = pd.DataFrame(zip(df0['district'].unique(), scores, [x[1] for x in arrests]), columns=[\"time\", \"kind\", \"data\"])\n",
      "plt.figure(figsize=(10, 6))\n",
      "sns.barplot(x=\"time\", hue=\"kind\", y=\"data\", data=temp)\n",
      "plt.show()\n",
      "85/302:\n",
      "ax = plt.subplot(111)\n",
      "x = df0['district'].unique()\n",
      "ax.bar(x, scores, width=0.2, color='b', align='center')\n",
      "ax.bar(x, [x[1] for x in arrests], width=0.2, color='g', align='center')\n",
      "85/303:\n",
      "ax = plt.subplot(111)\n",
      "x = df0['district'].unique()\n",
      "ax.bar(x, scores, width=0.2, color='b', align='center')\n",
      "ax.bar(x, [x[1] for x in arrests], width=0.2, color='g', align='center')\n",
      "plt.show()\n",
      "85/304:\n",
      "ax = plt.subplot(111)\n",
      "x = df0['district'].unique()\n",
      "ax.bar(x-0.2, scores, width=0.2, color='b', align='center')\n",
      "ax.bar(x+0.2, [x[1] for x in arrests], width=0.2, color='g', align='center')\n",
      "plt.show()\n",
      "85/305:\n",
      "ax = plt.subplot(1111)\n",
      "x = df0['district'].unique()\n",
      "ax.bar(x-0.2, scores, width=0.2, color='b', align='center')\n",
      "ax.bar(x+0.2, [x[1] for x in arrests], width=0.2, color='g', align='center')\n",
      "plt.show()\n",
      "85/306:\n",
      "ax = plt.subplot(111)\n",
      "x = df0['district'].unique()\n",
      "ax.bar(x-0.2, scores, width=0.2, color='b', align='center')\n",
      "ax.bar(x+0.2, [x[1] for x in arrests], width=0.2, color='g', align='center')\n",
      "plt.show()\n",
      "85/307:\n",
      "ax = plt.subplot(111)\n",
      "x = df0['district'].unique()\n",
      "ax.bar(x-0.2, scores, width=0.2, color='r', align='center')\n",
      "ax.bar(x+0.2, [x[1] for x in arrests], width=0.2, color='g', align='center')\n",
      "plt.show()\n",
      "85/308:\n",
      "ax = plt.subplot(111)\n",
      "x = df0['district'].unique()\n",
      "ax.bar(x-0.2, scores, width=0.2, color='g', align='center')\n",
      "ax.bar(x+0.2, [x[1] for x in arrests], width=0.2, color='r', align='center')\n",
      "plt.show()\n",
      "85/309:\n",
      "ax = plt.subplot(111)\n",
      "x = df0['district'].unique()\n",
      "ax.bar(x-0.5, scores, width=0.2, color='g', align='center')\n",
      "ax.bar(x+0.5, [x[1] for x in arrests], width=0.2, color='r', align='center')\n",
      "plt.show()\n",
      "85/310:\n",
      "ax = plt.subplot(111)\n",
      "x = df0['district'].unique()\n",
      "ax.bar(x-0.5, scores, width=0.5, color='g', align='center')\n",
      "ax.bar(x+0.5, [x[1] for x in arrests], width=0.5, color='r', align='center')\n",
      "plt.show()\n",
      "85/311:\n",
      "ax = plt.subplot(555)\n",
      "x = df0['district'].unique()\n",
      "ax.bar(x-0.5, scores, width=0.5, color='g', align='center')\n",
      "ax.bar(x+0.5, [x[1] for x in arrests], width=0.5, color='r', align='center')\n",
      "plt.show()\n",
      "85/312:\n",
      "ax = plt.subplot(222)\n",
      "x = df0['district'].unique()\n",
      "ax.bar(x-0.5, scores, width=0.5, color='g', align='center')\n",
      "ax.bar(x+0.5, [x[1] for x in arrests], width=0.5, color='r', align='center')\n",
      "plt.show()\n",
      "85/313:\n",
      "ax = plt.subplot(111)\n",
      "x = df0['district'].unique()\n",
      "ax.bar(x-0.5, scores, width=0.5, color='g', align='center')\n",
      "ax.bar(x+0.5, [x[1] for x in arrests], width=0.5, color='r', align='center')\n",
      "plt.show()\n",
      "85/314:\n",
      "ax = plt.subplot(111)\n",
      "x = df0['district'].unique()\n",
      "ax.bar(x-0.5, scores, width=0.5, color='g', align='center')\n",
      "ax.bar(x+0.5, [x[1] for x in arrests], width=0.5, color='r', align='center')\n",
      "ax.set_xticks(x + 0.5 +  0.5/2)\n",
      "plt.show()\n",
      "85/315:\n",
      "ax = plt.subplot(111)\n",
      "x = df0['district'].unique()\n",
      "ax.bar(x-0.5, scores, width=0.5, color='g', align='center')\n",
      "ax.bar(x+0.5, [x[1] for x in arrests], width=0.5, color='r', align='center')\n",
      "ax.set_xticks(x + 1 +  0.5/2)\n",
      "plt.show()\n",
      "85/316:\n",
      "ax = plt.subplot(111)\n",
      "x = df0['district'].unique()\n",
      "ax.bar(x-0.2, scores, width=0.2, color='g', align='center')\n",
      "ax.bar(x+0.2, [x[1] for x in arrests], width=0.2, color='r', align='center')\n",
      "plt.show()\n",
      "85/317:\n",
      "bnbs2 = []\n",
      "scores2 = []\n",
      "for x in df0['community_area'].unique():\n",
      "    BNB = BernoulliNB()\n",
      "    inds = df0.index[df0['community_area'] == x].tolist()\n",
      "    X = df2.iloc[inds].drop(['arrest'], axis=1)\n",
      "    y = df2.iloc[inds]['arrest']\n",
      "    BNB.fit(X,y)\n",
      "    s = BNB.score(X,y)\n",
      "#     print(s)\n",
      "    scores2.append(s)\n",
      "    bnbs2.append(BNB)\n",
      "85/318: df0['community_area'].unique()\n",
      "85/319:\n",
      "bnbs2 = []\n",
      "scores2 = []\n",
      "for x in df0['community_area'].unique():\n",
      "    print(x)\n",
      "    BNB = BernoulliNB()\n",
      "    inds = df0.index[df0['community_area'] == x].tolist()\n",
      "    X = df2.iloc[inds].drop(['arrest'], axis=1)\n",
      "    y = df2.iloc[inds]['arrest']\n",
      "    BNB.fit(X,y)\n",
      "    s = BNB.score(X,y)\n",
      "#     print(s)\n",
      "    scores2.append(s)\n",
      "    bnbs2.append(BNB)\n",
      "85/320: df0['community_area'].replace(np.nan, 0)\n",
      "85/321: df0['community_area'] = df0['community_area'].replace(np.nan, 0)\n",
      "85/322: df0['community_area'].unique()\n",
      "85/323:\n",
      "bnbs2 = []\n",
      "scores2 = []\n",
      "for x in df0['community_area'].unique():\n",
      "    BNB = BernoulliNB()\n",
      "    inds = df0.index[df0['community_area'] == x].tolist()\n",
      "    X = df2.iloc[inds].drop(['arrest'], axis=1)\n",
      "    y = df2.iloc[inds]['arrest']\n",
      "    BNB.fit(X,y)\n",
      "    s = BNB.score(X,y)\n",
      "#     print(s)\n",
      "    scores2.append(s)\n",
      "    bnbs2.append(BNB)\n",
      "85/324:\n",
      "arrests2 = []\n",
      "for x in df0['community_area'].unique():\n",
      "    inds = df0.index[df0['community_area'] == x].tolist()\n",
      "    arrests2.append((x, df2.iloc[inds]['arrest'].mean()))\n",
      "85/325: sorted(zip(df0['community_area'].unique(), scores2, [x[1] for x in arrests2]), key=lambda x: x[1], reverse=True)\n",
      "85/326: plt.hist(scores2)\n",
      "85/327: ca = pd.read_csv(\"income_levels.csv\")\n",
      "85/328: ca.head()\n",
      "85/329: ca\n",
      "85/330:\n",
      "incomes = []\n",
      "for x in df0['community_area'].unique():\n",
      "    if x != 0:\n",
      "        incomes.append(ca.iloc[x-1][\"PER CAPITA INCOME\"])\n",
      "85/331:\n",
      "incomes = []\n",
      "for x in df0['community_area'].unique():\n",
      "    print(x)\n",
      "    if x != 0:\n",
      "        incomes.append(ca.iloc[x-1][\"PER CAPITA INCOME\"])\n",
      "85/332:\n",
      "incomes = []\n",
      "for x in df0['community_area'].unique():\n",
      "    if x != 0:\n",
      "        incomes.append(ca.iloc[int(x)-1][\"PER CAPITA INCOME\"])\n",
      "85/333: ca.iloc[0]\n",
      "85/334: ca.iloc[0]['PER CAPITA INCOME']\n",
      "85/335: ca.columns\n",
      "85/336:\n",
      "incomes = []\n",
      "for x in df0['community_area'].unique():\n",
      "    if x != 0:\n",
      "        incomes.append(ca.iloc[int(x)-1][\"PER CAPITA INCOME \"])\n",
      "85/337: sorted(zip(df0['community_area'].unique(), scores2, [x[1] for x in arrests2], incomes), key=lambda x: x[1], reverse=True)\n",
      "85/338:\n",
      "incomes = []\n",
      "levels = []\n",
      "for x in df0['community_area'].unique():\n",
      "    if x != 0:\n",
      "        incomes.append(ca.iloc[int(x)-1][\"PER CAPITA INCOME \"])\n",
      "        levels.append(ca.iloc[int(x)-1][\"level\"])\n",
      "85/339: sorted(zip(df0['community_area'].unique(), scores2, [x[1] for x in arrests2], incomes, levels), key=lambda x: x[1], reverse=True)\n",
      "85/340: sorted(zip(df0['community_area'].unique(), scores2, [x[1] for x in arrests2], incomes, levels), key=lambda x: x[2], reverse=True)\n",
      "85/341: sorted(zip(df0['community_area'].unique(), scores2, [x[1] for x in arrests2], incomes, levels), key=lambda x: x[1], reverse=True)\n",
      "85/342: sorted(zip(df0['community_area'].unique(), scores2, [x[1] for x in arrests2], incomes, levels), key=lambda x: x[1], reverse=True)\n",
      "85/343: df0['hour'] = df0['date'].dt.hour//3\n",
      "85/344:\n",
      "df1 = df0.copy()\n",
      "df1 = pd.get_dummies(df1, columns = ['primary_type'])\n",
      "85/345: df1[['hour'] + [x for x in df1.columns if 'primary' in x]]\n",
      "85/346: df1[['hour'] + [x for x in df1.columns if 'primary' in x]].corr()\n",
      "85/347: df1[['hour'] + [x for x in df1.columns if 'primary' in x]].value_counts()\n",
      "85/348: df1['hour'].value_counts()\n",
      "85/349: df0['primary_type'].value_counts()\n",
      "85/350:\n",
      "inds = df0.index[df0['hour'] == 1].tolist()\n",
      "df0.iloc[inds]['primary_type'].value_counts()\n",
      "85/351:\n",
      "inds = df0.index[df0['hour'] == 1].tolist()\n",
      "df0.iloc[inds]['primary_type'].value_counts().sum()\n",
      "85/352:\n",
      "inds = df0.index[df0['hour'] == 1].tolist()\n",
      "df0.iloc[inds]['primary_type'].value_counts()/df0.iloc[inds]['primary_type'].value_counts()sum()\n",
      "85/353:\n",
      "inds = df0.index[df0['hour'] == 1].tolist()\n",
      "df0.iloc[inds]['primary_type'].value_counts()/df0.iloc[inds]['primary_type'].value_counts().sum()\n",
      "85/354:\n",
      "inds = df0.index[df0['hour'] == 5].tolist()\n",
      "df0.iloc[inds]['primary_type'].value_counts()/df0.iloc[inds]['primary_type'].value_counts().sum()\n",
      "85/355:\n",
      "inds = df0.index[df0['hour'] == 8].tolist()\n",
      "df0.iloc[inds]['primary_type'].value_counts()/df0.iloc[inds]['primary_type'].value_counts().sum()\n",
      "85/356:\n",
      "inds = df0.index[df0['hour'] == 4].tolist()\n",
      "df0.iloc[inds]['primary_type'].value_counts()/df0.iloc[inds]['primary_type'].value_counts().sum()\n",
      "85/357:\n",
      "inds = df0.index[df0['hour'] == 3].tolist()\n",
      "df0.iloc[inds]['primary_type'].value_counts()/df0.iloc[inds]['primary_type'].value_counts().sum()\n",
      "85/358:\n",
      "inds = df0.index[df0['hour'] == 4].tolist()\n",
      "df0.iloc[inds]['primary_type'].value_counts()/df0.iloc[inds]['primary_type'].value_counts().sum()\n",
      "85/359: models = [MultinomialNB(), DecisionTreeClassifier(), RandomForestClassifier(), LinearSVC(), SVC(), LogisticRegression()]\n",
      "85/360:\n",
      "from sklearn.naive_bayes import MultinomialNB\n",
      "from sklearn.tree import DecisionTreeClassifier\n",
      "from sklearn.ensemble import RandomForestClassifier\n",
      "from sklearn.svm import LinearSVC, SVC\n",
      "from sklearn.linear_model import LogisticRegression\n",
      "85/361: models = [MultinomialNB(), DecisionTreeClassifier(), RandomForestClassifier(), LinearSVC(), SVC(), LogisticRegression()]\n",
      "85/362:\n",
      "def predictx(col, models):\n",
      "    for m in models:\n",
      "        m.fit(train.drop([col], axis=1), train[col])\n",
      "        print m.score(test.drop([col], axis=1), test[col])\n",
      "    return models\n",
      "85/363:\n",
      "def predictx(col, models):\n",
      "    for m in models:\n",
      "        m.fit(train.drop([col], axis=1), train[col])\n",
      "        print(m.score(test.drop([col], axis=1), test[col]))\n",
      "    return models\n",
      "85/364: models = predictx('arrest', models)\n",
      "99/1:\n",
      "import numpy as np\n",
      "import pandas as pd\n",
      "import seaborn as sns\n",
      "99/2:\n",
      "from matplotlib import pyplot as plt\n",
      "%matplotlib inline\n",
      "pd.options.display.max_columns = 100\n",
      "99/3: df0 = pd.read_pickle('since2018')\n",
      "99/4: df0['hour'] = df0['date'].dt.hour//3\n",
      "99/5: df0.nunique()\n",
      "99/6: df0['primary_type'].value_counts()\n",
      "99/7: ca = pd.read_csv(\"income_levels.csv\")\n",
      "99/8: ca\n",
      "99/9: df = pd.read_csv('use_df.csv')\n",
      "99/10: df.head()\n",
      "99/11:\n",
      "df1 = df0.copy()\n",
      "df1 = pd.get_dummies(df1, columns = ['primary_type'])\n",
      "99/12: df[['date']].dtypes\n",
      "99/13: df['date'] = pd.to_datetime(df.date)\n",
      "99/14: df.nunique(axis=0)\n",
      "99/15: df.shape\n",
      "99/16:\n",
      "typeVals = df.dtypes\n",
      "typeVals = typeVals.replace('bool', 'int64')\n",
      "df = df.astype(typeVals)\n",
      "99/17: [x for x in df['location_description'].unique() if not x == None and ('FEDERAL' in x or 'GOVERNMENT' in x)]\n",
      "99/18:\n",
      "def getcounts(word):\n",
      "    cols = [x for x in df['location_description'].unique() if not x == None and (word in x)]\n",
      "    tot = 0\n",
      "    for c in cols:\n",
      "        tot += df['location_description'].value_counts()[c]\n",
      "    return tot\n",
      "def getcounts2(words):\n",
      "    tot = 0\n",
      "    for w in words:\n",
      "        tot += getcounts(w)\n",
      "    return tot\n",
      "99/19: getcounts2(['BAR', 'POOL', 'SPORTS'])\n",
      "99/20:\n",
      "df2 = df.copy()\n",
      "df2['hour'] = df2['date'].dt.hour//3\n",
      "df2 = df2.drop(['date'], axis=1)\n",
      "categorical_columns = ['iucr', 'location_description', 'community_area', 'hour']\n",
      "df2 = pd.get_dummies(df2, columns = categorical_columns)\n",
      "99/21: df2.head()\n",
      "99/22:\n",
      "trainInds = np.random.uniform(0,1,df2.shape[0])<=0.8\n",
      "train=df2.iloc[trainInds]\n",
      "test=df2.iloc[~trainInds]\n",
      "99/23: train.shape, test.shape\n",
      "99/24:\n",
      "from sklearn.naive_bayes import MultinomialNB\n",
      "from sklearn.tree import DecisionTreeClassifier\n",
      "from sklearn.ensemble import RandomForestClassifier\n",
      "from sklearn.svm import LinearSVC, SVC\n",
      "from sklearn.linear_model import LogisticRegression\n",
      "99/25: df0['iucr'] < 580\n",
      "99/26: int(df0['iucr']) < 580\n",
      "99/27: df0['iucr'].astype(int) < 580\n",
      "99/28: df0['iucr'][:-1].astype(int) < 580\n",
      "99/29: df0['iucr'][:3].astype(int) < 580\n",
      "99/30:\n",
      "\n",
      "df0['iucr'].astype(int) < 580\n",
      "99/31: df0.index[!df0['hour'].isdigit()].tolist()\n",
      "99/32: df0.index[not df0['hour'].isdigit()].tolist()\n",
      "99/33:\n",
      "violent = []\n",
      "for i in range(df0):\n",
      "    num = int(df0['iucr'].iloc[i][:3])\n",
      "99/34: df0['iucr'].iloc[0]\n",
      "99/35: df0['iucr'].iloc[0][:3]\n",
      "99/36: int(df0['iucr'].iloc[0][:3])\n",
      "99/37: df2['violent'] = 0\n",
      "99/38: df2\n",
      "99/39: df2['violent'] = 1\n",
      "99/40: df2\n",
      "99/41:\n",
      "df2['violent'] = 1\n",
      "for i in range(len(df0)):\n",
      "    num = int(df0['iucr'].iloc[i][:3])\n",
      "    if num in range(580, 585) or num in range(810-938):\n",
      "        df2['violent'],iloc[i] = 0\n",
      "99/42: df2\n",
      "99/43: df2['violent']\n",
      "99/44:\n",
      "df2['violent'] = 1\n",
      "for i in range(len(df0)):\n",
      "    num = int(df0['iucr'].iloc[i][:3])\n",
      "    if num in range(580, 585) or num in range(810, 938):\n",
      "        df2['violent'].iloc[i] = 0\n",
      "99/45: df2['violent']\n",
      "99/46: np.min(df2['violent'])\n",
      "99/47:\n",
      "df2['violent'] = 1\n",
      "for i in range(len(df0)):\n",
      "    num = int(df0['iucr'].iloc[i][:3])\n",
      "    if num in range(58, 59) or num in range(81, 94):\n",
      "        df2['violent'].iloc[i] = 0\n",
      "99/48:\n",
      "df2['violent'] = 1\n",
      "for i in range(len(df0)):\n",
      "    num = int(df0['iucr'].iloc[i][:3])\n",
      "    if num in range(58, 59) or num in range(81, 94):\n",
      "        df2['violent'].loc[,i] = 0\n",
      "99/49:\n",
      "df2['violent'] = 1\n",
      "for i in range(len(df0)):\n",
      "    num = int(df0['iucr'].iloc[i][:3])\n",
      "    if num in range(58, 59) or num in range(81, 94):\n",
      "        df2['violent'][i] = 0\n",
      "99/50:\n",
      "violent = [1]*len(df0)\n",
      "for i in range(len(df0)):\n",
      "    num = int(df0['iucr'].iloc[i][:3])\n",
      "    if num in range(58, 59) or num in range(81, 94):\n",
      "        violent[i] = 0\n",
      "99/51: np.min(violent)\n",
      "99/52:\n",
      "violent = [1]*len(df0)\n",
      "for i in range(len(df0)):\n",
      "    num = int(df0['iucr'].iloc[i][:3])\n",
      "    if num in range(58, 59) or num in range(81, 94) or num in range(105, 106) or num in range(111, 138) or num in range(142, 180) or num in range(181, 285) or num in range(286, 301) or num in range(330, 331) or num in range(361, 399) or num in range(431, 514):\n",
      "        violent[i] = 0\n",
      "99/53: np.sum(violent)\n",
      "99/54: len(violent)\n",
      "99/55: np.sum(violent)/len(violent)\n",
      "99/56: df2 = df2.drop(['violent'], axis=1)\n",
      "99/57: df2['violent'] = violent\n",
      "99/58: df2\n",
      "99/59: models = [MultinomialNB(), DecisionTreeClassifier(n_estimators=100), RandomForestClassifier(), LinearSVC(gamma='auto'), LogisticRegression()]\n",
      "99/60: models = [MultinomialNB(), DecisionTreeClassifier(), RandomForestClassifier(n_estimators=100), LinearSVC(gamma='auto'), LogisticRegression()]\n",
      "99/61: models = [MultinomialNB(), DecisionTreeClassifier(), RandomForestClassifier(n_estimators=100), LinearSVC(), LogisticRegression()]\n",
      "99/62:\n",
      "def predictx(col, models):\n",
      "    for m in models:\n",
      "        m.fit(train.drop([col], axis=1), train[col])\n",
      "        print(m.score(test.drop([col], axis=1), test[col]))\n",
      "    return models\n",
      "99/63: models = predictx('arrest', models)\n",
      "99/64:\n",
      "trainInds = np.random.uniform(0,1,df2.shape[0])<=0.8\n",
      "train=df2.iloc[trainInds]\n",
      "test=df2.iloc[~trainInds]\n",
      "99/65: train.shape, test.shape\n",
      "99/66: models = predictx('arrest', models)\n",
      "99/67: models2 = predictx('violent', [MultinomialNB(), DecisionTreeClassifier(), RandomForestClassifier(n_estimators=100), LinearSVC(), LogisticRegression()])\n",
      "99/68:\n",
      "def predictx(col, models, train, test):\n",
      "    for m in models:\n",
      "        m.fit(train.drop([col], axis=1), train[col])\n",
      "        print(m.score(test.drop([col], axis=1), test[col]))\n",
      "    return models\n",
      "99/69:\n",
      "df3 = df2.drop([x for x in df2.columns if 'iucr' in x], axis=1)\n",
      "trainInds2 = np.random.uniform(0,1,df3.shape[0])<=0.8\n",
      "train2=df3.iloc[trainInds2]\n",
      "test2=df3.iloc[~trainInds2]\n",
      "99/70: models2 = predictx('violent', [MultinomialNB(), DecisionTreeClassifier(), RandomForestClassifier(n_estimators=100), LinearSVC(), LogisticRegression()], train2, test2)\n",
      "99/71: sortedDict = sorted(zip(df2.columns, models2[2].feature_importances_), key=operator.itemgetter(1), reverse=True)\n",
      "99/72: sorted(zip(df2.columns, models2[2].feature_importances_), key=operator.itemgetter(1), reverse=True)\n",
      "99/73: import operator\n",
      "99/74: sorted(zip(df2.columns, models2[2].feature_importances_), key=operator.itemgetter(1), reverse=True)\n",
      "99/75: sorted(zip(df2.columns, models2[2].feature_importances_), key=operator.itemgetter(1), reverse=True)[:10]\n",
      "99/76: sorted(zip(df2.columns, models[2].feature_importances_), key=operator.itemgetter(1), reverse=True)[:10]\n",
      "99/77: sorted(zip(df3.columns, models2[2].feature_importances_), key=operator.itemgetter(1), reverse=True)[:10]\n",
      "99/78: sorted(zip(df2.drop(['arrest'], axis=1).columns, models[2].feature_importances_), key=operator.itemgetter(1), reverse=True)[:10]\n",
      "99/79: sorted(zip(df2.drop(['arrest', 'violent'], axis=1).columns, models[2].feature_importances_), key=operator.itemgetter(1), reverse=True)[:10]\n",
      "99/80: sorted(zip(df3.drop(['violent'], axis=1).columns, models2[2].feature_importances_), key=operator.itemgetter(1), reverse=True)[:10]\n",
      "99/81: len(models2[2].feature_importances_))\n",
      "99/82: models2[2].feature_importances_.shape\n",
      "99/83: df3.shape\n",
      "99/84: models[2].feature_importances_.shape\n",
      "99/85: df2.shape\n",
      "99/86: sorted(zip(df2.drop(['arrest'], axis=1).columns, models[2].feature_importances_), key=operator.itemgetter(1), reverse=True)[:10]\n",
      "99/87: df0['community_area']\n",
      "99/88: df0['community_area'][:5]\n",
      "99/89:\n",
      "df4 = df2.copy()\n",
      "incomes = [ca.iloc[x-1]['PER CAPITA INCOME '] for x in df0['community_area']]\n",
      "incomes\n",
      "99/90:\n",
      "df4 = df2.copy()\n",
      "incomes = [ca.iloc[int(x)-1]['PER CAPITA INCOME '] for x in df0['community_area']]\n",
      "incomes\n",
      "99/91: df0['community_area'] = df0['community_area'].replace(np.nan, 0)\n",
      "99/92:\n",
      "incomes = [ca.iloc[int(x)-1]['PER CAPITA INCOME '] for x in df0['community_area']]\n",
      "incomes\n",
      "99/93:\n",
      "df4['incomes'] = incomes\n",
      "df4.head()\n",
      "99/94:\n",
      "df4['incomes'] = incomes\n",
      "df4[:10]\n",
      "99/95:\n",
      "trainInds3 = np.random.uniform(0,1,df4.shape[0])<=0.8\n",
      "train3=df3.iloc[trainInds3]\n",
      "test3=df3.iloc[~trainInds3]\n",
      "99/96: models3 = predictx('arrest', [MultinomialNB(), DecisionTreeClassifier(), RandomForestClassifier(n_estimators=100), LinearSVC(), LogisticRegression()], train3, test3)\n",
      "99/97: train3.shape\n",
      "99/98:\n",
      "trainInds3 = np.random.uniform(0,1,df4.shape[0])<=0.8\n",
      "train3=df4.iloc[trainInds3]\n",
      "test3=df4.iloc[~trainInds3]\n",
      "99/99: models3 = predictx('arrest', [MultinomialNB(), DecisionTreeClassifier(), RandomForestClassifier(n_estimators=100), LinearSVC(), LogisticRegression()], train3, test3)\n",
      "99/100:\n",
      "df5 = df4.drop([x for x in df4.columns if 'iucr' in x], axis=1)\n",
      "trainInds4 = np.random.uniform(0,1,df5.shape[0])<=0.8\n",
      "train4=df5.iloc[trainInds4]\n",
      "test4=df5.iloc[~trainInds4]\n",
      "99/101: models4 = predictx('violent', [MultinomialNB(), DecisionTreeClassifier(), RandomForestClassifier(n_estimators=100), LinearSVC(), LogisticRegression()], train4, test4)\n",
      "99/102: sorted(zip(df4.drop(['arrest'], axis=1).columns, models3[2].feature_importances_), key=operator.itemgetter(1), reverse=True)[:10]\n",
      "99/103: sorted(zip(df5.drop(['violent'], axis=1).columns, models4[2].feature_importances_), key=operator.itemgetter(1), reverse=True)[:10]\n",
      "99/104: df2.head()\n",
      "99/105: df2\n",
      "99/106: df0['date']\n",
      "99/107: df0\n",
      "99/108: df0['date'].dt.week\n",
      "99/109: np.max(df0['date'].dt.week)\n",
      "99/110: np.min(df0['date'].dt.week)\n",
      "99/111: df0['date'].dt.week\n",
      "99/112: plt.hist(df0['date'].dt.week)\n",
      "99/113: plt.hist(df0['date'].dt.week,bins=52)\n",
      "99/114: np.min(df0['date'].dt.year)\n",
      "99/115: np.max(df0['date'].dt.year)\n",
      "99/116: plt.hist(df0['date'].dt.year)\n",
      "99/117: plt.hist(df0['date'].dt.week + (df0['date'].dt.year-2018)*52,bins=52)\n",
      "99/118: plt.hist(df0['date'].dt.day)\n",
      "99/119: plt.hist(df0['date'].dt.week + (df0['date'].dt.year-2018)*52)\n",
      "99/120: plt.hist(df0['date'].dt.dayofyear/3)\n",
      "99/121: plt.hist(df0['date'].dt.dayofyear//3 + (df0['date'].dt.year-2018)*121)\n",
      "99/122: df0['date'].dt.dayofyear//3\n",
      "99/123: np.max(df0['date'].dt.dayofyear//3)\n",
      "99/124: np.max(df0['date'].dt.dayofyear)\n",
      "99/125: plt.hist(df0['date'].dt.dayofyear//3 + (df0['date'].dt.year-2018)*73)\n",
      "99/126: plt.hist(df0['date'].dt.dayofyear//5 + (df0['date'].dt.year-2018)*73)\n",
      "99/127: df2.head()\n",
      "99/128: plt.hist(df0['date'].dt.weekday)\n",
      "99/129: np.max(df0['date'].dt.dayofyear//5 + (df0['date'].dt.year-2018)*73)\n",
      "99/130: plt.hist(df0['date'].dt.dayofyear//5 + (df0['date'].dt.year-2018)*73)\n",
      "99/131: df2['week'] = df0['date'].dt.dayofyear//5 + (df0['date'].dt.year-2018)*73\n",
      "99/132: df0['week'] = df0['date'].dt.dayofyear//5 + (df0['date'].dt.year-2018)*73\n",
      "99/133: df2['week'] = df0['week']\n",
      "99/134: df2['week'] = df0['week'].values\n",
      "99/135: df2['week']\n",
      "99/136: df0['week']\n",
      "99/137: df2['week']\n",
      "99/138: df0['week'].value_counts()\n",
      "99/139: list(df0['week'].value_counts()()\n",
      "99/140: df0['week'].value_counts()\n",
      "99/141: df0['week'].value_counts()[0]\n",
      "99/142: df0['week'].value_counts().shape\n",
      "99/143: df0['week'].value_counts()\n",
      "99/144:\n",
      "# df0['week'].value_counts()\n",
      "df0['week'].values\n",
      "99/145:\n",
      "# df0['week'].value_counts()\n",
      "df0['week'].unique()\n",
      "99/146: # df0['week'].value_counts()\n",
      "99/147: df0['week'].value_counts()\n",
      "99/148: df0['week'].value_counts()[0]\n",
      "99/149: df0['week'].value_counts()[30]\n",
      "99/150: df0['week'].value_counts()\n",
      "99/151: df0['week'].value_counts()[0]\n",
      "99/152: plt.plot([x for x in range(97)], df0['week'].value_counts())\n",
      "99/153: plt.hist(df0['date'].dt.dayofyear//5 + (df0['date'].dt.year-2018)*73, bins=97)\n",
      "99/154: plt.plot([x for x in range(97)], [df0['week'].value_counts()[x] for x in range(97)])\n",
      "99/155: # df0['week'].value_counts()\n",
      "99/156: from sklearn.linear_model import LinearRegression\n",
      "99/157: weekcounts = [df0['week'].value_counts()[x] for x in range(97)]\n",
      "99/158: plt.plot([x for x in range(97)], weekcounts)\n",
      "99/159:\n",
      "lr = LinearRegresion().fit(weekcounts[:96], weekcounts[1:])\n",
      "lr.score(weekcounts[:96], weekcounts[1:])\n",
      "99/160:\n",
      "lr = LinearRegression().fit(weekcounts[:96], weekcounts[1:])\n",
      "lr.score(weekcounts[:96], weekcounts[1:])\n",
      "99/161:\n",
      "lr = LinearRegression().fit(weekcounts[:96].reshape(1, -1), weekcounts[1:])\n",
      "lr.score(weekcounts[:96].reshape(1, -1), weekcounts[1:])\n",
      "99/162: weekcounts = np.array([df0['week'].value_counts()[x] for x in range(97)])\n",
      "99/163:\n",
      "lr = LinearRegression().fit(weekcounts[:96].reshape(1, -1), weekcounts[1:])\n",
      "lr.score(weekcounts[:96].reshape(1, -1), weekcounts[1:])\n",
      "99/164: weekcounts[1:].shape\n",
      "99/165: weekcounts[:96].reshape(1, -1)\n",
      "99/166: weekcounts[:96].reshape(1, -1).shape\n",
      "99/167:\n",
      "lr = LinearRegression().fit(weekcounts[:96].reshape(-1, 1), weekcounts[1:])\n",
      "lr.score(weekcounts[:96].reshape(1, -1), weekcounts[1:])\n",
      "99/168:\n",
      "lr = LinearRegression().fit(weekcounts[:96].reshape(-1, 1), weekcounts[1:])\n",
      "lr.score(weekcounts[:96].reshape(-1, -1), weekcounts[1:])\n",
      "99/169:\n",
      "lr = LinearRegression().fit(weekcounts[:96].reshape(-1, 1), weekcounts[1:])\n",
      "lr.score(weekcounts[:96].reshape(-1, 1), weekcounts[1:])\n",
      "99/170: testlen = 20\n",
      "99/171:\n",
      "lr = LinearRegression().fit(weekcounts[:96-testlen].reshape(-1, 1), weekcounts[1:97-testlen])\n",
      "lr.score(weekcounts[:96-testlen].reshape(-1, 1), weekcounts[1:97-testlen])\n",
      "99/172:\n",
      "lr = LinearRegression().fit(weekcounts[:96-testlen].reshape(-1, 1), weekcounts[1:97-testlen])\n",
      "lr.score(weekcounts[testlen:96].reshape(-1, 1), weekcounts[1+testlen:97])\n",
      "99/173:\n",
      "lr = LinearRegression().fit(weekcounts[:96-testlen].reshape(-1, 1), weekcounts[1:97-testlen])\n",
      "lr.score(weekcounts[97-testlen:96].reshape(-1, 1), weekcounts[97-1+testlen:97])\n",
      "99/174:\n",
      "lr = LinearRegression().fit(weekcounts[:96-testlen].reshape(-1, 1), weekcounts[1:97-testlen])\n",
      "lr.score(weekcounts[96-testlen:96].reshape(-1, 1), weekcounts[96-1+testlen:97])\n",
      "99/175: weekcounts[96-testlen:96]\n",
      "99/176: weekcounts[96-testlen:96].shape\n",
      "99/177: weekcounts[96-testlen:96].reshape(-1,1).shape\n",
      "99/178: weekcounts[96-1+testlen:97].shape\n",
      "99/179: weekcounts[96+1-testlen:97].shape\n",
      "99/180:\n",
      "lr = LinearRegression().fit(weekcounts[:96-testlen].reshape(-1, 1), weekcounts[1:97-testlen])\n",
      "lr.score(weekcounts[96-testlen:96].reshape(-1, 1), weekcounts[96-testlen+1:97])\n",
      "99/181: testlen = 0\n",
      "99/182:\n",
      "lr = LinearRegression().fit(weekcounts[:96-testlen].reshape(-1, 1), weekcounts[1:97-testlen])\n",
      "lr.score(weekcounts[96-testlen:96].reshape(-1, 1), weekcounts[96-testlen+1:97])\n",
      "99/183:\n",
      "lr = LinearRegression().fit(weekcounts[:96-testlen].reshape(-1, 1), weekcounts[1:97-testlen])\n",
      "lr.score(weekcounts[:96].reshape(-1, 1), weekcounts[1:97])\n",
      "# lr.score(weekcounts[96-testlen:96].reshape(-1, 1), weekcounts[96-testlen+1:97])\n",
      "99/184: ca.head()\n",
      "99/185: df17 = pd.read_csv(\"since2017.csv\")\n",
      "99/186: df17.head()\n",
      "99/187: df17.shape()\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "99/188: df17.shape\n",
      "99/189: np.min(df17['date'].dt.year)\n",
      "99/190: df17['date'] = pd.to_datetime(df17.date)\n",
      "99/191: np.min(df17['date'].dt.year)\n",
      "99/192: np.max(df17['date'].dt.year)\n",
      "99/193: plt.hist(df0['date'].dt.dayofyear//5 + (df0['date'].dt.year-2017)*73)\n",
      "99/194: min(df17.date)\n",
      "99/195: min(df17.date), max(df17.date)\n",
      "99/196: plt.hist(df17['date'].dt.dayofyear//5 + (df17['date'].dt.year-2017)*73)\n",
      "99/197: plt.hist(df17['date'].dt.dayofyear//5 + (df17['date'].dt.year-2017)*73, bins=170)\n",
      "99/198:\n",
      "plt.hist(df17['date'].dt.dayofyear//5 + (df17['date'].dt.year-2017)*73, bins=170)\n",
      "plt.show()\n",
      "99/199:\n",
      "plt.hist(df0['date'].dt.dayofyear//5 + (df0['date'].dt.year-2018)*73, bins=97)\n",
      "plt.show()\n",
      "99/200: weekcounts[:96-testlen].reshape(-1, 1).shape\n",
      "99/201: df17['week'] = df17['date'].dt.dayofyear//5 + (df17['date'].dt.year-2018)*73\n",
      "99/202: max(df17['week'])\n",
      "99/203: df17['week'] = df17['date'].dt.dayofyear//5 + (df17['date'].dt.year-2017)*73\n",
      "99/204: max(df17['week'])\n",
      "99/205: weekcounts = np.array([df17['week'].value_counts()[x] for x in range(170)])\n",
      "99/206: df16 = pd.read_pickle('since2016')\n",
      "99/207: plt.hist(df16['date'].dt.dayofyear//5 + (df16['date'].dt.year-2016)*73)\n",
      "99/208: plt.hist(df16['date'].dt.dayofyear//5 + (df16['date'].dt.year-2016)*73, bins=243)\n",
      "99/209: plt.hist(df16['date'].dt.dayofyear//5 + (df16['date'].dt.year-2016)*73, bins=244)\n",
      "99/210:\n",
      "plt.hist(df16['date'].dt.dayofyear//5 + (df16['date'].dt.year-2016)*73, bins=244)\n",
      "plt.show()\n",
      "99/211: max(df16['week'])\n",
      "99/212: df16['week'] = df16['date'].dt.dayofyear//5 + (df16['date'].dt.year-2016)*73\n",
      "99/213: max(df16['week'])\n",
      "99/214: weekcounts = np.array([df16['week'].value_counts()[x] for x in range(244)])\n",
      "99/215:\n",
      "lr = LinearRegression().fit(weekcounts[:243].reshape(-1, 1), weekcounts[1:244])\n",
      "lr.score(weekcounts[:243].reshape(-1, 1), weekcounts[1:244])\n",
      "99/216: [weekcounts[1:243], weekcounts[:242]]\n",
      "99/217: np.array([weekcounts[1:243], weekcounts[:242]]).shape\n",
      "99/218: np.array([weekcounts[1:243], weekcounts[:242]]).transpose().shape\n",
      "99/219: np.array([weekcounts[1:243], weekcounts[:242]]).transpose()\n",
      "99/220: np.array([weekcounts[1:243], weekcounts[:242]])\n",
      "99/221: np.array([weekcounts[1:243], weekcounts[:242]]).transpose()\n",
      "99/222:\n",
      "lr = LinearRegression().fit(np.array([weekcounts[1:243], weekcounts[:242]]).transpose(), weekcounts[2:244])\n",
      "lr.score(np.array([weekcounts[1:243], weekcounts[:242]]).transpose(), weekcounts[2:244])\n",
      "99/223: np.array([weekcounts[1:243-74], weekcounts[:242-74], weekcounts[74:]])\n",
      "99/224: np.array([weekcounts[1:243-74], weekcounts[:242-74], weekcounts[74:]]).shape\n",
      "99/225: [weekcounts[1:243-74]].shape\n",
      "99/226: len(weekcounts[1:243-74])\n",
      "99/227: len(weekcounts[74:])\n",
      "99/228: np.array([weekcounts[1:244-74], weekcounts[:244-74], weekcounts[74:]]).shape\n",
      "99/229: len(weekcounts[1:244-74])\n",
      "99/230: np.array([weekcounts[1:244-74], weekcounts[:244-74], weekcounts[74:243]]).shape\n",
      "99/231: len(weekcounts[74:243])\n",
      "99/232: len(weekcounts[1:243-73])\n",
      "99/233:\n",
      "lr = LinearRegression().fit(np.array([weekcounts[:243-73], weekcounts[72:242], weekcounts[73:243]]).transpose(), weekcounts[74:244])\n",
      "lr.score(np.array([weekcounts[:243-73], weekcounts[72:242], weekcounts[73:243]]).transpose(), weekcounts[74:244])\n",
      "99/234: np.array([weekcounts[:243-73], weekcounts[72:242], weekcounts[73:243]])\n",
      "99/235: plt.plot([x for x in range(244)], weekcounts)\n",
      "99/236: weekcounts = np.array([df16['week'].value_counts()[x] for x in range(243)])\n",
      "99/237: plt.plot([x for x in range(243)], weekcounts)\n",
      "99/238:\n",
      "lr = LinearRegression().fit(np.array([weekcounts[:242-73], weekcounts[72:241], weekcounts[73:242]]).transpose(), weekcounts[74:243])\n",
      "lr.score(np.array([weekcounts[:242-73], weekcounts[72:241], weekcounts[73:242]]).transpose(), weekcounts[74:243])\n",
      "99/239:\n",
      "lr = LinearRegression().fit(weekcounts[:242].reshape(-1, 1), weekcounts[1:243])\n",
      "lr.score(weekcounts[:242].reshape(-1, 1), weekcounts[1:243])\n",
      "99/240:\n",
      "lr = LinearRegression().fit(np.array([weekcounts[1:242], weekcounts[:241]]).transpose(), weekcounts[2:243])\n",
      "lr.score(np.array([weekcounts[1:242], weekcounts[:241]]).transpose(), weekcounts[2:243])\n",
      "99/241:\n",
      "lr = LinearRegression().fit(np.array([weekcounts[:242-73-t], weekcounts[72:241-t], weekcounts[73:242-t]]).transpose(), weekcounts[74:243-t])\n",
      "lr.score(np.array([weekcounts[243-t:242-73-t], weekcounts[243-t+72:241-t], weekcounts[243-t+73:242-t]]).transpose(), weekcounts[243-t:243])\n",
      "lr.score()\n",
      "99/242: t=40\n",
      "99/243:\n",
      "lr = LinearRegression().fit(np.array([weekcounts[:242-73-t], weekcounts[72:241-t], weekcounts[73:242-t]]).transpose(), weekcounts[74:243-t])\n",
      "lr.score(np.array([weekcounts[243-t:242-73-t], weekcounts[243-t+72:241-t], weekcounts[243-t+73:242-t]]).transpose(), weekcounts[243-t:243])\n",
      "lr.score()\n",
      "99/244: np.array([weekcounts[243-t:242-73-t], weekcounts[243-t+72:241-t], weekcounts[243-t+73:242-t]]).shape\n",
      "99/245: np.array([weekcounts[243-t:242-73], weekcounts[243-t+72:241], weekcounts[243-t+73:242]]).shape\n",
      "99/246:\n",
      "lr = LinearRegression().fit(np.array([weekcounts[:242-73-t], weekcounts[72:241-t], weekcounts[73:242-t]]).transpose(), weekcounts[74:243-t])\n",
      "lr.score(np.array([weekcounts[242-73-t:242], weekcounts[241-t:241], weekcounts[242-t:242]]).transpose(), weekcounts[243-t:243])\n",
      "lr.score()\n",
      "99/247: weekcounts[242-73-t:242].shape\n",
      "99/248: weekcounts[242-73-t:242].shape, weekcounts[241-t:241].shape\n",
      "99/249: weekcounts[242-73-t:242].shape, weekcounts[241-t:241].shape, weekcounts[242-t:242]\n",
      "99/250: weekcounts[242-73-t:242].shape, weekcounts[241-t:241].shape, weekcounts[242-t:242].shape\n",
      "99/251:\n",
      "lr = LinearRegression().fit(np.array([weekcounts[:242-73-t], weekcounts[72:241-t], weekcounts[73:242-t]]).transpose(), weekcounts[74:243-t])\n",
      "lr.score(np.array([weekcounts[242-73-t:242-73], weekcounts[241-t:241], weekcounts[242-t:242]]).transpose(), weekcounts[243-t:243])\n",
      "lr.score()\n",
      "99/252: weekcounts[242-73-t:242-73].shape, weekcounts[241-t:241].shape, weekcounts[242-t:242].shape\n",
      "99/253:\n",
      "lr = LinearRegression().fit(np.array([weekcounts[:242-73-t], weekcounts[72:241-t], weekcounts[73:242-t]]).transpose(), weekcounts[74:243-t])\n",
      "lr.score(np.array([weekcounts[242-73-t:242-73], weekcounts[241-t:241], weekcounts[242-t:242]]).transpose(), weekcounts[243-t:243])\n",
      "99/254: t=60\n",
      "99/255: weekcounts[242-73-t:242-73].shape, weekcounts[241-t:241].shape, weekcounts[242-t:242].shape\n",
      "99/256:\n",
      "lr = LinearRegression().fit(np.array([weekcounts[:242-73-t], weekcounts[72:241-t], weekcounts[73:242-t]]).transpose(), weekcounts[74:243-t])\n",
      "lr.score(np.array([weekcounts[242-73-t:242-73], weekcounts[241-t:241], weekcounts[242-t:242]]).transpose(), weekcounts[243-t:243])\n",
      "99/257: t=50\n",
      "99/258: weekcounts[242-73-t:242-73].shape, weekcounts[241-t:241].shape, weekcounts[242-t:242].shape\n",
      "99/259:\n",
      "lr = LinearRegression().fit(np.array([weekcounts[:242-73-t], weekcounts[72:241-t], weekcounts[73:242-t]]).transpose(), weekcounts[74:243-t])\n",
      "lr.score(np.array([weekcounts[242-73-t:242-73], weekcounts[241-t:241], weekcounts[242-t:242]]).transpose(), weekcounts[243-t:243])\n",
      "99/260: t=60\n",
      "99/261: weekcounts[242-73-t:242-73].shape, weekcounts[241-t:241].shape, weekcounts[242-t:242].shape\n",
      "99/262:\n",
      "lr = LinearRegression().fit(np.array([weekcounts[:242-73-t], weekcounts[72:241-t], weekcounts[73:242-t]]).transpose(), weekcounts[74:243-t])\n",
      "lr.score(np.array([weekcounts[242-73-t:242-73], weekcounts[241-t:241], weekcounts[242-t:242]]).transpose(), weekcounts[243-t:243])\n",
      "99/263:\n",
      "lr = LinearRegression().fit(np.array([weekcounts[:242-73-t], weekcounts[:242-72-t], weekcounts[72:241-t], weekcounts[73:242-t]]).transpose(), weekcounts[74:243-t])\n",
      "lr.score(np.array([weekcounts[242-73-t:242-73], [weekcounts[242-72-t:242-72], weekcounts[241-t:241], weekcounts[242-t:242]]).transpose(), weekcounts[243-t:243])\n",
      "99/264:\n",
      "lr = LinearRegression().fit(np.array([weekcounts[:242-73-t], weekcounts[:242-72-t], weekcounts[72:241-t], weekcounts[73:242-t]]).transpose(), weekcounts[74:243-t])\n",
      "lr.score(np.array([weekcounts[242-73-t:242-73], weekcounts[242-72-t:242-72], weekcounts[241-t:241], weekcounts[242-t:242]]).transpose(), weekcounts[243-t:243])\n",
      "99/265: np.array([weekcounts[:242-73-t], weekcounts[:242-72-t], weekcounts[72:241-t], weekcounts[73:242-t]]).transpose().shape\n",
      "99/266: np.array([weekcounts[:242-73-t], weekcounts[1:242-72-t], weekcounts[72:241-t], weekcounts[73:242-t]]).transpose().shape\n",
      "99/267:\n",
      "lr = LinearRegression().fit(np.array([weekcounts[:242-73-t], weekcounts[1:242-72-t], weekcounts[72:241-t], weekcounts[73:242-t]]).transpose(), weekcounts[74:243-t])\n",
      "lr.score(np.array([weekcounts[242-73-t:242-73], weekcounts[242-72-t:242-72], weekcounts[241-t:241], weekcounts[242-t:242]]).transpose(), weekcounts[243-t:243])\n",
      "99/268: t=73\n",
      "99/269: weekcounts[242-73-t:242-73].shape, weekcounts[241-t:241].shape, weekcounts[242-t:242].shape\n",
      "99/270:\n",
      "lr = LinearRegression().fit(np.array([weekcounts[:242-73-t], weekcounts[72:241-t], weekcounts[73:242-t]]).transpose(), weekcounts[74:243-t])\n",
      "lr.score(np.array([weekcounts[242-73-t:242-73], weekcounts[241-t:241], weekcounts[242-t:242]]).transpose(), weekcounts[243-t:243])\n",
      "99/271: np.array([weekcounts[:242-73-t], weekcounts[1:242-72-t], weekcounts[72:241-t], weekcounts[73:242-t]]).transpose().shape\n",
      "99/272:\n",
      "lr = LinearRegression().fit(np.array([weekcounts[:242-73-t], weekcounts[1:242-72-t], weekcounts[72:241-t], weekcounts[73:242-t]]).transpose(), weekcounts[74:243-t])\n",
      "lr.score(np.array([weekcounts[242-73-t:242-73], weekcounts[242-72-t:242-72], weekcounts[241-t:241], weekcounts[242-t:242]]).transpose(), weekcounts[243-t:243])\n",
      "99/273: t=70\n",
      "99/274: weekcounts[242-73-t:242-73].shape, weekcounts[241-t:241].shape, weekcounts[242-t:242].shape\n",
      "99/275:\n",
      "lr = LinearRegression().fit(np.array([weekcounts[:242-73-t], weekcounts[72:241-t], weekcounts[73:242-t]]).transpose(), weekcounts[74:243-t])\n",
      "lr.score(np.array([weekcounts[242-73-t:242-73], weekcounts[241-t:241], weekcounts[242-t:242]]).transpose(), weekcounts[243-t:243])\n",
      "99/276: np.array([weekcounts[:242-73-t], weekcounts[1:242-72-t], weekcounts[72:241-t], weekcounts[73:242-t]]).transpose().shape\n",
      "99/277:\n",
      "lr = LinearRegression().fit(np.array([weekcounts[:242-73-t], weekcounts[1:242-72-t], weekcounts[72:241-t], weekcounts[73:242-t]]).transpose(), weekcounts[74:243-t])\n",
      "lr.score(np.array([weekcounts[242-73-t:242-73], weekcounts[242-72-t:242-72], weekcounts[241-t:241], weekcounts[242-t:242]]).transpose(), weekcounts[243-t:243])\n",
      "99/278: t=60\n",
      "99/279: weekcounts[242-73-t:242-73].shape, weekcounts[241-t:241].shape, weekcounts[242-t:242].shape\n",
      "99/280:\n",
      "lr = LinearRegression().fit(np.array([weekcounts[:242-73-t], weekcounts[72:241-t], weekcounts[73:242-t]]).transpose(), weekcounts[74:243-t])\n",
      "lr.score(np.array([weekcounts[242-73-t:242-73], weekcounts[241-t:241], weekcounts[242-t:242]]).transpose(), weekcounts[243-t:243])\n",
      "99/281: np.array([weekcounts[:242-73-t], weekcounts[1:242-72-t], weekcounts[72:241-t], weekcounts[73:242-t]]).transpose().shape\n",
      "99/282:\n",
      "lr = LinearRegression().fit(np.array([weekcounts[:242-73-t], weekcounts[1:242-72-t], weekcounts[72:241-t], weekcounts[73:242-t]]).transpose(), weekcounts[74:243-t])\n",
      "lr.score(np.array([weekcounts[242-73-t:242-73], weekcounts[242-72-t:242-72], weekcounts[241-t:241], weekcounts[242-t:242]]).transpose(), weekcounts[243-t:243])\n",
      "99/283: t=50\n",
      "99/284: weekcounts[242-73-t:242-73].shape, weekcounts[241-t:241].shape, weekcounts[242-t:242].shape\n",
      "99/285:\n",
      "lr = LinearRegression().fit(np.array([weekcounts[:242-73-t], weekcounts[72:241-t], weekcounts[73:242-t]]).transpose(), weekcounts[74:243-t])\n",
      "lr.score(np.array([weekcounts[242-73-t:242-73], weekcounts[241-t:241], weekcounts[242-t:242]]).transpose(), weekcounts[243-t:243])\n",
      "99/286: np.array([weekcounts[:242-73-t], weekcounts[1:242-72-t], weekcounts[72:241-t], weekcounts[73:242-t]]).transpose().shape\n",
      "99/287:\n",
      "lr = LinearRegression().fit(np.array([weekcounts[:242-73-t], weekcounts[1:242-72-t], weekcounts[72:241-t], weekcounts[73:242-t]]).transpose(), weekcounts[74:243-t])\n",
      "lr.score(np.array([weekcounts[242-73-t:242-73], weekcounts[242-72-t:242-72], weekcounts[241-t:241], weekcounts[242-t:242]]).transpose(), weekcounts[243-t:243])\n",
      "99/288: t=70\n",
      "99/289: weekcounts[242-73-t:242-73].shape, weekcounts[241-t:241].shape, weekcounts[242-t:242].shape\n",
      "99/290:\n",
      "lr = LinearRegression().fit(np.array([weekcounts[:242-73-t], weekcounts[72:241-t], weekcounts[73:242-t]]).transpose(), weekcounts[74:243-t])\n",
      "lr.score(np.array([weekcounts[242-73-t:242-73], weekcounts[241-t:241], weekcounts[242-t:242]]).transpose(), weekcounts[243-t:243])\n",
      "99/291: np.array([weekcounts[:242-73-t], weekcounts[1:242-72-t], weekcounts[72:241-t], weekcounts[73:242-t]]).transpose().shape\n",
      "99/292:\n",
      "lr = LinearRegression().fit(np.array([weekcounts[:242-73-t], weekcounts[1:242-72-t], weekcounts[72:241-t], weekcounts[73:242-t]]).transpose(), weekcounts[74:243-t])\n",
      "lr.score(np.array([weekcounts[242-73-t:242-73], weekcounts[242-72-t:242-72], weekcounts[241-t:241], weekcounts[242-t:242]]).transpose(), weekcounts[243-t:243])\n",
      "99/293: t=60\n",
      "99/294: weekcounts[242-73-t:242-73].shape, weekcounts[241-t:241].shape, weekcounts[242-t:242].shape\n",
      "99/295:\n",
      "lr = LinearRegression().fit(np.array([weekcounts[:242-73-t], weekcounts[72:241-t], weekcounts[73:242-t]]).transpose(), weekcounts[74:243-t])\n",
      "lr.score(np.array([weekcounts[242-73-t:242-73], weekcounts[241-t:241], weekcounts[242-t:242]]).transpose(), weekcounts[243-t:243])\n",
      "99/296: np.array([weekcounts[:242-73-t], weekcounts[1:242-72-t], weekcounts[72:241-t], weekcounts[73:242-t]]).transpose().shape\n",
      "99/297:\n",
      "lr = LinearRegression().fit(np.array([weekcounts[:242-73-t], weekcounts[1:242-72-t], weekcounts[72:241-t], weekcounts[73:242-t]]).transpose(), weekcounts[74:243-t])\n",
      "lr.score(np.array([weekcounts[242-73-t:242-73], weekcounts[242-72-t:242-72], weekcounts[241-t:241], weekcounts[242-t:242]]).transpose(), weekcounts[243-t:243])\n",
      "99/298: from sklearn.neighbors import KNeighborsClassifier\n",
      "99/299: knn1 = predictx('arrest', [KNeighborsClassifier()], train, test)\n",
      "100/1: df16 = pd.read_pickle('since2016')\n",
      "100/2:\n",
      "import numpy as np\n",
      "import pandas as pd\n",
      "import seaborn as sns\n",
      "100/3:\n",
      "from matplotlib import pyplot as plt\n",
      "%matplotlib inline\n",
      "pd.options.display.max_columns = 100\n",
      "100/4:\n",
      "import numpy as np\n",
      "import pandas as pd\n",
      "import seaborn as sns\n",
      "100/5:\n",
      "from matplotlib import pyplot as plt\n",
      "%matplotlib inline\n",
      "pd.options.display.max_columns = 100\n",
      "100/6: df16 = pd.read_pickle('since2016')\n",
      "100/7: df16['week'] = df16['date'].dt.dayofyear//5 + (df16['date'].dt.year-2016)*73\n",
      "100/8: max(df16['week'])\n",
      "100/9:\n",
      "plt.hist(df16['date'].dt.dayofyear//5 + (df16['date'].dt.year-2016)*73, bins=244)\n",
      "plt.show()\n",
      "100/10: weekcounts = np.array([df16['week'].value_counts()[x] for x in range(243)])\n",
      "100/11: plt.plot([x for x in range(243)], weekcounts)\n",
      "100/12:\n",
      "lr = LinearRegression().fit(weekcounts[:242].reshape(-1, 1), weekcounts[1:243])\n",
      "lr.score(weekcounts[:242].reshape(-1, 1), weekcounts[1:243])\n",
      "100/13: from sklearn.linear_model import LinearRegression\n",
      "100/14:\n",
      "lr = LinearRegression().fit(weekcounts[:242].reshape(-1, 1), weekcounts[1:243])\n",
      "lr.score(weekcounts[:242].reshape(-1, 1), weekcounts[1:243])\n",
      "100/15:\n",
      "lr = LinearRegression().fit(np.array([weekcounts[1:242], weekcounts[:241]]).transpose(), weekcounts[2:243])\n",
      "lr.score(np.array([weekcounts[1:242], weekcounts[:241]]).transpose(), weekcounts[2:243])\n",
      "100/16: np.array([weekcounts[:243-73], weekcounts[72:242], weekcounts[73:243]]).shape\n",
      "100/17: len(weekcounts[1:243-73])\n",
      "100/18:\n",
      "lr = LinearRegression().fit(np.array([weekcounts[:242-73], weekcounts[72:241], weekcounts[73:242]]).transpose(), weekcounts[74:243])\n",
      "lr.score(np.array([weekcounts[:242-73], weekcounts[72:241], weekcounts[73:242]]).transpose(), weekcounts[74:243])\n",
      "100/19: t=60\n",
      "100/20: weekcounts[242-73-t:242-73].shape, weekcounts[241-t:241].shape, weekcounts[242-t:242].shape\n",
      "100/21:\n",
      "lr = LinearRegression().fit(np.array([weekcounts[:242-73-t], weekcounts[72:241-t], weekcounts[73:242-t]]).transpose(), weekcounts[74:243-t])\n",
      "lr.score(np.array([weekcounts[242-73-t:242-73], weekcounts[241-t:241], weekcounts[242-t:242]]).transpose(), weekcounts[243-t:243])\n",
      "100/22: np.array([weekcounts[:242-73-t], weekcounts[1:242-72-t], weekcounts[72:241-t], weekcounts[73:242-t]]).transpose().shape\n",
      "100/23:\n",
      "lr = LinearRegression().fit(np.array([weekcounts[:242-73-t], weekcounts[1:242-72-t], weekcounts[72:241-t], weekcounts[73:242-t]]).transpose(), weekcounts[74:243-t])\n",
      "lr.score(np.array([weekcounts[242-73-t:242-73], weekcounts[242-72-t:242-72], weekcounts[241-t:241], weekcounts[242-t:242]]).transpose(), weekcounts[243-t:243])\n",
      "100/24: preds = lr.predict(np.array([weekcounts[242-73-t:242-73], weekcounts[242-72-t:242-72], weekcounts[241-t:241], weekcounts[242-t:242]]).transpose())\n",
      "100/25: from sklearn.metrics import mean_squared_error\n",
      "100/26: mean_squared_error(weekcounts[243-t:243], preds)\n",
      "100/27: sqrt(mean_squared_error(weekcounts[243-t:243], preds))\n",
      "100/28: np.sqrt(mean_squared_error(weekcounts[243-t:243], preds))\n",
      "100/29: df0 = pd.read_pickle('since2018')\n",
      "100/30: df0['hour'] = df0['date'].dt.hour//3\n",
      "100/31: df0.nunique()\n",
      "100/32: df0['primary_type'].value_counts()\n",
      "100/33:\n",
      "violent = [1]*len(df0)\n",
      "for i in range(len(df0)):\n",
      "    num = int(df0['iucr'].iloc[i][:3])\n",
      "    if num in range(58, 59) or num in range(81, 94) or num in range(105, 106) or num in range(111, 138) or num in range(142, 180) or num in range(181, 285) or num in range(286, 301) or num in range(330, 331) or num in range(361, 399) or num in range(431, 514):\n",
      "        violent[i] = 0\n",
      "100/34: np.sum(violent)/len(violent)\n",
      "100/35: df2['violent'] = violent\n",
      "100/36: ca.head()\n",
      "100/37: df = pd.read_csv('use_df.csv')\n",
      "100/38: ca = pd.read_csv(\"income_levels.csv\")\n",
      "100/39: ca.head()\n",
      "100/40: df = pd.read_csv('use_df.csv')\n",
      "100/41: df.head()\n",
      "100/42:\n",
      "df1 = df0.copy()\n",
      "df1 = pd.get_dummies(df1, columns = ['primary_type'])\n",
      "100/43: df[['date']].dtypes\n",
      "100/44: df['date'] = pd.to_datetime(df.date)\n",
      "100/45: df.nunique(axis=0)\n",
      "100/46: df.shape\n",
      "100/47:\n",
      "typeVals = df.dtypes\n",
      "typeVals = typeVals.replace('bool', 'int64')\n",
      "df = df.astype(typeVals)\n",
      "100/48: [x for x in df['location_description'].unique() if not x == None and ('FEDERAL' in x or 'GOVERNMENT' in x)]\n",
      "100/49:\n",
      "def getcounts(word):\n",
      "    cols = [x for x in df['location_description'].unique() if not x == None and (word in x)]\n",
      "    tot = 0\n",
      "    for c in cols:\n",
      "        tot += df['location_description'].value_counts()[c]\n",
      "    return tot\n",
      "def getcounts2(words):\n",
      "    tot = 0\n",
      "    for w in words:\n",
      "        tot += getcounts(w)\n",
      "    return tot\n",
      "100/50: df2['violent'] = violent\n",
      "100/51: ca = pd.read_csv(\"income_levels.csv\")\n",
      "100/52:\n",
      "df2 = df.copy()\n",
      "df2['hour'] = df2['date'].dt.hour//3\n",
      "df2 = df2.drop(['date'], axis=1)\n",
      "categorical_columns = ['iucr', 'location_description', 'community_area', 'hour']\n",
      "df2 = pd.get_dummies(df2, columns = categorical_columns)\n",
      "100/53: df2['violent'] = violent\n",
      "100/54:\n",
      "trainInds = np.random.uniform(0,1,df2.shape[0])<=0.8\n",
      "train=df2.iloc[trainInds]\n",
      "test=df2.iloc[~trainInds]\n",
      "100/55: train.shape, test.shape\n",
      "100/56:\n",
      "from sklearn.naive_bayes import MultinomialNB\n",
      "from sklearn.tree import DecisionTreeClassifier\n",
      "from sklearn.ensemble import RandomForestClassifier\n",
      "from sklearn.svm import LinearSVC, SVC\n",
      "from sklearn.linear_model import LogisticRegression\n",
      "100/57: models = [MultinomialNB(), DecisionTreeClassifier(), RandomForestClassifier(n_estimators=100), LinearSVC(), LogisticRegression()]\n",
      "100/58:\n",
      "def predictx(col, models, train, test):\n",
      "    for m in models:\n",
      "        m.fit(train.drop([col], axis=1), train[col])\n",
      "        print(m.score(test.drop([col], axis=1), test[col]))\n",
      "    return models\n",
      "100/59: from sklearn.neighbors import KNeighborsClassifier\n",
      "100/60:\n",
      "df3 = df2.drop([x for x in df2.columns if 'iucr' in x], axis=1)\n",
      "trainInds2 = np.random.uniform(0,1,df3.shape[0])<=0.8\n",
      "train2=df3.iloc[trainInds2]\n",
      "test2=df3.iloc[~trainInds2]\n",
      "100/61: knn1 = predictx('arrest', [KNeighborsClassifier()], train, test)\n",
      "100/62: knn2 = predictx('violent', [KNeighborsClassifier()], train2, test2)\n",
      "100/63: knn2 = predictx('violent', [MultinomialNB()], train2, test2)\n",
      "100/64: knn2 = predictx('violent', [DecisionTreeClassifier()], train2, test2)\n",
      "100/65: train2.columns\n",
      "100/66:\n",
      "plt.plot([x for x in range(243)], weekcounts)\n",
      "plt.ylim(0, 4500)\n",
      "100/67:\n",
      "plt.plot([x for x in range(243)], weekcounts)\n",
      "plt.ylim(0, 5000)\n",
      "100/68:\n",
      "plt.plot([x for x in range(243)], weekcounts)\n",
      "plt.ylim(0, 4700)\n",
      "100/69:\n",
      "plt.plot([x for x in range(243)], weekcounts)\n",
      "plt.ylim(0, 4700)\n",
      "plt.title(\"Time series plot of Chicago crimes Jan 2016 - Apr 2019\")\n",
      "plt.xlabel(\"\")\n",
      "plt.show()\n",
      "100/70:\n",
      "plt.plot([x for x in range(243)], weekcounts)\n",
      "plt.ylim(0, 4700)\n",
      "plt.title(\"Time series graph of Chicago crimes 2016 - 2019\")\n",
      "plt.xlabel(\"Time (Week number)\")\n",
      "plt.ylabel(\"Number of crimes\")\n",
      "plt.show()\n",
      "100/71:\n",
      "plt.plot([x for x in range(243)], weekcounts)\n",
      "plt.ylim(0, 4700)\n",
      "plt.title(\"Time series graph of Chicago crimes since 2016\")\n",
      "plt.xlabel(\"Time (Week number)\")\n",
      "plt.ylabel(\"Number of crimes\")\n",
      "plt.show()\n",
      "100/72:\n",
      "lr = LinearRegression().fit(np.array([weekcounts[:242-73-t], weekcounts[1:242-72-t], weekcounts[2:242-71-t], weekcounts[72:241-t], weekcounts[73:242-t]]).transpose(), weekcounts[74:243-t])\n",
      "lr.score(np.array([weekcounts[242-73-t:242-73], weekcounts[242-72-t:242-72], weekcounts[242-71-t:242-71], weekcounts[241-t:241], weekcounts[242-t:242]]).transpose(), weekcounts[243-t:243])\n",
      "100/73: t=50\n",
      "100/74:\n",
      "lr = LinearRegression().fit(np.array([weekcounts[:242-73-t], weekcounts[1:242-72-t], weekcounts[2:242-71-t], weekcounts[72:241-t], weekcounts[73:242-t]]).transpose(), weekcounts[74:243-t])\n",
      "lr.score(np.array([weekcounts[242-73-t:242-73], weekcounts[242-72-t:242-72], weekcounts[242-71-t:242-71], weekcounts[241-t:241], weekcounts[242-t:242]]).transpose(), weekcounts[243-t:243])\n",
      "100/75: t=70\n",
      "100/76:\n",
      "lr = LinearRegression().fit(np.array([weekcounts[:242-73-t], weekcounts[1:242-72-t], weekcounts[2:242-71-t], weekcounts[72:241-t], weekcounts[73:242-t]]).transpose(), weekcounts[74:243-t])\n",
      "lr.score(np.array([weekcounts[242-73-t:242-73], weekcounts[242-72-t:242-72], weekcounts[242-71-t:242-71], weekcounts[241-t:241], weekcounts[242-t:242]]).transpose(), weekcounts[243-t:243])\n",
      "100/77:\n",
      "lr = LinearRegression().fit(np.array([weekcounts[:242-73-t], weekcounts[1:242-72-t], weekcounts[72:241-t], weekcounts[73:242-t]]).transpose(), weekcounts[74:243-t])\n",
      "lr.score(np.array([weekcounts[242-73-t:242-73], weekcounts[242-72-t:242-72], weekcounts[241-t:241], weekcounts[242-t:242]]).transpose(), weekcounts[243-t:243])\n",
      "100/78: t=60\n",
      "100/79:\n",
      "lr = LinearRegression().fit(np.array([weekcounts[:242-73-t], weekcounts[1:242-72-t], weekcounts[72:241-t], weekcounts[73:242-t]]).transpose(), weekcounts[74:243-t])\n",
      "lr.score(np.array([weekcounts[242-73-t:242-73], weekcounts[242-72-t:242-72], weekcounts[241-t:241], weekcounts[242-t:242]]).transpose(), weekcounts[243-t:243])\n",
      "100/80:\n",
      "lr = LinearRegression().fit(np.array([weekcounts[:242-73-t], weekcounts[1:242-72-t], weekcounts[2:242-71-t], weekcounts[72:241-t], weekcounts[73:242-t]]).transpose(), weekcounts[74:243-t])\n",
      "lr.score(np.array([weekcounts[242-73-t:242-73], weekcounts[242-72-t:242-72], weekcounts[242-71-t:242-71], weekcounts[241-t:241], weekcounts[242-t:242]]).transpose(), weekcounts[243-t:243])\n",
      "100/81: df16['week'].iloc[[1,2]]\n",
      "100/82: df16.index[df16['community_area'] == x and df16['year'] == 2019]\n",
      "100/83: df16.index[df16['community_area'] == 1 and df16['year'] == 2019]\n",
      "100/84: df16[df16['community_area'] == 1]\n",
      "100/85: df16[df16['community_area'] == 1].index[df16['year'] == 2019]\n",
      "100/86: df16[df16['community_area'] == 1].index[df16[df16['community_area'] == 1]['year'] == 2019]\n",
      "100/87: df16[df16['community_area'] == 1].index[df16[df16['community_area'] == 1]['year'] == 2019].shape\n",
      "100/88: df16[df16['community_area'] == 1].index[df16[df16['community_area'] == 1]['year'] == 2018].shape\n",
      "100/89: df16[df16['community_area'] == 1].index[df16[df16['community_area'] == 1]['year'] == 2017].shape\n",
      "100/90: df16[df16['community_area'] == 1].index[df16[df16['community_area'] == 1]['year'] == 2016].shape\n",
      "100/91: df16[df16['community_area'] == 1].index[df16[df16['community_area'] == 1]['year'] == 2017].shape\n",
      "100/92: df16[df16['community_area'] == 1].index[df16[df16['community_area'] == 1]['year'] == 2018].shape\n",
      "100/93: df16[df16['community_area'] == 1].index[df16[df16['community_area'] == 1]['year'] == 2016].shape\n",
      "100/94: df16[df16['community_area'] == 1].index[df16[df16['community_area'] == 1]['year'] == 2017].shape\n",
      "100/95: df16[df16['community_area'] == 1].index[df16[df16['community_area'] == 1]['year'] == 2018].shape\n",
      "100/96: df16[df16['community_area'] == 1].index[df16[df16['community_area'] == 1]['year'] == 2019].shape\n",
      "100/97: df16[df16['year']=2019]['weeks']\n",
      "100/98: df16[df16['year']=2019]['week']\n",
      "100/99: df16[df16['year']==2019]['week']\n",
      "100/100: df16[df16['year']==2019]['week'].value_counts()\n",
      "100/101: t=73\n",
      "100/102: weekcounts[242-73-t:242-73].shape, weekcounts[241-t:241].shape, weekcounts[242-t:242].shape\n",
      "100/103:\n",
      "lr = LinearRegression().fit(np.array([weekcounts[:242-73-t], weekcounts[72:241-t], weekcounts[73:242-t]]).transpose(), weekcounts[74:243-t])\n",
      "lr.score(np.array([weekcounts[242-73-t:242-73], weekcounts[241-t:241], weekcounts[242-t:242]]).transpose(), weekcounts[243-t:243])\n",
      "100/104: np.array([weekcounts[:242-73-t], weekcounts[1:242-72-t], weekcounts[72:241-t], weekcounts[73:242-t]]).transpose().shape\n",
      "100/105:\n",
      "lr = LinearRegression().fit(np.array([weekcounts[:242-73-t], weekcounts[1:242-72-t], weekcounts[72:241-t], weekcounts[73:242-t]]).transpose(), weekcounts[74:243-t])\n",
      "lr.score(np.array([weekcounts[242-73-t:242-73], weekcounts[242-72-t:242-72], weekcounts[241-t:241], weekcounts[242-t:242]]).transpose(), weekcounts[243-t:243])\n",
      "100/106: t=70\n",
      "100/107: weekcounts[242-73-t:242-73].shape, weekcounts[241-t:241].shape, weekcounts[242-t:242].shape\n",
      "100/108:\n",
      "lr = LinearRegression().fit(np.array([weekcounts[:242-73-t], weekcounts[72:241-t], weekcounts[73:242-t]]).transpose(), weekcounts[74:243-t])\n",
      "lr.score(np.array([weekcounts[242-73-t:242-73], weekcounts[241-t:241], weekcounts[242-t:242]]).transpose(), weekcounts[243-t:243])\n",
      "100/109: np.array([weekcounts[:242-73-t], weekcounts[1:242-72-t], weekcounts[72:241-t], weekcounts[73:242-t]]).transpose().shape\n",
      "100/110:\n",
      "lr = LinearRegression().fit(np.array([weekcounts[:242-73-t], weekcounts[1:242-72-t], weekcounts[72:241-t], weekcounts[73:242-t]]).transpose(), weekcounts[74:243-t])\n",
      "lr.score(np.array([weekcounts[242-73-t:242-73], weekcounts[242-72-t:242-72], weekcounts[241-t:241], weekcounts[242-t:242]]).transpose(), weekcounts[243-t:243])\n",
      "100/111: t=72\n",
      "100/112: weekcounts[242-73-t:242-73].shape, weekcounts[241-t:241].shape, weekcounts[242-t:242].shape\n",
      "100/113:\n",
      "lr = LinearRegression().fit(np.array([weekcounts[:242-73-t], weekcounts[72:241-t], weekcounts[73:242-t]]).transpose(), weekcounts[74:243-t])\n",
      "lr.score(np.array([weekcounts[242-73-t:242-73], weekcounts[241-t:241], weekcounts[242-t:242]]).transpose(), weekcounts[243-t:243])\n",
      "100/114: np.array([weekcounts[:242-73-t], weekcounts[1:242-72-t], weekcounts[72:241-t], weekcounts[73:242-t]]).transpose().shape\n",
      "100/115:\n",
      "lr = LinearRegression().fit(np.array([weekcounts[:242-73-t], weekcounts[1:242-72-t], weekcounts[72:241-t], weekcounts[73:242-t]]).transpose(), weekcounts[74:243-t])\n",
      "lr.score(np.array([weekcounts[242-73-t:242-73], weekcounts[242-72-t:242-72], weekcounts[241-t:241], weekcounts[242-t:242]]).transpose(), weekcounts[243-t:243])\n",
      "100/116: t=73\n",
      "100/117: weekcounts[242-73-t:242-73].shape, weekcounts[241-t:241].shape, weekcounts[242-t:242].shape\n",
      "100/118:\n",
      "lr = LinearRegression().fit(np.array([weekcounts[:242-73-t], weekcounts[72:241-t], weekcounts[73:242-t]]).transpose(), weekcounts[74:243-t])\n",
      "lr.score(np.array([weekcounts[242-73-t:242-73], weekcounts[241-t:241], weekcounts[242-t:242]]).transpose(), weekcounts[243-t:243])\n",
      "100/119: np.array([weekcounts[:242-73-t], weekcounts[1:242-72-t], weekcounts[72:241-t], weekcounts[73:242-t]]).transpose().shape\n",
      "100/120:\n",
      "lr = LinearRegression().fit(np.array([weekcounts[:242-73-t], weekcounts[1:242-72-t], weekcounts[72:241-t], weekcounts[73:242-t]]).transpose(), weekcounts[74:243-t])\n",
      "lr.score(np.array([weekcounts[242-73-t:242-73], weekcounts[242-72-t:242-72], weekcounts[241-t:241], weekcounts[242-t:242]]).transpose(), weekcounts[243-t:243])\n",
      "100/121: preds = lr.predict(np.array([weekcounts[242-73-t:242-73], weekcounts[242-72-t:242-72], weekcounts[241-t:241], weekcounts[242-t:242]]).transpose())\n",
      "100/122: from sklearn.metrics import mean_squared_error\n",
      "100/123: np.sqrt(mean_squared_error(weekcounts[243-t:243], preds))\n",
      "100/124: t=60\n",
      "100/125: weekcounts[242-73-t:242-73].shape, weekcounts[241-t:241].shape, weekcounts[242-t:242].shape\n",
      "100/126:\n",
      "lr = LinearRegression().fit(np.array([weekcounts[:242-73-t], weekcounts[72:241-t], weekcounts[73:242-t]]).transpose(), weekcounts[74:243-t])\n",
      "lr.score(np.array([weekcounts[242-73-t:242-73], weekcounts[241-t:241], weekcounts[242-t:242]]).transpose(), weekcounts[243-t:243])\n",
      "100/127: np.array([weekcounts[:242-73-t], weekcounts[1:242-72-t], weekcounts[72:241-t], weekcounts[73:242-t]]).transpose().shape\n",
      "100/128:\n",
      "lr = LinearRegression().fit(np.array([weekcounts[:242-73-t], weekcounts[1:242-72-t], weekcounts[72:241-t], weekcounts[73:242-t]]).transpose(), weekcounts[74:243-t])\n",
      "lr.score(np.array([weekcounts[242-73-t:242-73], weekcounts[242-72-t:242-72], weekcounts[241-t:241], weekcounts[242-t:242]]).transpose(), weekcounts[243-t:243])\n",
      "100/129: preds = lr.predict(np.array([weekcounts[242-73-t:242-73], weekcounts[242-72-t:242-72], weekcounts[241-t:241], weekcounts[242-t:242]]).transpose())\n",
      "100/130: from sklearn.metrics import mean_squared_error\n",
      "100/131: np.sqrt(mean_squared_error(weekcounts[243-t:243], preds))\n",
      "100/132:\n",
      "lrs = []\n",
      "lrscores = []\n",
      "for x in df16['community_area'].unique():\n",
      "    lr = LinearRegression()\n",
      "    \n",
      "    df16_ = df16[df16['community_area'] == x]\n",
      "    \n",
      "    weekcounts_ = np.array([df16_['week'].value_counts()[x] for x in range(243)])\n",
      "    \n",
      "    X = np.array([weekcounts_[:242-73-t], weekcounts_[1:242-72-t], weekcounts_[72:241-t], weekcounts_[73:242-t]]).transpose()\n",
      "    y = weekcounts_[74:243-t]\n",
      "    \n",
      "    Xt = np.array([weekcounts_[242-73-t:242-73], weekcounts_[242-72-t:242-72], weekcounts_[241-t:241], weekcounts_[242-t:242]]).transpose()\n",
      "    yt = weekcounts_[243-t:243]\n",
      "    \n",
      "    lr.fit(X, y)\n",
      "    s = lr.score(Xt,yt)\n",
      "    print(x + \": \" s)\n",
      "    lrscores.append(s)\n",
      "    lrs.append(lr)\n",
      "100/133:\n",
      "lrs = []\n",
      "lrscores = []\n",
      "for x in df16['community_area'].unique():\n",
      "    lr = LinearRegression()\n",
      "    \n",
      "    df16_ = df16[df16['community_area'] == x]\n",
      "    \n",
      "    weekcounts_ = np.array([df16_['week'].value_counts()[x] for x in range(243)])\n",
      "    \n",
      "    X = np.array([weekcounts_[:242-73-t], weekcounts_[1:242-72-t], weekcounts_[72:241-t], weekcounts_[73:242-t]]).transpose()\n",
      "    y = weekcounts_[74:243-t]\n",
      "    \n",
      "    Xt = np.array([weekcounts_[242-73-t:242-73], weekcounts_[242-72-t:242-72], weekcounts_[241-t:241], weekcounts_[242-t:242]]).transpose()\n",
      "    yt = weekcounts_[243-t:243]\n",
      "    \n",
      "    lr.fit(X, y)\n",
      "    s = lr.score(Xt,yt)\n",
      "    print(x + \": \" str(s))\n",
      "    lrscores.append(s)\n",
      "    lrs.append(lr)\n",
      "100/134:\n",
      "lrs = []\n",
      "lrscores = []\n",
      "for x in df16['community_area'].unique():\n",
      "    lr = LinearRegression()\n",
      "    \n",
      "    df16_ = df16[df16['community_area'] == x]\n",
      "    \n",
      "    weekcounts_ = np.array([df16_['week'].value_counts()[x] for x in range(243)])\n",
      "    \n",
      "    X = np.array([weekcounts_[:242-73-t], weekcounts_[1:242-72-t], weekcounts_[72:241-t], weekcounts_[73:242-t]]).transpose()\n",
      "    y = weekcounts_[74:243-t]\n",
      "    \n",
      "    Xt = np.array([weekcounts_[242-73-t:242-73], weekcounts_[242-72-t:242-72], weekcounts_[241-t:241], weekcounts_[242-t:242]]).transpose()\n",
      "    yt = weekcounts_[243-t:243]\n",
      "    \n",
      "    lr.fit(X, y)\n",
      "    s = lr.score(Xt,yt)\n",
      "    print(x + \": \" + str(s))\n",
      "    lrscores.append(s)\n",
      "    lrs.append(lr)\n",
      "100/135:\n",
      "lrs = []\n",
      "lrscores = []\n",
      "for x in df16['community_area'].unique():\n",
      "    lr = LinearRegression()\n",
      "    \n",
      "    df16_ = df16[df16['community_area'] == x]\n",
      "    \n",
      "    weekcounts_ = np.array([df16_['week'].value_counts()[x] for x in range(243)])\n",
      "    \n",
      "    X = np.array([weekcounts_[:242-73-t], weekcounts_[1:242-72-t], weekcounts_[72:241-t], weekcounts_[73:242-t]]).transpose()\n",
      "    y = weekcounts_[74:243-t]\n",
      "    \n",
      "    Xt = np.array([weekcounts_[242-73-t:242-73], weekcounts_[242-72-t:242-72], weekcounts_[241-t:241], weekcounts_[242-t:242]]).transpose()\n",
      "    yt = weekcounts_[243-t:243]\n",
      "    \n",
      "    lr.fit(X, y)\n",
      "    s = lr.score(Xt,yt)\n",
      "    print(str(x) + \": \" + str(s))\n",
      "    lrscores.append(s)\n",
      "    lrs.append(lr)\n",
      "100/136:\n",
      "lrs = []\n",
      "lrscores = []\n",
      "for x in df16['community_area'].unique():\n",
      "    lr = LinearRegression()\n",
      "    \n",
      "    df16_ = df16[df16['community_area'] == x].copy()\n",
      "    \n",
      "    weekcounts_ = np.array([df16_['week'].value_counts()[x] for x in range(243)])\n",
      "    \n",
      "    X = np.array([weekcounts_[:242-73-t], weekcounts_[1:242-72-t], weekcounts_[72:241-t], weekcounts_[73:242-t]]).transpose()\n",
      "    y = weekcounts_[74:243-t]\n",
      "    \n",
      "    Xt = np.array([weekcounts_[242-73-t:242-73], weekcounts_[242-72-t:242-72], weekcounts_[241-t:241], weekcounts_[242-t:242]]).transpose()\n",
      "    yt = weekcounts_[243-t:243]\n",
      "    \n",
      "    lr.fit(X, y)\n",
      "    s = lr.score(Xt,yt)\n",
      "    print(str(x) + \": \" + str(s))\n",
      "    lrscores.append(s)\n",
      "    lrs.append(lr)\n",
      "100/137:\n",
      "def lrx(k):\n",
      "    lr = LinearRegression()\n",
      "    \n",
      "    df16_ = df16[df16['community_area'] == k].copy()\n",
      "    \n",
      "    weekcounts_ = np.array([df16_['week'].value_counts()[x] for x in range(243)])\n",
      "    \n",
      "    X = np.array([weekcounts_[:242-73-t], weekcounts_[1:242-72-t], weekcounts_[72:241-t], weekcounts_[73:242-t]]).transpose()\n",
      "    y = weekcounts_[74:243-t]\n",
      "    \n",
      "    Xt = np.array([weekcounts_[242-73-t:242-73], weekcounts_[242-72-t:242-72], weekcounts_[241-t:241], weekcounts_[242-t:242]]).transpose()\n",
      "    yt = weekcounts_[243-t:243]\n",
      "    \n",
      "    lr.fit(X, y)\n",
      "    s = lr.score(Xt,yt)\n",
      "    print(str(x) + \": \" + str(s))\n",
      "    plt.plot()\n",
      "    \n",
      "    plt.plot([x for x in range(243)], weekcounts_)\n",
      "100/138:\n",
      "def lrx(k):\n",
      "    lr = LinearRegression()\n",
      "    \n",
      "    df16_ = df16[df16['community_area'] == k].copy()\n",
      "    \n",
      "    weekcounts_ = np.array([df16_['week'].value_counts()[x] for x in range(243)])\n",
      "    \n",
      "    X = np.array([weekcounts_[:242-73-t], weekcounts_[1:242-72-t], weekcounts_[72:241-t], weekcounts_[73:242-t]]).transpose()\n",
      "    y = weekcounts_[74:243-t]\n",
      "    \n",
      "    Xt = np.array([weekcounts_[242-73-t:242-73], weekcounts_[242-72-t:242-72], weekcounts_[241-t:241], weekcounts_[242-t:242]]).transpose()\n",
      "    yt = weekcounts_[243-t:243]\n",
      "    \n",
      "    lr.fit(X, y)\n",
      "    s = lr.score(Xt,yt)\n",
      "    print(str(x) + \": \" + str(s))\n",
      "    \n",
      "    plt.plot([x for x in range(243)], weekcounts_)\n",
      "100/139: lrx(1)\n",
      "100/140:\n",
      "lrs = []\n",
      "lrscores = []\n",
      "for x in df16['community_area'].unique():\n",
      "    lr = LinearRegression()\n",
      "    \n",
      "    df16_ = df16[df16['community_area'] == x].copy()\n",
      "    \n",
      "    weekcounts_ = np.array([])\n",
      "    for x in range(243):\n",
      "        if x in df16_['week']:\n",
      "            weekcounts_.append(df16_['week'].value_counts()[x])\n",
      "        else:\n",
      "            weekcounts_.append(0)\n",
      "    \n",
      "    X = np.array([weekcounts_[:242-73-t], weekcounts_[1:242-72-t], weekcounts_[72:241-t], weekcounts_[73:242-t]]).transpose()\n",
      "    y = weekcounts_[74:243-t]\n",
      "    \n",
      "    Xt = np.array([weekcounts_[242-73-t:242-73], weekcounts_[242-72-t:242-72], weekcounts_[241-t:241], weekcounts_[242-t:242]]).transpose()\n",
      "    yt = weekcounts_[243-t:243]\n",
      "    \n",
      "    lr.fit(X, y)\n",
      "    s = lr.score(Xt,yt)\n",
      "    print(str(x) + \": \" + str(s))\n",
      "    lrscores.append(s)\n",
      "    lrs.append(lr)\n",
      "100/141:\n",
      "lrs = []\n",
      "lrscores = []\n",
      "for x in df16['community_area'].unique():\n",
      "    lr = LinearRegression()\n",
      "    \n",
      "    df16_ = df16[df16['community_area'] == x].copy()\n",
      "    \n",
      "    weekcounts_ = []\n",
      "    for x in range(243):\n",
      "        if x in df16_['week']:\n",
      "            weekcounts_.append(df16_['week'].value_counts()[x])\n",
      "        else:\n",
      "            weekcounts_.append(0)\n",
      "    weekcounts_ = np.array(week_counts_)\n",
      "    \n",
      "    X = np.array([weekcounts_[:242-73-t], weekcounts_[1:242-72-t], weekcounts_[72:241-t], weekcounts_[73:242-t]]).transpose()\n",
      "    y = weekcounts_[74:243-t]\n",
      "    \n",
      "    Xt = np.array([weekcounts_[242-73-t:242-73], weekcounts_[242-72-t:242-72], weekcounts_[241-t:241], weekcounts_[242-t:242]]).transpose()\n",
      "    yt = weekcounts_[243-t:243]\n",
      "    \n",
      "    lr.fit(X, y)\n",
      "    s = lr.score(Xt,yt)\n",
      "    print(str(x) + \": \" + str(s))\n",
      "    lrscores.append(s)\n",
      "    lrs.append(lr)\n",
      "100/142:\n",
      "lrs = []\n",
      "lrscores = []\n",
      "for x in df16['community_area'].unique():\n",
      "    lr = LinearRegression()\n",
      "    \n",
      "    df16_ = df16[df16['community_area'] == x].copy()\n",
      "    \n",
      "    weekcounts_ = []\n",
      "    for x in range(243):\n",
      "        if x in df16_['week']:\n",
      "            weekcounts_.append(df16_['week'].value_counts()[x])\n",
      "        else:\n",
      "            weekcounts_.append(0)\n",
      "    weekcounts_ = np.array(weekcounts_)\n",
      "    \n",
      "    X = np.array([weekcounts_[:242-73-t], weekcounts_[1:242-72-t], weekcounts_[72:241-t], weekcounts_[73:242-t]]).transpose()\n",
      "    y = weekcounts_[74:243-t]\n",
      "    \n",
      "    Xt = np.array([weekcounts_[242-73-t:242-73], weekcounts_[242-72-t:242-72], weekcounts_[241-t:241], weekcounts_[242-t:242]]).transpose()\n",
      "    yt = weekcounts_[243-t:243]\n",
      "    \n",
      "    lr.fit(X, y)\n",
      "    s = lr.score(Xt,yt)\n",
      "    print(str(x) + \": \" + str(s))\n",
      "    lrscores.append(s)\n",
      "    lrs.append(lr)\n",
      "100/143:\n",
      "lrs = []\n",
      "lrscores = []\n",
      "for x in df16['community_area'].unique():\n",
      "    lr = LinearRegression()\n",
      "    \n",
      "    df16_ = df16[df16['community_area'] == x].copy()\n",
      "    \n",
      "    weekcounts_ = []\n",
      "    for x in range(243):\n",
      "        if x in df16_['week']:\n",
      "            print(x)\n",
      "            weekcounts_.append(df16_['week'].value_counts()[x])\n",
      "        else:\n",
      "            weekcounts_.append(0)\n",
      "    weekcounts_ = np.array(weekcounts_)\n",
      "    \n",
      "    X = np.array([weekcounts_[:242-73-t], weekcounts_[1:242-72-t], weekcounts_[72:241-t], weekcounts_[73:242-t]]).transpose()\n",
      "    y = weekcounts_[74:243-t]\n",
      "    \n",
      "    Xt = np.array([weekcounts_[242-73-t:242-73], weekcounts_[242-72-t:242-72], weekcounts_[241-t:241], weekcounts_[242-t:242]]).transpose()\n",
      "    yt = weekcounts_[243-t:243]\n",
      "    \n",
      "    lr.fit(X, y)\n",
      "    s = lr.score(Xt,yt)\n",
      "    print(str(x) + \": \" + str(s))\n",
      "    lrscores.append(s)\n",
      "    lrs.append(lr)\n",
      "100/144:\n",
      "lrs = []\n",
      "lrscores = []\n",
      "for x in df16['community_area'].unique():\n",
      "    lr = LinearRegression()\n",
      "    \n",
      "    df16_ = df16[df16['community_area'] == x].copy()\n",
      "    \n",
      "    weekcounts_ = []\n",
      "    for x in range(243):\n",
      "        if x in df16_['week']:\n",
      "            weekcounts_.append(df16_['week'].value_counts()[x])\n",
      "        else:\n",
      "            print(x)\n",
      "            weekcounts_.append(0)\n",
      "    weekcounts_ = np.array(weekcounts_)\n",
      "    \n",
      "    X = np.array([weekcounts_[:242-73-t], weekcounts_[1:242-72-t], weekcounts_[72:241-t], weekcounts_[73:242-t]]).transpose()\n",
      "    y = weekcounts_[74:243-t]\n",
      "    \n",
      "    Xt = np.array([weekcounts_[242-73-t:242-73], weekcounts_[242-72-t:242-72], weekcounts_[241-t:241], weekcounts_[242-t:242]]).transpose()\n",
      "    yt = weekcounts_[243-t:243]\n",
      "    \n",
      "    lr.fit(X, y)\n",
      "    s = lr.score(Xt,yt)\n",
      "    print(str(x) + \": \" + str(s))\n",
      "    lrscores.append(s)\n",
      "    lrs.append(lr)\n",
      "100/145:\n",
      "lrs = []\n",
      "lrscores = []\n",
      "for x in df16['community_area'].unique():\n",
      "    lr = LinearRegression()\n",
      "    \n",
      "    df16_ = df16[df16['community_area'] == x].copy()\n",
      "    \n",
      "    weekcounts_ = []\n",
      "    for x in range(243):\n",
      "        if x in df16_['week']:\n",
      "            weekcounts_.append(df16_['week'].value_counts()[x])\n",
      "        else:\n",
      "            weekcounts_.append(0)\n",
      "    weekcounts_ = np.array(weekcounts_)\n",
      "    \n",
      "    X = np.array([weekcounts_[:242-73-t], weekcounts_[1:242-72-t], weekcounts_[72:241-t], weekcounts_[73:242-t]]).transpose()\n",
      "    y = weekcounts_[74:243-t]\n",
      "    \n",
      "    Xt = np.array([weekcounts_[242-73-t:242-73], weekcounts_[242-72-t:242-72], weekcounts_[241-t:241], weekcounts_[242-t:242]]).transpose()\n",
      "    yt = weekcounts_[243-t:243]\n",
      "    \n",
      "    lr.fit(X, y)\n",
      "    s = lr.score(Xt,yt)\n",
      "    print(str(x) + \": \" + str(s))\n",
      "    lrscores.append(s)\n",
      "    lrs.append(lr)\n",
      "100/146:\n",
      "lrs = []\n",
      "lrscores = []\n",
      "for x in df16['community_area'].unique():\n",
      "    lr = LinearRegression()\n",
      "    \n",
      "    df16_ = df16[df16['community_area'] == x].copy()\n",
      "    \n",
      "    weekcounts_ = []\n",
      "    for i in range(243):\n",
      "        if i in df16_['week']:\n",
      "            weekcounts_.append(df16_['week'].value_counts()[x])\n",
      "        else:\n",
      "            weekcounts_.append(0)\n",
      "    weekcounts_ = np.array(weekcounts_)\n",
      "    \n",
      "    X = np.array([weekcounts_[:242-73-t], weekcounts_[1:242-72-t], weekcounts_[72:241-t], weekcounts_[73:242-t]]).transpose()\n",
      "    y = weekcounts_[74:243-t]\n",
      "    \n",
      "    Xt = np.array([weekcounts_[242-73-t:242-73], weekcounts_[242-72-t:242-72], weekcounts_[241-t:241], weekcounts_[242-t:242]]).transpose()\n",
      "    yt = weekcounts_[243-t:243]\n",
      "    \n",
      "    lr.fit(X, y)\n",
      "    s = lr.score(Xt,yt)\n",
      "    print(str(x) + \": \" + str(s))\n",
      "    lrscores.append(s)\n",
      "    lrs.append(lr)\n",
      "100/147:\n",
      "lrs = []\n",
      "lrscores = []\n",
      "for x in df16['community_area'].unique():\n",
      "    lr = LinearRegression()\n",
      "    \n",
      "    df16_ = df16[df16['community_area'] == x].copy()\n",
      "    \n",
      "    weekcounts_ = []\n",
      "    for i in range(243):\n",
      "        if i in df16_['week']:\n",
      "            weekcounts_.append(df16_['week'].value_counts()[i])\n",
      "        else:\n",
      "            weekcounts_.append(0)\n",
      "    weekcounts_ = np.array(weekcounts_)\n",
      "    \n",
      "    X = np.array([weekcounts_[:242-73-t], weekcounts_[1:242-72-t], weekcounts_[72:241-t], weekcounts_[73:242-t]]).transpose()\n",
      "    y = weekcounts_[74:243-t]\n",
      "    \n",
      "    Xt = np.array([weekcounts_[242-73-t:242-73], weekcounts_[242-72-t:242-72], weekcounts_[241-t:241], weekcounts_[242-t:242]]).transpose()\n",
      "    yt = weekcounts_[243-t:243]\n",
      "    \n",
      "    lr.fit(X, y)\n",
      "    s = lr.score(Xt,yt)\n",
      "    print(str(x) + \": \" + str(s))\n",
      "    lrscores.append(s)\n",
      "    lrs.append(lr)\n",
      "100/148:\n",
      "lrs = []\n",
      "lrscores = []\n",
      "for x in df16['community_area'].unique():\n",
      "    lr = LinearRegression()\n",
      "    \n",
      "    df16_ = df16[df16['community_area'] == x].copy()\n",
      "    \n",
      "    weekcounts_ = []\n",
      "    for i in range(243):\n",
      "        if i in df16_['week'].unique():\n",
      "            weekcounts_.append(df16_['week'].value_counts()[i])\n",
      "        else:\n",
      "            weekcounts_.append(0)\n",
      "    weekcounts_ = np.array(weekcounts_)\n",
      "    \n",
      "    X = np.array([weekcounts_[:242-73-t], weekcounts_[1:242-72-t], weekcounts_[72:241-t], weekcounts_[73:242-t]]).transpose()\n",
      "    y = weekcounts_[74:243-t]\n",
      "    \n",
      "    Xt = np.array([weekcounts_[242-73-t:242-73], weekcounts_[242-72-t:242-72], weekcounts_[241-t:241], weekcounts_[242-t:242]]).transpose()\n",
      "    yt = weekcounts_[243-t:243]\n",
      "    \n",
      "    lr.fit(X, y)\n",
      "    s = lr.score(Xt,yt)\n",
      "    print(str(x) + \": \" + str(s))\n",
      "    lrscores.append(s)\n",
      "    lrs.append(lr)\n",
      "100/149:\n",
      "split = 'district' # community_area\n",
      "lrs = []\n",
      "lrscores = []\n",
      "for x in df16[split].unique():\n",
      "    lr = LinearRegression()\n",
      "    \n",
      "    df16_ = df16[df16[split] == x].copy()\n",
      "    \n",
      "    weekcounts_ = []\n",
      "    for i in range(243):\n",
      "        if i in df16_['week'].unique():\n",
      "            weekcounts_.append(df16_['week'].value_counts()[i])\n",
      "        else:\n",
      "            weekcounts_.append(0)\n",
      "    weekcounts_ = np.array(weekcounts_)\n",
      "    \n",
      "    X = np.array([weekcounts_[:242-73-t], weekcounts_[1:242-72-t], weekcounts_[72:241-t], weekcounts_[73:242-t]]).transpose()\n",
      "    y = weekcounts_[74:243-t]\n",
      "    \n",
      "    Xt = np.array([weekcounts_[242-73-t:242-73], weekcounts_[242-72-t:242-72], weekcounts_[241-t:241], weekcounts_[242-t:242]]).transpose()\n",
      "    yt = weekcounts_[243-t:243]\n",
      "    \n",
      "    lr.fit(X, y)\n",
      "    s = lr.score(Xt,yt)\n",
      "    print(str(x) + \": \" + str(s))\n",
      "    lrscores.append(s)\n",
      "    lrs.append(lr)\n",
      "100/150: lrx(25)\n",
      "100/151:\n",
      "def lrx(k):\n",
      "    lr = LinearRegression()\n",
      "    \n",
      "    df16_ = df16[df16['community_area'] == k].copy()\n",
      "    \n",
      "    weekcounts_ = np.array([df16_['week'].value_counts()[x] for x in range(243)])\n",
      "    \n",
      "    X = np.array([weekcounts_[:242-73-t], weekcounts_[1:242-72-t], weekcounts_[72:241-t], weekcounts_[73:242-t]]).transpose()\n",
      "    y = weekcounts_[74:243-t]\n",
      "    \n",
      "    Xt = np.array([weekcounts_[242-73-t:242-73], weekcounts_[242-72-t:242-72], weekcounts_[241-t:241], weekcounts_[242-t:242]]).transpose()\n",
      "    yt = weekcounts_[243-t:243]\n",
      "    \n",
      "    lr.fit(X, y)\n",
      "    s = lr.score(Xt,yt)\n",
      "    print(str(k) + \": \" + str(s))\n",
      "    \n",
      "    plt.plot([x for x in range(243)], weekcounts_)\n",
      "100/152: lrx(25)\n",
      "100/153: plt.plot(weekcounts[243-t:243], preds)\n",
      "100/154: plt.plot(weekcounts[243-t:243], preds, \"o\")\n",
      "100/155: plt.plot(weekcounts[243-t:243]-preds, \"o\")\n",
      "100/156:\n",
      "plt.plot(weekcounts[243-t:243]-preds, \"o\")\n",
      "plt.title(\"Residuals\")\n",
      "plt.show()\n",
      "100/157: plt.plot([x for x in range(243-t, 243)], preds)\n",
      "100/158: t=70\n",
      "100/159: weekcounts[242-73-t:242-73].shape, weekcounts[241-t:241].shape, weekcounts[242-t:242].shape\n",
      "100/160:\n",
      "lr = LinearRegression().fit(np.array([weekcounts[:242-73-t], weekcounts[72:241-t], weekcounts[73:242-t]]).transpose(), weekcounts[74:243-t])\n",
      "lr.score(np.array([weekcounts[242-73-t:242-73], weekcounts[241-t:241], weekcounts[242-t:242]]).transpose(), weekcounts[243-t:243])\n",
      "100/161: np.array([weekcounts[:242-73-t], weekcounts[1:242-72-t], weekcounts[72:241-t], weekcounts[73:242-t]]).transpose().shape\n",
      "100/162:\n",
      "lr = LinearRegression().fit(np.array([weekcounts[:242-73-t], weekcounts[1:242-72-t], weekcounts[72:241-t], weekcounts[73:242-t]]).transpose(), weekcounts[74:243-t])\n",
      "lr.score(np.array([weekcounts[242-73-t:242-73], weekcounts[242-72-t:242-72], weekcounts[241-t:241], weekcounts[242-t:242]]).transpose(), weekcounts[243-t:243])\n",
      "100/163: preds = lr.predict(np.array([weekcounts[242-73-t:242-73], weekcounts[242-72-t:242-72], weekcounts[241-t:241], weekcounts[242-t:242]]).transpose())\n",
      "100/164:\n",
      "plt.plot(weekcounts[243-t:243]-preds, \"o\")\n",
      "plt.title(\"Residuals\")\n",
      "plt.show()\n",
      "100/165: plt.plot([x for x in range(243-t, 243)], preds)\n",
      "100/166: t=100\n",
      "100/167: weekcounts[242-73-t:242-73].shape, weekcounts[241-t:241].shape, weekcounts[242-t:242].shape\n",
      "100/168:\n",
      "lr = LinearRegression().fit(np.array([weekcounts[:242-73-t], weekcounts[72:241-t], weekcounts[73:242-t]]).transpose(), weekcounts[74:243-t])\n",
      "lr.score(np.array([weekcounts[242-73-t:242-73], weekcounts[241-t:241], weekcounts[242-t:242]]).transpose(), weekcounts[243-t:243])\n",
      "100/169: np.array([weekcounts[:242-73-t], weekcounts[1:242-72-t], weekcounts[72:241-t], weekcounts[73:242-t]]).transpose().shape\n",
      "100/170:\n",
      "lr = LinearRegression().fit(np.array([weekcounts[:242-73-t], weekcounts[1:242-72-t], weekcounts[72:241-t], weekcounts[73:242-t]]).transpose(), weekcounts[74:243-t])\n",
      "lr.score(np.array([weekcounts[242-73-t:242-73], weekcounts[242-72-t:242-72], weekcounts[241-t:241], weekcounts[242-t:242]]).transpose(), weekcounts[243-t:243])\n",
      "100/171: preds = lr.predict(np.array([weekcounts[242-73-t:242-73], weekcounts[242-72-t:242-72], weekcounts[241-t:241], weekcounts[242-t:242]]).transpose())\n",
      "100/172:\n",
      "plt.plot(weekcounts[243-t:243]-preds, \"o\")\n",
      "plt.title(\"Residuals\")\n",
      "plt.show()\n",
      "100/173: plt.plot([x for x in range(243-t, 243)], preds)\n",
      "100/174: t=97\n",
      "100/175: weekcounts[242-73-t:242-73].shape, weekcounts[241-t:241].shape, weekcounts[242-t:242].shape\n",
      "100/176:\n",
      "lr = LinearRegression().fit(np.array([weekcounts[:242-73-t], weekcounts[72:241-t], weekcounts[73:242-t]]).transpose(), weekcounts[74:243-t])\n",
      "lr.score(np.array([weekcounts[242-73-t:242-73], weekcounts[241-t:241], weekcounts[242-t:242]]).transpose(), weekcounts[243-t:243])\n",
      "100/177: np.array([weekcounts[:242-73-t], weekcounts[1:242-72-t], weekcounts[72:241-t], weekcounts[73:242-t]]).transpose().shape\n",
      "100/178:\n",
      "lr = LinearRegression().fit(np.array([weekcounts[:242-73-t], weekcounts[1:242-72-t], weekcounts[72:241-t], weekcounts[73:242-t]]).transpose(), weekcounts[74:243-t])\n",
      "lr.score(np.array([weekcounts[242-73-t:242-73], weekcounts[242-72-t:242-72], weekcounts[241-t:241], weekcounts[242-t:242]]).transpose(), weekcounts[243-t:243])\n",
      "100/179: preds = lr.predict(np.array([weekcounts[242-73-t:242-73], weekcounts[242-72-t:242-72], weekcounts[241-t:241], weekcounts[242-t:242]]).transpose())\n",
      "100/180:\n",
      "plt.plot(weekcounts[243-t:243]-preds, \"o\")\n",
      "plt.title(\"Residuals\")\n",
      "plt.show()\n",
      "100/181: plt.plot([x for x in range(243-t, 243)], preds)\n",
      "100/182: t=80\n",
      "100/183: weekcounts[242-73-t:242-73].shape, weekcounts[241-t:241].shape, weekcounts[242-t:242].shape\n",
      "100/184:\n",
      "lr = LinearRegression().fit(np.array([weekcounts[:242-73-t], weekcounts[72:241-t], weekcounts[73:242-t]]).transpose(), weekcounts[74:243-t])\n",
      "lr.score(np.array([weekcounts[242-73-t:242-73], weekcounts[241-t:241], weekcounts[242-t:242]]).transpose(), weekcounts[243-t:243])\n",
      "100/185: np.array([weekcounts[:242-73-t], weekcounts[1:242-72-t], weekcounts[72:241-t], weekcounts[73:242-t]]).transpose().shape\n",
      "100/186:\n",
      "lr = LinearRegression().fit(np.array([weekcounts[:242-73-t], weekcounts[1:242-72-t], weekcounts[72:241-t], weekcounts[73:242-t]]).transpose(), weekcounts[74:243-t])\n",
      "lr.score(np.array([weekcounts[242-73-t:242-73], weekcounts[242-72-t:242-72], weekcounts[241-t:241], weekcounts[242-t:242]]).transpose(), weekcounts[243-t:243])\n",
      "100/187: preds = lr.predict(np.array([weekcounts[242-73-t:242-73], weekcounts[242-72-t:242-72], weekcounts[241-t:241], weekcounts[242-t:242]]).transpose())\n",
      "100/188:\n",
      "plt.plot(weekcounts[243-t:243]-preds, \"o\")\n",
      "plt.title(\"Residuals\")\n",
      "plt.show()\n",
      "100/189: plt.plot([x for x in range(243-t, 243)], preds)\n",
      "100/190: from sklearn.metrics import mean_squared_error\n",
      "100/191: np.sqrt(mean_squared_error(weekcounts[243-t:243], preds))\n",
      "100/192: t=73\n",
      "100/193: t=75\n",
      "100/194: weekcounts[242-73-t:242-73].shape, weekcounts[241-t:241].shape, weekcounts[242-t:242].shape\n",
      "100/195:\n",
      "lr = LinearRegression().fit(np.array([weekcounts[:242-73-t], weekcounts[72:241-t], weekcounts[73:242-t]]).transpose(), weekcounts[74:243-t])\n",
      "lr.score(np.array([weekcounts[242-73-t:242-73], weekcounts[241-t:241], weekcounts[242-t:242]]).transpose(), weekcounts[243-t:243])\n",
      "100/196: np.array([weekcounts[:242-73-t], weekcounts[1:242-72-t], weekcounts[72:241-t], weekcounts[73:242-t]]).transpose().shape\n",
      "100/197:\n",
      "lr = LinearRegression().fit(np.array([weekcounts[:242-73-t], weekcounts[1:242-72-t], weekcounts[72:241-t], weekcounts[73:242-t]]).transpose(), weekcounts[74:243-t])\n",
      "lr.score(np.array([weekcounts[242-73-t:242-73], weekcounts[242-72-t:242-72], weekcounts[241-t:241], weekcounts[242-t:242]]).transpose(), weekcounts[243-t:243])\n",
      "100/198: t=70\n",
      "100/199: weekcounts[242-73-t:242-73].shape, weekcounts[241-t:241].shape, weekcounts[242-t:242].shape\n",
      "100/200:\n",
      "lr = LinearRegression().fit(np.array([weekcounts[:242-73-t], weekcounts[72:241-t], weekcounts[73:242-t]]).transpose(), weekcounts[74:243-t])\n",
      "lr.score(np.array([weekcounts[242-73-t:242-73], weekcounts[241-t:241], weekcounts[242-t:242]]).transpose(), weekcounts[243-t:243])\n",
      "100/201: np.array([weekcounts[:242-73-t], weekcounts[1:242-72-t], weekcounts[72:241-t], weekcounts[73:242-t]]).transpose().shape\n",
      "100/202:\n",
      "lr = LinearRegression().fit(np.array([weekcounts[:242-73-t], weekcounts[1:242-72-t], weekcounts[72:241-t], weekcounts[73:242-t]]).transpose(), weekcounts[74:243-t])\n",
      "lr.score(np.array([weekcounts[242-73-t:242-73], weekcounts[242-72-t:242-72], weekcounts[241-t:241], weekcounts[242-t:242]]).transpose(), weekcounts[243-t:243])\n",
      "100/203: preds = lr.predict(np.array([weekcounts[242-73-t:242-73], weekcounts[242-72-t:242-72], weekcounts[241-t:241], weekcounts[242-t:242]]).transpose())\n",
      "100/204:\n",
      "plt.plot(weekcounts[243-t:243]-preds, \"o\")\n",
      "plt.title(\"Residuals\")\n",
      "plt.show()\n",
      "100/205: plt.plot([x for x in range(243-t, 243)], preds)\n",
      "100/206: np.sqrt(mean_squared_error(weekcounts[243-t:243], preds))\n",
      "100/207: t=73\n",
      "100/208: weekcounts[242-73-t:242-73].shape, weekcounts[241-t:241].shape, weekcounts[242-t:242].shape\n",
      "100/209:\n",
      "lr = LinearRegression().fit(np.array([weekcounts[:242-73-t], weekcounts[72:241-t], weekcounts[73:242-t]]).transpose(), weekcounts[74:243-t])\n",
      "lr.score(np.array([weekcounts[242-73-t:242-73], weekcounts[241-t:241], weekcounts[242-t:242]]).transpose(), weekcounts[243-t:243])\n",
      "100/210: np.array([weekcounts[:242-73-t], weekcounts[1:242-72-t], weekcounts[72:241-t], weekcounts[73:242-t]]).transpose().shape\n",
      "100/211:\n",
      "lr = LinearRegression().fit(np.array([weekcounts[:242-73-t], weekcounts[1:242-72-t], weekcounts[72:241-t], weekcounts[73:242-t]]).transpose(), weekcounts[74:243-t])\n",
      "lr.score(np.array([weekcounts[242-73-t:242-73], weekcounts[242-72-t:242-72], weekcounts[241-t:241], weekcounts[242-t:242]]).transpose(), weekcounts[243-t:243])\n",
      "100/212: preds = lr.predict(np.array([weekcounts[242-73-t:242-73], weekcounts[242-72-t:242-72], weekcounts[241-t:241], weekcounts[242-t:242]]).transpose())\n",
      "100/213:\n",
      "plt.plot(weekcounts[243-t:243]-preds, \"o\")\n",
      "plt.title(\"Residuals\")\n",
      "plt.show()\n",
      "100/214: plt.plot([x for x in range(243-t, 243)], preds)\n",
      "100/215: np.sqrt(mean_squared_error(weekcounts[243-t:243], preds))\n",
      "100/216:\n",
      "plt.figure()\n",
      "plt.plot([x for x in range(243-t, 243)], preds)\n",
      "plt.plot([x for x in range(243-t, 243)], weekcounts[243-t:243])\n",
      "plt.show()\n",
      "100/217:\n",
      "plt.figure()\n",
      "plt.plot([x for x in range(243-t, 243)], preds)\n",
      "plt.plot([x for x in range(243-t, 243)], weekcounts[243-t:243])\n",
      "plt.legend()\n",
      "plt.show()\n",
      "100/218:\n",
      "plt.figure()\n",
      "plt.plot([x for x in range(243-t, 243)], preds, label=\"predicted\")\n",
      "plt.plot([x for x in range(243-t, 243)], weekcounts[243-t:243], label=\"actual\")\n",
      "plt.legend()\n",
      "plt.show()\n",
      "100/219:\n",
      "plt.figure()\n",
      "plt.plot([x for x in range(243-t, 243)], preds, label=\"predicted\")\n",
      "plt.plot([x for x in range(243-t, 243)], weekcounts[243-t:243], color=\"green\", label=\"actual\")\n",
      "plt.legend()\n",
      "plt.show()\n",
      "100/220:\n",
      "plt.figure()\n",
      "plt.plot([x for x in range(243-t, 243)], weekcounts[243-t:243], label=\"actual\")\n",
      "plt.plot([x for x in range(243-t, 243)], preds, label=\"predicted\")\n",
      "plt.legend()\n",
      "plt.show()\n",
      "100/221:\n",
      "plt.figure()\n",
      "plt.plot([x for x in range(243-t, 243)], weekcounts[243-t:243], label=\"actual\")\n",
      "plt.plot([x for x in range(243-t, 243)], preds, label=\"predicted\")\n",
      "plt.legend()\n",
      "plt.title(\"Chicago crime Apr 2018-Apr 2019\")\n",
      "plt.show()\n",
      "100/222:\n",
      "plt.figure()\n",
      "plt.plot([x for x in range(243-t, 243)], weekcounts[243-t:243], label=\"actual\")\n",
      "plt.plot([x for x in range(243-t, 243)], preds, label=\"predicted\")\n",
      "plt.legend()\n",
      "plt.title(\"Chicago crimes Apr 2018-Apr 2019\")\n",
      "plt.xlabel(\"Week\")\n",
      "plt.ylabel(\"Number of crimes\")\n",
      "plt.show()\n",
      "100/223:\n",
      "plt.plot([x for x in range(243)], weekcounts)\n",
      "plt.ylim(0, 4700)\n",
      "plt.title(\"Time series graph of Chicago crimes since 2016\")\n",
      "plt.xlabel(\"Week\")\n",
      "plt.ylabel(\"Number of crimes\")\n",
      "plt.show()\n",
      "100/224:\n",
      "split = 'district' # community_area\n",
      "lrs = []\n",
      "lrscores = []\n",
      "mses = []\n",
      "for x in df16[split].unique():\n",
      "    lr = LinearRegression()\n",
      "    \n",
      "    df16_ = df16[df16[split] == x].copy()\n",
      "    \n",
      "    weekcounts_ = []\n",
      "    for i in range(243):\n",
      "        if i in df16_['week'].unique():\n",
      "            weekcounts_.append(df16_['week'].value_counts()[i])\n",
      "        else:\n",
      "            weekcounts_.append(0)\n",
      "    weekcounts_ = np.array(weekcounts_)\n",
      "    \n",
      "    X = np.array([weekcounts_[:242-73-t], weekcounts_[1:242-72-t], weekcounts_[72:241-t], weekcounts_[73:242-t]]).transpose()\n",
      "    y = weekcounts_[74:243-t]\n",
      "    \n",
      "    Xt = np.array([weekcounts_[242-73-t:242-73], weekcounts_[242-72-t:242-72], weekcounts_[241-t:241], weekcounts_[242-t:242]]).transpose()\n",
      "    yt = weekcounts_[243-t:243]\n",
      "    \n",
      "    lr.fit(X, y)\n",
      "    s = lr.score(Xt,yt)\n",
      "    print(str(x) + \": \" + str(s))\n",
      "    lrscores.append(s)\n",
      "    lrs.append(lr)\n",
      "100/225:\n",
      "def lrx(k):\n",
      "    lr = LinearRegression()\n",
      "    \n",
      "    df16_ = df16[df16['community_area'] == k].copy()\n",
      "    \n",
      "    weekcounts_ = np.array([df16_['week'].value_counts()[x] for x in range(243)])\n",
      "    \n",
      "    X = np.array([weekcounts_[:242-73-t], weekcounts_[1:242-72-t], weekcounts_[72:241-t], weekcounts_[73:242-t]]).transpose()\n",
      "    y = weekcounts_[74:243-t]\n",
      "    \n",
      "    Xt = np.array([weekcounts_[242-73-t:242-73], weekcounts_[242-72-t:242-72], weekcounts_[241-t:241], weekcounts_[242-t:242]]).transpose()\n",
      "    yt = weekcounts_[243-t:243]\n",
      "    \n",
      "    lr.fit(X, y)\n",
      "    s = lr.score(Xt,yt)\n",
      "    print(str(k) + \": \" + str(s))\n",
      "    \n",
      "    plt.plot([x for x in range(243)], weekcounts_)\n",
      "100/226: lrx(25)\n",
      "100/227:\n",
      "split = 'district' # community_area\n",
      "lrs = []\n",
      "lrscores = []\n",
      "rmses = []\n",
      "for x in df16[split].unique():\n",
      "    lr = LinearRegression()\n",
      "    \n",
      "    df16_ = df16[df16[split] == x].copy()\n",
      "    \n",
      "    weekcounts_ = []\n",
      "    for i in range(243):\n",
      "        if i in df16_['week'].unique():\n",
      "            weekcounts_.append(df16_['week'].value_counts()[i])\n",
      "        else:\n",
      "            weekcounts_.append(0)\n",
      "    weekcounts_ = np.array(weekcounts_)\n",
      "    \n",
      "    X = np.array([weekcounts_[:242-73-t], weekcounts_[1:242-72-t], weekcounts_[72:241-t], weekcounts_[73:242-t]]).transpose()\n",
      "    y = weekcounts_[74:243-t]\n",
      "    \n",
      "    Xt = np.array([weekcounts_[242-73-t:242-73], weekcounts_[242-72-t:242-72], weekcounts_[241-t:241], weekcounts_[242-t:242]]).transpose()\n",
      "    yt = weekcounts_[243-t:243]\n",
      "    \n",
      "    lr.fit(X, y)\n",
      "    s = lr.score(Xt,yt)\n",
      "    yp = lr.predict(Xt)\n",
      "    \n",
      "    rmse = np.sqrt(mean_squared_error(yt, yp))\n",
      "    \n",
      "    print(str(x) + \": \" + str(s) + rmse)\n",
      "    lrscores.append(s)\n",
      "    lrs.append(lr)\n",
      "    rmses.append(rmse)\n",
      "100/228:\n",
      "split = 'district' # community_area\n",
      "lrs = []\n",
      "lrscores = []\n",
      "rmses = []\n",
      "for x in df16[split].unique():\n",
      "    lr = LinearRegression()\n",
      "    \n",
      "    df16_ = df16[df16[split] == x].copy()\n",
      "    \n",
      "    weekcounts_ = []\n",
      "    for i in range(243):\n",
      "        if i in df16_['week'].unique():\n",
      "            weekcounts_.append(df16_['week'].value_counts()[i])\n",
      "        else:\n",
      "            weekcounts_.append(0)\n",
      "    weekcounts_ = np.array(weekcounts_)\n",
      "    \n",
      "    X = np.array([weekcounts_[:242-73-t], weekcounts_[1:242-72-t], weekcounts_[72:241-t], weekcounts_[73:242-t]]).transpose()\n",
      "    y = weekcounts_[74:243-t]\n",
      "    \n",
      "    Xt = np.array([weekcounts_[242-73-t:242-73], weekcounts_[242-72-t:242-72], weekcounts_[241-t:241], weekcounts_[242-t:242]]).transpose()\n",
      "    yt = weekcounts_[243-t:243]\n",
      "    \n",
      "    lr.fit(X, y)\n",
      "    s = lr.score(Xt,yt)\n",
      "    yp = lr.predict(Xt)\n",
      "    \n",
      "    rmse = np.sqrt(mean_squared_error(yt, yp))\n",
      "    \n",
      "    print(str(x) + \": \" + str(s) + str(rmse))\n",
      "    lrscores.append(s)\n",
      "    lrs.append(lr)\n",
      "    rmses.append(rmse)\n",
      "100/229:\n",
      "split = 'district' # community_area\n",
      "lrs = []\n",
      "lrscores = []\n",
      "rmses = []\n",
      "for x in df16[split].unique():\n",
      "    lr = LinearRegression()\n",
      "    \n",
      "    df16_ = df16[df16[split] == x].copy()\n",
      "    \n",
      "    weekcounts_ = []\n",
      "    for i in range(243):\n",
      "        if i in df16_['week'].unique():\n",
      "            weekcounts_.append(df16_['week'].value_counts()[i])\n",
      "        else:\n",
      "            weekcounts_.append(0)\n",
      "    weekcounts_ = np.array(weekcounts_)\n",
      "    \n",
      "    X = np.array([weekcounts_[:242-73-t], weekcounts_[1:242-72-t], weekcounts_[72:241-t], weekcounts_[73:242-t]]).transpose()\n",
      "    y = weekcounts_[74:243-t]\n",
      "    \n",
      "    Xt = np.array([weekcounts_[242-73-t:242-73], weekcounts_[242-72-t:242-72], weekcounts_[241-t:241], weekcounts_[242-t:242]]).transpose()\n",
      "    yt = weekcounts_[243-t:243]\n",
      "    \n",
      "    lr.fit(X, y)\n",
      "    s = lr.score(Xt,yt)\n",
      "    yp = lr.predict(Xt)\n",
      "    \n",
      "    rmse = np.sqrt(mean_squared_error(yt, yp))\n",
      "    \n",
      "    print(str(x) + \": \" + str(s) + \" \" +  str(rmse))\n",
      "    lrscores.append(s)\n",
      "    lrs.append(lr)\n",
      "    rmses.append(rmse)\n",
      "100/230:\n",
      "def predictsplit(split)\n",
      "    lrs = []\n",
      "    lrscores = []\n",
      "    rmses = []\n",
      "    for x in df16[split].unique():\n",
      "        lr = LinearRegression()\n",
      "\n",
      "        df16_ = df16[df16[split] == x].copy()\n",
      "\n",
      "        weekcounts_ = []\n",
      "        for i in range(243):\n",
      "            if i in df16_['week'].unique():\n",
      "                weekcounts_.append(df16_['week'].value_counts()[i])\n",
      "            else:\n",
      "                weekcounts_.append(0)\n",
      "        weekcounts_ = np.array(weekcounts_)\n",
      "\n",
      "        X = np.array([weekcounts_[:242-73-t], weekcounts_[1:242-72-t], weekcounts_[72:241-t], weekcounts_[73:242-t]]).transpose()\n",
      "        y = weekcounts_[74:243-t]\n",
      "\n",
      "        Xt = np.array([weekcounts_[242-73-t:242-73], weekcounts_[242-72-t:242-72], weekcounts_[241-t:241], weekcounts_[242-t:242]]).transpose()\n",
      "        yt = weekcounts_[243-t:243]\n",
      "\n",
      "        lr.fit(X, y)\n",
      "        s = lr.score(Xt,yt)\n",
      "        yp = lr.predict(Xt)\n",
      "\n",
      "        rmse = np.sqrt(mean_squared_error(yt, yp))\n",
      "\n",
      "        print(str(x) + \": \" + str(s) + \" \" + str(rmse))\n",
      "        lrscores.append(s)\n",
      "        lrs.append(lr)\n",
      "        rmses.append(rmse)\n",
      "    return lrs, lrscores, rmses\n",
      "100/231:\n",
      "def predictsplit(split):\n",
      "    lrs = []\n",
      "    lrscores = []\n",
      "    rmses = []\n",
      "    for x in df16[split].unique():\n",
      "        lr = LinearRegression()\n",
      "\n",
      "        df16_ = df16[df16[split] == x].copy()\n",
      "\n",
      "        weekcounts_ = []\n",
      "        for i in range(243):\n",
      "            if i in df16_['week'].unique():\n",
      "                weekcounts_.append(df16_['week'].value_counts()[i])\n",
      "            else:\n",
      "                weekcounts_.append(0)\n",
      "        weekcounts_ = np.array(weekcounts_)\n",
      "\n",
      "        X = np.array([weekcounts_[:242-73-t], weekcounts_[1:242-72-t], weekcounts_[72:241-t], weekcounts_[73:242-t]]).transpose()\n",
      "        y = weekcounts_[74:243-t]\n",
      "\n",
      "        Xt = np.array([weekcounts_[242-73-t:242-73], weekcounts_[242-72-t:242-72], weekcounts_[241-t:241], weekcounts_[242-t:242]]).transpose()\n",
      "        yt = weekcounts_[243-t:243]\n",
      "\n",
      "        lr.fit(X, y)\n",
      "        s = lr.score(Xt,yt)\n",
      "        yp = lr.predict(Xt)\n",
      "\n",
      "        rmse = np.sqrt(mean_squared_error(yt, yp))\n",
      "\n",
      "        print(str(x) + \": \" + str(s) + \" \" + str(rmse))\n",
      "        lrscores.append(s)\n",
      "        lrs.append(lr)\n",
      "        rmses.append(rmse)\n",
      "    return lrs, lrscores, rmses\n",
      "100/232:\n",
      "def lrx(k):\n",
      "    lr = LinearRegression()\n",
      "    \n",
      "    df16_ = df16[df16['community_area'] == k].copy()\n",
      "    \n",
      "    weekcounts_ = np.array([df16_['week'].value_counts()[x] for x in range(243)])\n",
      "    \n",
      "    X = np.array([weekcounts_[:242-73-t], weekcounts_[1:242-72-t], weekcounts_[72:241-t], weekcounts_[73:242-t]]).transpose()\n",
      "    y = weekcounts_[74:243-t]\n",
      "    \n",
      "    Xt = np.array([weekcounts_[242-73-t:242-73], weekcounts_[242-72-t:242-72], weekcounts_[241-t:241], weekcounts_[242-t:242]]).transpose()\n",
      "    yt = weekcounts_[243-t:243]\n",
      "    \n",
      "    lr.fit(X, y)\n",
      "    s = lr.score(Xt,yt)\n",
      "    rmse = np.sqrt(mean_squared_error(yt, yp))\n",
      "    print(str(k) + \": \" + str(s) + \" \" + rmse)\n",
      "    \n",
      "    plt.plot([x for x in range(243)], weekcounts_)\n",
      "100/233: lrx(25)\n",
      "100/234:\n",
      "def lrx(k):\n",
      "    lr = LinearRegression()\n",
      "    \n",
      "    df16_ = df16[df16['community_area'] == k].copy()\n",
      "    \n",
      "    weekcounts_ = np.array([df16_['week'].value_counts()[x] for x in range(243)])\n",
      "    \n",
      "    X = np.array([weekcounts_[:242-73-t], weekcounts_[1:242-72-t], weekcounts_[72:241-t], weekcounts_[73:242-t]]).transpose()\n",
      "    y = weekcounts_[74:243-t]\n",
      "    \n",
      "    Xt = np.array([weekcounts_[242-73-t:242-73], weekcounts_[242-72-t:242-72], weekcounts_[241-t:241], weekcounts_[242-t:242]]).transpose()\n",
      "    yt = weekcounts_[243-t:243]\n",
      "    \n",
      "    lr.fit(X, y)\n",
      "    s = lr.score(Xt,yt)\n",
      "    rmse = np.sqrt(mean_squared_error(yt, yp))\n",
      "    print(str(k) + \": \" + str(s) + \" \" + str(rmse))\n",
      "    \n",
      "    plt.plot([x for x in range(243)], weekcounts_)\n",
      "100/235: lrx(25)\n",
      "100/236:\n",
      "def lrx(k):\n",
      "    lr = LinearRegression()\n",
      "    \n",
      "    df16_ = df16[df16['community_area'] == k].copy()\n",
      "    \n",
      "    weekcounts_ = np.array([df16_['week'].value_counts()[x] for x in range(243)])\n",
      "    \n",
      "    X = np.array([weekcounts_[:242-73-t], weekcounts_[1:242-72-t], weekcounts_[72:241-t], weekcounts_[73:242-t]]).transpose()\n",
      "    y = weekcounts_[74:243-t]\n",
      "    \n",
      "    Xt = np.array([weekcounts_[242-73-t:242-73], weekcounts_[242-72-t:242-72], weekcounts_[241-t:241], weekcounts_[242-t:242]]).transpose()\n",
      "    yt = weekcounts_[243-t:243]\n",
      "    \n",
      "    lr.fit(X, y)\n",
      "    s = lr.score(Xt,yt)\n",
      "    yp = lr.predict(Xt)\n",
      "    rmse = np.sqrt(mean_squared_error(yt, yp))\n",
      "    print(str(k) + \": \" + str(s) + \" \" + str(rmse))\n",
      "    \n",
      "    plt.plot([x for x in range(243)], weekcounts_)\n",
      "100/237: lrx(25)\n",
      "100/238: lrs_ca, lrscores_ca, rmses_ca = predictsplit('community_area')\n",
      "100/239: lrs_d, lrscores_d, rmses_d = predictsplit('district')\n",
      "100/240:\n",
      "def lrx(k, split):\n",
      "    lr = LinearRegression()\n",
      "    \n",
      "    df16_ = df16[df16[split] == k].copy()\n",
      "    \n",
      "    weekcounts_ = np.array([df16_['week'].value_counts()[x] for x in range(243)])\n",
      "    \n",
      "    X = np.array([weekcounts_[:242-73-t], weekcounts_[1:242-72-t], weekcounts_[72:241-t], weekcounts_[73:242-t]]).transpose()\n",
      "    y = weekcounts_[74:243-t]\n",
      "    \n",
      "    Xt = np.array([weekcounts_[242-73-t:242-73], weekcounts_[242-72-t:242-72], weekcounts_[241-t:241], weekcounts_[242-t:242]]).transpose()\n",
      "    yt = weekcounts_[243-t:243]\n",
      "    \n",
      "    lr.fit(X, y)\n",
      "    s = lr.score(Xt,yt)\n",
      "    yp = lr.predict(Xt)\n",
      "    rmse = np.sqrt(mean_squared_error(yt, yp))\n",
      "    print(str(k) + \": \" + str(s) + \" \" + str(rmse))\n",
      "    \n",
      "    plt.plot([x for x in range(243)], weekcounts_)\n",
      "100/241: lrx(25, 'community_area')\n",
      "100/242:\n",
      "def lrx(k, split):\n",
      "    lr = LinearRegression()\n",
      "    \n",
      "    df16_ = df16[df16[split] == k].copy()\n",
      "    \n",
      "    weekcounts_ = np.array([df16_['week'].value_counts()[x] for x in range(243)])\n",
      "    \n",
      "    X = np.array([weekcounts_[:242-73-t], weekcounts_[1:242-72-t], weekcounts_[72:241-t], weekcounts_[73:242-t]]).transpose()\n",
      "    y = weekcounts_[74:243-t]\n",
      "    \n",
      "    Xt = np.array([weekcounts_[242-73-t:242-73], weekcounts_[242-72-t:242-72], weekcounts_[241-t:241], weekcounts_[242-t:242]]).transpose()\n",
      "    yt = weekcounts_[243-t:243]\n",
      "    \n",
      "    lr.fit(X, y)\n",
      "    s = lr.score(Xt,yt)\n",
      "    yp = lr.predict(Xt)\n",
      "    rmse = np.sqrt(mean_squared_error(yt, yp))\n",
      "    print(str(k) + \": \" + str(s) + \" \" + str(rmse))\n",
      "    \n",
      "    plt.plot([x for x in range(243)], weekcounts_)\n",
      "    plt.figure()\n",
      "    plt.plot([x for x in range(243-t, 243)], weekcounts_[243-t:243], label=\"actual\")\n",
      "    plt.plot([x for x in range(243-t, 243)], yp, label=\"predicted\")\n",
      "    plt.legend()\n",
      "100/243: lrx(25, 'community_area')\n",
      "100/244: lrx(5, 'district')\n",
      "100/245: df16v = df16[df16['violent']==1].copy()\n",
      "100/246: df2['violent'].shape\n",
      "100/247: ## Gaussian Process Regression\n",
      "100/248:\n",
      "lr = LinearRegression().fit(np.array([weekcounts[:242-73], weekcounts[72:241], weekcounts[73:242]]).transpose(), weekcounts[74:243])\n",
      "lr.score(np.array([weekcounts[:242-73], weekcounts[72:241], weekcounts[73:242]]).transpose(), weekcounts[74:243])\n",
      "100/249: lr.coef_\n",
      "100/250:\n",
      "lr = LinearRegression().fit(weekcounts[:242].reshape(-1, 1), weekcounts[1:243])\n",
      "lr.score(weekcounts[:242].reshape(-1, 1), weekcounts[1:243])\n",
      "100/251: lr.coef_\n",
      "100/252:\n",
      "lr = LinearRegression().fit(np.array([weekcounts[1:242], weekcounts[:241]]).transpose(), weekcounts[2:243])\n",
      "lr.score(np.array([weekcounts[1:242], weekcounts[:241]]).transpose(), weekcounts[2:243])\n",
      "100/253:\n",
      "lr = LinearRegression().fit(np.array([weekcounts[:242-73], weekcounts[72:241], weekcounts[73:242]]).transpose(), weekcounts[74:243])\n",
      "lr.score(np.array([weekcounts[:242-73], weekcounts[72:241], weekcounts[73:242]]).transpose(), weekcounts[74:243])\n",
      "100/254:\n",
      "lr = LinearRegression().fit(np.array([weekcounts[1:242], weekcounts[:241]]).transpose(), weekcounts[2:243])\n",
      "lr.score(np.array([weekcounts[1:242], weekcounts[:241]]).transpose(), weekcounts[2:243])\n",
      "100/255: lr.coef_\n",
      "100/256: from sklearn.gaussian_process import GaussianProcessRegressor\n",
      "100/257:\n",
      "X = np.array([weekcounts[:242-73-t], weekcounts[1:242-72-t], weekcounts[72:241-t], weekcounts[73:242-t]]).transpose()\n",
      "y = weekcounts[74:243-t]\n",
      "100/258:\n",
      "Xt = np.array([weekcounts[242-73-t:242-73], weekcounts[242-72-t:242-72], weekcounts[241-t:241], weekcounts[242-t:242]]).transpose()\n",
      "yt = weekcounts[243-t:243]\n",
      "100/259:\n",
      "gp = GaussianProcessRegressor()\n",
      "gp.fit(X, y)\n",
      "preds = gp.predict(Xt)\n",
      "np.sqrt(mean_squared_error(yt, preds)) # RMSE\n",
      "100/260:\n",
      "gp = GaussianProcessRegressor(normalize_y=True)\n",
      "gp.fit(X, y)\n",
      "preds = gp.predict(Xt)\n",
      "np.sqrt(mean_squared_error(yt, preds)) # RMSE\n",
      "100/261:\n",
      "gp = GaussianProcessRegressor(normalize_y=True)\n",
      "gp.fit(X, y)\n",
      "gp.score(X, y)\n",
      "preds = gp.predict(Xt)\n",
      "np.sqrt(mean_squared_error(yt, preds)) # RMSE\n",
      "100/262:\n",
      "gp = GaussianProcessRegressor(normalize_y=True)\n",
      "gp.fit(X, y)\n",
      "print(gp.score(X, y))\n",
      "preds = gp.predict(Xt)\n",
      "np.sqrt(mean_squared_error(yt, preds)) # RMSE\n",
      "100/263:\n",
      "gp = GaussianProcessRegressor(normalize_y=True)\n",
      "gp.fit(X, y)\n",
      "print(gp.score(X, y))\n",
      "preds = gp.predict(Xt)\n",
      "print(gp.score(Xt, yt))\n",
      "np.sqrt(mean_squared_error(yt, preds)) # RMSE\n",
      "100/264:\n",
      "gp = GaussianProcessRegressor(normalize_y=True)\n",
      "gp.fit(X, y)\n",
      "print(gp.score(X, y))\n",
      "preds = gp.predict(Xt)\n",
      "np.sqrt(mean_squared_error(yt, preds)) # RMSE\n",
      "100/265:\n",
      "gp = GaussianProcessRegressor()\n",
      "gp.fit(X, y)\n",
      "print(gp.score(X, y))\n",
      "preds = gp.predict(Xt)\n",
      "np.sqrt(mean_squared_error(yt, preds)) # RMSE\n",
      "100/266:\n",
      "gp = GaussianProcessRegressor(normalize_y=True)\n",
      "gp.fit(X, y)\n",
      "print(gp.score(X, y))\n",
      "preds = gp.predict(Xt)\n",
      "np.sqrt(mean_squared_error(yt, preds)) # RMSE\n",
      "100/267: plt.plot([x for x in range(243-t, 243)], preds, label=\"predicted\")\n",
      "100/268: from sklearn.ensemble import RandomForestRegressor\n",
      "100/269:\n",
      "gp = RandomForestRegressor(normalize_y=True)\n",
      "gp.fit(X, y)\n",
      "print(gp.score(X, y))\n",
      "preds = gp.predict(Xt)\n",
      "np.sqrt(mean_squared_error(yt, preds)) # RMSE\n",
      "100/270:\n",
      "gp = RandomForestRegressor()\n",
      "gp.fit(X, y)\n",
      "print(gp.score(X, y))\n",
      "preds = gp.predict(Xt)\n",
      "np.sqrt(mean_squared_error(yt, preds)) # RMSE\n",
      "100/271:\n",
      "gp = RandomForestRegressor(n_estimators=100)\n",
      "gp.fit(X, y)\n",
      "print(gp.score(X, y))\n",
      "preds = gp.predict(Xt)\n",
      "np.sqrt(mean_squared_error(yt, preds)) # RMSE\n",
      "100/272: plt.plot([x for x in range(243-t, 243)], preds, label=\"predicted\")\n",
      "100/273:\n",
      "plt.plot([x for x in range(243-t, 243)], weekcounts[243-t:243], label=\"actual\")\n",
      "\n",
      "plt.plot([x for x in range(243-t, 243)], preds, label=\"predicted\")\n",
      "100/274:\n",
      "plt.plot([x for x in range(243-t, 243)], weekcounts[243-t:243], label=\"actual\")\n",
      "plt.plot([x for x in range(243-t, 243)], preds, label=\"predicted\")\n",
      "100/275:\n",
      "gp = GaussianProcessRegressor()\n",
      "gp.fit(X, y)\n",
      "print(gp.score(X, y))\n",
      "preds = gp.predict(Xt)\n",
      "np.sqrt(mean_squared_error(yt, preds)) # RMSE\n",
      "100/276: plt.plot([x for x in range(243-t, 243)], preds, label=\"predicted\")\n",
      "100/277:\n",
      "gp = GaussianProcessRegressor(normalize_y=True)\n",
      "gp.fit(X, y)\n",
      "print(gp.score(X, y))\n",
      "preds = gp.predict(Xt)\n",
      "np.sqrt(mean_squared_error(yt, preds)) # RMSE\n",
      "100/278:\n",
      "from sklearn.gaussian_process import GaussianProcessRegressor\n",
      "from sklearn.gaussian_process.kernels import ExpSineSquared\n",
      "100/279:\n",
      "gp = GaussianProcessRegressor(kernel=ExpSineSquared(periodicity=73), normalize_y=True)\n",
      "gp.fit(X, y)\n",
      "print(gp.score(X, y))\n",
      "preds = gp.predict(Xt)\n",
      "np.sqrt(mean_squared_error(yt, preds)) # RMSE\n",
      "100/280:\n",
      "gp = GaussianProcessRegressor(kernel=ExpSineSquared(), normalize_y=True)\n",
      "gp.fit(X, y)\n",
      "print(gp.score(X, y))\n",
      "preds = gp.predict(Xt)\n",
      "np.sqrt(mean_squared_error(yt, preds)) # RMSE\n",
      "100/281:\n",
      "gp = GaussianProcessRegressor(kernel=ExpSineSquared(length_scale=1.44,periodicity=1), normalize_y=True)\n",
      "gp.fit(X, y)\n",
      "print(gp.score(X, y))\n",
      "preds = gp.predict(Xt)\n",
      "np.sqrt(mean_squared_error(yt, preds)) # RMSE\n",
      "100/282:\n",
      "gp = GaussianProcessRegressor(, normalize_y=True)\n",
      "gp.fit(X, y)\n",
      "print(gp.score(X, y))\n",
      "preds = gp.predict(Xt)\n",
      "np.sqrt(mean_squared_error(yt, preds)) # RMSE\n",
      "100/283:\n",
      "gp = GaussianProcessRegressor(normalize_y=True)\n",
      "gp.fit(X, y)\n",
      "print(gp.score(X, y))\n",
      "preds = gp.predict(Xt)\n",
      "np.sqrt(mean_squared_error(yt, preds)) # RMSE\n",
      "100/284: plt.plot([x for x in range(243-t, 243)], preds, label=\"predicted\")\n",
      "100/285:\n",
      "plt.figure()\n",
      "plt.plot([x for x in range(243-t, 243)], weekcounts[243-t:243], label=\"actual\")\n",
      "plt.plot([x for x in range(243-t, 243)], preds, label=\"predicted\")\n",
      "plt.plot([x for x in range(243-t, 243)], weekcounts[242-t:242], label=\"persistence\")\n",
      "\n",
      "plt.legend()\n",
      "plt.title(\"Chicago crimes Apr 2018-Apr 2019\")\n",
      "plt.xlabel(\"Week\")\n",
      "plt.ylabel(\"Number of crimes\")\n",
      "plt.show()\n",
      "100/286:\n",
      "lr = LinearRegression().fit(np.array([weekcounts[:242-73-t], weekcounts[1:242-72-t], weekcounts[72:241-t], weekcounts[73:242-t]]).transpose(), weekcounts[74:243-t])\n",
      "lr.score(np.array([weekcounts[242-73-t:242-73], weekcounts[242-72-t:242-72], weekcounts[241-t:241], weekcounts[242-t:242]]).transpose(), weekcounts[243-t:243])\n",
      "100/287: preds = lr.predict(np.array([weekcounts[242-73-t:242-73], weekcounts[242-72-t:242-72], weekcounts[241-t:241], weekcounts[242-t:242]]).transpose())\n",
      "100/288:\n",
      "plt.plot(weekcounts[243-t:243]-preds, \"o\")\n",
      "plt.title(\"Residuals\")\n",
      "plt.show()\n",
      "100/289:\n",
      "plt.figure()\n",
      "plt.plot([x for x in range(243-t, 243)], weekcounts[243-t:243], label=\"actual\")\n",
      "plt.plot([x for x in range(243-t, 243)], preds, label=\"predicted\")\n",
      "plt.plot([x for x in range(243-t, 243)], weekcounts[242-t:242], label=\"persistence\")\n",
      "\n",
      "plt.legend()\n",
      "plt.title(\"Chicago crimes Apr 2018-Apr 2019\")\n",
      "plt.xlabel(\"Week\")\n",
      "plt.ylabel(\"Number of crimes\")\n",
      "plt.show()\n",
      "100/290: np.sqrt(mean_squared_error(weekcounts[243-t:243], weekcounts[242-t:242])) # RMSE\n",
      "100/291:\n",
      "lr = LinearRegression()\n",
      "lr.fit(np.array(weekcounts[242-t:242]).reshape(-1, 1), weekcounts[243-t:243])\n",
      "lr.score(np.array(weekcounts[242-t:242]).reshape(-1, 1), weekcounts[243-t:243])\n",
      "100/292: t=70\n",
      "100/293: weekcounts[242-73-t:242-73].shape, weekcounts[241-t:241].shape, weekcounts[242-t:242].shape\n",
      "100/294:\n",
      "lr = LinearRegression().fit(np.array([weekcounts[:242-73-t], weekcounts[72:241-t], weekcounts[73:242-t]]).transpose(), weekcounts[74:243-t])\n",
      "lr.score(np.array([weekcounts[242-73-t:242-73], weekcounts[241-t:241], weekcounts[242-t:242]]).transpose(), weekcounts[243-t:243])\n",
      "100/295: np.array([weekcounts[:242-73-t], weekcounts[1:242-72-t], weekcounts[72:241-t], weekcounts[73:242-t]]).transpose().shape\n",
      "100/296:\n",
      "lr = LinearRegression().fit(np.array([weekcounts[:242-73-t], weekcounts[1:242-72-t], weekcounts[72:241-t], weekcounts[73:242-t]]).transpose(), weekcounts[74:243-t])\n",
      "lr.score(np.array([weekcounts[242-73-t:242-73], weekcounts[242-72-t:242-72], weekcounts[241-t:241], weekcounts[242-t:242]]).transpose(), weekcounts[243-t:243])\n",
      "100/297: preds = lr.predict(np.array([weekcounts[242-73-t:242-73], weekcounts[242-72-t:242-72], weekcounts[241-t:241], weekcounts[242-t:242]]).transpose())\n",
      "100/298:\n",
      "plt.plot(weekcounts[243-t:243]-preds, \"o\")\n",
      "plt.title(\"Residuals\")\n",
      "plt.show()\n",
      "100/299:\n",
      "plt.figure()\n",
      "plt.plot([x for x in range(243-t, 243)], weekcounts[243-t:243], label=\"actual\")\n",
      "plt.plot([x for x in range(243-t, 243)], preds, label=\"predicted\")\n",
      "plt.plot([x for x in range(243-t, 243)], weekcounts[242-t:242], label=\"persistence\")\n",
      "\n",
      "plt.legend()\n",
      "plt.title(\"Chicago crimes Apr 2018-Apr 2019\")\n",
      "plt.xlabel(\"Week\")\n",
      "plt.ylabel(\"Number of crimes\")\n",
      "plt.show()\n",
      "100/300: np.sqrt(mean_squared_error(weekcounts[243-t:243], preds)) # RMSE\n",
      "100/301: np.sqrt(mean_squared_error(weekcounts[243-t:243], weekcounts[242-t:242])) # RMSE\n",
      "100/302:\n",
      "plt.figure()\n",
      "plt.plot([x for x in range(243-t, 243)], weekcounts[243-t:243], label=\"actual\")\n",
      "plt.plot([x for x in range(243-t, 243)], preds, label=\"predicted\")\n",
      "# plt.plot([x for x in range(243-t, 243)], weekcounts[242-t:242], label=\"persistence\")\n",
      "\n",
      "plt.legend()\n",
      "plt.title(\"Chicago crimes Apr 2018-Apr 2019\")\n",
      "plt.xlabel(\"Week\")\n",
      "plt.ylabel(\"Number of crimes\")\n",
      "plt.show()\n",
      "100/303:\n",
      "plt.figure()\n",
      "# plt.plot([x for x in range(243-t, 243)], weekcounts[243-t:243], label=\"actual\")\n",
      "plt.plot([x for x in range(243)], weekcounts[:243], label=\"actual\")\n",
      "\n",
      "plt.plot([x for x in range(243-t, 243)], preds, label=\"predicted\")\n",
      "# plt.plot([x for x in range(243-t, 243)], weekcounts[242-t:242], label=\"persistence\")\n",
      "\n",
      "plt.legend()\n",
      "plt.title(\"Chicago crimes Apr 2018-Apr 2019\")\n",
      "plt.xlabel(\"Week\")\n",
      "plt.ylabel(\"Number of crimes\")\n",
      "plt.show()\n",
      "100/304:\n",
      "lrp = lr.predict(np.array(weekcounts[242-t:242]).reshape(-1, 1))\n",
      "np.sqrt(mean_squared_error(weekcounts[243-t:243], lrp))\n",
      "100/305:\n",
      "lrp = lr.predict(np.array(weekcounts[242-t:242]).reshape(-1, 1))\n",
      "np.sqrt(mean_squared_error(weekcounts[243-t:243], lrp)\n",
      "100/306:\n",
      "lrp = lr.predict(np.array(weekcounts[242-t:242]).reshape(-1, 1))\n",
      "np.sqrt(mean_squared_error(weekcounts[243-t:243], lrp))\n",
      "100/307:\n",
      "lr = LinearRegression()\n",
      "lr.fit(np.array(weekcounts[242-t:242]).reshape(-1, 1), weekcounts[243-t:243])\n",
      "lr.score(np.array(weekcounts[242-t:242]).reshape(-1, 1), weekcounts[243-t:243])\n",
      "100/308:\n",
      "lrp = lr.predict(np.array(weekcounts[242-t:242]).reshape(-1, 1))\n",
      "np.sqrt(mean_squared_error(weekcounts[243-t:243], lrp))\n",
      "100/309: np.sqrt(mean_squared_error(weekcounts[243-t:243], weekcounts[242-t:242])) # RMSE\n",
      "100/310: plt.plot([x for x in range(243-t:243)], lrp)\n",
      "100/311: plt.plot([x for x in range(243-t,243)], lrp)\n",
      "100/312:\n",
      "plt.plot([x for x in range(243-t,243)], weekcounts[243-t:243])\n",
      "plt.plot([x for x in range(243-t,243)], lrp)\n",
      "100/313:\n",
      "plt.figure()\n",
      "plt.plot([x for x in range(243-t, 243)], weekcounts[243-t:243], label=\"actual\")\n",
      "# plt.plot([x for x in range(243)], weekcounts[:243], label=\"actual\")\n",
      "\n",
      "plt.plot([x for x in range(243-t, 243)], preds, label=\"predicted\")\n",
      "# plt.plot([x for x in range(243-t, 243)], weekcounts[242-t:242], label=\"persistence\")\n",
      "\n",
      "plt.legend()\n",
      "plt.title(\"Chicago crimes Apr 2018-Apr 2019\")\n",
      "plt.xlabel(\"Week\")\n",
      "plt.ylabel(\"Number of crimes\")\n",
      "plt.show()\n",
      "100/314: t=48\n",
      "100/315: weekcounts[242-73-t:242-73].shape, weekcounts[241-t:241].shape, weekcounts[242-t:242].shape\n",
      "100/316:\n",
      "lr = LinearRegression().fit(np.array([weekcounts[:242-73-t], weekcounts[72:241-t], weekcounts[73:242-t]]).transpose(), weekcounts[74:243-t])\n",
      "lr.score(np.array([weekcounts[242-73-t:242-73], weekcounts[241-t:241], weekcounts[242-t:242]]).transpose(), weekcounts[243-t:243])\n",
      "100/317: np.array([weekcounts[:242-73-t], weekcounts[1:242-72-t], weekcounts[72:241-t], weekcounts[73:242-t]]).transpose().shape\n",
      "100/318:\n",
      "lr = LinearRegression().fit(np.array([weekcounts[:242-73-t], weekcounts[1:242-72-t], weekcounts[72:241-t], weekcounts[73:242-t]]).transpose(), weekcounts[74:243-t])\n",
      "lr.score(np.array([weekcounts[242-73-t:242-73], weekcounts[242-72-t:242-72], weekcounts[241-t:241], weekcounts[242-t:242]]).transpose(), weekcounts[243-t:243])\n",
      "100/319: preds = lr.predict(np.array([weekcounts[242-73-t:242-73], weekcounts[242-72-t:242-72], weekcounts[241-t:241], weekcounts[242-t:242]]).transpose())\n",
      "100/320:\n",
      "plt.plot(weekcounts[243-t:243]-preds, \"o\")\n",
      "plt.title(\"Residuals\")\n",
      "plt.show()\n",
      "100/321:\n",
      "plt.figure()\n",
      "plt.plot([x for x in range(243-t, 243)], weekcounts[243-t:243], label=\"actual\")\n",
      "# plt.plot([x for x in range(243)], weekcounts[:243], label=\"actual\")\n",
      "\n",
      "plt.plot([x for x in range(243-t, 243)], preds, label=\"predicted\")\n",
      "# plt.plot([x for x in range(243-t, 243)], weekcounts[242-t:242], label=\"persistence\")\n",
      "\n",
      "plt.legend()\n",
      "plt.title(\"Chicago crimes Apr 2018-Apr 2019\")\n",
      "plt.xlabel(\"Week\")\n",
      "plt.ylabel(\"Number of crimes\")\n",
      "plt.show()\n",
      "100/322: np.sqrt(mean_squared_error(weekcounts[243-t:243], preds)) # RMSE\n",
      "100/323: np.sqrt(mean_squared_error(weekcounts[243-t:243], weekcounts[242-t:242])) # RMSE\n",
      "100/324: t=70\n",
      "100/325: weekcounts[242-73-t:242-73].shape, weekcounts[241-t:241].shape, weekcounts[242-t:242].shape\n",
      "100/326:\n",
      "lr = LinearRegression().fit(np.array([weekcounts[:242-73-t], weekcounts[72:241-t], weekcounts[73:242-t]]).transpose(), weekcounts[74:243-t])\n",
      "lr.score(np.array([weekcounts[242-73-t:242-73], weekcounts[241-t:241], weekcounts[242-t:242]]).transpose(), weekcounts[243-t:243])\n",
      "100/327: np.array([weekcounts[:242-73-t], weekcounts[1:242-72-t], weekcounts[72:241-t], weekcounts[73:242-t]]).transpose().shape\n",
      "100/328:\n",
      "lr = LinearRegression().fit(np.array([weekcounts[:242-73-t], weekcounts[1:242-72-t], weekcounts[72:241-t], weekcounts[73:242-t]]).transpose(), weekcounts[74:243-t])\n",
      "lr.score(np.array([weekcounts[242-73-t:242-73], weekcounts[242-72-t:242-72], weekcounts[241-t:241], weekcounts[242-t:242]]).transpose(), weekcounts[243-t:243])\n",
      "100/329: preds = lr.predict(np.array([weekcounts[242-73-t:242-73], weekcounts[242-72-t:242-72], weekcounts[241-t:241], weekcounts[242-t:242]]).transpose())\n",
      "100/330:\n",
      "plt.plot(weekcounts[243-t:243]-preds, \"o\")\n",
      "plt.title(\"Residuals\")\n",
      "plt.show()\n",
      "100/331:\n",
      "plt.figure()\n",
      "plt.plot([x for x in range(243-t, 243)], weekcounts[243-t:243], label=\"actual\")\n",
      "# plt.plot([x for x in range(243)], weekcounts[:243], label=\"actual\")\n",
      "\n",
      "plt.plot([x for x in range(243-t, 243)], preds, label=\"predicted\")\n",
      "# plt.plot([x for x in range(243-t, 243)], weekcounts[242-t:242], label=\"persistence\")\n",
      "\n",
      "plt.legend()\n",
      "plt.title(\"Chicago crimes Apr 2018-Apr 2019\")\n",
      "plt.xlabel(\"Week\")\n",
      "plt.ylabel(\"Number of crimes\")\n",
      "plt.show()\n",
      "100/332: np.sqrt(mean_squared_error(weekcounts[243-t:243], preds)) # RMSE\n",
      "100/333: np.sqrt(mean_squared_error(weekcounts[243-t:243], weekcounts[242-t:242])) # RMSE\n",
      "100/334: np.sqrt(mean_squared_error(weekcounts[243-t:243], [mean(weekcounts[242-t:242])]*len(weekcounts[242-t:242]))) # RMSE\n",
      "100/335: np.sqrt(mean_squared_error(weekcounts[243-t:243], [np.mean(weekcounts[242-t:242])]*len(weekcounts[242-t:242]))) # RMSE\n",
      "100/336:\n",
      "violent = [1]*len(df16)\n",
      "for i in range(len(df16)):\n",
      "    num = int(df16['iucr'].iloc[i][:3])\n",
      "    if num in range(58, 59) or num in range(81, 94) or num in range(105, 106) or num in range(111, 138) or num in range(142, 180) or num in range(181, 285) or num in range(286, 301) or num in range(330, 331) or num in range(361, 399) or num in range(431, 514):\n",
      "        violent[i] = 0\n",
      "100/337:\n",
      "df16v = df16[df16['violent']==1].copy()\n",
      "df16v.head()\n",
      "100/338:\n",
      "df16v['violent'] = violent\n",
      "df16v = df16[df16['violent']==1].copy()\n",
      "df16v.head()\n",
      "100/339:\n",
      "df16['violent'] = violent\n",
      "df16v = df16[df16['violent']==1].copy()\n",
      "df16v.head()\n",
      "100/340:\n",
      "from statsmodels.tsa.stattools import adfuller\n",
      "from pandas import Series\n",
      "100/341:\n",
      "from statsmodels.tsa.stattools import adfuller\n",
      "from pandas import Series\n",
      "100/342:\n",
      "result = adfuller(weekcounts)\n",
      "print('ADF Statistic: %f' % result[0])\n",
      "print('p-value: %f' % result[1])\n",
      "print('Critical Values:')\n",
      "for key, value in result[4].items():\n",
      "    print('\\t%s: %.3f' % (key, value))\n",
      "100/343:\n",
      "result = adfuller(weekcounts_)\n",
      "print('ADF Statistic: %f' % result[0])\n",
      "print('p-value: %f' % result[1])\n",
      "print('Critical Values:')\n",
      "for key, value in result[4].items():\n",
      "    print('\\t%s: %.3f' % (key, value))\n",
      "100/344:\n",
      "result = adfuller(weekcounts)\n",
      "print('ADF Statistic: %f' % result[0])\n",
      "print('p-value: %f' % result[1])\n",
      "print('Critical Values:')\n",
      "for key, value in result[4].items():\n",
      "    print('\\t%s: %.3f' % (key, value))\n",
      "100/345:\n",
      "plt.figure()\n",
      "plt.plot([x for x in range(243-t, 243)], weekcounts[243-t:243], label=\"actual\")\n",
      "# plt.plot([x for x in range(243)], weekcounts[:243], label=\"actual\")\n",
      "\n",
      "plt.plot([x for x in range(243-t, 243)], preds, label=\"predicted\")\n",
      "# plt.plot([x for x in range(243-t, 243)], weekcounts[242-t:242], label=\"persistence\")\n",
      "\n",
      "plt.legend()\n",
      "plt.title(\"Chicago crimes Apr 2018-Apr 2019\")\n",
      "plt.xlabel(\"Week\")\n",
      "plt.ylabel(\"Number of crimes\")\n",
      "\n",
      "plt.figure()\n",
      "# plt.plot([x for x in range(243-t, 243)], weekcounts[243-t:243], label=\"actual\")\n",
      "plt.plot([x for x in range(243)], weekcounts[:243], label=\"actual\")\n",
      "\n",
      "plt.plot([x for x in range(243-t, 243)], preds, label=\"predicted\")\n",
      "\n",
      "plt.legend()\n",
      "plt.title(\"Chicago crimes 2016-2019\")\n",
      "plt.xlabel(\"Week\")\n",
      "plt.ylabel(\"Number of crimes\")\n",
      "\n",
      "plt.show()\n",
      "100/346: lrs_w, lrscores_w, rmses_w = predictsplit('ward')\n",
      "100/347: lrs_b, lrscores_b, rmses_b = predictsplit('beat')\n",
      "100/348: lrs_w, lrscores_w, rmses_w = predictsplit('ward')\n",
      "100/349:\n",
      "plt.figure()\n",
      "plt.plot([x for x in range(243-t, 243)], weekcounts[243-t:243], label=\"actual\")\n",
      "plt.plot([x for x in range(243-t, 243)], preds, label=\"predicted\")\n",
      "# plt.plot([x for x in range(243-t, 243)], weekcounts[242-t:242], label=\"persistence\")\n",
      "\n",
      "plt.legend()\n",
      "plt.title(\"Chicago crimes Apr 2018-Apr 2019\")\n",
      "plt.xlabel(\"Week\")\n",
      "plt.ylabel(\"Number of crimes\")\n",
      "\n",
      "plt.figure()\n",
      "plt.plot([x for x in range(243)], weekcounts[:243], label=\"actual\")\n",
      "plt.plot([x for x in range(243-t, 243)], preds, label=\"predicted\")\n",
      "\n",
      "plt.legend()\n",
      "plt.title(\"Chicago crimes 2016-2019\")\n",
      "plt.xlabel(\"Week\")\n",
      "plt.ylabel(\"Number of crimes\")\n",
      "plt.ylim(0, 4700)\n",
      "\n",
      "plt.show()\n",
      "100/350: df2.head()\n",
      "100/351: weekcountsv = np.array([df16v['week'].value_counts()[x] for x in range(243)])\n",
      "100/352: plt.plot([x for x in range(243), weekcountsv])\n",
      "100/353: plt.plot([x for x in range(243)], weekcountsv])\n",
      "100/354: plt.plot([x for x in range(243)], weekcountsv)\n",
      "100/355:\n",
      "plt.plot([x for x in range(243)], weekcountsv)\n",
      "plt.ylim(0, 1800)\n",
      "100/356:\n",
      "def makeXY(weekcounts, t):\n",
      "    X = np.array([weekcounts[:242-73-t], weekcounts[1:242-72-t], weekcounts[72:241-t], weekcounts[73:242-t]]).transpose()\n",
      "    y = weekcounts[74:243-t]\n",
      "    Xt = np.array([weekcounts[242-73-t:242-73], weekcounts[242-72-t:242-72], weekcounts[241-t:241], weekcounts[242-t:242]]).transpose()\n",
      "    yt = weekcounts[243-t:243]\n",
      "    return X, y, Xt, yt\n",
      "100/357: X, y, Xt, yt = makeXY(weekcountsv, t)\n",
      "100/358:\n",
      "lr = LinearRegression().fit(X, y)\n",
      "lr.score(Xt, yt)\n",
      "100/359:\n",
      "p = lr.predict(Xt)\n",
      "np.sqrt(mean_squared_error(yt, p))\n",
      "100/360: np.sqrt(mean_squared_error(yt, np.mean(weekcountsv[242-t:242])))\n",
      "100/361: np.sqrt(mean_squared_error(yt, [np.mean(weekcountsv[242-t:242])]*len(weekcountsv[243-t:243])))\n",
      "100/362: X, y, Xt, yt = makeXY(weekcountsv, 60)\n",
      "100/363:\n",
      "lr = LinearRegression().fit(X, y)\n",
      "lr.score(Xt, yt)\n",
      "100/364: X, y, Xt, yt = makeXY(weekcountsv, 50)\n",
      "100/365:\n",
      "lr = LinearRegression().fit(X, y)\n",
      "lr.score(Xt, yt)\n",
      "100/366: X, y, Xt, yt = makeXY(weekcountsv, 60)\n",
      "100/367:\n",
      "lr = LinearRegression().fit(X, y)\n",
      "lr.score(Xt, yt)\n",
      "100/368:\n",
      "p = lr.predict(Xt)\n",
      "np.sqrt(mean_squared_error(yt, p))\n",
      "100/369: np.sqrt(mean_squared_error(yt, [np.mean(weekcountsv[242-t:242])]*len(weekcountsv[243-t:243])))\n",
      "100/370: X, y, Xt, yt = makeXY(weekcountsv, t)\n",
      "100/371:\n",
      "lr = LinearRegression().fit(X, y)\n",
      "lr.score(Xt, yt)\n",
      "100/372:\n",
      "p = lr.predict(Xt)\n",
      "np.sqrt(mean_squared_error(yt, p))\n",
      "100/373: np.sqrt(mean_squared_error(yt, [np.mean(weekcountsv[242-t:242])]*len(weekcountsv[243-t:243])))\n",
      "100/374: from sklearn.decomposition import LatentDirichletAllocation\n",
      "100/375:\n",
      "ldas2 = []\n",
      "scores = []\n",
      "ns = [3,5,10,15,20,25]\n",
      "for n in ns:\n",
      "    print(n)\n",
      "    lda = LatentDirichletAllocation(n_components=n, learning_method='online')\n",
      "    ldas2.append(lda.fit(train.values))\n",
      "    score = lda.score(test.values)\n",
      "    print(score)\n",
      "    scores.append(score)\n",
      "100/376:\n",
      "trainInds = np.random.uniform(0,1,df2.shape[0])<=0.8\n",
      "train=df2.iloc[trainInds]\n",
      "test=df2.iloc[~trainInds]\n",
      "100/377:\n",
      "ldas2 = []\n",
      "scores = []\n",
      "ns = [3,5,10,15,20,25]\n",
      "for n in ns:\n",
      "    print(n)\n",
      "    lda = LatentDirichletAllocation(n_components=n, learning_method='online')\n",
      "    ldas2.append(lda.fit(train.values))\n",
      "    score = lda.score(test.values)\n",
      "    print(score)\n",
      "    scores.append(score)\n",
      "100/378:\n",
      "pcas = []\n",
      "ns2 = [3,5,10,15,20,25,30,35,40,45,50]\n",
      "for n in ns2:\n",
      "    print(n)\n",
      "    pcas.append(PCA(n_components=n).fit(df2.values))\n",
      "100/379: from sklearn.decomposition import PCA\n",
      "100/380:\n",
      "pcas = []\n",
      "ns2 = [3,5,10,15,20,25,30,35,40,45,50]\n",
      "for n in ns2:\n",
      "    print(n)\n",
      "    pcas.append(PCA(n_components=n).fit(df2.values))\n",
      "100/381: plt.plot([x for x in range(60)], pcas[7].singular_values_)\n",
      "100/382: plt.plot([x for x in range(ns2[7])], pcas[7].singular_values_)\n",
      "100/383:\n",
      "sums = []\n",
      "tot = 0\n",
      "for x in pcas[7].singular_values_:\n",
      "    tot += x\n",
      "    sums.append(tot)\n",
      "plt.plot([x for x in range(60)], sums)\n",
      "100/384:\n",
      "sums = []\n",
      "tot = 0\n",
      "for x in pcas[7].singular_values_:\n",
      "    tot += x\n",
      "    sums.append(tot)\n",
      "plt.plot([x for x in range(ns2[7])], sums)\n",
      "100/385:\n",
      "def pcacmp(pca, i, j, l):\n",
      "    c1 = pca.components_[i]\n",
      "    c2 = pca.components_[j]\n",
      "    x = []\n",
      "    y = []\n",
      "    for k in range(l):\n",
      "        x.append(np.dot(df2.iloc[k], c1))\n",
      "        y.append(np.dot(df2.iloc[k], c2))\n",
      "    plt.plot(x, y, \"o\")\n",
      "    plt.xlabel(\"PCA \" + str(i+1))\n",
      "    plt.ylabel(\"PCA \" + str(j+1))\n",
      "100/386: pcacmp(pcas[7], 0, 1, 5000)\n",
      "100/387:\n",
      "for i in range(15):\n",
      "    plt.figure()\n",
      "    pcacmp(pcas[7], i, i+1, 5000)\n",
      "100/388: pcacmp(pcas[7], 2, 3, 5000)\n",
      "100/389:\n",
      "def pcacmps(pca, i, j):\n",
      "    inds = []\n",
      "    for k in range(len(pca.components_[i])):\n",
      "        if abs(pca.components_[i][k]-pca.components_[j][k]) > 0.2:\n",
      "            inds.append(k)\n",
      "    for k in range(len(inds)):\n",
      "        print(df2.columns[k])\n",
      "100/390: pcacmps(pcas[7], 0, 1)\n",
      "100/391:\n",
      "def pcacmp2(pca, i, j, thresh, axis):\n",
      "    c1 = pca.components_[i]\n",
      "    c2 = pca.components_[j]\n",
      "    x = []\n",
      "    y = []\n",
      "    avg1 = [0]*428\n",
      "    count1 = 0\n",
      "    avg2 = [0]*428\n",
      "    count2 = 0\n",
      "    for k in range(1000):\n",
      "        x1 = np.dot(df2.iloc[k], c1)\n",
      "        x.append(x1)\n",
      "        if axis==0:\n",
      "            if x1 > thresh:\n",
      "                avg1 += df2.iloc[k]\n",
      "                count1 += 1\n",
      "            else:\n",
      "                avg2 += df2.iloc[k]\n",
      "                count2 += 1\n",
      "        y1 = np.dot(df2.iloc[k], c2)\n",
      "        if axis==1:\n",
      "            if y1 > thresh:\n",
      "                avg1 += df2.iloc[k]\n",
      "                count1 += 1\n",
      "            else:\n",
      "                avg2 += df2.iloc[k]\n",
      "                count2 += 1\n",
      "        y.append(y1)\n",
      "    \n",
      "    avg1 = avg1/count1\n",
      "    avg2 = avg2/count2\n",
      "    \n",
      "    for k in range(428):\n",
      "        if abs(avg1[k]-avg2[k]) > 0.05:\n",
      "            print(df2.columns[k], avg1[k], avg2[k])\n",
      "    \n",
      "    return\n",
      "100/392: pcacmp2(pcas[7], 3, 4, 0.5, 1)\n",
      "100/393:\n",
      "def pcacmp2(pca, i, j, thresh, axis):\n",
      "    c1 = pca.components_[i]\n",
      "    c2 = pca.components_[j]\n",
      "    x = []\n",
      "    y = []\n",
      "    avg1 = [0]*429\n",
      "    count1 = 0\n",
      "    avg2 = [0]*429\n",
      "    count2 = 0\n",
      "    for k in range(1000):\n",
      "        x1 = np.dot(df2.iloc[k], c1)\n",
      "        x.append(x1)\n",
      "        if axis==0:\n",
      "            if x1 > thresh:\n",
      "                avg1 += df2.iloc[k]\n",
      "                count1 += 1\n",
      "            else:\n",
      "                avg2 += df2.iloc[k]\n",
      "                count2 += 1\n",
      "        y1 = np.dot(df2.iloc[k], c2)\n",
      "        if axis==1:\n",
      "            if y1 > thresh:\n",
      "                avg1 += df2.iloc[k]\n",
      "                count1 += 1\n",
      "            else:\n",
      "                avg2 += df2.iloc[k]\n",
      "                count2 += 1\n",
      "        y.append(y1)\n",
      "    \n",
      "    avg1 = avg1/count1\n",
      "    avg2 = avg2/count2\n",
      "    \n",
      "    for k in range(429):\n",
      "        if abs(avg1[k]-avg2[k]) > 0.05:\n",
      "            print(df2.columns[k], avg1[k], avg2[k])\n",
      "    \n",
      "    return\n",
      "100/394: pcacmp2(pcas[7], 3, 4, 0.5, 1)\n",
      "100/395: pcacmp2(pcas[7], 3, 4, 0.25, 0)\n",
      "100/396: pcacmp2(pcas[7], 4, 5, 0.5, 0)\n",
      "100/397: pcacmp2(pcas[7], 4, 5, 0.25, 1)\n",
      "100/398: pcacmp2(pcas[7], 1, 2, 0.5, 0)\n",
      "100/399: pcacmp2(pcas[7], 0, 1, 0.75, 0)\n",
      "100/400: pcacmp2(pcas[7], 6, 7, 0.25, 0)\n",
      "100/401: pcacmp2(pcas[7], 6, 7, 0.25, 1)\n",
      "100/402: pcacmp2(pcas[7], 11, 12, 0.5, 0)\n",
      "100/403: pcacmp2(pcas[7], 12, 13 0.25, 0)\n",
      "100/404: pcacmp2(pcas[7], 12, 13, 0.25, 0)\n",
      "100/405: pcacmp2(pcas[7], 14, 15, 0.5, 0)\n",
      "100/406: pcacmp2(pcas[7], 14, 15, 0.5, 1)\n",
      "100/407:\n",
      "# PCA 16 separates arrest/sexual assault/violent/street/transportation/vehicle trespass from\n",
      "# \n",
      "pcacmp2(pcas[7], 14, 15, 0.25, 1)\n",
      "100/408: plt.plot(ns, scores)\n",
      "100/409: transformed = ldas[0].transform(df2.values)\n",
      "100/410: transformed = ldas2[0].transform(df2.values)\n",
      "100/411:\n",
      "plt.figure()\n",
      "for i in range(3):\n",
      "    plt.hist(transformed[:, i], alpha=0.3, label=str(i+1),\n",
      "            range=(0,1), bins=20)\n",
      "#     plt.show()\n",
      "plt.xlabel(\"User Weight in Component i\", fontsize=20)\n",
      "plt.ylabel(\"Count\", fontsize=20)\n",
      "plt.tick_params(labelsize=15)\n",
      "plt.legend(ncol=1, bbox_to_anchor=(1, 1.05))\n",
      "plt.title(\"LDA\")\n",
      "plt.show()\n",
      "100/412:\n",
      "feature_distribution = ldas2[0].components_ / ldas2[0].components_.sum(axis=1)[:, np.newaxis]\n",
      "\n",
      "\n",
      "fd1 = feature_distribution[0, :]\n",
      "print fd1.shape\n",
      "\n",
      "\n",
      "plt.figure()\n",
      "plt.hist(fd1)\n",
      "plt.xlabel(\"Feature Proportion in Latent User 1\", fontsize=20)\n",
      "plt.ylabel(\"Frequency\", fontsize=20)\n",
      "plt.tick_params(labelsize=15)\n",
      "plt.show()\n",
      "100/413:\n",
      "feature_distribution = ldas2[0].components_ / ldas2[0].components_.sum(axis=1)[:, np.newaxis]\n",
      "\n",
      "\n",
      "fd1 = feature_distribution[0, :]\n",
      "\n",
      "\n",
      "plt.figure()\n",
      "plt.hist(fd1)\n",
      "plt.xlabel(\"Feature Proportion in Latent User 1\", fontsize=20)\n",
      "plt.ylabel(\"Frequency\", fontsize=20)\n",
      "plt.tick_params(labelsize=15)\n",
      "plt.show()\n",
      "100/414:\n",
      "feature_distribution = ldas2[0].components_ / ldas2[0].components_.sum(axis=1)[:, np.newaxis]\n",
      "\n",
      "\n",
      "fd1 = feature_distribution[1, :]\n",
      "\n",
      "\n",
      "plt.figure()\n",
      "plt.hist(fd1)\n",
      "plt.xlabel(\"Feature Proportion in Latent User 2\", fontsize=20)\n",
      "plt.ylabel(\"Frequency\", fontsize=20)\n",
      "plt.tick_params(labelsize=15)\n",
      "plt.show()\n",
      "100/415:\n",
      "feature_distribution = ldas2[0].components_ / ldas2[0].components_.sum(axis=1)[:, np.newaxis]\n",
      "\n",
      "\n",
      "fd1 = feature_distribution[2, :]\n",
      "\n",
      "\n",
      "plt.figure()\n",
      "plt.hist(fd1)\n",
      "plt.xlabel(\"Feature Proportion in Latent User 3\", fontsize=20)\n",
      "plt.ylabel(\"Frequency\", fontsize=20)\n",
      "plt.tick_params(labelsize=15)\n",
      "plt.show()\n",
      "100/416:\n",
      "plt.figure()\n",
      "for i in range(3):\n",
      "    plt.hist(transformed[:, i], alpha=0.3, label=str(i+1),\n",
      "            range=(0,1), bins=20)\n",
      "#     plt.show()\n",
      "plt.xlabel(\"Crime Weight in Component i\", fontsize=20)\n",
      "plt.ylabel(\"Count\", fontsize=20)\n",
      "plt.tick_params(labelsize=15)\n",
      "plt.legend(ncol=1, bbox_to_anchor=(1, 1.05))\n",
      "plt.title(\"LDA\")\n",
      "plt.show()\n",
      "100/417:\n",
      "c1 = ldas[0].components_[0, :]\n",
      "print(c1.shape)\n",
      "100/418:\n",
      "c1 = ldas2[0].components_[0, :]\n",
      "print(c1.shape)\n",
      "100/419:\n",
      "c1_indices = np.where(c1 > np.percentile(c1, 99))[0]\n",
      "\n",
      "top_c1_df = pd.DataFrame({'Component': df2.columns.values[c1_indices],\n",
      "                         'Weight': c1[c1_indices]})\n",
      "\n",
      "top_c1_df.sort_values('Weight', inplace=True, ascending=False)\n",
      "\n",
      "top_c1_df.head(n=10)\n",
      "100/420:\n",
      "def gettop(i):\n",
      "    c1 = ldas2[0].components_[i, :]\n",
      "    c1_indices = np.where(c1 > np.percentile(c1, 70))[0]\n",
      "\n",
      "    top_c1_df = pd.DataFrame({'Component': df2.columns.values[c1_indices],\n",
      "                             'Weight': c1[c1_indices]})\n",
      "\n",
      "    top_c1_df.sort_values('Weight', inplace=True, ascending=False)\n",
      "\n",
      "    print(top_c1_df.head(n=10))\n",
      "100/421: gettop(0)\n",
      "100/422: gettop(1)\n",
      "100/423: gettop(2)\n",
      "100/424:\n",
      "df3 = df2.drop([x for x in df2.columns if 'iucr' in x], axis=1)\n",
      "trainInds2 = np.random.uniform(0,1,df3.shape[0])<=0.8\n",
      "train2=df3.iloc[trainInds2]\n",
      "test2=df3.iloc[~trainInds2]\n",
      "100/425: models2 = predictx('violent', [MultinomialNB(), DecisionTreeClassifier(), RandomForestClassifier(n_estimators=100), LinearSVC(), LogisticRegression()], train2, test2)\n",
      "100/426: models = [MultinomialNB(), DecisionTreeClassifier(), RandomForestClassifier(n_estimators=100), LinearSVC(), LogisticRegression()]\n",
      "100/427:\n",
      "def predictx(col, models, train, test):\n",
      "    for m in models:\n",
      "        m.fit(train.drop([col], axis=1), train[col])\n",
      "        print(m.score(test.drop([col], axis=1), test[col]))\n",
      "    return models\n",
      "100/428: models = predictx('arrest', models, train, test)\n",
      "100/429:\n",
      "df3 = df2.drop([x for x in df2.columns if 'iucr' in x], axis=1)\n",
      "trainInds2 = np.random.uniform(0,1,df3.shape[0])<=0.8\n",
      "train2=df3.iloc[trainInds2]\n",
      "test2=df3.iloc[~trainInds2]\n",
      "100/430: models2 = predictx('violent', [MultinomialNB(), DecisionTreeClassifier(), RandomForestClassifier(n_estimators=100), LinearSVC(), LogisticRegression()], train2, test2)\n",
      "100/431: sorted(zip(df2.drop(['arrest'], axis=1).columns, models[2].feature_importances_), key=operator.itemgetter(1), reverse=True)[:10]\n",
      "100/432: import operator\n",
      "100/433: sorted(zip(df2.drop(['arrest'], axis=1).columns, models[2].feature_importances_), key=operator.itemgetter(1), reverse=True)[:10]\n",
      "100/434: sorted(zip(df3.drop(['violent'], axis=1).columns, models2[2].feature_importances_), key=operator.itemgetter(1), reverse=True)[:10]\n",
      "100/435:\n",
      "data = df.copy()\n",
      "def showCategorical(cat):\n",
      "    for t in pd.unique(data[cat]):\n",
      "        d = data[data[cat] == t]\n",
      "        plt.scatter(d.longitude, d.latitude, s=.001)\n",
      "    plt.show()\n",
      "100/436: showCategorical(\"primary_type\")\n",
      "100/437:\n",
      "data = df.copy()\n",
      "def showCategorical(cat):\n",
      "    for t in pd.unique(data[cat]):\n",
      "        d = data[data[cat] == t]\n",
      "        plt.scatter(d.longitude, d.latitude, s=.001)\n",
      "    plt.show()\n",
      "100/438: showCategorical(\"primary_type\")\n",
      "100/439:\n",
      "data = df0.copy()\n",
      "def showCategorical(cat):\n",
      "    for t in pd.unique(data[cat]):\n",
      "        d = data[data[cat] == t]\n",
      "        plt.scatter(d.longitude, d.latitude, s=.001)\n",
      "    plt.show()\n",
      "100/440: showCategorical(\"primary_type\")\n",
      "100/441:\n",
      "get_type = []\n",
      "crime_data = df.copy()\n",
      "for i in range(0,crime_data.shape[0]):\n",
      "    primary = crime_data.iloc[i].primary_type\n",
      "    get_index = -1\n",
      "    \n",
      "    for j in range(0, len(get_type)):\n",
      "        if (get_type[j][0] == primary):\n",
      "            get_index = j\n",
      "            get_type[j][1]+=1\n",
      "    \n",
      "    if get_index == -1:\n",
      "        get_type.append([primary, 1])\n",
      "\n",
      "type_data = pd.DataFrame(columns=['Type', 'count'], data=get_type)\n",
      "fig1, ax1 = plt.subplots()\n",
      "fig1.set_size_inches(18.5, 10.5)\n",
      "ax1.pie(type_data['count'], labels=type_data['Type'], autopct='%1.1f%%',startangle=90)\n",
      "ax1.axis('equal')  # Equal aspect ratio ensures that pie is drawn as a circle.\n",
      "plt.title(\"Chicago Crimes 2018-2019 by Type\")\n",
      "plt.show()\n",
      "100/442:\n",
      "get_type = []\n",
      "crime_data = df0.copy()\n",
      "for i in range(0,crime_data.shape[0]):\n",
      "    primary = crime_data.iloc[i].primary_type\n",
      "    get_index = -1\n",
      "    \n",
      "    for j in range(0, len(get_type)):\n",
      "        if (get_type[j][0] == primary):\n",
      "            get_index = j\n",
      "            get_type[j][1]+=1\n",
      "    \n",
      "    if get_index == -1:\n",
      "        get_type.append([primary, 1])\n",
      "\n",
      "type_data = pd.DataFrame(columns=['Type', 'count'], data=get_type)\n",
      "fig1, ax1 = plt.subplots()\n",
      "fig1.set_size_inches(18.5, 10.5)\n",
      "ax1.pie(type_data['count'], labels=type_data['Type'], autopct='%1.1f%%',startangle=90)\n",
      "ax1.axis('equal')  # Equal aspect ratio ensures that pie is drawn as a circle.\n",
      "plt.title(\"Chicago Crimes 2018-2019 by Type\")\n",
      "plt.show()\n",
      "100/443: pcacmp2(pcas[7], 14, 15, -0.3, 0)\n",
      "100/444: np.median(lrscores_d)\n",
      "100/445: np.median(rmses_d)\n",
      "100/446:\n",
      "def predictsplit(split):\n",
      "    lrs = []\n",
      "    lrscores = []\n",
      "    rmses = []\n",
      "    ws = []\n",
      "    for x in df16[split].unique():\n",
      "        lr = LinearRegression()\n",
      "\n",
      "        df16_ = df16[df16[split] == x].copy()\n",
      "\n",
      "        weekcounts_ = []\n",
      "        for i in range(243):\n",
      "            if i in df16_['week'].unique():\n",
      "                weekcounts_.append(df16_['week'].value_counts()[i])\n",
      "            else:\n",
      "                weekcounts_.append(0)\n",
      "        weekcounts_ = np.array(weekcounts_)\n",
      "\n",
      "        X = np.array([weekcounts_[:242-73-t], weekcounts_[1:242-72-t], weekcounts_[72:241-t], weekcounts_[73:242-t]]).transpose()\n",
      "        y = weekcounts_[74:243-t]\n",
      "\n",
      "        Xt = np.array([weekcounts_[242-73-t:242-73], weekcounts_[242-72-t:242-72], weekcounts_[241-t:241], weekcounts_[242-t:242]]).transpose()\n",
      "        yt = weekcounts_[243-t:243]\n",
      "\n",
      "        lr.fit(X, y)\n",
      "        s = lr.score(Xt,yt)\n",
      "        yp = lr.predict(Xt)\n",
      "\n",
      "        rmse = np.sqrt(mean_squared_error(yt, yp))\n",
      "\n",
      "        print(str(x) + \": \" + str(s) + \" \" + str(rmse))\n",
      "        lrscores.append(s)\n",
      "        lrs.append(lr)\n",
      "        rmses.append(rmse)\n",
      "        ws.append(weekcounts_)\n",
      "    return lrs, lrscores, rmses, ws\n",
      "100/447: lrs_d, lrscores_d, rmses_d = predictsplit('district')\n",
      "100/448: lrs_d, lrscores_d, rmses_d, ws_d = predictsplit('district')\n",
      "100/449: np.median(rmses_d), np.median(ws_d)\n",
      "100/450: lrs_ca, lrscores_ca, rmses_ca, ws_ca = predictsplit('community_area')\n",
      "100/451: np.median(rmses_ca), np.median(ws_ca)\n",
      "100/452:\n",
      "lr = LinearRegression().fit(np.array([weekcounts[:242-73-t]]).transpose(), weekcounts[74:243-t])\n",
      "lr.score(np.array([weekcounts[242-73-t:242-73]]).transpose(), weekcounts[243-t:243])\n",
      "100/453:\n",
      "lr = LinearRegression().fit(np.array([weekcounts[:242-t]]).transpose(), weekcounts[74:243-t])\n",
      "lr.score(np.array([weekcounts[242-t:242]]).transpose(), weekcounts[243-t:243])\n",
      "100/454:\n",
      "lr = LinearRegression().fit(np.array([weekcounts[73:242-t]]).transpose(), weekcounts[74:243-t])\n",
      "lr.score(np.array([weekcounts[242-t:242]]).transpose(), weekcounts[243-t:243])\n",
      "100/455:\n",
      "lr = LinearRegression().fit(np.array([weekcounts[73:242-t]]).transpose(), weekcounts[74:243-t])\n",
      "lr.score(np.array([weekcounts[242-t:242]]).transpose(), weekcounts[243-t:243])\n",
      "preds = lr.predict(np.array([weekcounts[242-t:242]]).transpose())\n",
      "np.sqrt(mean_squared_error(weekcounts[243-t:243], preds)) # RMSE\n",
      "100/456:\n",
      "lr = LinearRegression().fit(np.array([weekcounts[73:242-t]]).transpose(), weekcounts[74:243-t])\n",
      "print(lr.score(np.array([weekcounts[242-t:242]]).transpose(), weekcounts[243-t:243]))\n",
      "preds = lr.predict(np.array([weekcounts[242-t:242]]).transpose())\n",
      "np.sqrt(mean_squared_error(weekcounts[243-t:243], preds)) # RMSE\n",
      "100/457:\n",
      "lr = LinearRegression().fit(np.array([weekcounts[:242-73-t], weekcounts[72:241-t], weekcounts[73:242-t]]).transpose(), weekcounts[74:243-t])\n",
      "lr.score(np.array([weekcounts[242-73-t:242-73], weekcounts[241-t:241], weekcounts[242-t:242]]).transpose(), weekcounts[243-t:243])\n",
      "preds = lr.predict(np.array([weekcounts[242-73-t:242-73], weekcounts[241-t:241], weekcounts[242-t:242]]).transpose())\n",
      "np.sqrt(mean_squared_error(weekcounts[243-t:243], preds)) # RMSE\n",
      "100/458:\n",
      "lr = LinearRegression().fit(np.array([ weekcounts[72:241-t], weekcounts[73:242-t]]).transpose(), weekcounts[74:243-t])\n",
      "lr.score(np.array([weekcounts[241-t:241], weekcounts[242-t:242]]).transpose(), weekcounts[243-t:243])\n",
      "preds = lr.predict(np.array([weekcounts[241-t:241], weekcounts[242-t:242]]).transpose())\n",
      "np.sqrt(mean_squared_error(weekcounts[243-t:243], preds)) # RMSE\n",
      "100/459:\n",
      "lr = LinearRegression().fit(np.array([ weekcounts[72:241-t], weekcounts[73:242-t]]).transpose(), weekcounts[74:243-t])\n",
      "print(lr.score(np.array([weekcounts[241-t:241], weekcounts[242-t:242]]).transpose(), weekcounts[243-t:243]))\n",
      "preds = lr.predict(np.array([weekcounts[241-t:241], weekcounts[242-t:242]]).transpose())\n",
      "np.sqrt(mean_squared_error(weekcounts[243-t:243], preds)) # RMSE\n",
      "100/460: from sklearn.metrics import roc_curve\n",
      "100/461:\n",
      "from sklearn.metrics import roc_curve\n",
      "import pylab\n",
      "100/462: showCategorical(\"community_area\")\n",
      "100/463: showCategorical(\"community_area\", \"Community Areas\", 20)\n",
      "100/464:\n",
      "data = df0.copy()\n",
      "def showCategorical(cat, title, f):\n",
      "    for t in pd.unique(data[cat]):\n",
      "        d = data[data[cat] == t]\n",
      "        plt.scatter(d.longitude, d.latitude, s=.001)\n",
      "    plt.title(title, fontsize=f)\n",
      "    plt.show()\n",
      "100/465: showCategorical(\"community_area\", \"Community Areas\", 20)\n",
      "100/466:\n",
      "data = df0.copy()\n",
      "def showCategorical(cat, title, f):\n",
      "    for t in pd.unique(data[cat]):\n",
      "        d = data[data[cat] == t]\n",
      "        plt.scatter(d.longitude, d.latitude, s=.001)\n",
      "    plt.title(title, fontsize=f)\n",
      "    plt.legend()\n",
      "    plt.show()\n",
      "100/467: showCategorical(\"community_area\", \"Community Areas\", 20)\n",
      "100/468:\n",
      "data = df0.copy()\n",
      "def showCategorical(cat, title, f):\n",
      "    for t in pd.unique(data[cat]):\n",
      "        d = data[data[cat] == t]\n",
      "        plt.scatter(d.longitude, d.latitude, s=.001, label=t)\n",
      "    plt.title(title, fontsize=f)\n",
      "    plt.legend()\n",
      "    plt.show()\n",
      "100/469: showCategorical(\"community_area\", \"Community Areas\", 20)\n",
      "100/470:\n",
      "data = df0.copy()\n",
      "def showCategorical(cat, title, f, legend):\n",
      "    for t in pd.unique(data[cat]):\n",
      "        d = data[data[cat] == t]\n",
      "        plt.scatter(d.longitude, d.latitude, s=.001, label=t)\n",
      "    plt.title(title, fontsize=f)\n",
      "    if legend:\n",
      "        plt.legend()\n",
      "    plt.show()\n",
      "100/471: showCategorical(\"community_area\", \"Community Areas\", 20, False)\n",
      "100/472: showCategorical(\"arrest\", \"Arrest\", 20, True)\n",
      "100/473:\n",
      "data = df0.copy()\n",
      "def showCategorical(cat, title, f, legend):\n",
      "    for t in pd.unique(data[cat]):\n",
      "        d = data[data[cat] == t]\n",
      "        if legend:\n",
      "            plt.scatter(d.longitude, d.latitude, s=.001, label=t)\n",
      "        else:\n",
      "            plt.scatter(d.longitude, d.latitude, s=.001, label=t, color=('b' if t else 'o')) \n",
      "    plt.title(title, fontsize=f)\n",
      "    if legend:\n",
      "        plt.legend()\n",
      "    plt.show()\n",
      "100/474: showCategorical(\"community_area\", \"Community Areas\", 20, False)\n",
      "100/475:\n",
      "data = df0.copy()\n",
      "def showCategorical(cat, title, f, legend):\n",
      "    for t in pd.unique(data[cat]):\n",
      "        d = data[data[cat] == t]\n",
      "        if not legend:\n",
      "            plt.scatter(d.longitude, d.latitude, s=.001, label=t)\n",
      "        else:\n",
      "            plt.scatter(d.longitude, d.latitude, s=.001, label=t, color=('b' if t else 'r')) \n",
      "    plt.title(title, fontsize=f)\n",
      "    if legend:\n",
      "        plt.legend()\n",
      "    plt.show()\n",
      "100/476: showCategorical(\"community_area\", \"Community Areas\", 20, False)\n",
      "100/477: showCategorical(\"arrest\", \"Arrest\", 20, True)\n",
      "100/478:\n",
      "data = df0.copy()\n",
      "def showCategorical(cat, title, f, legend):\n",
      "    for t in pd.unique(data[cat]):\n",
      "        d = data[data[cat] == t]\n",
      "        plt.scatter(d.longitude, d.latitude, s=.001, label=t)\n",
      "    plt.title(title, fontsize=f)\n",
      "    if legend:\n",
      "        lg = plt.legend()\n",
      "        lg.legendHandles[0]._legmarker.set_markersize(6)\n",
      "        lg.legendHandles[1]._legmarker.set_markersize(6)\n",
      "    plt.show()\n",
      "100/479: showCategorical(\"arrest\", \"Arrest\", 20, True)\n",
      "100/480:\n",
      "data = df0.copy()\n",
      "def showCategorical(cat, title, f, legend):\n",
      "    for t in pd.unique(data[cat]):\n",
      "        d = data[data[cat] == t]\n",
      "        plt.scatter(d.longitude, d.latitude, s=.001, label=t)\n",
      "    plt.title(title, fontsize=f)\n",
      "    if legend:\n",
      "        lgnd = plt.legend()\n",
      "        lgnd.legendHandles[0]._sizes = [30]\n",
      "        lgnd.legendHandles[1]._sizes = [30]\n",
      "    plt.show()\n",
      "100/481: showCategorical(\"arrest\", \"Arrest\", 20, True)\n",
      "100/482:\n",
      "from sklearn.metrics import roc_curve, roc_auc_score\n",
      "import pylab\n",
      "100/483:\n",
      "plot.figure()\n",
      "labels = [\"Multinomial Naive Bayes, Decision Tree Classifier, RandomForestClassifier(n_estimators=100), LinearSVC(), LogisticRegression()\"]\n",
      "for i in range(len(models)):\n",
      "    \n",
      "\n",
      "pylab.plot(np.linspace(0, 1, 100), np.linspace(0, 1, 100), ls='dashed', c='k')\n",
      "plot.legend()\n",
      "plot.xlabel(\"FPR\")\n",
      "plot.ylabel(\"TPR\")\n",
      "100/484:\n",
      "plot.figure()\n",
      "labels = [\"Multinomial Naive Bayes\", \"DecisionTreeClassifier, RandomForestClassifier, LinearSVC(), LogisticRegression()\"]\n",
      "for i in range(len(models)):\n",
      "    \n",
      "\n",
      "pylab.plot(np.linspace(0, 1, 100), np.linspace(0, 1, 100), ls='dashed', c='k')\n",
      "plot.legend()\n",
      "plot.xlabel(\"FPR\")\n",
      "plot.ylabel(\"TPR\")\n",
      "100/485:\n",
      "fprs = []\n",
      "tprs = []\n",
      "for m in models:\n",
      "    fpr, tpr, _ = roc_curve(, m.predict_proba(test)[:,1])\n",
      "    fprs.append(fpr)\n",
      "    tprs.append(tpr)\n",
      "100/486:\n",
      "fprs = []\n",
      "tprs = []\n",
      "for m in models:\n",
      "    fpr, tpr, _ = roc_curve(test['arrest'], m.predict_proba(test.drop(['arrest'], axis=1))[:,1])\n",
      "    fprs.append(fpr)\n",
      "    tprs.append(tpr)\n",
      "100/487:\n",
      "fprs = []\n",
      "tprs = []\n",
      "for i in range(len(models)):\n",
      "    print(i)\n",
      "    if i == 3:\n",
      "        fpr, tpr, _ = roc_curve(test['arrest'], models[i].decision_function(test.drop(['arrest'], axis=1))[:,1])\n",
      "    else:\n",
      "        fpr, tpr, _ = roc_curve(test['arrest'], models[i].predict_proba(test.drop(['arrest'], axis=1))[:,1])\n",
      "    fprs.append(fpr)\n",
      "    tprs.append(tpr)\n",
      "100/488:\n",
      "fprs = []\n",
      "tprs = []\n",
      "for i in range(len(models)):\n",
      "    print(i)\n",
      "    if i == 3:\n",
      "        fpr, tpr, _ = roc_curve(test['arrest'], models[i].decision_function(test.drop(['arrest'], axis=1)))\n",
      "    else:\n",
      "        fpr, tpr, _ = roc_curve(test['arrest'], models[i].predict_proba(test.drop(['arrest'], axis=1))[:,1])\n",
      "    fprs.append(fpr)\n",
      "    tprs.append(tpr)\n",
      "100/489:\n",
      "fprk, tprk, _ = roc_curve(test['arrest'], knn1.predict_proba(test.drop(['arrest'], axis=1))[:,1])\n",
      "fprs.append(fprk)\n",
      "tprs.append(tprk)\n",
      "100/490:\n",
      "fprk, tprk, _ = roc_curve(test['arrest'], knn1[0].predict_proba(test.drop(['arrest'], axis=1))[:,1])\n",
      "fprs.append(fprk)\n",
      "tprs.append(tprk)\n",
      "102/1:\n",
      "from sklearn.naive_bayes import MultinomialNB\n",
      "from sklearn.tree import DecisionTreeClassifier\n",
      "from sklearn.ensemble import RandomForestClassifier\n",
      "from sklearn.svm import LinearSVC, SVC\n",
      "from sklearn.linear_model import LogisticRegression\n",
      "102/2: models = [MultinomialNB(), DecisionTreeClassifier(), RandomForestClassifier(n_estimators=100), LinearSVC(), LogisticRegression()]\n",
      "102/3:\n",
      "def predictx(col, models, train, test):\n",
      "    for m in models:\n",
      "        m.fit(train.drop([col], axis=1), train[col])\n",
      "        print(m.score(test.drop([col], axis=1), test[col]))\n",
      "    return models\n",
      "102/4:\n",
      "import numpy as np\n",
      "import pandas as pd\n",
      "import seaborn as sns\n",
      "102/5:\n",
      "from matplotlib import pyplot as plt\n",
      "%matplotlib inline\n",
      "pd.options.display.max_columns = 100\n",
      "102/6: df0 = pd.read_pickle('since2018')\n",
      "102/7: df0['hour'] = df0['date'].dt.hour//3\n",
      "102/8: df0.nunique()\n",
      "102/9: df0['primary_type'].value_counts()\n",
      "102/10:\n",
      "violent = [1]*len(df0)\n",
      "for i in range(len(df0)):\n",
      "    num = int(df0['iucr'].iloc[i][:3])\n",
      "    if num in range(58, 59) or num in range(81, 94) or num in range(105, 106) or num in range(111, 138) or num in range(142, 180) or num in range(181, 285) or num in range(286, 301) or num in range(330, 331) or num in range(361, 399) or num in range(431, 514):\n",
      "        violent[i] = 0\n",
      "102/11: np.sum(violent)/len(violent)\n",
      "102/12: df2['violent'] = violent\n",
      "102/13: df = pd.read_csv('use_df.csv')\n",
      "102/14: df.head()\n",
      "102/15:\n",
      "df1 = df0.copy()\n",
      "df1 = pd.get_dummies(df1, columns = ['primary_type'])\n",
      "102/16: df[['date']].dtypes\n",
      "102/17: df['date'] = pd.to_datetime(df.date)\n",
      "102/18: df.nunique(axis=0)\n",
      "102/19: df.shape\n",
      "102/20:\n",
      "typeVals = df.dtypes\n",
      "typeVals = typeVals.replace('bool', 'int64')\n",
      "df = df.astype(typeVals)\n",
      "102/21: [x for x in df['location_description'].unique() if not x == None and ('FEDERAL' in x or 'GOVERNMENT' in x)]\n",
      "102/22:\n",
      "def getcounts(word):\n",
      "    cols = [x for x in df['location_description'].unique() if not x == None and (word in x)]\n",
      "    tot = 0\n",
      "    for c in cols:\n",
      "        tot += df['location_description'].value_counts()[c]\n",
      "    return tot\n",
      "def getcounts2(words):\n",
      "    tot = 0\n",
      "    for w in words:\n",
      "        tot += getcounts(w)\n",
      "    return tot\n",
      "102/23: getcounts2(['BAR', 'POOL', 'SPORTS'])\n",
      "102/24:\n",
      "df2 = df.copy()\n",
      "df2['hour'] = df2['date'].dt.hour//3\n",
      "df2 = df2.drop(['date'], axis=1)\n",
      "categorical_columns = ['iucr', 'location_description', 'community_area', 'hour']\n",
      "df2 = pd.get_dummies(df2, columns = categorical_columns)\n",
      "102/25: df2['violent'] = violent\n",
      "102/26: df2.head()\n",
      "102/27:\n",
      "trainInds = np.random.uniform(0,1,df2.shape[0])<=0.8\n",
      "train=df2.iloc[trainInds]\n",
      "test=df2.iloc[~trainInds]\n",
      "102/28: train.shape, test.shape\n",
      "102/29: models = predictx('arrest', models, train, test)\n",
      "102/30:\n",
      "from sklearn.metrics import roc_curve, roc_auc_score\n",
      "import pylab\n",
      "102/31:\n",
      "fprs = []\n",
      "tprs = []\n",
      "for i in range(len(models)):\n",
      "    print(i)\n",
      "    if i == 3:\n",
      "        fpr, tpr, _ = roc_curve(test['arrest'], models[i].decision_function(test.drop(['arrest'], axis=1)))\n",
      "    else:\n",
      "        fpr, tpr, _ = roc_curve(test['arrest'], models[i].predict_proba(test.drop(['arrest'], axis=1))[:,1])\n",
      "    fprs.append(fpr)\n",
      "    tprs.append(tpr)\n",
      "102/32:\n",
      "plt.figure()\n",
      "labels = [\"Multinomial Naive Bayes\", \"DecisionTreeClassifier\", \"RandomForestClassifier\", \"SVM\", \"Logistic Regression\"]\n",
      "for i in range(len(models)):\n",
      "    plt.plot(fprs[i], tprs[i], label=labels[i])\n",
      "\n",
      "pylab.plot(np.linspace(0, 1, 100), np.linspace(0, 1, 100), ls='dashed', c='k')\n",
      "plt.legend()\n",
      "plt.xlabel(\"FPR\")\n",
      "plt.ylabel(\"TPR\")\n",
      "plt.show()\n",
      "102/33:\n",
      "def getroc(var)\n",
      "    fprs = []\n",
      "    tprs = []\n",
      "    for i in range(len(models)):\n",
      "        print(i)\n",
      "        if i == 3:\n",
      "            fpr, tpr, _ = roc_curve(test[var], models[i].decision_function(test.drop([var], axis=1)))\n",
      "        else:\n",
      "            fpr, tpr, _ = roc_curve(test[var], models[i].predict_proba(test.drop([var], axis=1))[:,1])\n",
      "        fprs.append(fpr)\n",
      "        tprs.append(tpr)\n",
      "    return fprs, tprs\n",
      "102/34:\n",
      "def getroc(var, models)\n",
      "    fprs = []\n",
      "    tprs = []\n",
      "    for i in range(len(models)):\n",
      "        print(i)\n",
      "        if i == 3:\n",
      "            fpr, tpr, _ = roc_curve(test[var], models[i].decision_function(test.drop([var], axis=1)))\n",
      "        else:\n",
      "            fpr, tpr, _ = roc_curve(test[var], models[i].predict_proba(test.drop([var], axis=1))[:,1])\n",
      "        fprs.append(fpr)\n",
      "        tprs.append(tpr)\n",
      "    return fprs, tprs\n",
      "102/35:\n",
      "def getroc(var, models):\n",
      "    fprs = []\n",
      "    tprs = []\n",
      "    for i in range(len(models)):\n",
      "        print(i)\n",
      "        if i == 3:\n",
      "            fpr, tpr, _ = roc_curve(test[var], models[i].decision_function(test.drop([var], axis=1)))\n",
      "        else:\n",
      "            fpr, tpr, _ = roc_curve(test[var], models[i].predict_proba(test.drop([var], axis=1))[:,1])\n",
      "        fprs.append(fpr)\n",
      "        tprs.append(tpr)\n",
      "    return fprs, tprs\n",
      "102/36:\n",
      "plt.figure()\n",
      "labels = [\"Multinomial Naive Bayes\", \"DecisionTreeClassifier\", \"RandomForestClassifier\", \"SVM\", \"Logistic Regression\"]\n",
      "for i in range(len(models)):\n",
      "    plt.plot(fprs[i], tprs[i], label=labels[i])\n",
      "\n",
      "pylab.plot(np.linspace(0, 1, 100), np.linspace(0, 1, 100), ls='dashed', c='k')\n",
      "plt.legend()\n",
      "plt.xlabel(\"FPR\")\n",
      "plt.ylabel(\"TPR\")\n",
      "plt.title(\"ROC curves for Predicting Arrest\")\n",
      "plt.show()\n",
      "102/37:\n",
      "plt.figure()\n",
      "labels = [\"Multinomial Naive Bayes\", \"DecisionTreeClassifier\", \"RandomForestClassifier\", \"SVM\", \"Logistic Regression\"]\n",
      "for i in range(len(models)):\n",
      "    plt.plot(fprs[i], tprs[i], label=labels[i])\n",
      "\n",
      "pylab.plot(np.linspace(0, 1, 100), np.linspace(0, 1, 100), ls='dashed', c='k')\n",
      "plt.legend()\n",
      "plt.xlabel(\"FPR\")\n",
      "plt.ylabel(\"TPR\")\n",
      "plt.title(\"ROC curves for Predicting Arrest\", fontsize=20)\n",
      "plt.show()\n",
      "102/38:\n",
      "plt.figure()\n",
      "labels = [\"Multinomial Naive Bayes\", \"DecisionTreeClassifier\", \"RandomForestClassifier\", \"SVM\", \"Logistic Regression\"]\n",
      "for i in range(len(models)):\n",
      "    plt.plot(fprs[i], tprs[i], label=labels[i])\n",
      "\n",
      "pylab.plot(np.linspace(0, 1, 100), np.linspace(0, 1, 100), ls='dashed', c='k')\n",
      "plt.legend()\n",
      "plt.xlabel(\"FPR\")\n",
      "plt.ylabel(\"TPR\")\n",
      "plt.title(\"ROC curves for predicting Arrest\", fontsize=20)\n",
      "plt.show()\n",
      "102/39:\n",
      "plt.figure()\n",
      "labels = [\"Multinomial Naive Bayes\", \"DecisionTreeClassifier\", \"RandomForestClassifier\", \"SVM\", \"Logistic Regression\"]\n",
      "for i in range(len(models)):\n",
      "    plt.plot(fprs[i], tprs[i], label=labels[i])\n",
      "\n",
      "pylab.plot(np.linspace(0, 1, 100), np.linspace(0, 1, 100), ls='dashed', c='k')\n",
      "plt.legend()\n",
      "plt.xlabel(\"FPR\")\n",
      "plt.ylabel(\"TPR\")\n",
      "plt.title(\"ROC curves for predicting Arrest\", fontsize=18)\n",
      "plt.show()\n",
      "102/40: models2 = predictx('violent', [MultinomialNB(), DecisionTreeClassifier(), RandomForestClassifier(n_estimators=100), LinearSVC(), LogisticRegression()], train2, test2)\n",
      "102/41:\n",
      "df3 = df2.drop([x for x in df2.columns if 'iucr' in x], axis=1)\n",
      "trainInds2 = np.random.uniform(0,1,df3.shape[0])<=0.8\n",
      "train2=df3.iloc[trainInds2]\n",
      "test2=df3.iloc[~trainInds2]\n",
      "102/42: models2 = predictx('violent', [MultinomialNB(), DecisionTreeClassifier(), RandomForestClassifier(n_estimators=100), LinearSVC(), LogisticRegression()], train2, test2)\n",
      "102/43: fprs2, tprs2 = getroc('violent', models2)\n",
      "102/44:\n",
      "def getroc(var, models, test):\n",
      "    fprs = []\n",
      "    tprs = []\n",
      "    for i in range(len(models)):\n",
      "        print(i)\n",
      "        if i == 3:\n",
      "            fpr, tpr, _ = roc_curve(test[var], models[i].decision_function(test.drop([var], axis=1)))\n",
      "        else:\n",
      "            fpr, tpr, _ = roc_curve(test[var], models[i].predict_proba(test.drop([var], axis=1))[:,1])\n",
      "        fprs.append(fpr)\n",
      "        tprs.append(tpr)\n",
      "    return fprs, tprs\n",
      "102/45: fprs2, tprs2 = getroc('violent', models2, test2)\n",
      "102/46:\n",
      "plt.figure()\n",
      "for i in range(len(models)):\n",
      "    plt.plot(fprs2[i], tprs2[i], label=labels[i])\n",
      "\n",
      "pylab.plot(np.linspace(0, 1, 100), np.linspace(0, 1, 100), ls='dashed', c='k')\n",
      "plt.legend()\n",
      "plt.xlabel(\"FPR\")\n",
      "plt.ylabel(\"TPR\")\n",
      "plt.title(\"ROC curves for predicting Violent crimes\", fontsize=18)\n",
      "plt.show()\n",
      "102/47:\n",
      "plt.figure()\n",
      "for i in range(len(models)):\n",
      "    plt.plot(fprs2[i], tprs2[i], label=labels[i])\n",
      "\n",
      "pylab.plot(np.linspace(0, 1, 100), np.linspace(0, 1, 100), ls='dashed', c='k')\n",
      "plt.legend()\n",
      "plt.xlabel(\"FPR\")\n",
      "plt.ylabel(\"TPR\")\n",
      "plt.title(\"ROC curves for predicting Violent crimes\", fontsize=16)\n",
      "plt.show()\n",
      "102/48:\n",
      "def getauc(var, models, test):\n",
      "    aucs = []\n",
      "    for i in range(len(models)):\n",
      "        print(i)\n",
      "        if i == 3:\n",
      "            auc = roc_auc_score(test[var], models[i].decision_function(test.drop([var], axis=1)))\n",
      "        else:\n",
      "            auc = roc_auc_score(test[var], models[i].predict_proba(test.drop([var], axis=1))[:,1])\n",
      "        aucs.append(auc)\n",
      "    return aucs\n",
      "102/49: aucs = getauc('arrest', models, test)\n",
      "102/50: print(aucs)\n",
      "102/51: aucs2 = getauc('violent', models2, test2)\n",
      "102/52: print(aucs2)\n",
      "102/53:\n",
      "fprk, tprk, _ = roc_curve(test['arrest'], knn1[0].predict_proba(test.drop(['arrest'], axis=1))[:,1])\n",
      "fprs.append(fprk)\n",
      "tprs.append(tprk)\n",
      "102/54: from sklearn.metrics import mean_squared_error\n",
      "102/55: df16 = pd.read_pickle('since2016')\n",
      "102/56: df16['week'] = df16['date'].dt.dayofyear//5 + (df16['date'].dt.year-2016)*73\n",
      "102/57: max(df16['week'])\n",
      "102/58:\n",
      "plt.hist(df16['date'].dt.dayofyear//5 + (df16['date'].dt.year-2016)*73, bins=244)\n",
      "plt.show()\n",
      "102/59: weekcounts = np.array([df16['week'].value_counts()[x] for x in range(243)])\n",
      "102/60:\n",
      "plt.plot([x for x in range(243)], weekcounts)\n",
      "plt.ylim(0, 4700)\n",
      "plt.title(\"Time series graph of Chicago crimes since 2016\")\n",
      "plt.xlabel(\"Week\")\n",
      "plt.ylabel(\"Number of crimes\")\n",
      "plt.show()\n",
      "102/61:\n",
      "lr = LinearRegression().fit(weekcounts[:242].reshape(-1, 1), weekcounts[1:243])\n",
      "lr.score(weekcounts[:242].reshape(-1, 1), weekcounts[1:243])\n",
      "102/62: lr.coef_\n",
      "102/63:\n",
      "lr = LinearRegression().fit(np.array([weekcounts[1:242], weekcounts[:241]]).transpose(), weekcounts[2:243])\n",
      "lr.score(np.array([weekcounts[1:242], weekcounts[:241]]).transpose(), weekcounts[2:243])\n",
      "102/64: from sklearn.linear_model import LinearRegression\n",
      "102/65:\n",
      "lr = LinearRegression().fit(weekcounts[:242].reshape(-1, 1), weekcounts[1:243])\n",
      "lr.score(weekcounts[:242].reshape(-1, 1), weekcounts[1:243])\n",
      "102/66: lr.coef_\n",
      "102/67:\n",
      "lr = LinearRegression().fit(np.array([weekcounts[1:242], weekcounts[:241]]).transpose(), weekcounts[2:243])\n",
      "lr.score(np.array([weekcounts[1:242], weekcounts[:241]]).transpose(), weekcounts[2:243])\n",
      "102/68: lr.coef_\n",
      "102/69: np.array([weekcounts[:243-73], weekcounts[72:242], weekcounts[73:243]]).shape\n",
      "102/70: len(weekcounts[1:243-73])\n",
      "102/71:\n",
      "lr = LinearRegression().fit(np.array([weekcounts[:242-73], weekcounts[72:241], weekcounts[73:242]]).transpose(), weekcounts[74:243])\n",
      "lr.score(np.array([weekcounts[:242-73], weekcounts[72:241], weekcounts[73:242]]).transpose(), weekcounts[74:243])\n",
      "102/72: t=70\n",
      "102/73:\n",
      "lr = LinearRegression().fit(np.array([weekcounts[73:242-t]]).transpose(), weekcounts[74:243-t])\n",
      "print(lr.score(np.array([weekcounts[242-t:242]]).transpose(), weekcounts[243-t:243]))\n",
      "preds = lr.predict(np.array([weekcounts[242-t:242]]).transpose())\n",
      "np.sqrt(mean_squared_error(weekcounts[243-t:243], preds)) # RMSE\n",
      "102/74:\n",
      "lr = LinearRegression().fit(np.array([ weekcounts[72:241-t], weekcounts[73:242-t]]).transpose(), weekcounts[74:243-t])\n",
      "print(lr.score(np.array([weekcounts[241-t:241], weekcounts[242-t:242]]).transpose(), weekcounts[243-t:243]))\n",
      "preds = lr.predict(np.array([weekcounts[241-t:241], weekcounts[242-t:242]]).transpose())\n",
      "np.sqrt(mean_squared_error(weekcounts[243-t:243], preds)) # RMSE\n",
      "102/75: weekcounts[242-73-t:242-73].shape, weekcounts[241-t:241].shape, weekcounts[242-t:242].shape\n",
      "102/76:\n",
      "lr = LinearRegression().fit(np.array([weekcounts[:242-73-t], weekcounts[72:241-t], weekcounts[73:242-t]]).transpose(), weekcounts[74:243-t])\n",
      "lr.score(np.array([weekcounts[242-73-t:242-73], weekcounts[241-t:241], weekcounts[242-t:242]]).transpose(), weekcounts[243-t:243])\n",
      "preds = lr.predict(np.array([weekcounts[242-73-t:242-73], weekcounts[241-t:241], weekcounts[242-t:242]]).transpose())\n",
      "np.sqrt(mean_squared_error(weekcounts[243-t:243], preds)) # RMSE\n",
      "102/77: np.array([weekcounts[:242-73-t], weekcounts[1:242-72-t], weekcounts[72:241-t], weekcounts[73:242-t]]).transpose().shape\n",
      "102/78:\n",
      "lr = LinearRegression().fit(np.array([weekcounts[:242-73-t], weekcounts[1:242-72-t], weekcounts[72:241-t], weekcounts[73:242-t]]).transpose(), weekcounts[74:243-t])\n",
      "lr.score(np.array([weekcounts[242-73-t:242-73], weekcounts[242-72-t:242-72], weekcounts[241-t:241], weekcounts[242-t:242]]).transpose(), weekcounts[243-t:243])\n",
      "102/79: preds = lr.predict(np.array([weekcounts[242-73-t:242-73], weekcounts[242-72-t:242-72], weekcounts[241-t:241], weekcounts[242-t:242]]).transpose())\n",
      "102/80:\n",
      "plt.plot(weekcounts[243-t:243]-preds, \"o\")\n",
      "plt.title(\"Residuals\")\n",
      "plt.show()\n",
      "102/81:\n",
      "plt.figure()\n",
      "plt.plot([x for x in range(243-t, 243)], weekcounts[243-t:243], label=\"actual\")\n",
      "plt.plot([x for x in range(243-t, 243)], preds, label=\"predicted\")\n",
      "# plt.plot([x for x in range(243-t, 243)], weekcounts[242-t:242], label=\"persistence\")\n",
      "\n",
      "plt.legend()\n",
      "plt.title(\"Chicago crimes Apr 2018-Apr 2019\")\n",
      "plt.xlabel(\"Week\")\n",
      "plt.ylabel(\"Number of crimes\")\n",
      "\n",
      "plt.figure()\n",
      "plt.plot([x for x in range(243)], weekcounts[:243], label=\"actual\")\n",
      "plt.plot([x for x in range(243-t, 243)], preds, label=\"predicted\")\n",
      "\n",
      "plt.legend()\n",
      "plt.title(\"Chicago crimes 2016-2019\")\n",
      "plt.xlabel(\"Week\")\n",
      "plt.ylabel(\"Number of crimes\")\n",
      "\n",
      "plt.ylim(0, 4700)\n",
      "\n",
      "plt.show()\n",
      "102/82: np.sqrt(mean_squared_error(weekcounts[243-t:243], preds)) # RMSE\n",
      "102/83:\n",
      "plt.figure()\n",
      "plt.plot([x for x in range(243-t, 243)], weekcounts[243-t:243], label=\"actual\")\n",
      "plt.plot([x for x in range(243-t, 243)], preds, label=\"predicted\")\n",
      "# plt.plot([x for x in range(243-t, 243)], weekcounts[242-t:242], label=\"persistence\")\n",
      "\n",
      "plt.legend()\n",
      "plt.title(\"Chicago crimes Apr 2018-Apr 2019\")\n",
      "plt.xlabel(\"Week\")\n",
      "plt.ylabel(\"Number of crimes\")\n",
      "\n",
      "plt.figure()\n",
      "plt.plot([x for x in range(243)], weekcounts[:243], label=\"actual\")\n",
      "plt.plot([x for x in range(243-t, 243)], preds, label=\"predicted\")\n",
      "\n",
      "plt.legend()\n",
      "plt.title(\"Chicago crimes 2016-2019\", fontsize=20)\n",
      "plt.xlabel(\"Week\")\n",
      "plt.ylabel(\"Number of crimes\")\n",
      "\n",
      "plt.ylim(0, 4700)\n",
      "\n",
      "plt.show()\n",
      "102/84:\n",
      "plt.figure()\n",
      "plt.plot([x for x in range(243-t, 243)], weekcounts[243-t:243], label=\"actual\")\n",
      "plt.plot([x for x in range(243-t, 243)], preds, label=\"predicted\")\n",
      "# plt.plot([x for x in range(243-t, 243)], weekcounts[242-t:242], label=\"persistence\")\n",
      "\n",
      "plt.legend()\n",
      "plt.title(\"Chicago crimes Apr 2018-Apr 2019\", fontsize=20)\n",
      "plt.xlabel(\"Week\")\n",
      "plt.ylabel(\"Number of crimes\")\n",
      "\n",
      "plt.figure()\n",
      "plt.plot([x for x in range(243)], weekcounts[:243], label=\"actual\")\n",
      "plt.plot([x for x in range(243-t, 243)], preds, label=\"predicted\")\n",
      "\n",
      "plt.legend()\n",
      "plt.title(\"Chicago crimes 2016-2019\", fontsize=20)\n",
      "plt.xlabel(\"Week\")\n",
      "plt.ylabel(\"Number of crimes\")\n",
      "\n",
      "plt.ylim(0, 4700)\n",
      "\n",
      "plt.show()\n",
      "102/85: from sklearn.decomposition import PCA\n",
      "102/86: df2.head()\n",
      "102/87:\n",
      "pcas = []\n",
      "ns2 = [3,5,10,15,20,25,30,35,40,45,50]\n",
      "for n in ns2:\n",
      "    print(n)\n",
      "    pcas.append(PCA(n_components=n).fit(df2.values))\n",
      "102/88:\n",
      "pcas = []\n",
      "ns2 = [3,5,10,15,20,25,30,35,40,45,50,55,60]\n",
      "for n in ns2:\n",
      "    print(n)\n",
      "    pcas.append(PCA(n_components=n).fit(df2.values))\n",
      "102/89: plt.plot([x for x in range(ns2[len(ns2)-1])], pcas[len(pcas)-1].singular_values_)\n",
      "102/90:\n",
      "plt.plot([x for x in range(ns2[len(ns2)-1])], pcas[len(pcas)-1].singular_values_)\n",
      "plt.title(\"PCA\")\n",
      "102/91:\n",
      "sums = []\n",
      "tot = 0\n",
      "for x in pcas[len(pcas)-1].singular_values_:\n",
      "    tot += x\n",
      "    sums.append(tot)\n",
      "plt.plot([x for x in range(ns2[len(ns2)-1])], sums)\n",
      "102/92:\n",
      "plt.plot([x for x in range(ns2[len(ns2)-1])], pcas[len(pcas)-1].singular_values_)\n",
      "plt.title(\"PCA Scree Plot\")\n",
      "102/93:\n",
      "plt.plot([x for x in range(ns2[len(ns2)-1])], pcas[len(pcas)-1].singular_values_)\n",
      "plt.title(\"PCA Scree Plot\", fontsize=18)\n",
      "plt.show()\n",
      "102/94:\n",
      "plt.plot([x for x in range(ns2[len(ns2)-1])], pcas[len(pcas)-1].singular_values_)\n",
      "plt.title(\"PCA Scree Plot\", fontsize=12)\n",
      "plt.show()\n",
      "102/95:\n",
      "plt.plot([x for x in range(ns2[len(ns2)-1])], pcas[len(pcas)-1].singular_values_)\n",
      "plt.title(\"PCA Scree Plot\", fontsize=15)\n",
      "plt.show()\n",
      "102/96:\n",
      "plt.plot([x for x in range(ns2[len(ns2)-1])], pcas[len(pcas)-1].singular_values_)\n",
      "plt.title(\"PCA Scree Plot\", fontsize=14)\n",
      "plt.show()\n",
      "102/97:\n",
      "def pcacmp(pca, i, j, l):\n",
      "    c1 = pca.components_[i]\n",
      "    c2 = pca.components_[j]\n",
      "    x = []\n",
      "    y = []\n",
      "    for k in range(l):\n",
      "        x.append(np.dot(df2.iloc[k], c1))\n",
      "        y.append(np.dot(df2.iloc[k], c2))\n",
      "    plt.plot(x, y, \"o\")\n",
      "    plt.xlabel(\"PCA \" + str(i+1))\n",
      "    plt.ylabel(\"PCA \" + str(j+1))\n",
      "102/98:\n",
      "pcacmp(pcas[7], 0, 1, 5000)\n",
      "plt.title(\"PCA component 1 vs component 2\")\n",
      "102/99:\n",
      "pcacmp(pcas[7], 0, 1, 5000)\n",
      "plt.title(\"Principal Component 1 vs 2\", fontsize=14)\n",
      "102/100:\n",
      "plt.plot([x for x in range(ns2[len(ns2)-1])], pcas[len(pcas)-1].singular_values_)\n",
      "plt.title(\"PCA Scree Plot\", fontsize=14)\n",
      "plt.xlabel(\"Component number\")\n",
      "plt.ylabel(\"Eigenvalue\")\n",
      "plt.show()\n",
      "102/101:\n",
      "plt.plot([x for x in range(ns2[len(ns2)-1])], np.sqrt(pcas[len(pcas)-1].singular_values_))\n",
      "plt.title(\"PCA Scree Plot\", fontsize=14)\n",
      "plt.xlabel(\"Component number\")\n",
      "plt.ylabel(\"Eigenvalue\")\n",
      "plt.show()\n",
      "102/102:\n",
      "plt.plot([x for x in range(ns2[len(ns2)-1])], pcas[len(pcas)-1].singular_values_)\n",
      "plt.title(\"PCA Scree Plot\", fontsize=14)\n",
      "plt.xlabel(\"Component number\")\n",
      "plt.ylabel(\"Eigenvalue\")\n",
      "plt.show()\n",
      "102/103:\n",
      "pcacmp(pcas[7], 14, 15, 5000)\n",
      "plt.title(\"Principal Component 15 vs 16\", fontsize=14)\n",
      "102/104: plt.plot(ns, scores)\n",
      "102/105:\n",
      "ldas2 = []\n",
      "scores = []\n",
      "ns = [3,5,10,15,20,25]\n",
      "for n in ns:\n",
      "    print(n)\n",
      "    lda = LatentDirichletAllocation(n_components=n, learning_method='online')\n",
      "    ldas2.append(lda.fit(train.values))\n",
      "    score = lda.score(test.values)\n",
      "    print(score)\n",
      "    scores.append(score)\n",
      "102/106: from sklearn.decomposition import LatentDirichletAllocation\n",
      "102/107:\n",
      "trainInds = np.random.uniform(0,1,df2.shape[0])<=0.8\n",
      "train=df2.iloc[trainInds]\n",
      "test=df2.iloc[~trainInds]\n",
      "102/108:\n",
      "ldas2 = []\n",
      "scores = []\n",
      "ns = [3,5,10,15,20,25]\n",
      "for n in ns:\n",
      "    print(n)\n",
      "    lda = LatentDirichletAllocation(n_components=n, learning_method='online')\n",
      "    ldas2.append(lda.fit(train.values))\n",
      "    score = lda.score(test.values)\n",
      "    print(score)\n",
      "    scores.append(score)\n",
      "102/109: plt.plot(ns, scores)\n",
      "102/110:\n",
      "plt.plot(ns, scores)\n",
      "plt.title(\"LDA Log Likelihood\", fontsize=12)\n",
      "plt.xlabel(\"Number of components\")\n",
      "plt.ylabel(\"Log likelihood\")\n",
      "102/111:\n",
      "plt.plot(ns, scores)\n",
      "plt.title(\"LDA Log Likelihood\", fontsize=14)\n",
      "plt.xlabel(\"Number of components\")\n",
      "plt.ylabel(\"Log likelihood\")\n",
      "102/112: print(aucs)\n",
      "102/113: from sklearn.metrics import confusion_matrix\n",
      "102/114:\n",
      "def metrics(var, models, test):\n",
      "    preds = models[i].predict(test.drop([var], axis=1))\n",
      "    tn, fp, fn, tp = confusion_matrix(test[var], preds).ravel()\n",
      "    precision = tp/(tp+fp)\n",
      "    recall = tp/(tp+fn)\n",
      "    f1 = 2*precision*recall/(precision+recall)\n",
      "102/115:\n",
      "def metrics(var, models, test):\n",
      "    preds = models[i].predict(test.drop([var], axis=1))\n",
      "    tn, fp, fn, tp = confusion_matrix(test[var], preds).ravel()\n",
      "    precision = tp/(tp+fp)\n",
      "    recall = tp/(tp+fn)\n",
      "    f1 = 2*precision*recall/(precision+recall)\n",
      "    return precision, recall, f1\n",
      "102/116: print(metrics('arrest', models, test))\n",
      "102/117: print(metrics('violent', models2, test2))\n",
      "102/118:\n",
      "def metrics(var, models, test):\n",
      "    ret = []\n",
      "    for i in range(len(models)):\n",
      "        preds = models[i].predict(test.drop([var], axis=1))\n",
      "        tn, fp, fn, tp = confusion_matrix(test[var], preds).ravel()\n",
      "        precision = tp/(tp+fp)\n",
      "        recall = tp/(tp+fn)\n",
      "        f1 = 2*precision*recall/(precision+recall)\n",
      "        ret.append((precision, recall, f1))\n",
      "    return ret\n",
      "102/119: print(metrics('arrest', models, test))\n",
      "102/120: print(metrics('violent', models2, test2))\n",
      "102/121:\n",
      "a2 = metrics('violent', models2, test2)\n",
      "for a in a2:\n",
      "    print a\n",
      "102/122:\n",
      "a2 = metrics('violent', models2, test2)\n",
      "for i in len(a2):\n",
      "    print(labels[i], a2[i])\n",
      "102/123:\n",
      "a2 = metrics('violent', models2, test2)\n",
      "for i in range(len(a2)):\n",
      "    print(labels[i], a2[i])\n",
      "102/124:\n",
      "a1 = metrics('arrest', models, test)\n",
      "for i in range(len(a1)):\n",
      "    print(labels[i], a1[i])\n",
      "102/125: train\n",
      "102/126:\n",
      "from sklearn.mixture import GaussianMixture\n",
      "# WARNING TAKES >= 6 HOURS, DO NOT RUN!\n",
      "gmms = []\n",
      "for k in [3,5,10,20,30,40,50]:\n",
      "gmms.append(GaussianMixture(k).fit(train))\n",
      "102/127:\n",
      "from sklearn.mixture import GaussianMixture\n",
      "# WARNING TAKES >= 6 HOURS, DO NOT RUN!\n",
      "gmms = []\n",
      "for k in [3,5,10,20,30,40,50]:\n",
      "    gmms.append(GaussianMixture(k).fit(train))\n",
      "102/128:\n",
      "gscores = []\n",
      "for gmm in gmms:\n",
      "    s = gmm.score(test)\n",
      "    gscores.append(s)\n",
      "102/129:\n",
      "for x in gscores:\n",
      "    print(g)\n",
      "102/130:\n",
      "for g in gscores:\n",
      "    print(g)\n",
      "102/131: plt.plot([3,5,10,20,30,40,50], gscores)\n",
      "102/132: plt.plot([3,5,10,20,30,40,50], gscores-1800)\n",
      "102/133: plt.plot([3,5,10,20,30,40,50], np.array(gscores)-1800)\n",
      "102/134: np.argmax(np.array(gscores))\n",
      "102/135: np.max(np.array(gscores))-1800\n",
      "102/136:\n",
      "gmmBest = gmms[1]\n",
      "Zgmm = gmmBest.predict_proba(test)\n",
      "means = gmmBest.means_\n",
      "reconstructedMatrix = Zgmm @ means\n",
      "error = (test - reconstructedMatrix)**2\n",
      "error.sum().sum() / error.shape[0]\n",
      "102/137:\n",
      "Zgmm2 = gmmBest.predict_proba(train)\n",
      "reconstructedMatrix2 = Zgmm2 @ means\n",
      "error2 = (train - reconstructedMatrix2)**2\n",
      "error2.sum().sum() / error2.shape[0]\n",
      "102/138:\n",
      "plt.plot([3,5,10,20,30,40,50], np.array(gscores)-1800)\n",
      "plt.title(\"GMM Log-Likelihood\", fontsize=14)\n",
      "plt.xlabel(\"Number of components\")\n",
      "plt.ylabel(\"Log-likelihood\")\n",
      "102/139:\n",
      "plt.figure()\n",
      "plt.plot([x for x in range(243-t, 243)], weekcounts[243-t:243], label=\"actual\")\n",
      "plt.plot([x for x in range(243-t, 243)], preds, label=\"predicted\")\n",
      "# plt.plot([x for x in range(243-t, 243)], weekcounts[242-t:242], label=\"persistence\")\n",
      "\n",
      "plt.legend()\n",
      "plt.title(\"Chicago crimes Apr 2018-Apr 2019\", fontsize=20)\n",
      "plt.xlabel(\"Week\")\n",
      "plt.ylabel(\"Number of crimes\")\n",
      "\n",
      "plt.figure()\n",
      "plt.plot([x for x in range(243)], weekcounts[:243], label=\"actual\")\n",
      "plt.plot([x for x in range(243-t, 243)], preds, label=\"predicted\")\n",
      "\n",
      "plt.legend()\n",
      "plt.title(\"Chicago crimes 2016-2019\", fontsize=20)\n",
      "plt.xlabel(\"Week\")\n",
      "plt.ylabel(\"Number of crimes\")\n",
      "\n",
      "plt.ylim(0, 4700)\n",
      "\n",
      "plt.show()\n",
      "102/140: np.mean(weekcounts[242-t:242])\n",
      "102/141: np.median(lrscores_d)\n",
      "102/142: lrs_d, lrscores_d, rmses_d, ws_d = predictsplit('district')\n",
      "102/143:\n",
      "def predictsplit(split):\n",
      "    lrs = []\n",
      "    lrscores = []\n",
      "    rmses = []\n",
      "    ws = []\n",
      "    for x in df16[split].unique():\n",
      "        lr = LinearRegression()\n",
      "\n",
      "        df16_ = df16[df16[split] == x].copy()\n",
      "\n",
      "        weekcounts_ = []\n",
      "        for i in range(243):\n",
      "            if i in df16_['week'].unique():\n",
      "                weekcounts_.append(df16_['week'].value_counts()[i])\n",
      "            else:\n",
      "                weekcounts_.append(0)\n",
      "        weekcounts_ = np.array(weekcounts_)\n",
      "\n",
      "        X = np.array([weekcounts_[:242-73-t], weekcounts_[1:242-72-t], weekcounts_[72:241-t], weekcounts_[73:242-t]]).transpose()\n",
      "        y = weekcounts_[74:243-t]\n",
      "\n",
      "        Xt = np.array([weekcounts_[242-73-t:242-73], weekcounts_[242-72-t:242-72], weekcounts_[241-t:241], weekcounts_[242-t:242]]).transpose()\n",
      "        yt = weekcounts_[243-t:243]\n",
      "\n",
      "        lr.fit(X, y)\n",
      "        s = lr.score(Xt,yt)\n",
      "        yp = lr.predict(Xt)\n",
      "\n",
      "        rmse = np.sqrt(mean_squared_error(yt, yp))\n",
      "\n",
      "        print(str(x) + \": \" + str(s) + \" \" + str(rmse))\n",
      "        lrscores.append(s)\n",
      "        lrs.append(lr)\n",
      "        rmses.append(rmse)\n",
      "        ws.append(weekcounts_)\n",
      "    return lrs, lrscores, rmses, ws\n",
      "102/144: lrs_d, lrscores_d, rmses_d, ws_d = predictsplit('district')\n",
      "102/145: np.median(lrscores_d)\n",
      "102/146: np.median(lrscores_d), np.max(lrscores_d)\n",
      "102/147: np.median(lrscores_d), np.max(lrscores_d[:len(lrscores_d)-1])\n",
      "102/148: lrs_ca, lrscores_ca, rmses_ca, ws_ca = predictsplit('community_area')\n",
      "102/149: np.median(lrscores_ca), np.max(lrscores_ca[:len(lrscores_ca)-1])\n",
      "102/150: np.median(lrscores_ca), np.max([x for x in lrscores_ca if not x[0].isnan])\n",
      "102/151: np.median(lrscores_ca), np.max([x for x in lrscores_ca if not x== 1)\n",
      "102/152: np.median(lrscores_ca), np.max([x for x in lrscores_ca if not x is 1)\n",
      "102/153: np.median(lrscores_ca), np.max([x for x in lrscores_ca if not x is 1])\n",
      "102/154: np.median(lrscores_ca), np.max([x for x in lrscores_ca if not x == 1])\n",
      "102/155: lrs_w, lrscores_w, rmses_w = predictsplit('ward')\n",
      "102/156: lrs_w, lrscores_w, rmses_w = predictsplit('ward')\n",
      "102/157: np.median(lrscores_w), np.max([x for x in lrscores_w if not x == 1])\n",
      "102/158: lrs_w, lrscores_w, rmses_w, ws_w = predictsplit('ward')\n",
      "102/159: np.median(lrscores_w), np.max([x for x in lrscores_w if not x == 1])\n",
      "102/160: len(df0), len(df16)\n",
      "102/161: df0['location_description'].value_counts()\n",
      "102/162: df2['location_description'].value_counts()\n",
      "102/163: df2['location'].value_counts()\n",
      "102/164: [x for x in df2.columns if 'location' in x]\n",
      "102/165: [(x, sum(df2[x]) for x in df2.columns if 'location' in x]\n",
      "102/166: [(x, sum(df2[x])) for x in df2.columns if 'location' in x]\n",
      "102/167: showCategorical(\"violent\", \"Violent\", 20, True)\n",
      "102/168:\n",
      "data = df0.copy()\n",
      "def showCategorical(cat, title, f, legend):\n",
      "    for t in pd.unique(data[cat]):\n",
      "        d = data[data[cat] == t]\n",
      "        plt.scatter(d.longitude, d.latitude, s=.001, label=t)\n",
      "    plt.title(title, fontsize=f)\n",
      "    if legend:\n",
      "        lgnd = plt.legend()\n",
      "        lgnd.legendHandles[0]._sizes = [30]\n",
      "        lgnd.legendHandles[1]._sizes = [30]\n",
      "    plt.show()\n",
      "102/169: showCategorical(\"violent\", \"Violent\", 20, True)\n",
      "102/170:\n",
      "def showCategorical(cat, title, f, legend, data):\n",
      "    data = data.copy()\n",
      "    for t in pd.unique(data[cat]):\n",
      "        d = data[data[cat] == t]\n",
      "        plt.scatter(d.longitude, d.latitude, s=.001, label=t)\n",
      "    plt.title(title, fontsize=f)\n",
      "    if legend:\n",
      "        lgnd = plt.legend()\n",
      "        lgnd.legendHandles[0]._sizes = [30]\n",
      "        lgnd.legendHandles[1]._sizes = [30]\n",
      "    plt.show()\n",
      "102/171: showCategorical(\"violent\", \"Violent\", 20, True, df2)\n",
      "102/172:\n",
      "df0['violent'] = df2['violent'].values\n",
      "showCategorical(\"violent\", \"Violent\", 20, True, df2)\n",
      "102/173:\n",
      "df0['violent'] = df2['violent'].values\n",
      "showCategorical(\"violent\", \"Violent\", 20, True, df0)\n",
      "102/174:\n",
      "data = df0.copy()\n",
      "def showCategorical(cat, title, f, legend):\n",
      "    for t in pd.unique(data[cat]):\n",
      "        d = data[data[cat] == t]\n",
      "        plt.scatter(d.longitude, d.latitude, s=.001, label=t)\n",
      "    plt.title(title, fontsize=f)\n",
      "    if legend:\n",
      "        lgnd = plt.legend()\n",
      "        lgnd.legendHandles[0]._sizes = [30]\n",
      "        lgnd.legendHandles[1]._sizes = [30]\n",
      "    plt.show()\n",
      "102/175:\n",
      "df0['violent'] = df2['violent'].values\n",
      "showCategorical(\"violent\", \"Violent\", 20, True)\n",
      "102/176: sum(df0['violent'])\n",
      "102/177: sum(df0['violent'])/len(df0)\n",
      "102/178: showCategorical(\"district\", \"District\", 20, True)\n",
      "102/179:\n",
      "get_type = []\n",
      "crime_data = df0.copy()\n",
      "for i in range(0,crime_data.shape[0]):\n",
      "    primary = crime_data.iloc[i].primary_type\n",
      "    get_index = -1\n",
      "    \n",
      "    for j in range(0, len(get_type)):\n",
      "        if (get_type[j][0] == primary):\n",
      "            get_index = j\n",
      "            get_type[j][1]+=1\n",
      "    \n",
      "    if get_index == -1:\n",
      "        get_type.append([primary, 1])\n",
      "\n",
      "type_data = pd.DataFrame(columns=['Type', 'count'], data=get_type)\n",
      "fig1, ax1 = plt.subplots()\n",
      "fig1.set_size_inches(18.5, 10.5)\n",
      "ax1.pie(type_data['count'], labels=type_data['Type'], autopct='%1.1f%%',startangle=90)\n",
      "ax1.axis('equal')  # Equal aspect ratio ensures that pie is drawn as a circle.\n",
      "plt.title(\"Chicago Crimes 2018-2019 by Type\")\n",
      "plt.tight_layout()\n",
      "plt.show()\n",
      "102/180: showCategorical(\"district\", \"District\", 20, False)\n",
      "102/181: sum(df0['arrest'])/len(df0)\n",
      "102/182: sum(df0['violent'])/len(df0)\n",
      "102/183: showCategorical(\"district\", \"District\", 20, False)\n",
      "107/1: import pandas as pd\n",
      "107/2: from sklearn import linear_model\n",
      "107/3: data = pd.read_csv('provincial_level.csv')\n",
      "107/4: data.head()\n",
      "107/5: prov = pd.read_csv('provincial_level.csv')\n",
      "107/6: prov.head()\n",
      "107/7: priv = pd.read_csv('private_expenditure')\n",
      "107/8: priv = pd.read_csv('private_expenditure.csv')\n",
      "107/9: pub = pd.read_csv('national_spending_by_sector.csv')\n",
      "107/10: priv.head()''\n",
      "107/11: priv.head()\n",
      "107/12: pub.head()\n",
      "107/13: import pyplot as plt\n",
      "107/14: from matplotlib import pyplot as plt\n",
      "107/15: pub.summary()\n",
      "107/16: len(pub)\n",
      "107/17: pub.shape\n",
      "107/18: pub\n",
      "107/19: pub[:43]\n",
      "107/20: pub[:44]\n",
      "107/21: plt.plot(pub['private-sector']/pub['total'], pub['year'])\n",
      "107/22: plt.plot(pub['Private-sector']/pub['Total'], pub['Year'])\n",
      "107/23: pub['Year']\n",
      "107/24: pub.columns\n",
      "107/25: plt.plot(pub['Year'], pub['Private-sector ']/pub['Total'])\n",
      "107/26: plt.plot(pub['Year'], pub['Private-sector ']/pub['Total '])\n",
      "107/27: plt.plot(pub['Year'], pub['Private-sector ']/pub['Total '], \"o\")\n",
      "107/28:\n",
      "for i in range(5):\n",
      "    plt.figure()\n",
      "    pub_ = pub[i*44:i*44+44]\n",
      "    plt.plot(pub_['Year'], pub_['Private-sector ']/pub_['Total '], \"o\"))\n",
      "107/29:\n",
      "for i in range(5):\n",
      "    plt.figure()\n",
      "    pub_ = pub[i*44:i*44+44]\n",
      "    plt.plot(pub_['Year'], pub_['Private-sector ']/pub_['Total '], \"o\")\n",
      "107/30:\n",
      "plt.plot(pub['Year'], pub['Private-sector ']/pub['Total '], \"o\")\n",
      "plt.title('Proportion of private sector expenditure, 1975-2018')\n",
      "107/31:\n",
      "for i in range(5):\n",
      "    plt.figure()\n",
      "    pub_ = pub[i*44:i*44+44]\n",
      "    plt.plot(pub_['Year'], pub_['Private-sector ']/pub_['Total '], \"o\")\n",
      "    plt.title('Proportion of private sector expenditure, 1975-2018')\n",
      "107/32:\n",
      "pub1 = pub[:44]\n",
      "plt.plot(pub1['Year'], pub1['Private-sector ']/pub1['Total '], \"o\")\n",
      "plt.title('Proportion of private sector expenditure, 1975-2018')\n",
      "107/33: pub.isNaN()\n",
      "107/34: pub.null()\n",
      "107/35: priv\n",
      "107/36: priv.columns\n",
      "107/37: priv[0]\n",
      "107/38: priv.iloc[0]\n",
      "107/39: priv.iloc[,0]\n",
      "107/40: priv.iloc[0,:]\n",
      "107/41: p88 = priv.iloc[0,:]\n",
      "107/42:\n",
      "labels = priv.columns[2:len(priv.columns)-1]\n",
      "vals = [p88[label] for label in labels]\n",
      "plt.pie(vals, labels=labels)\n",
      "107/43: p88 = priv.iloc[0,:]\n",
      "107/44: priv.iloc[0,:]\n",
      "107/45:\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "labels = [x for x in priv.columns[2:len(priv.columns)-1] if x is not np.NaN]\n",
      "vals = [p88[label] for label in labels]\n",
      "plt.pie(vals, labels=labels)\n",
      "107/46: import numpy as np\n",
      "107/47:\n",
      "labels = [x for x in priv.columns[2:len(priv.columns)-1] if x is not np.NaN]\n",
      "vals = [p88[label] for label in labels]\n",
      "plt.pie(vals, labels=labels)\n",
      "107/48: priv.iloc[0,:]\n",
      "107/49:\n",
      "labels = [x for x in priv.columns[2:len(priv.columns)-1] if x is not np.NaN]\n",
      "print labels\n",
      "vals = [p88[label] for label in labels]\n",
      "plt.pie(vals, labels=labels)\n",
      "107/50:\n",
      "labels = [x for x in priv.columns[2:len(priv.columns)-1] if x is not np.NaN]\n",
      "print(labels)\n",
      "vals = [p88[label] for label in labels]\n",
      "plt.pie(vals, labels=labels)\n",
      "107/51:\n",
      "labels = [x for x in priv.columns[2:len(priv.columns)-1] if x is not 'Public Health ']\n",
      "vals = [p88[label] for label in labels]\n",
      "plt.pie(vals, labels=labels)\n",
      "107/52:\n",
      "labels = [x for x in priv.columns[2:len(priv.columns)-1] if x != 'Public Health ']\n",
      "\n",
      "vals = [p88[label] for label in labels]\n",
      "plt.pie(vals, labels=labels)\n",
      "107/53:\n",
      "labels = [x for x in priv.columns[2:len(priv.columns)-1] if x != 'Public Health ']\n",
      "\n",
      "vals = [p88[label] for label in labels]\n",
      "plt.pie(vals, labels=labels)\n",
      "plt.show()\n",
      "107/54:\n",
      "p88 = priv.iloc[0,:]\n",
      "labels = [x for x in priv.columns[2:len(priv.columns)-1] if x != 'Public Health ']\n",
      "vals = [p88[label] for label in labels]\n",
      "plt.pie(vals, labels=labels)\n",
      "plt.show()\n",
      "107/55:\n",
      "p18 = priv.iloc[44,:]\n",
      "labels = [x for x in priv.columns[2:len(priv.columns)-1] if x != 'Public Health ']\n",
      "vals = [p88[label] for label in labels]\n",
      "plt.pie(vals, labels=labels)\n",
      "plt.show()\n",
      "107/56: priv.iloc[44,:]\n",
      "107/57: priv.iloc[43,:]\n",
      "107/58: priv.iloc[40,:]\n",
      "107/59: priv.iloc[39,:]\n",
      "107/60: priv.iloc[30,:]\n",
      "107/61: priv.iloc[29,:]\n",
      "107/62: priv.iloc[28,:]\n",
      "107/63:\n",
      "p16 = priv.iloc[28,:]\n",
      "labels = [x for x in priv.columns[2:len(priv.columns)-1] if x != 'Public Health ']\n",
      "vals = [p16[label] for label in labels]\n",
      "plt.pie(vals, labels=labels)\n",
      "plt.show()\n",
      "107/64:\n",
      "p88 = priv.iloc[0,:]\n",
      "labels = [x for x in priv.columns[2:len(priv.columns)-1] if x != 'Public Health ']\n",
      "vals = [p88[label] for label in labels]\n",
      "patches, texts = plt.pie(vals, labels=labels)\n",
      "plt.legend(patches, labels, loc=\"best\")\n",
      "plt.axis('equal')\n",
      "plt.tight_layout()\n",
      "plt.show()\n",
      "107/65:\n",
      "p88 = priv.iloc[0,:]\n",
      "labels = [x for x in priv.columns[2:len(priv.columns)-1] if x != 'Public Health ']\n",
      "vals = [p88[label] for label in labels]\n",
      "patches, texts = plt.pie(vals, labels=labels)\n",
      "plt.legend(patches, labels, loc=\"best\")\n",
      "plt.tight_layout()\n",
      "plt.show()\n",
      "107/66:\n",
      "p88 = priv.iloc[0,:]\n",
      "labels = [x for x in priv.columns[2:len(priv.columns)-1] if x != 'Public Health ']\n",
      "vals = [p88[label] for label in labels]\n",
      "patches, texts = plt.pie(vals, labels=labels)\n",
      "plt.legend(patches, labels, loc=\"best\")\n",
      "\n",
      "plt.show()\n",
      "107/67:\n",
      "p88 = priv.iloc[0,:]\n",
      "labels = [x for x in priv.columns[2:len(priv.columns)-1] if x != 'Public Health ']\n",
      "vals = [p88[label] for label in labels]\n",
      "patches, texts = plt.pie(vals, labels=labels)\n",
      "plt.legend(patches, labels, loc=\"right\")\n",
      "plt.show()\n",
      "107/68:\n",
      "p88 = priv.iloc[0,:]\n",
      "labels = [x for x in priv.columns[2:len(priv.columns)-1] if x != 'Public Health ']\n",
      "vals = [p88[label] for label in labels]\n",
      "patches, texts = plt.pie(vals, labels=labels)\n",
      "plt.legend(patches, labels)\n",
      "plt.show()\n",
      "107/69:\n",
      "p88 = priv.iloc[0,:]\n",
      "labels = [x for x in priv.columns[2:len(priv.columns)-1] if x != 'Public Health ']\n",
      "vals = [p88[label] for label in labels]\n",
      "plt.pie(vals, labels=labels)\n",
      "plt.show()\n",
      "107/70:\n",
      "p88 = priv.iloc[0,:]\n",
      "labels = [x for x in priv.columns[2:len(priv.columns)-1] if x != 'Public Health ']\n",
      "vals = [p88[label] for label in labels]\n",
      "plt.pie(vals, labels=labels, radius=1000)\n",
      "plt.show()\n",
      "107/71:\n",
      "p88 = priv.iloc[0,:]\n",
      "labels = [x for x in priv.columns[2:len(priv.columns)-1] if x != 'Public Health ']\n",
      "vals = [p88[label] for label in labels]\n",
      "plt.pie(vals, labels=labels, radius=100)\n",
      "plt.show()\n",
      "107/72:\n",
      "p88 = priv.iloc[0,:]\n",
      "labels = [x for x in priv.columns[2:len(priv.columns)-1] if x != 'Public Health ']\n",
      "vals = [p88[label] for label in labels]\n",
      "plt.pie(vals, labels=labels, radius=10)\n",
      "plt.show()\n",
      "107/73:\n",
      "p88 = priv.iloc[0,:]\n",
      "labels = [x for x in priv.columns[2:len(priv.columns)-1] if x != 'Public Health ']\n",
      "vals = [p88[label] for label in labels]\n",
      "plt.pie(vals, labels=labels, radius=5)\n",
      "plt.show()\n",
      "107/74:\n",
      "p88 = priv.iloc[0,:]\n",
      "labels = [x for x in priv.columns[2:len(priv.columns)-1] if x != 'Public Health ']\n",
      "vals = [p88[label] for label in labels]\n",
      "plt.pie(vals, labels=labels, radius=3)\n",
      "plt.show()\n",
      "107/75:\n",
      "p16 = priv.iloc[28,:]\n",
      "labels = [x for x in priv.columns[2:len(priv.columns)-1] if x != 'Public Health ']\n",
      "vals = [p16[label] for label in labels]\n",
      "plt.pie(vals, labels=labels, radius=3)\n",
      "plt.show()\n",
      "107/76:\n",
      "p88 = priv.iloc[0,:]\n",
      "labels = [x for x in priv.columns[2:len(priv.columns)-1] if x != 'Public Health ']\n",
      "vals = [p88[label] for label in labels]\n",
      "plt.pie(vals, labels=labels, radius=2)\n",
      "plt.show()\n",
      "107/77:\n",
      "p16 = priv.iloc[28,:]\n",
      "labels = [x for x in priv.columns[2:len(priv.columns)-1] if x != 'Public Health ']\n",
      "vals = [p16[label] for label in labels]\n",
      "plt.pie(vals, labels=labels, radius=2)\n",
      "plt.show()\n",
      "107/78:\n",
      "pdrugs = priv[:29]\n",
      "plt.plot(pdrugs['Year'], pdrugs['Prescribed Drugs '])\n",
      "107/79:\n",
      "pdrugs = priv[:29]\n",
      "plt.plot(pdrugs['Year '], pdrugs['Prescribed Drugs '])\n",
      "107/80:\n",
      "p88 = priv.iloc[0,:]\n",
      "labels = [x for x in priv.columns[2:len(priv.columns)-1] if x != 'Public Health ' and 'Total' not in x]\n",
      "vals = [p88[label] for label in labels]\n",
      "plt.pie(vals, labels=labels, radius=2)\n",
      "plt.show()\n",
      "107/81:\n",
      "p16 = priv.iloc[28,:]\n",
      "labels = [x for x in priv.columns[2:len(priv.columns)-1] if x != 'Public Health ' and 'Total' not in x]\n",
      "vals = [p16[label] for label in labels]\n",
      "plt.pie(vals, labels=labels, radius=2)\n",
      "plt.show()\n",
      "107/82:\n",
      "pdrugs = priv[:29]\n",
      "plt.plot(pdrugs['Year '], pdrugs['Prescribed Drugs ']/pdrugs['Total Drugs '])\n",
      "107/83:\n",
      "pdrugs = priv[:29]\n",
      "plt.plot(pdrugs['Year '], pdrugs['Prescribed Drugs ']/pdrugs['Total Drugs'])\n",
      "107/84: plt.plot(pdrugs['Year '], pdrugs['Prescribed Drugs ']/pdrugs['Grand Total '])\n",
      "107/85:\n",
      "p88 = priv.iloc[0,:]\n",
      "labels = [x for x in priv.columns[2:len(priv.columns)-1] if x != 'Grand Total ' and ':' not in x]\n",
      "vals = [p88[label] for label in labels]\n",
      "plt.pie(vals, labels=labels, radius=2)\n",
      "plt.show()\n",
      "107/86:\n",
      "p88 = priv.iloc[0,:]\n",
      "labels = [x for x in priv.columns[2:len(priv.columns)-1] if x != 'Grand Total ' and x != 'Public Health ' and ':' not in x]\n",
      "vals = [p88[label] for label in labels]\n",
      "plt.pie(vals, labels=labels, radius=2)\n",
      "plt.show()\n",
      "107/87:\n",
      "p16 = priv.iloc[28,:]\n",
      "labels = [x for x in priv.columns[2:len(priv.columns)-1] if x != 'Grand Total ' and x != 'Public Health ' and ':' not in x]\n",
      "vals = [p16[label] for label in labels]\n",
      "plt.pie(vals, labels=labels, radius=2)\n",
      "plt.show()\n",
      "107/88:\n",
      "p88 = priv.iloc[0,:]\n",
      "labels = [x for x in priv.columns[2:len(priv.columns)-1] if x not in ['Grand Total ', 'Public Health ', 'Prescribed Drugs ', 'Over-the-counter Drugs'\\\n",
      "                                                                      and ':' not in x]\n",
      "vals = [p88[label] for label in labels]\n",
      "plt.pie(vals, labels=labels, radius=2)\n",
      "plt.show()\n",
      "107/89:\n",
      "p88 = priv.iloc[0,:]\n",
      "labels = [x for x in priv.columns[2:len(priv.columns)-1] if x not in ['Grand Total ', 'Public Health ', 'Prescribed Drugs ', 'Over-the-counter Drugs']\\\n",
      "                                                                      and ':' not in x]\n",
      "vals = [p88[label] for label in labels]\n",
      "plt.pie(vals, labels=labels, radius=2)\n",
      "plt.show()\n",
      "107/90:\n",
      "p88 = priv.iloc[0,:]\n",
      "labels = [x for x in priv.columns[2:len(priv.columns)-1] if x not in ['Grand Total ', 'Public Health ', 'Prescribed Drugs ', 'Over-the-Counter Drugs']\\\n",
      "                                                                      and ':' not in x]\n",
      "vals = [p88[label] for label in labels]\n",
      "plt.pie(vals, labels=labels, radius=2)\n",
      "plt.show()\n",
      "107/91:\n",
      "p16 = priv.iloc[28,:]\n",
      "labels = [x for x in priv.columns[2:len(priv.columns)-1] if x not in ['Grand Total ', 'Public Health ', 'Prescribed Drugs ', 'Over-the-Counter Drugs']\\\n",
      "                                                                      and ':' not in x]\n",
      "vals = [p16[label] for label in labels]\n",
      "plt.pie(vals, labels=labels, radius=2)\n",
      "plt.show()\n",
      "107/92:\n",
      "pdrugs = priv[:29]\n",
      "plt.plot(pdrugs['Year '], pdrugs['Administration ']/pdrugs['Grand Total '])\n",
      "107/93: from linear_model import LinearRegression\n",
      "107/94: from sklearn.linear_model import LinearRegression\n",
      "107/95: labels = [x for x in priv.columns[2:len(priv.columns)-1] if x != 'Public Health ' and 'Total ' not in x]\n",
      "107/96:\n",
      "X = pdrugs['Prescribed Drugs ']\n",
      "lr = LinearRegression().fit(X, pdrugs['Grand Total '])\n",
      "107/97:\n",
      "X = pdrugs['Prescribed Drugs '].reshape(-1, 1)\n",
      "lr = LinearRegression().fit(X, pdrugs['Grand Total '])\n",
      "107/98:\n",
      "X = np.array(pdrugs['Prescribed Drugs ']).reshape(-1, 1)\n",
      "lr = LinearRegression().fit(X, pdrugs['Grand Total '])\n",
      "107/99:\n",
      "X = np.array(pdrugs['Prescribed Drugs ']).reshape(-1, 1)\n",
      "lr = LinearRegression().fit(X, pdrugs['Grand Total '])\n",
      "lr.score()\n",
      "107/100:\n",
      "X = np.array(pdrugs['Prescribed Drugs ']).reshape(-1, 1)\n",
      "y = pdrugs['Grand Total ']\n",
      "lr = LinearRegression().fit(X, y)\n",
      "lr.score(X, y)\n",
      "107/101:\n",
      "X = np.array(pdrugs['Administration ']).reshape(-1, 1)\n",
      "y = pdrugs['Grand Total ']\n",
      "lr = LinearRegression().fit(X, y)\n",
      "lr.score(X, y)\n",
      "107/102:\n",
      "X = np.array(pdrugs['Physicians ']).reshape(-1, 1)\n",
      "y = pdrugs['Grand Total ']\n",
      "lr = LinearRegression().fit(X, y)\n",
      "lr.score(X, y)\n",
      "107/103:\n",
      "X = np.array(pdrugs['Hospitals ']).reshape(-1, 1)\n",
      "y = pdrugs['Grand Total ']\n",
      "lr = LinearRegression().fit(X, y)\n",
      "lr.score(X, y)\n",
      "107/104:\n",
      "X = np.array(pdrugs['Hospitals ']).reshape(-1, 1)\n",
      "y = pdrugs['Grand Total ']\n",
      "lr = LinearRegression().fit(X, y)\n",
      "lr.score(X, y)\n",
      "lr.coef_\n",
      "107/105:\n",
      "X = np.array(pdrugs['Prescribed Drugs ']).reshape(-1, 1)\n",
      "y = pdrugs['Grand Total ']\n",
      "lr = LinearRegression().fit(X, y)\n",
      "lr.score(X, y)\n",
      "lr.coef_\n",
      "107/106:\n",
      "X = np.array(pdrugs['Prescribed Drugs ']).reshape(-1, 1)\n",
      "y = pdrugs['Grand Total ']\n",
      "lr = LinearRegression().fit(X, y)\n",
      "lr.score(X, y), lr.coef_\n",
      "107/107:\n",
      "X = np.array(pdrugs['Prescribed Drugs ']).reshape(-1, 1)\n",
      "y = pdrugs['Grand Total ']\n",
      "lr = LinearRegression().fit(X, y)\n",
      "lr.score(X, y), lr.coef_[0]\n",
      "107/108:\n",
      "X = np.array(pdrugs['Hospitals ']).reshape(-1, 1)\n",
      "y = pdrugs['Grand Total ']\n",
      "lr = LinearRegression().fit(X, y)\n",
      "lr.score(X, y)\n",
      "107/109:\n",
      "plt.figure()\n",
      "plt.plot(pdrugs['Year '], pdrugs['Grand Total '])\n",
      "107/110:\n",
      "plt.figure()\n",
      "plt.plot(pdrugs['Year '], pdrugs['Grand Total '])\n",
      "plt.plot(pdrugs['Year'], pdrugs['Prescribed Drugs '])\n",
      "107/111:\n",
      "plt.figure()\n",
      "plt.plot(pdrugs['Year '], pdrugs['Grand Total '])\n",
      "plt.plot(pdrugs['Year '], pdrugs['Prescribed Drugs '])\n",
      "107/112:\n",
      "plt.figure()\n",
      "plt.plot(pdrugs['Year '], pdrugs['Grand Total '])\n",
      "plt.plot(pdrugs['Year '], pdrugs['Prescribed Drugs '])\n",
      "plt.plot(pdrugs['Year '], pdrugs['Hospitals'])\n",
      "107/113:\n",
      "plt.figure()\n",
      "plt.plot(pdrugs['Year '], pdrugs['Grand Total '])\n",
      "plt.plot(pdrugs['Year '], pdrugs['Prescribed Drugs '])\n",
      "plt.plot(pdrugs['Year '], pdrugs['Hospitals '])\n",
      "107/114:\n",
      "plt.figure()\n",
      "plt.plot(pdrugs['Year '], pdrugs['Grand Total '])\n",
      "plt.plot(pdrugs['Year '], pdrugs['Prescribed Drugs '])\n",
      "plt.plot(pdrugs['Year '], pdrugs['Hospitals '])\n",
      "plt.plot(pdrugs['Year '], pdrugs['Administration '])\n",
      "107/115:\n",
      "X = np.array(pdrugs['Prescribed Drugs ']/pdrugs['Grand Total ']).reshape(-1, 1)\n",
      "y = pdrugs['Grand Total ']\n",
      "lr = LinearRegression().fit(X, y)\n",
      "lr.score(X, y), lr.coef_[0]\n",
      "107/116:\n",
      "X = np.array(pdrugs['Hospital ']/pdrugs['Grand Total ']).reshape(-1, 1)\n",
      "y = pdrugs['Grand Total ']\n",
      "lr = LinearRegression().fit(X, y)\n",
      "lr.score(X, y), lr.coef_[0]\n",
      "107/117:\n",
      "X = np.array(pdrugs['Hospitals ']/pdrugs['Grand Total ']).reshape(-1, 1)\n",
      "y = pdrugs['Grand Total ']\n",
      "lr = LinearRegression().fit(X, y)\n",
      "lr.score(X, y), lr.coef_[0]\n",
      "107/118:\n",
      "X = np.array(pdrugs['Prescribed Drugs ']/pdrugs['Grand Total ']).reshape(-1, 1)\n",
      "y = pdrugs['Grand Total ']\n",
      "lr = LinearRegression().fit(X, y)\n",
      "lr.score(X, y), lr.coef_[0]\n",
      "107/119: pdrugs[28]\n",
      "107/120: pdrugs.iloc[28]\n",
      "107/121: pdrugs.iloc[28][x]\n",
      "107/122: pdrugs.iloc[28]['Grand Total ']\n",
      "107/123:\n",
      "props = []\n",
      "for x in labels:\n",
      "    X = np.array(pdrugs[x]).reshape(-1, 1)\n",
      "    y = pdrugs['Grand Total ']\n",
      "    lr = LinearRegression().fit(X, y)\n",
      "    props.append(labels, lr.score(X,y), lr.coef_[0], (pdrugs.iloc[28][x]-pdrugs.iloc[0][x])/pdrugs.iloc[0][x])\n",
      "107/124:\n",
      "props = []\n",
      "for x in labels:\n",
      "    X = np.array(pdrugs[x]).reshape(-1, 1)\n",
      "    y = pdrugs['Grand Total ']\n",
      "    lr = LinearRegression().fit(X, y)\n",
      "    props.append((labels, lr.score(X,y), lr.coef_[0], (pdrugs.iloc[28][x]-pdrugs.iloc[0][x])/pdrugs.iloc[0][x]))\n",
      "107/125: print props\n",
      "107/126: print(props)\n",
      "107/127:\n",
      "props = []\n",
      "for x in labels:\n",
      "    X = np.array(pdrugs[x]).reshape(-1, 1)\n",
      "    y = pdrugs['Grand Total ']\n",
      "    lr = LinearRegression().fit(X, y)\n",
      "    props.append((x, lr.score(X,y), lr.coef_[0], (pdrugs.iloc[28][x]-pdrugs.iloc[0][x])/pdrugs.iloc[0][x]))\n",
      "107/128: print(props)\n",
      "107/129: sorted(props, key=lambda x: x[1])\n",
      "107/130: sorted(props, key=lambda x: x[1], reverse=True)\n",
      "107/131:\n",
      "props = []\n",
      "for x in labels:\n",
      "    X = np.array(pdrugs[x]).reshape(-1, 1)\n",
      "    y = pdrugs['Grand Total ']\n",
      "    lr = LinearRegression().fit(X, y)\n",
      "    props.append((x, lr.score(X,y), (pdrugs.iloc[28][x]-pdrugs.iloc[0][x])/pdrugs.iloc[0][x]))\n",
      "107/132: sorted(props, key=lambda x: x[2], reverse=True)\n",
      "107/133: sorted(props, key=lambda x: x[1], reverse=True)\n",
      "107/134:\n",
      "x = 'Grand Total '\n",
      "(pdrugs.iloc[28][x]-pdrugs.iloc[0][x])/pdrugs.iloc[0][x]\n",
      "107/135:\n",
      "props = []\n",
      "for x in labels:\n",
      "    X = np.array(pdrugs[x]).reshape(-1, 1)\n",
      "    y = pdrugs['Grand Total ']\n",
      "    lr = LinearRegression().fit(X, y)\n",
      "    X2 = np.array(pdrugs[x]/y).reshape(-1, 1)\n",
      "#     (pdrugs.iloc[28][x]-pdrugs.iloc[0][x])/pdrugs.iloc[0][x])\n",
      "    lr2 = LinearRegression().fit(X2, y)\n",
      "    props.append((x, lr.score(X,y), l2.score(X2, y))\n",
      "107/136:\n",
      "props = []\n",
      "for x in labels:\n",
      "    X = np.array(pdrugs[x]).reshape(-1, 1)\n",
      "    y = pdrugs['Grand Total ']\n",
      "    lr = LinearRegression().fit(X, y)\n",
      "    X2 = np.array(pdrugs[x]/y).reshape(-1, 1)\n",
      "#     (pdrugs.iloc[28][x]-pdrugs.iloc[0][x])/pdrugs.iloc[0][x])\n",
      "    lr2 = LinearRegression().fit(X2, y)\n",
      "    props.append((x, lr.score(X,y), l2.score(X2,y)))\n",
      "107/137:\n",
      "props = []\n",
      "for x in labels:\n",
      "    X = np.array(pdrugs[x]).reshape(-1, 1)\n",
      "    y = pdrugs['Grand Total ']\n",
      "    lr = LinearRegression().fit(X, y)\n",
      "    X2 = np.array(pdrugs[x]/y).reshape(-1, 1)\n",
      "#     (pdrugs.iloc[28][x]-pdrugs.iloc[0][x])/pdrugs.iloc[0][x])\n",
      "    lr2 = LinearRegression().fit(X2, y)\n",
      "    props.append((x, lr.score(X,y), lr2.score(X2,y)))\n",
      "107/138: sorted(props, key=lambda x: x[1], reverse=True)\n",
      "107/139:\n",
      "props = []\n",
      "for x in labels:\n",
      "    X = np.array(pdrugs[x]).reshape(-1, 1)\n",
      "    y = pdrugs['Grand Total ']\n",
      "    lr = LinearRegression().fit(X, y)\n",
      "    X2 = np.array(np.log(pdrugs[x])).reshape(-1, 1)\n",
      "#     (pdrugs.iloc[28][x]-pdrugs.iloc[0][x])/pdrugs.iloc[0][x])\n",
      "    lr2 = LinearRegression().fit(X2, np.log(y))\n",
      "    props.append((x, lr.score(X,y), lr2.score(X2,np.log(y))))\n",
      "107/140:\n",
      "x = 'Grand Total '\n",
      "(pdrugs.iloc[28][x]-pdrugs.iloc[0][x])/pdrugs.iloc[0][x]\n",
      "107/141: sorted(props, key=lambda x: x[1], reverse=True)\n",
      "107/142: len(y)''\n",
      "107/143: len(y)\n",
      "107/144:\n",
      "props = []\n",
      "for x in labels:\n",
      "    X = np.array(pdrugs[x]).reshape(-1, 1)\n",
      "    y = pdrugs['Grand Total ']\n",
      "    lr = LinearRegression().fit(X, y)\n",
      "    X2 = np.array(np.log(pdrugs[x])).reshape(-1, 1)\n",
      "    lr2 = LinearRegression().fit(X2, np.log(y))\n",
      "    X3 = np.array(pdrugs[x]/y).reshape(-1, 1)\n",
      "    lr3 = LinearRegression().fit(X3, np.log(y))\n",
      "    props.append((x, lr.score(X,y), lr2.score(X2,np.log(y)), lr3.score(X3, np.log(y))))\n",
      "107/145: sorted(props, key=lambda x: x[1], reverse=True)\n",
      "107/146:\n",
      "props = []\n",
      "for x in labels:\n",
      "    X = np.array(pdrugs[x]).reshape(-1, 1)\n",
      "    y = pdrugs['Grand Total ']\n",
      "    lr = LinearRegression().fit(X, y)\n",
      "    X2 = np.array(np.log(pdrugs[x])).reshape(-1, 1)\n",
      "    lr2 = LinearRegression().fit(X2, np.log(y))\n",
      "    X3 = np.array(log(pdrugs[x]/y)).reshape(-1, 1)\n",
      "    lr3 = LinearRegression().fit(X3, np.log(y))\n",
      "    props.append((x, lr.score(X,y), lr2.score(X2,np.log(y)), lr3.score(X3, np.log(y))))\n",
      "107/147: sorted(props, key=lambda x: x[1], reverse=True)\n",
      "107/148:\n",
      "props = []\n",
      "for x in labels:\n",
      "    X = np.array(pdrugs[x]).reshape(-1, 1)\n",
      "    y = pdrugs['Grand Total ']\n",
      "    lr = LinearRegression().fit(X, y)\n",
      "    X2 = np.array(np.log(pdrugs[x])).reshape(-1, 1)\n",
      "    lr2 = LinearRegression().fit(X2, np.log(y))\n",
      "    X3 = np.array(np.log(pdrugs[x]/y)).reshape(-1, 1)\n",
      "    lr3 = LinearRegression().fit(X3, np.log(y))\n",
      "    props.append((x, lr.score(X,y), lr2.score(X2,np.log(y)), lr3.score(X3, np.log(y))))\n",
      "107/149: sorted(props, key=lambda x: x[1], reverse=True)\n",
      "107/150: pdrugs.iloc[28]['Grand Total ']\n",
      "107/151:\n",
      "props = []\n",
      "for x in labels:\n",
      "    X = np.array(pdrugs[x]).reshape(-1, 1)\n",
      "    y = pdrugs['Grand Total ']\n",
      "    lr = LinearRegression().fit(X, y)\n",
      "    X2 = np.array(np.log(pdrugs[x])).reshape(-1, 1)\n",
      "    lr2 = LinearRegression().fit(X2, np.log(y))\n",
      "    X3 = np.array(np.log(pdrugs[x]/y)).reshape(-1, 1)\n",
      "    lr3 = LinearRegression().fit(X3, np.log(y))\n",
      "    X4 = np.array(pdrugs[x]/y).reshape(-1, 1)\n",
      "    lr4 = LinearRegression().fit(X4, np.log(y))\n",
      "    props.append((x, lr.score(X,y), lr2.score(X2,np.log(y)), lr3.score(X3, np.log(y)), lr4.score(X4, np.log(y))))\n",
      "107/152: sorted(props, key=lambda x: x[1], reverse=True)\n",
      "107/153:\n",
      "props = []\n",
      "for x in labels:\n",
      "    X = np.array(pdrugs[x]).reshape(-1, 1)\n",
      "    y = pdrugs['Grand Total ']\n",
      "    lr = LinearRegression().fit(X, y)\n",
      "    X2 = np.array(np.log(pdrugs[x])).reshape(-1, 1)\n",
      "    lr2 = LinearRegression().fit(X2, np.log(y))\n",
      "    X3 = np.array(np.log(pdrugs[x]/y)).reshape(-1, 1)\n",
      "    lr3 = LinearRegression().fit(X3, np.log(y))\n",
      "    X4 = np.array(pdrugs[x]/y).reshape(-1, 1)\n",
      "    lr4 = LinearRegression().fit(X4, y)\n",
      "    props.append((x, lr.score(X,y), lr2.score(X2,np.log(y)), lr3.score(X3, np.log(y)), lr4.score(X4, y)))\n",
      "107/154: sorted(props, key=lambda x: x[1], reverse=True)\n",
      "107/155:\n",
      "props = []\n",
      "for x in labels:\n",
      "    X = np.array(pdrugs[x]).reshape(-1, 1)\n",
      "    y = pdrugs['Grand Total ']\n",
      "    lr = LinearRegression().fit(X, y)\n",
      "    X2 = np.array(np.log(pdrugs[x])).reshape(-1, 1)\n",
      "    lr2 = LinearRegression().fit(X2, np.log(y))\n",
      "    X3 = np.array(np.log(pdrugs[x]/y)).reshape(-1, 1)\n",
      "    lr3 = LinearRegression().fit(X3, np.log(y))\n",
      "    X4 = np.array(pdrugs[x]/y).reshape(-1, 1)\n",
      "    lr4 = LinearRegression().fit(X4, np.log(y))\n",
      "    props.append((x, lr.score(X,y), lr2.score(X2,np.log(y)), lr3.score(X3, np.log(y)), lr4.score(X4, np.log(y))))\n",
      "107/156: sorted(props, key=lambda x: x[1], reverse=True)\n",
      "107/157:\n",
      "pub1 = pub[4*44:4*44+44]\n",
      "plt.plot(pub1['Year'], pub1['Private-sector ']/pub1['Total '], \"o\")\n",
      "plt.title('Proportion of private sector expenditure per capita, 1975-2018')\n",
      "107/158: lr = LinearRegression().fit(np.array(pub1['Year'].reshape(-1,1), np.array(pub1['Private-sector ']/pub1['Total ']))\n",
      "107/159:\n",
      "lr = LinearRegression().fit(np.array(pub1['Year']).reshape(-1,1), np.array(pub1['Private-sector ']/pub1['Total ']))\n",
      "lr.score()\n",
      "107/160:\n",
      "lr = LinearRegression().fit(np.array(pub1['Year']).reshape(-1,1), np.array(pub1['Private-sector ']/pub1['Total ']))\n",
      "lr.score(np.array(pub1['Year']).reshape(-1,1), np.array(pub1['Private-sector ']/pub1['Total ']))\n",
      "107/161:\n",
      "lr = LinearRegression().fit(np.array(pub1['Year']).reshape(-1,1), np.array(pub1['Private-sector ']/pub1['Total ']))\n",
      "print(lr.score(np.array(pub1['Year']).reshape(-1,1), np.array(pub1['Private-sector ']/pub1['Total '])))\n",
      "plt.figure()\n",
      "plt.plot(pub1['Year'], pub1['Private-sector ']/pub1['Total '], \"o\", pub1['Year'], lr.coef_[0]*pub1['Year']+lr.intercept_)\n",
      "plt.title('Proportion of private sector expenditure per capita, 1975-2018')\n",
      "107/162: prov['Age Groups ']\n",
      "107/163: prov['Age Groups']\n",
      "107/164: prov['Age Groups'].values()\n",
      "107/165: prov['Age Groups'].nunique()\n",
      "107/166: set(prov['Age Groups'])\n",
      "107/167: pd.get_dummies(prov, columns=['Age Groups'])\n",
      "107/168: prov\n",
      "107/169: prov2 = pd.get_dummies(prov, columns=['Age Groups'])\n",
      "107/170: prov2.head()\n",
      "107/171: prov2.where(prov['Age Groups_55-59']==1)\n",
      "107/172: prov2.where(prov2['Age Groups_55-59']==1)\n",
      "107/173: prov2.iloc[prov2.where(prov2['Age Groups_55-59']==1)]\n",
      "107/174: prov2[prov2.where(prov2['Age Groups_55-59']==1)]\n",
      "107/175: prov2.loc[prov2.where(prov2['Age Groups_55-59']==1)]\n",
      "107/176: prov2[prov2.where(prov2['Age Groups_55-59']==1)]\n",
      "107/177: prov2[prov2['Age Groups_55-59']==1]\n",
      "107/178: prov2[prov2['Age Groups_55-59']==1 and prov2['Category'] == 'E.2.1.2: Expenditure per year; both sexes']\n",
      "107/179: prov2[prov2['Age Groups_55-59']==1][prov2['Category'] == 'E.2.1.2: Expenditure per year; both sexes']\n",
      "107/180: prov2[prov2['Age Groups_55-59']==1][prov2['Category'] == 'E.2.1.1: Expenditure per year; both sexes']\n",
      "107/181: prov2[prov2['Age Groups_55-59']==1]\n",
      "107/182: prov2[prov2['Category']=='E.2.1.2: Expenditure per year; both sexes']\n",
      "107/183: import re\n",
      "107/184: prov2[re.search(prov2['Category'], 'E.2.\\d+.2: Expenditure per year; both sexes') is not None]\n",
      "107/185: [re.search(x, 'E.2.\\d+.2: Expenditure per year; both sexes') for x in prov2['Category']\n",
      "107/186: [re.search(x, 'E.2.\\d+.2: Expenditure per year; both sexes')==None for x in prov2['Category']]\n",
      "107/187: [re.search(x, 'E.2.\\d+.2: Expenditure per year; both sexes')==None for x in prov2['Category']].nunique()\n",
      "107/188: [re.search(x, 'E.2.\\d+.2: Expenditure per year; both sexes')==None for x in prov2['Category']]\n",
      "107/189: inds = [re.search(x, 'E.2.\\d+.2: Expenditure per year; both sexes')==None for x in prov2['Category']]\n",
      "107/190: inds = [re.search(x, 'E.2.\\d+.2: Expenditure per year; both sexes')!=None for x in prov2['Category']]\n",
      "107/191: sum(inds)\n",
      "107/192: inds = [re.search(x, 'E.2.\\d+.2: Expenditure per year; both sexes')==None for x in prov2['Category']]\n",
      "107/193: sum(inds)\n",
      "107/194: sum(inds)/len(inds)\n",
      "107/195: inds = [re.search(x, 'E.2.21.2: Expenditure per year; both sexes')==None for x in prov2['Category']]\n",
      "107/196: sum(inds)/len(inds)\n",
      "107/197: inds = [re.search(x, 'E.2.\\d.2: Expenditure per year; both sexes')==None for x in prov2['Category']]\n",
      "107/198: sum(inds)/len(inds)\n",
      "107/199: inds = [re.search(x, 'E.2.[0-9].2: Expenditure per year; both sexes')==None for x in prov2['Category']]\n",
      "107/200: sum(inds)/len(inds)\n",
      "107/201: inds = [re.search(x, 'E.2.*.2: Expenditure per year; both sexes')==None for x in prov2['Category']]\n",
      "107/202: sum(inds)/len(inds)\n",
      "107/203: inds = [re.search(x, 'E.2.21.2: Expenditure per year; both sexes')==None for x in prov2['Category']]\n",
      "107/204: sum(inds)/len(inds)\n",
      "107/205: inds = [re.search(x, 'E.2.20.2: Expenditure per year; both sexes')==None for x in prov2['Category']]\n",
      "107/206: sum(inds)/len(inds)\n",
      "107/207: inds = [re.search(x, 'E.2.20.2: Expenditure per year; both sexes')!=None for x in prov2['Category']]\n",
      "107/208: sum(inds)/len(inds)\n",
      "107/209: inds = [re.search(x, 'E.2.?.2: Expenditure per year; both sexes')!=None for x in prov2['Category']]\n",
      "107/210: sum(inds)/len(inds)\n",
      "107/211: inds = [re.search(x, 'Expenditure per year; both sexes')!=None for x in prov2['Category']]\n",
      "107/212: sum(inds)/len(inds)\n",
      "107/213: inds = [re.match(x, 'Expenditure per year; both sexes')!=None for x in prov2['Category']]\n",
      "107/214: sum(inds)/len(inds)\n",
      "107/215: inds = [re.search(x, 'Expenditure per year; both sexes')!=None for x in prov2['Category']]\n",
      "107/216: sum(inds)/len(inds)\n",
      "107/217: inds = [re.search('E.2.\\d+.2: Expenditure per year; both sexes', x)!=None for x in prov2['Category']]\n",
      "107/218: sum(inds)/len(inds)\n",
      "107/219: inds = [re.search('E.2.21.2: Expenditure per year; both sexes', x)!=None for x in prov2['Category']]\n",
      "107/220: sum(inds)/len(inds)\n",
      "107/221: inds = [re.search('E.2.\\d+.2: Expenditure per year; both sexes', x)!=None for x in prov2['Category']]\n",
      "107/222: sum(inds)/len(inds)\n",
      "107/223: prov2.iloc[inds]\n",
      "107/224: len(set(prov['Age Groups']))\n",
      "107/225: set(prov['Age Groups'])\n",
      "107/226: prov2[prov2['Age Groups_55-59']==1]['Population' is in prov2['Category']]\n",
      "107/227: prov2[prov2['Age Groups_55-59']==1]\n",
      "107/228: ['Population' is in x for x in prov2[prov2['Age Groups_55-59']==1]['Category']]\n",
      "107/229: ['Population' in x for x in prov2[prov2['Age Groups_55-59']==1]['Category']]\n",
      "107/230: prov2.iloc['Population' in x for x in prov2[prov2['Age Groups_55-59']==1]['Category']]\n",
      "107/231: prov2.iloc[['Population' in x for x in prov2[prov2['Age Groups_55-59']==1]['Category']]]\n",
      "107/232: prov2[prov2['Age Groups_55-59']==1]\n",
      "107/233: temp = prov2[prov2['Age Groups_55-59']==1]\n",
      "107/234: temp[['Population' in x for x in temp['Category']]]\n",
      "107/235: temp[['Population by age: both sexes' in x for x in temp['Category']]]\n",
      "107/236: temp2 = temp[['Population by age: both sexes' in x for x in temp['Category']]]\n",
      "107/237: temp2.head()\n",
      "107/238: plt.plot(temp2['Year'], temp2['Canada'])\n",
      "107/239: prov2[prov2['Age Groups_55-59']==1][['Population by age: both sexes' in x for x in temp['Category']]]\n",
      "107/240:\n",
      "byage = {}\n",
      "for age in [x for x in prov2.columns if Age in x]:\n",
      "    byage[age] = prov2[prov2[age]==1][['Population by age: both sexes' in x for x in temp['Category']]]\n",
      "107/241:\n",
      "byage = {}\n",
      "for age in [x for x in prov2.columns if 'Age' in x]:\n",
      "    byage[age] = prov2[prov2[age]==1][['Population by age: both sexes' in x for x in temp['Category']]]\n",
      "107/242:\n",
      "byage = {}\n",
      "for age in [x for x in prov2.columns if 'Age' in x]:\n",
      "    print(age)\n",
      "#     byage[age] = prov2[prov2[age]==1][['Population by age: both sexes' in x for x in temp['Category']]]\n",
      "107/243:\n",
      "byage = {}\n",
      "for age in [x for x in prov2.columns if 'Age' in x]:\n",
      "    prov2[prov2[age]==1][['Population by age: both sexes' in x for x in temp['Category']]]\n",
      "107/244:\n",
      "byage = {}\n",
      "for age in [x for x in prov2.columns if 'Age' in x and 'All' not in x]:\n",
      "    byage[age] = prov2[prov2[age]==1][['Population by age: both sexes' in x for x in temp['Category']]]\n",
      "107/245: prov2[prov2['Age Groups_All']==1]\n",
      "107/246:\n",
      "byage = {}\n",
      "for age in [x for x in prov2.columns if 'Age' in x and 'All' not in x]:\n",
      "    byage[age] = prov2[prov2[age]==1][['Population by age: both sexes' in x for x in temp['Category']]][prov2['Year']==1996]\n",
      "107/247: byage\n",
      "107/248:\n",
      "byage = {}\n",
      "for age in [x for x in prov2.columns if 'Age' in x and 'All' not in x]:\n",
      "    byage[age] = prov2[prov2[age]==1][['Population by age: both sexes' in x for x in temp['Category']]]\n",
      "107/249: byage\n",
      "107/250: byage['Age Groups_1-4']\n",
      "107/251: byage['Age Groups_1-4'][['Population by age: both sexes' in x for x in temp['Category']]]\n",
      "107/252:\n",
      "byage = {}\n",
      "for age in [x for x in prov2.columns if 'Age' in x and 'All' not in x]:\n",
      "    byage[age] = prov2[prov2[age]==1][['Population by age: both sexes' in x for x in prov2['Category']]]\n",
      "107/253:\n",
      "byage = {}\n",
      "for age in [x for x in prov2.columns if 'Age' in x and 'All' not in x]:\n",
      "    temp = prov2[prov2[age]==1]\n",
      "    byage[age] = temp[['Population by age: both sexes' in x for x in temp['Category']]]\n",
      "107/254: byage['Age Groups_1-4']\n",
      "107/255:\n",
      "byage = {}\n",
      "for age in [x for x in prov2.columns if 'Age' in x and 'All' not in x]:\n",
      "    temp = prov2[prov2[age]==1]\n",
      "    temp2 = temp[['Population by age: both sexes' in x for x in temp['Category']]]\n",
      "    byage[age] = temp2[temp2['Year'] == 1996]\n",
      "107/256: byage['Age Groups_1-4'][]\n",
      "107/257: byage['Age Groups_1-4']\n",
      "107/258: byage['Age Groups_1-4']['Canada']\n",
      "107/259:\n",
      "x = np.arange(len(byage))\n",
      "pops = [byage[age]['Canada'] for age in byage]\n",
      "pop\n",
      "107/260:\n",
      "x = np.arange(len(byage))\n",
      "pops = [byage[age]['Canada'] for age in byage]\n",
      "pops\n",
      "107/261: plt.bar(pops, byage.keys())\n",
      "107/262: plt.bar(x, pops)\n",
      "107/263:\n",
      "x = np.arange(len(byage))\n",
      "pops = [byage[age]['Canada'].value() for age in byage]\n",
      "pops\n",
      "107/264:\n",
      "x = np.arange(len(byage))\n",
      "pops = [byage[age]['Canada'].values() for age in byage]\n",
      "pops\n",
      "107/265:\n",
      "x = np.arange(len(byage))\n",
      "pops = [byage[age]['Canada'] for age in byage]\n",
      "pops\n",
      "107/266:\n",
      "x = np.arange(len(byage))\n",
      "pops = [byage[age]['Canada'] for age in byage]\n",
      "pops[0]\n",
      "107/267:\n",
      "x = np.arange(len(byage))\n",
      "pops = [byage.at[age,'Canada'] for age in byage]\n",
      "pops\n",
      "107/268:\n",
      "x = np.arange(len(byage))\n",
      "pops = [byage[age].at['Canada'] for age in byage]\n",
      "pops\n",
      "107/269:\n",
      "x = np.arange(len(byage))\n",
      "pops = [byage[age]['Canada'] for age in byage]\n",
      "pops\n",
      "107/270:\n",
      "def getbyage(year):\n",
      "    byage = {}\n",
      "    for age in [x for x in prov2.columns if 'Age' in x and 'All' not in x]:\n",
      "        temp = prov2[prov2[age]==1]\n",
      "        temp2 = temp[['Population by age: both sexes' in x for x in temp['Category']]]\n",
      "        byage[age] = temp2[temp2['Year'] == year]\n",
      "    return byage\n",
      "107/271: getbyage(1996)['Age Groups_1-4']['Canada']\n",
      "107/272: byage = getbyage(1996)['Age Groups_1-4']['Canada']\n",
      "107/273:\n",
      "x = np.arange(len(byage))\n",
      "pops = [byage[age]['Canada'] for age in byage]\n",
      "pops\n",
      "107/274:\n",
      "x = np.arange(len(byage))\n",
      "pops = [byage[age]['Canada'] for age in byage]\n",
      "plt.hist(pops)\n",
      "107/275:\n",
      "x = np.arange(len(byage))\n",
      "pops = [byage[age]['Canada'] for age in byage]\n",
      "pops\n",
      "107/276:\n",
      "x = np.arange(len(byage))\n",
      "pops = [byage[age]['Canada'] for age in byage]\n",
      "pops\n",
      "107/277: byage\n",
      "107/278: byage = getbyage(1996)\n",
      "107/279: byage['Age Groups_1-4']['Canada']\n",
      "107/280:\n",
      "x = np.arange(len(byage))\n",
      "pops = [byage[age]['Canada'] for age in byage]\n",
      "pops\n",
      "107/281: plt.hist(pops)\n",
      "107/282:\n",
      "x = np.arange(len(byage))\n",
      "pops = [byage[age]['Canada'] for age in byage]\n",
      "pops.shape\n",
      "107/283:\n",
      "x = np.arange(len(byage))\n",
      "pops = [byage[age]['Canada'] for age in byage]\n",
      "pops\n",
      "107/284: byage['Age Groups_1-4']['Canada'].get_value()\n",
      "107/285: byage['Age Groups_1-4']['Canada']\n",
      "107/286: byage['Age Groups_1-4'].at['Canada']\n",
      "107/287: byage['Age Groups_1-4'].at[,1'Canada']\n",
      "107/288: byage['Age Groups_1-4'].at[1,'Canada']\n",
      "107/289: byage['Age Groups_1-4']['Canada']\n",
      "107/290: byage['Age Groups_1-4'].at[1565,'Canada']\n",
      "107/291: byage.shape\n",
      "107/292: byage['Age Groups_1-4'].shape\n",
      "107/293: byage['Age Groups_1-4'].values\n",
      "107/294: byage['Age Groups_1-4']['Canada'].values\n",
      "107/295:\n",
      "x = np.arange(len(byage))\n",
      "pops = [byage[age]['Canada'].values[0] for age in byage]\n",
      "pops\n",
      "107/296: plt.bar(x, pops)\n",
      "107/297:\n",
      "plt.bar(x, pops)\n",
      "plt.xticks(x, byage.keys())\n",
      "107/298:\n",
      "plt.bar(x, pops)\n",
      "plt.xticks(x, byage.keys())\n",
      "plt.show()\n",
      "107/299:\n",
      "plt.figure(figsize=(20,10))\n",
      "plt.bar(x, pops)\n",
      "plt.xticks(x, [byage.keys())\n",
      "plt.show()\n",
      "107/300:\n",
      "plt.figure(figsize=(20,10))\n",
      "plt.bar(x, pops)\n",
      "plt.xticks(x, byage.keys())\n",
      "plt.show()\n",
      "107/301:\n",
      "x = np.arange(len(byage))\n",
      "keys = byage.keys()\n",
      "pops = [byage[age]['Canada'].values[0] for age in keys]\n",
      "pops\n",
      "107/302:\n",
      "plt.figure(figsize=(20,10))\n",
      "plt.bar(x, pops)\n",
      "plt.xticks(x, [k[11:] for k in keys])\n",
      "plt.show()\n",
      "107/303:\n",
      "byage16 = getbyage(2016)\n",
      "x = np.arange(len(byage16))\n",
      "keys = byage16.keys()\n",
      "pops = [byage16[age]['Canada'].values[0] for age in keys]\n",
      "plt.figure(figsize=(20,10))\n",
      "plt.bar(x, pops)\n",
      "plt.xticks(x, [k[11:] for k in keys])\n",
      "plt.show()\n",
      "107/304:\n",
      "plt.figure(figsize=(16,8))\n",
      "plt.bar(x, pops)\n",
      "plt.xticks(x, [k[11:] for k in keys])\n",
      "plt.show()\n",
      "107/305:\n",
      "byage16 = getbyage(2016)\n",
      "x = np.arange(len(byage16))\n",
      "keys = byage16.keys()\n",
      "pops = [byage16[age]['Canada'].values[0] for age in keys]\n",
      "plt.figure(figsize=(16,8))\n",
      "plt.bar(x, pops)\n",
      "plt.xticks(x, [k[11:] for k in keys])\n",
      "plt.show()\n",
      "107/306:\n",
      "byage16 = getbyage(2016)\n",
      "x = np.arange(len(byage16))\n",
      "keys = byage16.keys()\n",
      "pops = [byage16[age]['Canada'].values[0] for age in keys]\n",
      "plt.figure(figsize=(16,8))\n",
      "plt.bar(x, pops)\n",
      "plt.xticks(x, [k[11:] for k in keys])\n",
      "plt.show()\n",
      "107/307:\n",
      "x = np.arange(len(byage))\n",
      "keys = byage.keys()\n",
      "pops = [byage[age]['Canada'].values[0] for age in keys]\n",
      "plt.figure(figsize=(16,8))\n",
      "plt.bar(x, pops)\n",
      "plt.xticks(x, [k[11:] for k in keys])\n",
      "plt.show()\n",
      "107/308: keys()\n",
      "107/309: keys\n",
      "107/310: keys = ['Age Groups_1-4', 'Age Groups_5-9', 'Age Groups_10-14', 'Age Groups_15-19', 'Age Groups_20-24', 'Age Groups_25-29', 'Age Groups_30-34', 'Age Groups_35-39', 'Age Groups_40-44', 'Age Groups_45-49', 'Age Groups_50-54', 'Age Groups_55-59', 'Age Groups_60-64', 'Age Groups_65-69', 'Age Groups_70-74', 'Age Groups_75-79', 'Age Groups_80-84', 'Age Groups_85-89', 'Age Groups_90+', 'Age Groups_<1']\n",
      "107/311:\n",
      "x = np.arange(len(byage))\n",
      "pops = [byage[age]['Canada'].values[0] for age in keys]\n",
      "plt.figure(figsize=(16,8))\n",
      "plt.bar(x, pops)\n",
      "plt.xticks(x, [k[11:] for k in keys])\n",
      "plt.show()\n",
      "107/312:\n",
      "byage16 = getbyage(2016)\n",
      "x = np.arange(len(byage16))\n",
      "pops = [byage16[age]['Canada'].values[0] for age in keys]\n",
      "plt.figure(figsize=(16,8))\n",
      "plt.bar(x, pops)\n",
      "plt.xticks(x, [k[11:] for k in keys])\n",
      "plt.show()\n",
      "107/313:\n",
      "def getbyage(year, cat):\n",
      "    byage = {}\n",
      "    for age in [x for x in prov2.columns if 'Age' in x and 'All' not in x]:\n",
      "        temp = prov2[prov2[age]==1]\n",
      "        temp2 = temp[[cat in x for x in temp['Category']]]\n",
      "        byage[age] = temp2[temp2['Year'] == year]\n",
      "    return byage\n",
      "107/314: byage = getbyage(1996, 'Population by age: both sexes')\n",
      "107/315:\n",
      "def getbyage(data, year, cat):\n",
      "    byage = {}\n",
      "    for age in [x for x in data.columns if 'Age' in x and 'All' not in x]:\n",
      "        temp = data[data[age]==1]\n",
      "        temp2 = temp[[cat in x for x in temp['Category']]]\n",
      "        byage[age] = temp2[temp2['Year'] == year]\n",
      "    return byage\n",
      "107/316: byage = getbyage(prov2, 1996, 'Population by age: both sexes')\n",
      "107/317:\n",
      "byage16 = getbyage(prov2, 2016, 'Population by age: both sexes')\n",
      "x = np.arange(len(byage16))\n",
      "pops = [byage16[age]['Canada'].values[0] for age in keys]\n",
      "plt.figure(figsize=(16,8))\n",
      "plt.bar(x, pops)\n",
      "plt.xticks(x, [k[11:] for k in keys])\n",
      "plt.show()\n",
      "107/318: spend = prov2.iloc[inds]\n",
      "107/319: spend\n",
      "107/320: spendbyage96 = getbyage(spend, 1996, '')\n",
      "107/321: spendbyage96\n",
      "107/322:\n",
      "spendbyage96\n",
      "x = np.arange(len(spendbyage96))\n",
      "pops = [spendbyage96[age]['Canada'].values[0] for age in keys]\n",
      "plt.figure(figsize=(16,8))\n",
      "plt.bar(x, pops)\n",
      "plt.xticks(x, [k[11:] for k in keys])\n",
      "plt.show()\n",
      "107/323:\n",
      "spendbyage16 = getbyage(spend, 2016, '')\n",
      "x = np.arange(len(spendbyage16))\n",
      "pops = [spendbyage16[age]['Canada'].values[0] for age in keys]\n",
      "plt.figure(figsize=(16,8))\n",
      "plt.bar(x, pops)\n",
      "plt.xticks(x, [k[11:] for k in keys])\n",
      "plt.show()\n",
      "107/324: import seaborn as sns\n",
      "107/325:\n",
      "spendbyage96 = getbyage(spend, 1996, '')\n",
      "x = np.arange(len(spendbyage96))\n",
      "s96 = [spendbyage96[age]['Canada'].values[0] for age in keys]\n",
      "plt.figure(figsize=(16,8))\n",
      "plt.bar(x, s96)\n",
      "plt.xticks(x, [k[11:] for k in keys])\n",
      "plt.show()\n",
      "107/326:\n",
      "spendbyage16 = getbyage(spend, 2016, '')\n",
      "x = np.arange(len(spendbyage16))\n",
      "s16 = [spendbyage16[age]['Canada'].values[0] for age in keys]\n",
      "plt.figure(figsize=(16,8))\n",
      "plt.bar(x, 216)\n",
      "plt.xticks(x, [k[11:] for k in keys])\n",
      "plt.show()\n",
      "107/327:\n",
      "spendbyage16 = getbyage(spend, 2016, '')\n",
      "x = np.arange(len(spendbyage16))\n",
      "s16 = [spendbyage16[age]['Canada'].values[0] for age in keys]\n",
      "plt.figure(figsize=(16,8))\n",
      "plt.bar(x, s16)\n",
      "plt.xticks(x, [k[11:] for k in keys])\n",
      "plt.show()\n",
      "107/328:\n",
      "df = pd.DataFrame(zip(x*3, [\"y\"]*3+[\"z\"]*3+[\"k\"]*3, y+z+k), columns=[\"age\", \"year\", \"expenditure\"])\n",
      "plt.figure(figsize=(10, 6))\n",
      "sns.barplot(x=\"age\", hue=\"year\", y=\"expenditure\", data=df)\n",
      "plt.show()\n",
      "107/329:\n",
      "df = pd.DataFrame(zip(s96, 216), columns=[\"age\", \"year\", \"expenditure\"])\n",
      "plt.figure(figsize=(10, 6))\n",
      "sns.barplot(x=\"age\", hue=\"year\", y=\"expenditure\", data=df)\n",
      "plt.show()\n",
      "107/330:\n",
      "df = pd.DataFrame(zip(s96, s16), columns=[\"age\", \"year\", \"expenditure\"])\n",
      "plt.figure(figsize=(10, 6))\n",
      "sns.barplot(x=\"age\", hue=\"year\", y=\"expenditure\", data=df)\n",
      "plt.show()\n",
      "107/331:\n",
      "df = pd.DataFrame(zip(s96, s16), columns=[\"age\", \"year\", \"expenditure\"])\n",
      "df\n",
      "107/332:\n",
      "df = pd.DataFrame(zip(s96, s16), columns=[\"age\", \"year\"])\n",
      "df\n",
      "107/333:\n",
      "dfd = {1996: s96, 2016: s16}\n",
      "df = pd.DataFrame(dfd, index=keys)\n",
      "df\n",
      "107/334:\n",
      "plt.figure(figsize=(10, 6))\n",
      "sns.barplot(x=\"age\", hue=\"year\", y=\"expenditure\", data=df)\n",
      "plt.show()\n",
      "107/335:\n",
      "dfd = {1996: s96, 2016: s16}\n",
      "df = pd.DataFrame(dfd, index=keys, columns=['age'])\n",
      "df\n",
      "107/336:\n",
      "dfd = {1996: s96, 2016: s16}\n",
      "df = pd.DataFrame(dfd, index=keys)\n",
      "df\n",
      "107/337:\n",
      "plt.figure(figsize=(10, 6))\n",
      "sns.barplot(data=df)\n",
      "plt.show()\n",
      "107/338:\n",
      "plt.figure(figsize=(10, 6))\n",
      "sns.barplot(hue=\"year\", data=df)\n",
      "plt.show()\n",
      "107/339:\n",
      "df = pd.DataFrame({\n",
      "    'Factor': ['Growth', 'Value'],\n",
      "    'Weight': [0.10, 0.20],\n",
      "    'Variance': [0.15, 0.35]\n",
      "})\n",
      "107/340:\n",
      "df = pd.DataFrame({\n",
      "    'Factor': ['Growth', 'Value'],\n",
      "    'Weight': [0.10, 0.20],\n",
      "    'Variance': [0.15, 0.35]\n",
      "})\n",
      "df\n",
      "107/341:\n",
      "\n",
      "df = pd.DataFrame({\n",
      "    'Age': keys,\n",
      "    '1996': s96,\n",
      "    '2016': s16\n",
      "})\n",
      "df\n",
      "107/342:\n",
      "df = pd.DataFrame({\n",
      "    'Age': keys,\n",
      "    '1996': s96,\n",
      "    '2016': s16\n",
      "})\n",
      "df\n",
      "107/343:\n",
      "plt.figure(figsize=(10, 6))\n",
      "sns.barplot(x=\"Age\", hue=\"year\", y=\"expenditure\", data=df)\n",
      "plt.show()\n",
      "107/344:\n",
      "plt.figure(figsize=(10, 6))\n",
      "sns.barplot(x=\"Age\", data=df)\n",
      "plt.show()\n",
      "107/345:\n",
      "# plt.figure(figsize=(10, 6))\n",
      "# sns.barplot(x=\"Age\", data=df)\n",
      "# plt.show()\n",
      "fig, ax1 = pyplot.subplots(figsize=(10, 10))\n",
      "tidy = df.melt(id_vars='Age').rename(columns=str.title)\n",
      "sns.barplot(x='Factor', y='Value', hue='Variable', data=tidy, ax=ax1)\n",
      "sns.despine(fig)\n",
      "107/346:\n",
      "# plt.figure(figsize=(10, 6))\n",
      "# sns.barplot(x=\"Age\", data=df)\n",
      "# plt.show()\n",
      "fig, ax1 = plt.subplots(figsize=(10, 10))\n",
      "tidy = df.melt(id_vars='Age').rename(columns=str.title)\n",
      "sns.barplot(x='Factor', y='Value', hue='Variable', data=tidy, ax=ax1)\n",
      "sns.despine(fig)\n",
      "107/347:\n",
      "# plt.figure(figsize=(10, 6))\n",
      "# sns.barplot(x=\"Age\", data=df)\n",
      "# plt.show()\n",
      "fig, ax1 = plt.subplots(figsize=(10, 10))\n",
      "tidy = df.melt(id_vars='Age').rename(columns=str.title)\n",
      "sns.barplot(x='Age', y='Value', hue='Variable', data=tidy, ax=ax1)\n",
      "sns.despine(fig)\n",
      "107/348:\n",
      "fig, ax1 = plt.subplots(figsize=(20, 10))\n",
      "tidy = df.melt(id_vars='Age').rename(columns=str.title)\n",
      "sns.barplot(x='Age', y='Value', hue='Variable', data=tidy, ax=ax1)\n",
      "sns.despine(fig)\n",
      "107/349:\n",
      "df = pd.DataFrame({\n",
      "    'Age': [x[:11] for x in keys],\n",
      "    '1996': s96,\n",
      "    '2016': s16\n",
      "})\n",
      "df\n",
      "107/350:\n",
      "fig, ax1 = plt.subplots(figsize=(20, 10))\n",
      "tidy = df.melt(id_vars='Age').rename(columns=str.title)\n",
      "sns.barplot(x='Age', y='Value', hue='Variable', data=tidy, ax=ax1)\n",
      "sns.despine(fig)\n",
      "107/351:\n",
      "df = pd.DataFrame({\n",
      "    'Age': [x[11:] for x in keys],\n",
      "    '1996': s96,\n",
      "    '2016': s16\n",
      "})\n",
      "df\n",
      "107/352:\n",
      "fig, ax1 = plt.subplots(figsize=(20, 10))\n",
      "tidy = df.melt(id_vars='Age').rename(columns=str.title)\n",
      "sns.barplot(x='Age', y='Value', hue='Variable', data=tidy, ax=ax1)\n",
      "sns.despine(fig)\n",
      "107/353:\n",
      "fig, ax1 = plt.subplots(figsize=(20, 10))\n",
      "tidy = df.melt(id_vars='Age').rename(columns=str.title)\n",
      "sns.barplot(x='Age', y='Hospital expenditure per capita', hue='Year', data=tidy, ax=ax1)\n",
      "sns.despine(fig)\n",
      "107/354:\n",
      "fig, ax1 = plt.subplots(figsize=(20, 10))\n",
      "tidy = df.melt(id_vars='Age').rename(columns=str.title)\n",
      "sns.barplot(x='Age', y='Value', hue='Year', data=tidy, ax=ax1)\n",
      "sns.despine(fig)\n",
      "107/355:\n",
      "# plt.figure(figsize=(10, 6))\n",
      "# sns.barplot(x=\"Age\", data=df)\n",
      "# plt.show()\n",
      "fig, ax1 = plt.subplots(figsize=(20, 10))\n",
      "tidy = df.melt(id_vars='Age').rename(columns=str.title)\n",
      "sns.barplot(x='Age', y='Value', hue='Variable', data=tidy, ax=ax1)\n",
      "sns.despine(fig)\n",
      "107/356:\n",
      "# plt.figure(figsize=(10, 6))\n",
      "# sns.barplot(x=\"Age\", data=df)\n",
      "# plt.show()\n",
      "fig, ax1 = plt.subplots(figsize=(20, 10))\n",
      "tidy = df.melt(id_vars='Age').rename(columns=str.title)\n",
      "sns.barplot(x='Age', y='Expenditure', hue='Variable', data=tidy, ax=ax1)\n",
      "sns.despine(fig)\n",
      "107/357:\n",
      "# plt.figure(figsize=(10, 6))\n",
      "# sns.barplot(x=\"Age\", data=df)\n",
      "# plt.show()\n",
      "fig, ax1 = plt.subplots(figsize=(20, 10))\n",
      "tidy = df.melt(id_vars='Age').rename(columns=str.title)\n",
      "sns.barplot(x='Age', y='Value', hue='Variable', data=tidy, ax=ax1)\n",
      "sns.despine(fig)\n",
      "107/358:\n",
      "df = pd.DataFrame({\n",
      "    'Age': [x[11:] for x in keys],\n",
      "    '1996': s96,\n",
      "    '2016': s16\n",
      "})\n",
      "df\n",
      "107/359:\n",
      "# plt.figure(figsize=(10, 6))\n",
      "# sns.barplot(x=\"Age\", data=df)\n",
      "# plt.show()\n",
      "fig, ax1 = plt.subplots(figsize=(20, 10))\n",
      "tidy = df.melt(id_vars='Age').rename(columns=str.title)\n",
      "sns.barplot(x='Age', y='Value', hue='Variable', data=tidy, ax=ax1)\n",
      "sns.despine(fig)\n",
      "107/360:\n",
      "fig, ax1 = plt.subplots(figsize=(20, 10))\n",
      "tidy = df.melt(id_vars='Age').rename(columns=str.title)\n",
      "sns.barplot(x='Age', y='Value', hue='Variable', data=tidy, ax=ax1)\n",
      "sns.despine(fig)\n",
      "107/361:\n",
      "fig, ax1 = plt.subplots(figsize=(20, 10))\n",
      "tidy = df.melt(id_vars='Age').rename(columns=str.title)\n",
      "sns.barplot(x='Age', y='Value', hue='Year', data=tidy, ax=ax1)\n",
      "sns.despine(fig)\n",
      "107/362:\n",
      "fig, ax1 = plt.subplots(figsize=(20, 10))\n",
      "tidy = df.melt(id_vars='Age').rename(columns=str.title)\n",
      "sns.barplot(x='Age', y='Value', hue='Variable', data=tidy, ax=ax1)\n",
      "sns.despine(fig)\n",
      "107/363:\n",
      "df = pd.DataFrame({\n",
      "    'Age': [x[11:] for x in keys],\n",
      "    '1996': s96,\n",
      "    '2016': s16\n",
      "})\n",
      "df\n",
      "107/364:\n",
      "fig, ax1 = plt.subplots(figsize=(20, 10))\n",
      "tidy = df.melt(id_vars='Age').rename(columns=str.title)\n",
      "sns.barplot(x='Age', y='Value', hue='Variable', data=tidy, ax=ax1)\n",
      "sns.despine(fig)\n",
      "107/365:\n",
      "fig, ax1 = plt.subplots(figsize=(20, 10))\n",
      "tidy = df.melt(id_vars='Age').rename(columns=str.title)\n",
      "sns.barplot(x='Age', y='Value', hue='Var', data=tidy, ax=ax1)\n",
      "sns.despine(fig)\n",
      "107/366:\n",
      "fig, ax1 = plt.subplots(figsize=(20, 10))\n",
      "tidy = df.melt(id_vars='Age').rename(columns=str.title)\n",
      "sns.barplot(x='Age', y='Value', hue='Variable', data=tidy, ax=ax1)\n",
      "sns.despine(fig)\n",
      "107/367:\n",
      "fig, ax1 = plt.subplots(figsize=(20, 10))\n",
      "tidy = df.melt(id_vars='Age').rename(columns=str.title)\n",
      "sns.barplot(x='Age', y='Value', hue='Variable', data=tidy, ax=ax1)\n",
      "sns.despine(fig)\n",
      "plt.title(\"Hospital Expenditure per capita\")\n",
      "107/368:\n",
      "fig, ax1 = plt.subplots(figsize=(20, 10))\n",
      "tidy = df.melt(id_vars='Age').rename(columns=str.title)\n",
      "sns.barplot(x='Age', y='Value', hue='Variable', data=tidy, ax=ax1)\n",
      "sns.despine(fig)\n",
      "plt.title(\"Hospital Expenditure per capita\", fontsize=20)\n",
      "107/369:\n",
      "fig, ax1 = plt.subplots(figsize=(20, 10))\n",
      "tidy = df.melt(id_vars='Age').rename(columns=str.title)\n",
      "sns.barplot(x='Age', y='Value', hue='Variable', data=tidy, ax=ax1)\n",
      "sns.despine(fig)\n",
      "plt.title(\"Hospital Expenditure per capita; both sexes, 1996 and 2016\", fontsize=20)\n",
      "107/370:\n",
      "fig, ax1 = plt.subplots(figsize=(20, 10))\n",
      "tidy = df.melt(id_vars='Age').rename(columns=str.title)\n",
      "sns.barplot(x='Age', y='Value', hue='Variable', data=tidy, ax=ax1)\n",
      "sns.despine(fig)\n",
      "plt.title(\"Hospital Expenditure per capita for both sexes, 1996 and 2016\", fontsize=20)\n",
      "107/371:\n",
      "x = np.arange(len(byage))\n",
      "pops96 = [byage[age]['Canada'].values[0] for age in keys]\n",
      "plt.figure(figsize=(16,8))\n",
      "plt.bar(x, pops)\n",
      "plt.xticks(x, [k[11:] for k in keys])\n",
      "plt.show()\n",
      "107/372:\n",
      "x = np.arange(len(byage))\n",
      "pops96 = [byage[age]['Canada'].values[0] for age in keys]\n",
      "plt.figure(figsize=(16,8))\n",
      "plt.bar(x, pops96)\n",
      "plt.xticks(x, [k[11:] for k in keys])\n",
      "plt.show()\n",
      "107/373:\n",
      "byage16 = getbyage(prov2, 2016, 'Population by age: both sexes')\n",
      "x = np.arange(len(byage16))\n",
      "pops16 = [byage16[age]['Canada'].values[0] for age in keys]\n",
      "plt.figure(figsize=(16,8))\n",
      "plt.bar(x, pops16)\n",
      "plt.xticks(x, [k[11:] for k in keys])\n",
      "plt.show()\n",
      "107/374:\n",
      "df2 = pd.DataFrame({\n",
      "    'Age': [x[11:] for x in keys],\n",
      "    '1996': pops96,\n",
      "    '2016': pops16\n",
      "})\n",
      "df2\n",
      "107/375:\n",
      "fig, ax1 = plt.subplots(figsize=(20, 10))\n",
      "tidy = df2.melt(id_vars='Age').rename(columns=str.title)\n",
      "sns.barplot(x='Age', y='Value', hue='Variable', data=tidy, ax=ax1)\n",
      "sns.despine(fig)\n",
      "plt.title(\"Hospital Expenditure per capita for both sexes, 1996 and 2016\", fontsize=20)\n",
      "107/376:\n",
      "fig, ax1 = plt.subplots(figsize=(20, 10))\n",
      "tidy = df2.melt(id_vars='Age').rename(columns=str.title)\n",
      "sns.barplot(x='Age', y='Value', hue='Variable', data=tidy, ax=ax1)\n",
      "sns.despine(fig)\n",
      "plt.title(\"Age Distribution of Canadian Population, 1996 and 2016\", fontsize=20)\n",
      "107/377:\n",
      "fig, ax1 = plt.subplots(figsize=(20, 10))\n",
      "tidy = df2.melt(id_vars='Age').rename(columns=str.title)\n",
      "sns.barplot(x='Age', y='Value', hue='Variable', data=tidy, ax=ax1)\n",
      "sns.despine(fig)\n",
      "plt.title(\"Age Distribution of Canadian Population, 1996 and 2016\", fontsize=20)\n",
      "plt.ylabel(\"Population\")\n",
      "107/378:\n",
      "fig, ax1 = plt.subplots(figsize=(20, 10))\n",
      "tidy = df2.melt(id_vars='Age').rename(columns=str.title)\n",
      "sns.barplot(x='Age', y='Value', hue='Variable', data=tidy, ax=ax1)\n",
      "sns.despine(fig)\n",
      "plt.title(\"Age Distribution of Canadian Population, 1996 and 2016\", fontsize=20)\n",
      "plt.ylabel(\"Population\")\n",
      "plt.legend(title=\"Year\")\n",
      "107/379:\n",
      "fig, ax1 = plt.subplots(figsize=(20, 10))\n",
      "tidy = df2.melt(id_vars='Age').rename(columns=str.title)\n",
      "sns.barplot(x='Age', y='Value', hue='Variable', data=tidy, ax=ax1)\n",
      "sns.despine(fig)\n",
      "plt.title(\"Age Distribution of Canadian Population, 1996 and 2016\", fontsize=20)\n",
      "plt.ylabel(\"Population (thousands)\")\n",
      "plt.legend(title=\"Year\")\n",
      "107/380:\n",
      "fig, ax1 = plt.subplots(figsize=(20, 10))\n",
      "tidy = df2.melt(id_vars='Age').rename(columns=str.title)\n",
      "sns.barplot(x='Age', y='Value', hue='Variable', data=tidy, ax=ax1)\n",
      "sns.despine(fig)\n",
      "plt.title(\"Age Distribution of Canadian Population, 1996 and 2016\", fontsize=20)\n",
      "plt.ylabel(\"Population (thousands)\", fontsize=15)\n",
      "plt.legend(title=\"Year\")\n",
      "107/381:\n",
      "fig, ax1 = plt.subplots(figsize=(20, 10))\n",
      "tidy = df2.melt(id_vars='Age').rename(columns=str.title)\n",
      "sns.barplot(x='Age', y='Value', hue='Variable', data=tidy, ax=ax1)\n",
      "sns.despine(fig)\n",
      "plt.title(\"Age Distribution of Canadian Population, 1996 and 2016\", fontsize=24)\n",
      "plt.ylabel(\"Population (thousands)\", fontsize=15)\n",
      "plt.xlabel(\"Age\", fontsize=16)\n",
      "plt.legend(title=\"Year\", fontsize=15)\n",
      "107/382:\n",
      "fig, ax1 = plt.subplots(figsize=(20, 10))\n",
      "tidy = df2.melt(id_vars='Age').rename(columns=str.title)\n",
      "sns.barplot(x='Age', y='Value', hue='Variable', data=tidy, ax=ax1)\n",
      "sns.despine(fig)\n",
      "plt.title(\"Age Distribution of Canadian Population, 1996 and 2016\", fontsize=24)\n",
      "plt.ylabel(\"Population (thousands)\", fontsize=15)\n",
      "plt.xlabel(\"Age\", fontsize=15)\n",
      "plt.legend(title=\"Year\", fontsize=15)\n",
      "107/383:\n",
      "fig, ax1 = plt.subplots(figsize=(20, 10))\n",
      "tidy = df.melt(id_vars='Age').rename(columns=str.title)\n",
      "sns.barplot(x='Age', y='Value', hue='Variable', data=tidy, ax=ax1)\n",
      "sns.despine(fig)\n",
      "plt.title(\"Hospital Expenditure per capita for both sexes, 1996 and 2016\", fontsize=24)\n",
      "plt.ylabel(\"Expenditure per capita\", fontsize=15)\n",
      "plt.xlabel(\"Age\", fontsize=15)\n",
      "plt.legend(title=\"Year\", fontsize=15)\n",
      "107/384:\n",
      "fig, ax1 = plt.subplots(figsize=(20, 10))\n",
      "tidy = df.melt(id_vars='Age').rename(columns=str.title)\n",
      "sns.barplot(x='Age', y='Value', hue='Variable', data=tidy, ax=ax1)\n",
      "sns.despine(fig)\n",
      "plt.title(\"Government Hospital Expenditure per capita for both sexes, 1996 and 2016\", fontsize=24)\n",
      "plt.ylabel(\"Expenditure per capita\", fontsize=15)\n",
      "plt.xlabel(\"Age\", fontsize=15)\n",
      "plt.legend(title=\"Year\", fontsize=15)\n",
      "107/385:\n",
      "p88 = priv.iloc[0,:]\n",
      "labels = [x for x in priv.columns[2:len(priv.columns)-1] if x != 'Public Health ' and 'Total' not in x]\n",
      "vals = [p88[label] for label in labels]\n",
      "plt.pie(vals, labels=labels, radius=2)\n",
      "plt.show()\n",
      "plt.title(\"Breakdown of Private Health Spending in Canada\")\n",
      "107/386:\n",
      "p88 = priv.iloc[0,:]\n",
      "labels = [x for x in priv.columns[2:len(priv.columns)-1] if x != 'Public Health ' and 'Total' not in x]\n",
      "vals = [p88[label] for label in labels]\n",
      "plt.pie(vals, labels=labels, radius=2)\n",
      "plt.title(\"Breakdown of Private Health Spending in Canada\")\n",
      "plt.show()\n",
      "107/387:\n",
      "p88 = priv.iloc[0,:]\n",
      "labels = [x for x in priv.columns[2:len(priv.columns)-1] if x != 'Public Health ' and 'Total' not in x]\n",
      "vals = [p88[label] for label in labels]\n",
      "plt.pie(vals, labels=labels, radius=2)\n",
      "plt.title(\"Breakdown of Private Health Spending in Canada\", fontsize=24)\n",
      "plt.show()\n",
      "107/388:\n",
      "p88 = priv.iloc[0,:]\n",
      "labels = [x for x in priv.columns[2:len(priv.columns)-1] if x != 'Public Health ' and 'Total' not in x]\n",
      "vals = [p88[label] for label in labels]\n",
      "plt.pie(vals, labels=labels, radius=2)\n",
      "plt.title(\"Breakdown of Private Health Spending in Canada\", fontsize=24, loc=\"top\")\n",
      "plt.show()\n",
      "107/389:\n",
      "p88 = priv.iloc[0,:]\n",
      "labels = [x for x in priv.columns[2:len(priv.columns)-1] if x != 'Public Health ' and 'Total' not in x]\n",
      "vals = [p88[label] for label in labels]\n",
      "plt.figure()\n",
      "plt.pie(vals, labels=labels, radius=2)\n",
      "plt.title(\"Breakdown of Private Health Spending in Canada\", fontsize=24, loc=\"top\")\n",
      "plt.show()\n",
      "107/390:\n",
      "p88 = priv.iloc[0,:]\n",
      "labels = [x for x in priv.columns[2:len(priv.columns)-1] if x != 'Public Health ' and 'Total' not in x]\n",
      "vals = [p88[label] for label in labels]\n",
      "plt.figure()\n",
      "plt.pie(vals, labels=labels, radius=2)\n",
      "plt.title(\"Breakdown of Private Health Spending in Canada\", fontsize=20)\n",
      "plt.show()\n",
      "107/391:\n",
      "p88 = priv.iloc[0,:]\n",
      "labels = [x for x in priv.columns[2:len(priv.columns)-1] if x != 'Public Health ' and 'Total' not in x]\n",
      "vals = [p88[label] for label in labels]\n",
      "plt.figure()\n",
      "plt.pie(vals, labels=labels, radius=2)\n",
      "plt.show()\n",
      "107/392:\n",
      "p88 = priv.iloc[0,:]\n",
      "labels = [x for x in priv.columns[2:len(priv.columns)-1] if x != 'Public Health ' and 'Total' not in x]\n",
      "vals = [p88[label] for label in labels]\n",
      "plt.figure()\n",
      "plt.pie(vals, labels=labels, radius=2.5)\n",
      "plt.show()\n",
      "107/393:\n",
      "p16 = priv.iloc[28,:]\n",
      "labels = [x for x in priv.columns[2:len(priv.columns)-1] if x != 'Public Health ' and 'Total' not in x]\n",
      "vals = [p16[label] for label in labels]\n",
      "plt.pie(vals, labels=labels, radius=2.5)\n",
      "plt.show()\n",
      "107/394:\n",
      "p88 = priv.iloc[0,:]\n",
      "labels = [x for x in priv.columns[2:len(priv.columns)-1] if x != 'Public Health ' and 'Total' not in x]\n",
      "vals = [p88[label] for label in labels]\n",
      "plt.figure()\n",
      "plt.pie(vals, labels=labels, radius=2)\n",
      "plt.title(\"Breakdown of Private Health Spending in Canada\", fontsize=20, pad=20)\n",
      "plt.show()\n",
      "107/395:\n",
      "p88 = priv.iloc[0,:]\n",
      "labels = [x for x in priv.columns[2:len(priv.columns)-1] if x != 'Public Health ' and 'Total' not in x]\n",
      "vals = [p88[label] for label in labels]\n",
      "plt.figure()\n",
      "plt.pie(vals, labels=labels, radius=2)\n",
      "plt.title(\"Breakdown of Private Health Spending in Canada\", fontsize=20, pad=50)\n",
      "plt.show()\n",
      "107/396:\n",
      "p88 = priv.iloc[0,:]\n",
      "labels = [x for x in priv.columns[2:len(priv.columns)-1] if x != 'Public Health ' and 'Total' not in x]\n",
      "vals = [p88[label] for label in labels]\n",
      "plt.figure()\n",
      "plt.pie(vals, labels=labels, radius=2)\n",
      "plt.title(\"Breakdown of Private Health Spending in Canada\", fontsize=20, pad=100)\n",
      "plt.show()\n",
      "107/397:\n",
      "p88 = priv.iloc[0,:]\n",
      "labels = [x for x in priv.columns[2:len(priv.columns)-1] if x != 'Public Health ' and 'Total' not in x]\n",
      "vals = [p88[label] for label in labels]\n",
      "plt.figure()\n",
      "plt.pie(vals, labels=labels, radius=2)\n",
      "plt.title(\"Breakdown of Private Health Spending in Canada\", fontsize=18, pad=100)\n",
      "plt.show()\n",
      "107/398:\n",
      "p88 = priv.iloc[0,:]\n",
      "labels = [x for x in priv.columns[2:len(priv.columns)-1] if x != 'Public Health ' and 'Total' not in x]\n",
      "vals = [p88[label] for label in labels]\n",
      "plt.figure()\n",
      "plt.pie(vals, labels=labels, radius=2.2)\n",
      "plt.title(\"Breakdown of Private Health Spending in Canada\", fontsize=18, pad=100)\n",
      "plt.show()\n",
      "107/399:\n",
      "p88 = priv.iloc[0,:]\n",
      "labels = [x for x in priv.columns[2:len(priv.columns)-1] if x != 'Public Health ' and 'Total' not in x]\n",
      "vals = [p88[label] for label in labels]\n",
      "plt.figure()\n",
      "plt.pie(vals, labels=labels, radius=2.25)\n",
      "plt.title(\"Breakdown of Private Health Spending in Canada\", fontsize=18, pad=100)\n",
      "plt.show()\n",
      "107/400:\n",
      "p88 = priv.iloc[0,:]\n",
      "labels = [x for x in priv.columns[2:len(priv.columns)-1] if x != 'Public Health ' and 'Total' not in x]\n",
      "vals = [p88[label] for label in labels]\n",
      "plt.figure()\n",
      "plt.pie(vals, labels=labels, radius=2.5)\n",
      "plt.title(\"Breakdown of Private Health Spending in Canada\", fontsize=18, pad=100)\n",
      "plt.show()\n",
      "107/401:\n",
      "p88 = priv.iloc[0,:]\n",
      "labels = [x for x in priv.columns[2:len(priv.columns)-1] if x != 'Public Health ' and 'Total' not in x]\n",
      "vals = [p88[label] for label in labels]\n",
      "plt.figure()\n",
      "plt.pie(vals, labels=labels, radius=2.5)\n",
      "plt.title(\"Breakdown of Private Health Spending in Canada\", fontsize=18, pad=150)\n",
      "plt.show()\n",
      "107/402:\n",
      "p88 = priv.iloc[0,:]\n",
      "labels = [x for x in priv.columns[2:len(priv.columns)-1] if x != 'Public Health ' and 'Total' not in x]\n",
      "vals = [p88[label] for label in labels]\n",
      "plt.figure()\n",
      "plt.pie(vals, labels=labels, radius=2.5)\n",
      "plt.title(\"Breakdown of Private Health Spending in Canada\", fontsize=20, pad=150)\n",
      "plt.show()\n",
      "107/403:\n",
      "p88 = priv.iloc[0,:]\n",
      "labels = [x for x in priv.columns[2:len(priv.columns)-1] if x != 'Public Health ' and 'Total' not in x]\n",
      "vals = [p88[label] for label in labels]\n",
      "plt.figure()\n",
      "plt.pie(vals, labels=labels, radius=2.5)\n",
      "plt.title(\"Breakdown of Private Health Spending in Canada, 1988\", fontsize=20, pad=150)\n",
      "plt.show()\n",
      "107/404:\n",
      "p16 = priv.iloc[28,:]\n",
      "labels = [x for x in priv.columns[2:len(priv.columns)-1] if x != 'Public Health ' and 'Total' not in x]\n",
      "vals = [p16[label] for label in labels]\n",
      "plt.pie(vals, labels=labels, radius=2.5)\n",
      "plt.title(\"Breakdown of Private Health Spending in Canada, 2016\", fontsize=20, pad=150)\n",
      "plt.show()\n",
      "107/405:\n",
      "lr = LinearRegression().fit(np.array(pub1['Year']).reshape(-1,1), np.array(pub1['Private-sector ']/pub1['Total ']))\n",
      "print(lr.score(np.array(pub1['Year']).reshape(-1,1), np.array(pub1['Private-sector ']/pub1['Total '])))\n",
      "plt.figure((20,10))\n",
      "plt.plot(pub1['Year'], pub1['Private-sector ']/pub1['Total '], \"o\", pub1['Year'], lr.coef_[0]*pub1['Year']+lr.intercept_)\n",
      "plt.title('Proportion of private sector expenditure per capita, 1975-2018')\n",
      "107/406:\n",
      "lr = LinearRegression().fit(np.array(pub1['Year']).reshape(-1,1), np.array(pub1['Private-sector ']/pub1['Total ']))\n",
      "print(lr.score(np.array(pub1['Year']).reshape(-1,1), np.array(pub1['Private-sector ']/pub1['Total '])))\n",
      "plt.figure(figsize=(20,10))\n",
      "plt.plot(pub1['Year'], pub1['Private-sector ']/pub1['Total '], \"o\", pub1['Year'], lr.coef_[0]*pub1['Year']+lr.intercept_)\n",
      "plt.title('Proportion of private sector expenditure per capita, 1975-2018')\n",
      "107/407:\n",
      "lr = LinearRegression().fit(np.array(pub1['Year']).reshape(-1,1), np.array(pub1['Private-sector ']/pub1['Total ']))\n",
      "print(lr.score(np.array(pub1['Year']).reshape(-1,1), np.array(pub1['Private-sector ']/pub1['Total '])))\n",
      "plt.figure(figsize=(14,7))\n",
      "plt.plot(pub1['Year'], pub1['Private-sector ']/pub1['Total '], \"o\", pub1['Year'], lr.coef_[0]*pub1['Year']+lr.intercept_)\n",
      "plt.title('Proportion of private sector expenditure per capita, 1975-2018')\n",
      "107/408:\n",
      "lr = LinearRegression().fit(np.array(pub1['Year']).reshape(-1,1), np.array(pub1['Private-sector ']/pub1['Total ']))\n",
      "print(lr.score(np.array(pub1['Year']).reshape(-1,1), np.array(pub1['Private-sector ']/pub1['Total '])))\n",
      "plt.figure(figsize=(12,6))\n",
      "plt.plot(pub1['Year'], pub1['Private-sector ']/pub1['Total '], \"o\", pub1['Year'], lr.coef_[0]*pub1['Year']+lr.intercept_)\n",
      "plt.title('Proportion of private sector expenditure per capita, 1975-2018')\n",
      "107/409:\n",
      "lr = LinearRegression().fit(np.array(pub1['Year']).reshape(-1,1), np.array(pub1['Private-sector ']/pub1['Total ']))\n",
      "print(lr.score(np.array(pub1['Year']).reshape(-1,1), np.array(pub1['Private-sector ']/pub1['Total '])))\n",
      "plt.figure(figsize=(12,8))\n",
      "plt.plot(pub1['Year'], pub1['Private-sector ']/pub1['Total '], \"o\", pub1['Year'], lr.coef_[0]*pub1['Year']+lr.intercept_)\n",
      "plt.title('Proportion of private sector expenditure per capita, 1975-2018')\n",
      "107/410:\n",
      "lr = LinearRegression().fit(np.array(pub1['Year']).reshape(-1,1), np.array(pub1['Private-sector ']/pub1['Total ']))\n",
      "print(lr.score(np.array(pub1['Year']).reshape(-1,1), np.array(pub1['Private-sector ']/pub1['Total '])))\n",
      "plt.figure(figsize=(8,6))\n",
      "plt.plot(pub1['Year'], pub1['Private-sector ']/pub1['Total '], \"o\", pub1['Year'], lr.coef_[0]*pub1['Year']+lr.intercept_)\n",
      "plt.title('Proportion of private sector expenditure per capita, 1975-2018')\n",
      "107/411:\n",
      "lr = LinearRegression().fit(np.array(pub1['Year']).reshape(-1,1), np.array(pub1['Private-sector ']/pub1['Total ']))\n",
      "print(lr.score(np.array(pub1['Year']).reshape(-1,1), np.array(pub1['Private-sector ']/pub1['Total '])))\n",
      "plt.figure(figsize=(8,6))\n",
      "plt.plot(pub1['Year'], pub1['Private-sector ']/pub1['Total '], \"o\", pub1['Year'], lr.coef_[0]*pub1['Year']+lr.intercept_)\n",
      "plt.title('Proportion of private sector expenditure per capita, 1975-2018', fontsize=20)\n",
      "107/412:\n",
      "lr = LinearRegression().fit(np.array(pub1['Year']).reshape(-1,1), np.array(pub1['Private-sector ']/pub1['Total ']))\n",
      "print(lr.score(np.array(pub1['Year']).reshape(-1,1), np.array(pub1['Private-sector ']/pub1['Total '])))\n",
      "plt.figure(figsize=(8,6))\n",
      "plt.plot(pub1['Year'], pub1['Private-sector ']/pub1['Total '], \"o\", pub1['Year'], lr.coef_[0]*pub1['Year']+lr.intercept_)\n",
      "plt.title('Proportion of private sector expenditure out of total expenditure per capita, 1975-2018', fontsize=16)\n",
      "107/413:\n",
      "lr = LinearRegression().fit(np.array(pub1['Year']).reshape(-1,1), np.array(pub1['Private-sector ']/pub1['Total ']))\n",
      "print(lr.score(np.array(pub1['Year']).reshape(-1,1), np.array(pub1['Private-sector ']/pub1['Total '])))\n",
      "plt.figure(figsize=(8,6))\n",
      "plt.plot(pub1['Year'], pub1['Private-sector ']/pub1['Total '], \"o\", pub1['Year'], lr.coef_[0]*pub1['Year']+lr.intercept_)\n",
      "plt.title('Proportion of private sector expenditure per capita, 1975-2018', fontsize=16)\n",
      "107/414:\n",
      "plt.figure()\n",
      "plt.plot(pdrugs['Year '], pdrugs['Grand Total '])\n",
      "plt.plot(pdrugs['Year '], pdrugs['Prescribed Drugs '])\n",
      "plt.plot(pdrugs['Year '], pdrugs['Hospitals '])\n",
      "plt.plot(pdrugs['Year '], pdrugs['Administration '])\n",
      "plt.plot(pdrugs['Year '], pdrugs['Other Professionals: Dental Services '])\n",
      "107/415:\n",
      "plt.figure(figsize=(8,6))\n",
      "plt.plot(pdrugs['Year '], pdrugs['Prescribed Drugs ']/pdrugs['Grand Total '])\n",
      "plt.title('')\n",
      "plt.show()\n",
      "107/416:\n",
      "plt.figure(figsize=(8,6))\n",
      "plt.plot(pdrugs['Year '], pdrugs['Prescribed Drugs ']/pdrugs['Grand Total '])\n",
      "plt.title('Proportion of Private Health Spending from Prescribed Drugs, 1988-2018')\n",
      "plt.show()\n",
      "107/417:\n",
      "plt.figure(figsize=(8,6))\n",
      "plt.plot(pdrugs['Year '], pdrugs['Prescribed Drugs ']/pdrugs['Grand Total '])\n",
      "plt.title('Proportion of Private Health Spending from Prescribed Drugs, 1988-2018', fontsize=16)\n",
      "plt.show()\n",
      "107/418:\n",
      "plt.figure(figsize=(8,6))\n",
      "plt.plot(pdrugs['Year '], pdrugs['Prescribed Drugs ']/pdrugs['Grand Total '])\n",
      "plt.title('Proportion of Private Health Spending from Prescribed Drugs, 1988-2018', fontsize=14)\n",
      "plt.show()\n",
      "107/419:\n",
      "plt.figure(figsize=(8,6))\n",
      "plt.plot(pdrugs['Year '], pdrugs['Prescribed Drugs ']/pdrugs['Grand Total '])\n",
      "plt.title('Proportion of Private Health Spending from Prescribed Drugs, 1988-2018', fontsize=14)\n",
      "plt.ylabel(\"proportion\")\n",
      "plt.show()\n",
      "107/420:\n",
      "plt.figure(figsize=(8,6))\n",
      "plt.plot(pdrugs['Year '], pdrugs['Prescribed Drugs ']/pdrugs['Grand Total '])\n",
      "plt.title('Proportion of Private Health Spending from Prescribed Drugs, 1988-2018', fontsize=14)\n",
      "plt.ylabel(\"Proportion\")\n",
      "plt.xlabel(\"Year\")\n",
      "plt.show()\n",
      "107/421: sorted(props, key=lambda x: x[2], reverse=True)\n",
      "107/422: sorted(props, key=lambda x: x[3], reverse=True)\n",
      "107/423:\n",
      "p16 = priv.iloc[28,:]\n",
      "labels = [x for x in priv.columns[2:len(priv.columns)-1] if x != 'Public Health ' and 'Total' not in x]\n",
      "vals = [p16[label] for label in labels]\n",
      "plt.pie(vals, labels=labels, radius=2.5, autopct='%1.0f%%')\n",
      "plt.title(\"Breakdown of Private Health Spending in Canada, 2016\", fontsize=20, pad=150)\n",
      "plt.show()\n",
      "107/424:\n",
      "p88 = priv.iloc[0,:]\n",
      "labels = [x for x in priv.columns[2:len(priv.columns)-1] if x != 'Public Health ' and 'Total' not in x]\n",
      "vals = [p88[label] for label in labels]\n",
      "plt.figure()\n",
      "plt.pie(vals, labels=labels, radius=2.5, autopct='%1.0f%%')\n",
      "plt.title(\"Breakdown of Private Health Spending in Canada, 1988\", fontsize=20, pad=150)\n",
      "plt.show()\n",
      "107/425:\n",
      "lr = LinearRegression().fit(np.array(pub1['Year']).reshape(-1,1), np.array(pub1['Private-sector ']/pub1['Total ']))\n",
      "print(lr.score(np.array(pub1['Year']).reshape(-1,1), np.array(pub1['Private-sector ']/pub1['Total '])))\n",
      "plt.figure(figsize=(12,6))\n",
      "plt.plot(pub1['Year'], pub1['Private-sector ']/pub1['Total '], \"o\", pub1['Year'], lr.coef_[0]*pub1['Year']+lr.intercept_)\n",
      "plt.title('Proportion of private sector expenditure per capita, 1975-2018', fontsize=16)\n",
      "107/426:\n",
      "lr = LinearRegression().fit(np.array(pub1['Year']).reshape(-1,1), np.array(pub1['Private-sector ']/pub1['Total ']))\n",
      "print(lr.score(np.array(pub1['Year']).reshape(-1,1), np.array(pub1['Private-sector ']/pub1['Total '])))\n",
      "plt.figure(figsize=(16,5))\n",
      "plt.plot(pub1['Year'], pub1['Private-sector ']/pub1['Total '], \"o\", pub1['Year'], lr.coef_[0]*pub1['Year']+lr.intercept_)\n",
      "plt.title('Proportion of private sector expenditure per capita, 1975-2018', fontsize=16)\n",
      "107/427:\n",
      "plt.figure(figsize=(16,5))\n",
      "plt.plot(pdrugs['Year '], pdrugs['Prescribed Drugs ']/pdrugs['Grand Total '])\n",
      "plt.title('Proportion of Private Health Spending from Prescribed Drugs, 1988-2018', fontsize=14)\n",
      "plt.ylabel(\"Proportion\")\n",
      "plt.xlabel(\"Year\")\n",
      "plt.show()\n",
      "107/428:\n",
      "fig, ax1 = plt.subplots(figsize=(16, 5))\n",
      "tidy = df.melt(id_vars='Age').rename(columns=str.title)\n",
      "sns.barplot(x='Age', y='Value', hue='Variable', data=tidy, ax=ax1)\n",
      "sns.despine(fig)\n",
      "plt.title(\"Government Hospital Expenditure per capita for both sexes, 1996 and 2016\", fontsize=24)\n",
      "plt.ylabel(\"Expenditure per capita\", fontsize=15)\n",
      "plt.xlabel(\"Age\", fontsize=15)\n",
      "plt.legend(title=\"Year\", fontsize=15)\n",
      "107/429:\n",
      "fig, ax1 = plt.subplots(figsize=(16, 5))\n",
      "tidy = df2.melt(id_vars='Age').rename(columns=str.title)\n",
      "sns.barplot(x='Age', y='Value', hue='Variable', data=tidy, ax=ax1)\n",
      "sns.despine(fig)\n",
      "plt.title(\"Age Distribution of Canadian Population, 1996 and 2016\", fontsize=24)\n",
      "plt.ylabel(\"Population (thousands)\", fontsize=15)\n",
      "plt.xlabel(\"Age\", fontsize=15)\n",
      "plt.legend(title=\"Year\", fontsize=15)\n",
      "107/430: pub1['Private-sector ']/pub1['Total ']\n",
      "107/431: pub1['Year']\n",
      "108/1: %matplotlib inline\n",
      "108/2:\n",
      "import torch\n",
      "import torchvision\n",
      "import torchvision.transforms as transforms\n",
      "109/1:\n",
      "import torch\n",
      "import torchvision\n",
      "import torchvision.transforms as transforms\n",
      "110/1: import torch\n",
      "114/1:\n",
      "import torch\n",
      "import torchvision\n",
      "import torchvision.transforms as transforms\n",
      "114/2:\n",
      "transform = transforms.Compose(\n",
      "    [transforms.ToTensor(),\n",
      "     transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))])\n",
      "\n",
      "trainset = torchvision.datasets.CIFAR10(root='./data', train=True,\n",
      "                                        download=True, transform=transform)\n",
      "trainloader = torch.utils.data.DataLoader(trainset, batch_size=4,\n",
      "                                          shuffle=True, num_workers=2)\n",
      "\n",
      "testset = torchvision.datasets.CIFAR10(root='./data', train=False,\n",
      "                                       download=True, transform=transform)\n",
      "testloader = torch.utils.data.DataLoader(testset, batch_size=4,\n",
      "                                         shuffle=False, num_workers=2)\n",
      "\n",
      "classes = ('plane', 'car', 'bird', 'cat',\n",
      "           'deer', 'dog', 'frog', 'horse', 'ship', 'truck')\n",
      "114/3:\n",
      "import matplotlib.pyplot as plt\n",
      "import numpy as np\n",
      "\n",
      "# functions to show an image\n",
      "\n",
      "\n",
      "def imshow(img):\n",
      "    img = img / 2 + 0.5     # unnormalize\n",
      "    npimg = img.numpy()\n",
      "    plt.imshow(np.transpose(npimg, (1, 2, 0)))\n",
      "    plt.show()\n",
      "\n",
      "\n",
      "# get some random training images\n",
      "dataiter = iter(trainloader)\n",
      "images, labels = dataiter.next()\n",
      "print('Shape of images tensor: {}'.format(images.size()))\n",
      "# show images\n",
      "imshow(torchvision.utils.make_grid(images))\n",
      "# print labels\n",
      "print(' '.join('%5s' % classes[labels[j]] for j in range(4)))\n",
      "114/4:\n",
      "import matplotlib.pyplot as plt\n",
      "import numpy as np\n",
      "\n",
      "# functions to show an image\n",
      "plt.figure()\n",
      "\n",
      "def imshow(img):\n",
      "    img = img / 2 + 0.5     # unnormalize\n",
      "    npimg = img.numpy()\n",
      "    plt.imshow(np.transpose(npimg, (1, 2, 0)))\n",
      "    plt.show()\n",
      "\n",
      "\n",
      "# get some random training images\n",
      "dataiter = iter(trainloader)\n",
      "images, labels = dataiter.next()\n",
      "print('Shape of images tensor: {}'.format(images.size()))\n",
      "# show images\n",
      "imshow(torchvision.utils.make_grid(images))\n",
      "# print labels\n",
      "print(' '.join('%5s' % classes[labels[j]] for j in range(4)))\n",
      "114/5:\n",
      "import torch.nn as nn\n",
      "import torch.nn.functional as F\n",
      "\n",
      "\n",
      "class Net(nn.Module):\n",
      "    def __init__(self):\n",
      "        super(Net, self).__init__()\n",
      "        self.conv1 = nn.Conv2d(3, 6, 5)\n",
      "        self.pool = nn.MaxPool2d(2, 2)\n",
      "        self.conv2 = nn.Conv2d(6, 16, 5)\n",
      "        self.fc1 = nn.Linear(16 * 5 * 5, 120)\n",
      "        self.fc2 = nn.Linear(120, 84)\n",
      "        self.fc3 = nn.Linear(84, 10)\n",
      "\n",
      "    def forward(self, x):\n",
      "        x = self.pool(F.relu(self.conv1(x)))\n",
      "        x = self.pool(F.relu(self.conv2(x)))\n",
      "        x = x.view(-1, 16 * 5 * 5)\n",
      "        x = F.relu(self.fc1(x))\n",
      "        x = F.relu(self.fc2(x))\n",
      "        x = self.fc3(x)\n",
      "        return x\n",
      "\n",
      "\n",
      "net = Net()\n",
      "114/6:\n",
      "import torch.optim as optim\n",
      "\n",
      "criterion = nn.CrossEntropyLoss()\n",
      "optimizer = optim.SGD(net.parameters(), lr=0.001, momentum=0.9)\n",
      "114/7:\n",
      "for epoch in range(2):  # loop over the dataset multiple times\n",
      "\n",
      "    running_loss = 0.0\n",
      "    for i, data in enumerate(trainloader, 0):\n",
      "        # get the inputs; data is a list of [inputs, labels]\n",
      "        inputs, labels = data\n",
      "\n",
      "        # zero the parameter gradients\n",
      "        optimizer.zero_grad()\n",
      "\n",
      "        # forward + backward + optimize\n",
      "        outputs = net(inputs)\n",
      "        loss = criterion(outputs, labels)\n",
      "        loss.backward()\n",
      "        optimizer.step()\n",
      "\n",
      "        # print statistics\n",
      "        running_loss += loss.item()\n",
      "        if i % 2000 == 1999:    # print every 2000 mini-batches\n",
      "            print('[%d, %5d] loss: %.3f' %\n",
      "                  (epoch + 1, i + 1, running_loss / 2000))\n",
      "            running_loss = 0.0\n",
      "\n",
      "print('Finished Training')\n",
      "114/8:\n",
      "dataiter = iter(testloader)\n",
      "images, labels = dataiter.next()\n",
      "\n",
      "# print images\n",
      "imshow(torchvision.utils.make_grid(images))\n",
      "print('GroundTruth: ', ' '.join('%5s' % classes[labels[j]] for j in range(4)))\n",
      "114/9:\n",
      "outputs = net(images)\n",
      "print(outputs)\n",
      "114/10:\n",
      "_, predicted = torch.max(outputs, 1)\n",
      "\n",
      "print('Predicted: ', ' '.join('%5s' % classes[predicted[j]]\n",
      "                              for j in range(4)))\n",
      "114/11:\n",
      "correct = 0\n",
      "total = 0\n",
      "with torch.no_grad():\n",
      "    for data in testloader:\n",
      "        images, labels = data\n",
      "        outputs = net(images)\n",
      "        _, predicted = torch.max(outputs.data, 1)\n",
      "        total += labels.size(0)\n",
      "        correct += (predicted == labels).sum().item()\n",
      "\n",
      "print('Accuracy of the network on the 10000 test images: %d %%' % (\n",
      "    100 * correct / total))\n",
      "114/12:\n",
      "class_correct = list(0. for i in range(10))\n",
      "class_total = list(0. for i in range(10))\n",
      "with torch.no_grad():\n",
      "    for data in testloader:\n",
      "        images, labels = data\n",
      "        outputs = net(images)\n",
      "        _, predicted = torch.max(outputs, 1)\n",
      "        c = (predicted == labels).squeeze()\n",
      "        for i in range(4):\n",
      "            label = labels[i]\n",
      "            class_correct[label] += c[i].item()\n",
      "            class_total[label] += 1\n",
      "\n",
      "\n",
      "for i in range(10):\n",
      "    print('Accuracy of %5s : %2d %%' % (\n",
      "        classes[i], 100 * class_correct[i] / class_total[i]))\n",
      "114/13:\n",
      "# print images\n",
      "imshow(torchvision.utils.make_grid(images))\n",
      "print('GroundTruth: ', ' '.join('%5s' % classes[labels[j]] for j in range(4)))\n",
      "outputs = net(images)\n",
      "_, predicted = torch.max(outputs, 1)\n",
      "\n",
      "print('Predicted: ', ' '.join('%5s' % classes[predicted[j]]\n",
      "                              for j in range(4)))\n",
      "114/14:\n",
      "def modify_input(images):\n",
      "    # We clone the images so that making changes to modified_images doesn't change the original set of images\n",
      "    modified_images = images.clone().detach()\n",
      "    '''\n",
      "    USE THIS SECTION TO MAKE MODIFICATIONS TO modified_images\n",
      "    '''\n",
      "    return modified_images\n",
      "114/15:\n",
      "modified_images = modify_input(images)\n",
      "\n",
      "# print images\n",
      "imshow(torchvision.utils.make_grid(modified_images))\n",
      "print('GroundTruth: ', ' '.join('%5s' % classes[labels[j]] for j in range(4)))\n",
      "outputs = net(modified_images)\n",
      "_, predicted = torch.max(outputs, 1)\n",
      "\n",
      "print('Predicted: ', ' '.join('%5s' % classes[predicted[j]]\n",
      "                              for j in range(4)))\n",
      "116/1: import numpy as np\n",
      "116/2: s = 1\n",
      "116/3:\n",
      "def gauss2(x):\n",
      "    return (1/np.sqrt(2*np.pi))*(x*x-1)*np.exp(-x*x/2)\n",
      "116/4: gauss2(1)\n",
      "116/5:\n",
      "import numpy as np\n",
      "import matplotlib.pyplot as plt\n",
      "116/6: xs = np.linspace(-1,25)\n",
      "116/7: xs\n",
      "116/8: plt.plot(xs, [gauss2(x) for x in xs])\n",
      "116/9: xs = np.linspace(-50,50)\n",
      "116/10: plt.plot(xs, [gauss2(x) for x in xs])\n",
      "116/11: xs = np.linspace(-25,25)\n",
      "116/12: plt.plot(xs, [gauss2(x) for x in xs])\n",
      "116/13: xs = np.linspace(-25,25,100)\n",
      "116/14: plt.plot(xs, [gauss2(x) for x in xs])\n",
      "116/15: xs = np.linspace(-15,15,100)\n",
      "116/16: plt.plot(xs, [gauss2(x) for x in xs])\n",
      "116/17:\n",
      "def gauss2(x, s):\n",
      "    return (1/(np.sqrt(2*np.pi)*s**3))*(x*x/s**2-1)*np.exp(-x*x/(2*s**2))\n",
      "116/18: xs = np.linspace(-15,15,100)\n",
      "116/19: plt.plot(xs, [gauss2(x, 1) for x in xs])\n",
      "116/20:\n",
      "def D(x, k):\n",
      "    return (gauss2(x, k*1)-gauss2(x,1))/(k*1-1)\n",
      "116/21:\n",
      "for k in [1.2, 1.4, 1.6, 1.8, 2.0]:\n",
      "    plt.plot(xs, [D(x,k) for x in xs])\n",
      "116/22:\n",
      "for k in [1.2, 1.4, 1.6, 1.8, 2.0]:\n",
      "    plt.plot(xs, [D(x,k) for x in xs])\n",
      "plt.legend()\n",
      "116/23:\n",
      "\n",
      "for k in [1.2, 1.4, 1.6, 1.8, 2.0]:\n",
      "    plt.plot(xs, [D(x,k) for x in xs], label=k)\n",
      "plt.legend()\n",
      "116/24:\n",
      "\n",
      "for k in [1.2, 1.4, 1.6, 1.8, 2.0]:\n",
      "    plt.plot(xs, [D(x,k) for x in xs], label=\"k = \" + k)\n",
      "plt.legend()\n",
      "116/25:\n",
      "\n",
      "for k in [1.2, 1.4, 1.6, 1.8, 2.0]:\n",
      "    plt.plot(xs, [D(x,k) for x in xs], label=\"k = \" + str(k))\n",
      "plt.legend()\n",
      "116/26:\n",
      "plt.plot(xs, [gauss2(x, 1) for x in xs], label=\"Actual\")\n",
      "for k in [1.2, 1.4, 1.6, 1.8, 2.0]:\n",
      "    plt.plot(xs, [D(x,k) for x in xs], label=\"k = \" + str(k))\n",
      "plt.legend()\n",
      "116/27:\n",
      "plt.figure((20,10))\n",
      "plt.plot(xs, [gauss2(x, 1) for x in xs], label=\"Actual\")\n",
      "for k in [1.2, 1.4, 1.6, 1.8, 2.0]:\n",
      "    plt.plot(xs, [D(x,k) for x in xs], label=\"k = \" + str(k))\n",
      "plt.legend()\n",
      "116/28:\n",
      "plt.figure(size=(20,10))\n",
      "plt.plot(xs, [gauss2(x, 1) for x in xs], label=\"Actual\")\n",
      "for k in [1.2, 1.4, 1.6, 1.8, 2.0]:\n",
      "    plt.plot(xs, [D(x,k) for x in xs], label=\"k = \" + str(k))\n",
      "plt.legend()\n",
      "116/29:\n",
      "plt.figure(figsize=(20,10))\n",
      "plt.plot(xs, [gauss2(x, 1) for x in xs], label=\"Actual\")\n",
      "for k in [1.2, 1.4, 1.6, 1.8, 2.0]:\n",
      "    plt.plot(xs, [D(x,k) for x in xs], label=\"k = \" + str(k))\n",
      "plt.legend()\n",
      "116/30:\n",
      "def gauss2(x, s):\n",
      "    return (1/(np.sqrt(2*np.pi)*s**3))*(x*x/(s**2)-1)*np.exp(-x*x/(2*s**2))\n",
      "116/31:\n",
      "def D(x, k):\n",
      "    return (gauss2(x, k*1)-gauss2(x,1))/(k*1-1)\n",
      "116/32:\n",
      "plt.figure(figsize=(20,10))\n",
      "plt.plot(xs, [gauss2(x, 1) for x in xs], label=\"Actual\")\n",
      "for k in [1.2, 1.4, 1.6, 1.8, 2.0]:\n",
      "    plt.plot(xs, [D(x,k) for x in xs], label=\"k = \" + str(k))\n",
      "plt.legend()\n",
      "116/33:\n",
      "plt.figure(figsize=(20,10))\n",
      "plt.plot(xs, [gauss2(x, 1) for x in xs], label=\"Actual\")\n",
      "for k in [1.2, 1.4, 1.6, 1.8, 2.0]:\n",
      "    plt.plot(xs, [-1*D(x,k) for x in xs], label=\"k = \" + str(k))\n",
      "plt.legend()\n",
      "116/34:\n",
      "plt.figure(figsize=(20,10))\n",
      "plt.plot(xs, [gauss2(x, 1) for x in xs], label=\"Actual\")\n",
      "for k in [1.2, 1.4, 1.6, 1.8, 2.0]:\n",
      "    plt.plot(xs, [D(x,k) for x in xs], label=\"k = \" + str(k))\n",
      "plt.legend()\n",
      "116/35:\n",
      "def D(x, k):\n",
      "    return (gauss(x, k*1)-gauss(x,1))/(k*1-1)\n",
      "116/36:\n",
      "def gauss(x, s):\n",
      "    (1/(np.sqrt(2*np.pi)*s))*np.exp(-x*x/(2*s**2))\n",
      "116/37:\n",
      "plt.figure(figsize=(20,10))\n",
      "plt.plot(xs, [gauss2(x, 1) for x in xs], label=\"Actual\")\n",
      "for k in [1.2, 1.4, 1.6, 1.8, 2.0]:\n",
      "    plt.plot(xs, [D(x,k) for x in xs], label=\"k = \" + str(k))\n",
      "plt.legend()\n",
      "116/38:\n",
      "def D(x, k):\n",
      "    return (gauss(x, k*1)-gauss(x,1))/(k*1-1)\n",
      "116/39:\n",
      "plt.figure(figsize=(20,10))\n",
      "plt.plot(xs, [gauss2(x, 1) for x in xs], label=\"Actual\")\n",
      "for k in [1.2, 1.4, 1.6, 1.8, 2.0]:\n",
      "    plt.plot(xs, [D(x,k) for x in xs], label=\"k = \" + str(k))\n",
      "plt.legend()\n",
      "116/40:\n",
      "def gauss(x, s):\n",
      "    return (1/(np.sqrt(2*np.pi)*s))*np.exp(-x*x/(2*s**2))\n",
      "116/41:\n",
      "plt.figure(figsize=(20,10))\n",
      "plt.plot(xs, [gauss2(x, 1) for x in xs], label=\"Actual\")\n",
      "for k in [1.2, 1.4, 1.6, 1.8, 2.0]:\n",
      "    plt.plot(xs, [D(x,k) for x in xs], label=\"k = \" + str(k))\n",
      "plt.legend()\n",
      "117/1: import numpy as np\n",
      "117/2:\n",
      "def gprime(x,s):\n",
      "        return (-1/(np.sqrt(2*np.pi)*s**3))*x*np.exp(x**2/(2*s**2))\n",
      "\n",
      "def gprimekernel(halfw,s):\n",
      "    return [gprime(x,s) for x in np.linspace(-halfw, halfw)]\n",
      "\n",
      "halfw = 3*sigma\n",
      "\n",
      "x = np.expand_dims(gprimekernel(halfw,sigma),0)\n",
      "117/3:\n",
      "def gprime(x,s):\n",
      "        return (-1/(np.sqrt(2*np.pi)*s**3))*x*np.exp(x**2/(2*s**2))\n",
      "\n",
      "def gprimekernel(halfw,s):\n",
      "    return [gprime(x,s) for x in np.linspace(-halfw, halfw)]\n",
      "\n",
      "halfw = 3*1\n",
      "\n",
      "x = np.expand_dims(gprimekernel(halfw,sigma),0)\n",
      "117/4: sigma = 1\n",
      "117/5:\n",
      "def gprime(x,s):\n",
      "        return (-1/(np.sqrt(2*np.pi)*s**3))*x*np.exp(x**2/(2*s**2))\n",
      "\n",
      "def gprimekernel(halfw,s):\n",
      "    return [gprime(x,s) for x in np.linspace(-halfw, halfw)]\n",
      "\n",
      "halfw = 3*sigma\n",
      "\n",
      "x = np.expand_dims(gprimekernel(halfw,sigma),0)\n",
      "117/6: x\n",
      "117/7:\n",
      "import numpy as np\n",
      "import matplotlib.pyplot as plt\n",
      "117/8: plt.plot(np.linspace(-halfw, halfw), gprimekernel(halfw,sigma))\n",
      "117/9: plt.plot(np.linspace(-halfw, halfw), gprimekernel(halfw,sigma))\n",
      "117/10:\n",
      "def gprime(x,s):\n",
      "        return (-1/(np.sqrt(2*np.pi)*s**3))*x*np.exp(x**2/(2*s**2))\n",
      "\n",
      "def gprimekernel(halfw,s):\n",
      "    return [gprime(x,s) for x in np.linspace(-halfw, halfw)]\n",
      "\n",
      "halfw = 30*sigma\n",
      "\n",
      "x = np.expand_dims(gprimekernel(halfw,sigma),0)\n",
      "117/11: plt.plot(np.linspace(-halfw, halfw), gprimekernel(halfw,sigma))\n",
      "117/12:\n",
      "def gprime(x,s):\n",
      "        return (-1/(np.sqrt(2*np.pi)*s**3))*x*np.exp(x**2/(2*s**2))\n",
      "\n",
      "def gprimekernel(halfw,s):\n",
      "    return [gprime(x,s) for x in np.linspace(-halfw, halfw)]\n",
      "\n",
      "halfw = 3*sigma\n",
      "\n",
      "x = np.expand_dims(gprimekernel(halfw,sigma),0)\n",
      "117/13: plt.plot(np.linspace(-halfw, halfw), gprimekernel(halfw,sigma))\n",
      "117/14: plt.plot(np.linspace(-halfw, halfw), [gprime(halfw,sigma) for x in np.linspace(-halfw, halfw)])\n",
      "117/15: plt.plot(np.linspace(-halfw, halfw), [gprime(x,sigma) for x in np.linspace(-halfw, halfw)])\n",
      "117/16: halfw = 20\n",
      "117/17: plt.plot(np.linspace(-halfw, halfw), [gprime(x,sigma) for x in np.linspace(-halfw, halfw)])\n",
      "117/18:\n",
      "def gprime(x,s):\n",
      "        return (-1/(np.sqrt(2*np.pi)*s**3))*x*np.exp(x**2/(2*s**2))\n",
      "\n",
      "def gprimekernel(halfw,s):\n",
      "    return [gprime(x,s) for x in np.linspace(-halfw, halfw)]\n",
      "\n",
      "halfw = 3*sigma\n",
      "\n",
      "x = np.expand_dims(gprimekernel(halfw,sigma),0)\n",
      "117/19: plt.plot(np.linspace(-halfw, halfw), [gprime(x,sigma) for x in np.linspace(-halfw, halfw)])\n",
      "117/20:\n",
      "def gprime(x,s):\n",
      "        return (-1/(np.sqrt(2*np.pi)*s**3))*x*np.exp(-x**2/(2*s**2))\n",
      "\n",
      "def gprimekernel(halfw,s):\n",
      "    return [gprime(x,s) for x in np.linspace(-halfw, halfw)]\n",
      "\n",
      "halfw = 3*sigma\n",
      "\n",
      "x = np.expand_dims(gprimekernel(halfw,sigma),0)\n",
      "117/21: plt.plot(np.linspace(-halfw, halfw), gprimekernel(halfw,sigma))\n",
      "117/22: plt.plot(np.linspace(-halfw, halfw), [gprime(x,sigma) for x in np.linspace(-halfw, halfw)])\n",
      "117/23:\n",
      "def g(x,s):\n",
      "        return (1/(np.sqrt(2*np.pi)*s))*np.exp(-x**2/(2*s**2))\n",
      "117/24:\n",
      "def g(x,s):\n",
      "        return (1/(np.sqrt(2*np.pi)*s))*np.exp(-x**2/(2*s**2))\n",
      "    \n",
      "def gprimekernel(halfw,s):\n",
      "    return [gprime(x,s) for x in np.linspace(-halfw, halfw)]\n",
      "117/25:\n",
      "def g(x,s):\n",
      "        return (1/(np.sqrt(2*np.pi)*s))*np.exp(-x**2/(2*s**2))\n",
      "    \n",
      "def gprimekernel(halfw,s):\n",
      "    return [g(x,s) for x in np.linspace(-halfw, halfw)]\n",
      "117/26: plt.plot(np.linspace(-halfw, halfw), gprimekernel(halfw,sigma))\n",
      "117/27: gprimekernel(halfw,sigma)\n",
      "118/1:\n",
      "import numpy as np\n",
      "import matplotlib.pyplot as plt\n",
      "118/2:\n",
      "def gauss2(x, s):\n",
      "    return (1/(np.sqrt(2*np.pi)*s**3))*(x*x/(s**2)-1)*np.exp(-x*x/(2*s**2))\n",
      "118/3:\n",
      "def gauss(x, s):\n",
      "    return (1/(np.sqrt(2*np.pi)*s))*np.exp(-x*x/(2*s**2))\n",
      "118/4: xs = np.linspace(-15,15,100)\n",
      "118/5:\n",
      "# Second derivative of Gaussian\n",
      "plt.title(\"Second derivative of Gaussian\")\n",
      "plt.plot(xs, [gauss2(x, 1) for x in xs])\n",
      "plt.show()\n",
      "118/6:\n",
      "# Difference of Gaussians\n",
      "def D(x, k):\n",
      "    return (gauss(x, k*1)-gauss(x,1))/(k*1-1)\n",
      "118/7:\n",
      "plt.figure(figsize=(20,10))\n",
      "plt.title(\"Approximating Second Derivative of Gaussian with Difference of Gaussians\")\n",
      "plt.plot(xs, [gauss2(x, 1) for x in xs], label=\"Actual\")\n",
      "for k in [1.2, 1.4, 1.6, 1.8, 2.0]:\n",
      "    plt.plot(xs, [D(x,k) for x in xs], label=\"k = \" + str(k))\n",
      "plt.legend()\n",
      "plt.show()\n",
      "118/8: # We see that k=1.2 gives the best approximation\n",
      "119/1:\n",
      "import xml.etree.ElementTree as ET\n",
      "from itertools import reduce\n",
      "import os\n",
      "from typing import List\n",
      "119/2:\n",
      "import xml.etree.ElementTree as ET\n",
      "import reduce\n",
      "import os\n",
      "from typing import List\n",
      "119/3:\n",
      "import xml.etree.ElementTree as ET\n",
      "import os\n",
      "from typing import List\n",
      "119/4:\n",
      "class Doc:\n",
      "    def __init__(self, id: int, descriptors: List[str], text: str):\n",
      "        self.id = id\n",
      "        self.descriptors = descriptors\n",
      "        self.text = text\n",
      "119/5:\n",
      "def parse_doc(file: str) -> Doc:\n",
      "    tree = ET.parse(file)\n",
      "    root = tree.getroot()\n",
      "    fulltext = root.find('body').find('body.content').find(\"*[@class='full_text']\")\n",
      "    text = reduce(lambda x, y: x + y, [x.text for x in fulltext.findall('p')])\n",
      "    id = int(root.find('head').find('docdata').find('doc-id').attrib['id-string'])\n",
      "    classifiers = root.find('head').find('docdata').find('identified-content')\n",
      "    classifiers = classifiers.findall(\"*[@class='indexing_service'][@type='descriptor']\")\n",
      "    descriptors = [c.text for c in classifiers]\n",
      "    return Doc(id, descriptors, text)\n",
      "119/6:\n",
      "def parse_doc(file: str) -> Doc:\n",
      "    tree = ET.parse(file)\n",
      "    root = tree.getroot()\n",
      "    fulltext = root.find('body').find('body.content').find(\"*[@class='full_text']\")\n",
      "    text = reduce(lambda x, y: x + y, [x.text for x in fulltext.findall('p')])\n",
      "    id = int(root.find('head').find('docdata').find('doc-id').attrib['id-string'])\n",
      "    classifiers = root.find('head').find('docdata').find('identified-content')\n",
      "    classifiers = classifiers.findall(\"*[@class='indexing_service'][@type='descriptor']\")\n",
      "    descriptors = [c.text for c in classifiers]\n",
      "    return Doc(id, descriptors, text)\n",
      "119/7:\n",
      "docs = []\n",
      "for file in os.listdir('01/01'):\n",
      "    docs.append(parse_doc(file))\n",
      "119/8:\n",
      "docs = []\n",
      "for file in os.listdir('01/01'):\n",
      "    docs.append(parse_doc('01/01/' + file))\n",
      "119/9: import itertools\n",
      "119/10: from itertools import reduce\n",
      "119/11: from functools import reduce\n",
      "119/12:\n",
      "import xml.etree.ElementTree as ET\n",
      "import os\n",
      "from typing import List\n",
      "from functools import reduce\n",
      "119/13:\n",
      "docs = []\n",
      "for file in os.listdir('01/01'):\n",
      "    docs.append(parse_doc('01/01/' + file))\n",
      "119/14:\n",
      "def parse_doc(file: str) -> Doc:\n",
      "    tree = ET.parse(file)\n",
      "    root = tree.getroot()\n",
      "    fulltext = root.find('body').find('body.content').find(\"*[@class='full_text']\")\n",
      "    text = reduce(lambda x, y: x + y, [x.text for x in fulltext.findall('p')])\n",
      "    id = int(root.find('head').find('docdata').find('doc-id').attrib['id-string'])\n",
      "    classifiers = root.find('head').find('docdata').find('identified-content')\n",
      "    classifiers = classifiers.findall(\"*[@class='indexing_service'][@type='descriptor']\")\n",
      "    descriptors = [c.text for c in classifiers]\n",
      "    return Doc(id, descriptors, text)\n",
      "119/15:\n",
      "docs = []\n",
      "for file in os.listdir('01/01'):\n",
      "    docs.append(parse_doc('01/01/' + file))\n",
      "119/16:\n",
      "def parse_doc(file: str) -> Doc:\n",
      "    tree = ET.parse(file)\n",
      "    print(tree)\n",
      "    root = tree.getroot()\n",
      "    fulltext = root.find('body').find('body.content').find(\"*[@class='full_text']\")\n",
      "    text = reduce(lambda x, y: x + y, [x.text for x in fulltext.findall('p')])\n",
      "    id = int(root.find('head').find('docdata').find('doc-id').attrib['id-string'])\n",
      "    classifiers = root.find('head').find('docdata').find('identified-content')\n",
      "    classifiers = classifiers.findall(\"*[@class='indexing_service'][@type='descriptor']\")\n",
      "    descriptors = [c.text for c in classifiers]\n",
      "    return Doc(id, descriptors, text)\n",
      "119/17:\n",
      "docs = []\n",
      "for file in os.listdir('01/01'):\n",
      "    docs.append(parse_doc('01/01/' + file))\n",
      "119/18: docs[0]\n",
      "119/19: docs[0].text\n",
      "119/20: len(docs)\n",
      "119/21: docs[0].descriptors\n",
      "119/22: file\n",
      "119/23:\n",
      "tree = ET.parse('01/01/' + file)\n",
      "root = tree.getroot()\n",
      "fulltext = root.find('body').find('body.content').find(\"*[@class='full_text']\")\n",
      "text = reduce(lambda x, y: x + y, [x.text for x in fulltext.findall('p')])\n",
      "id = int(root.find('head').find('docdata').find('doc-id').attrib['id-string'])\n",
      "119/24:\n",
      "tree = ET.parse('01/01/' + file)\n",
      "root = tree.getroot()\n",
      "fulltext = root.find('body').find('body.content').find(\"*[@class='full_text']\")\n",
      "fulltext\n",
      "119/25: file\n",
      "119/26:\n",
      "def parse_doc(file: str) -> Doc:\n",
      "    tree = ET.parse(file)\n",
      "    root = tree.getroot()\n",
      "    try:\n",
      "        fulltext = root.find('body').find('body.content').find(\"*[@class='full_text']\")\n",
      "        text = reduce(lambda x, y: x + y, [x.text for x in fulltext.findall('p')])\n",
      "        id = int(root.find('head').find('docdata').find('doc-id').attrib['id-string'])\n",
      "        classifiers = root.find('head').find('docdata').find('identified-content')\n",
      "        classifiers = classifiers.findall(\"*[@class='indexing_service'][@type='descriptor']\")\n",
      "        descriptors = [c.text for c in classifiers]\n",
      "        return Doc(id, descriptors, text)\n",
      "    except:\n",
      "        return None\n",
      "119/27:\n",
      "docs = []\n",
      "for file in os.listdir('01/01'):\n",
      "    doc = parse_doc('01/01/' + file)\n",
      "    if doc:\n",
      "        docs.append(doc)\n",
      "119/28: len(docs)\n",
      "119/29:\n",
      "class Doc:\n",
      "    def __init__(self, id: int, descriptors: List[str], text: str) -> None:\n",
      "        self.id = id\n",
      "        self.descriptors = descriptors\n",
      "        self.text = text\n",
      "119/30:\n",
      "def parse_doc(file: str) -> Doc:\n",
      "    tree = ET.parse(file)\n",
      "    root = tree.getroot()\n",
      "    try:\n",
      "        fulltext = root.find('body').find('body.content').find(\"*[@class='full_text']\")\n",
      "        text = reduce(lambda x, y: x + y, [x.text for x in fulltext.findall('p')])\n",
      "        id = int(root.find('head').find('docdata').find('doc-id').attrib['id-string'])\n",
      "        classifiers = root.find('head').find('docdata').find('identified-content')\n",
      "        classifiers = classifiers.findall(\"*[@class='indexing_service'][@type='descriptor']\")\n",
      "        descriptors = [c.text for c in classifiers]\n",
      "        return Doc(id, descriptors, text)\n",
      "    except:\n",
      "        return None\n",
      "119/31:\n",
      "docs = []\n",
      "for file in os.listdir('01/01'):\n",
      "    doc = parse_doc('01/01/' + file)\n",
      "    if doc:\n",
      "        docs.append(doc)\n",
      "119/32:\n",
      "docs = []\n",
      "for day in os.listdir('01/'):\n",
      "    for file in os.listdir('01/' + day + '/'):\n",
      "    doc = parse_doc('01/' + day + '/' + file)\n",
      "    if doc:\n",
      "        docs.append(doc)\n",
      "119/33:\n",
      "docs = []\n",
      "for day in os.listdir('01/'):\n",
      "    for file in os.listdir('01/' + day + '/'):\n",
      "        doc = parse_doc('01/' + day + '/' + file)\n",
      "        if doc:\n",
      "            docs.append(doc)\n",
      "119/34:\n",
      "docs = []\n",
      "for day in range(1,32):\n",
      "    for file in os.listdir(f'01/{day:2d}/'):\n",
      "        doc = parse_doc('01/' + day + '/' + file)\n",
      "        if doc:\n",
      "            docs.append(doc)\n",
      "119/35:\n",
      "docs = []\n",
      "for day in range(1,32):\n",
      "    for file in os.listdir(f'01/{day:02}/'):\n",
      "        doc = parse_doc('01/' + day + '/' + file)\n",
      "        if doc:\n",
      "            docs.append(doc)\n",
      "119/36:\n",
      "docs = []\n",
      "for day in range(1,32):\n",
      "    for file in os.listdir(f'01/{day:02d}/'):\n",
      "        doc = parse_doc('01/' + day + '/' + file)\n",
      "        if doc:\n",
      "            docs.append(doc)\n",
      "119/37:\n",
      "docs = []\n",
      "for day in range(1,32):\n",
      "    for file in os.listdir(f'01/{day:%02d}/'):\n",
      "        doc = parse_doc('01/' + day + '/' + file)\n",
      "        if doc:\n",
      "            docs.append(doc)\n",
      "119/38:\n",
      "docs = []\n",
      "for day in range(1,32):\n",
      "    for file in os.listdir(f'01/{day:02}/'):\n",
      "        doc = parse_doc('01/' + day + '/' + file)\n",
      "        if doc:\n",
      "            docs.append(doc)\n",
      "119/39:\n",
      "docs = []\n",
      "for day in range(1,32):\n",
      "    for file in os.listdir(f'01/{day:02}/'):\n",
      "        doc = parse_doc(f'01/{day:02}/{file}')\n",
      "        if doc:\n",
      "            docs.append(doc)\n",
      "119/40: len(docs)\n",
      "119/41:\n",
      "alldescriptors = set()\n",
      "for doc in docs:\n",
      "    alldescriptors.update(doc.descriptors)\n",
      "119/42: print(alldescriptors)\n",
      "119/43:\n",
      "alld = {}\n",
      "for doc in docs:\n",
      "    for d in doc.descriptors:\n",
      "        alld[d] = alld.get(d, 0) + 1\n",
      "119/44: print(sorted(alld))\n",
      "119/45: print(alld)\n",
      "119/46: print(sorted(alld, key=lambda x: alld[x]))\n",
      "119/47:\n",
      "alldlist = sorted([(d, alld[d]) for d in alld], key=lambda x: x[1])\n",
      "print(alldlist)\n",
      "119/48:\n",
      "alldlist = sorted([(d, alld[d]) for d in alld], key=lambda x: x[1], reverse=True)\n",
      "print(alldlist)\n",
      "119/49:\n",
      "alldlist = sorted([(d, alld[d]) for d in alld], key=lambda x: x[1], reverse=True)\n",
      "print(alldlist[:20])\n",
      "119/50:\n",
      "alldlist = sorted([(d, alld[d]) for d in alld], key=lambda x: x[1], reverse=True)\n",
      "for x in range(20):\n",
      "    print(alldlist[x])\n",
      "119/51:\n",
      "alldlist = sorted([(d, alld[d]) for d in alld], key=lambda x: x[1], reverse=True)\n",
      "for x in range(25):\n",
      "    print(alldlist[x])\n",
      "119/52:\n",
      "alldlist = sorted([(d, alld[d]) for d in alld], key=lambda x: x[1], reverse=True)\n",
      "for x in range(25):\n",
      "    print(alldlist[x])\n",
      "119/53:\n",
      "import xml.etree.ElementTree as ET\n",
      "import os\n",
      "from typing import List\n",
      "from functools import reduce\n",
      "from calendar import monthrange\n",
      "119/54:\n",
      "def get_docs_by_month(year: int, month: int) -> List[Doc]:\n",
      "    days = monthrange(year, month)[1]\n",
      "    docs = []\n",
      "    for day in range(1, days+1):\n",
      "        for file in os.listdir(f'{month:02}/{day:02}/'):\n",
      "            doc = parse_doc(f'{month:02}/{day:02}/{file}')\n",
      "            if doc:\n",
      "                docs.append(doc)\n",
      "119/55: jan07 = get_docs_by_month(2007, 1)\n",
      "119/56: len(jan07)\n",
      "119/57:\n",
      "def get_docs_by_month(year: int, month: int) -> List[Doc]:\n",
      "    days = monthrange(year, month)[1]\n",
      "    docs = []\n",
      "    for day in range(1, days+1):\n",
      "        for file in os.listdir(f'{month:02}/{day:02}/'):\n",
      "            doc = parse_doc(f'{month:02}/{day:02}/{file}')\n",
      "            if doc:\n",
      "                docs.append(doc)\n",
      "    return docs\n",
      "119/58: jan07 = get_docs_by_month(2007, 1)\n",
      "119/59: len(jan07)\n",
      "119/60:\n",
      "all07 = []\n",
      "for i in range(1,7):\n",
      "    all07 += get_docs_by_month(2007, i)\n",
      "119/61:\n",
      "all07 = []\n",
      "for i in range(1,6):\n",
      "    all07 += get_docs_by_month(2007, i)\n",
      "119/62: len(all07)\n",
      "119/63:\n",
      "alld07 = {}\n",
      "for doc in all07:\n",
      "    for d in doc.descriptors:\n",
      "        alld07[d] = alld07.get(d, 0) + 1\n",
      "119/64:\n",
      "alldlist07 = sorted([(d, alld07[d]) for d in alld07], key=lambda x: x[1], reverse=True)\n",
      "for x in range(25):\n",
      "    print(alldlist07[x])\n",
      "119/65:\n",
      "def parse_doc(file: str) -> Doc:\n",
      "    tree = ET.parse(file)\n",
      "    root = tree.getroot()\n",
      "    try:\n",
      "        fulltext = root.find('body').find('body.content').find(\"*[@class='full_text']\")\n",
      "        text = reduce(lambda x, y: x + y, [x.text for x in fulltext.findall('p')])\n",
      "        id = int(root.find('head').find('docdata').find('doc-id').attrib['id-string'])\n",
      "        classifiers = root.find('head').find('docdata').find('identified-content')\n",
      "        classifiers = classifiers.findall(\"*[@class='online_producer'][@type='general_descriptor']\")\n",
      "        descriptors = [c.text for c in classifiers]\n",
      "        return Doc(id, descriptors, text)\n",
      "    except:\n",
      "        return None\n",
      "119/66:\n",
      "def get_docs_by_month(year: int, month: int) -> List[Doc]:\n",
      "    days = monthrange(year, month)[1]\n",
      "    docs = []\n",
      "    for day in range(1, days+1):\n",
      "        for file in os.listdir(f'{month:02}/{day:02}/'):\n",
      "            doc = parse_doc(f'{month:02}/{day:02}/{file}')\n",
      "            if doc:\n",
      "                docs.append(doc)\n",
      "    return docs\n",
      "119/67: jan07 = get_docs_by_month(2007, 1)\n",
      "119/68: len(jan07)\n",
      "119/69:\n",
      "alld = {}\n",
      "for doc in docs:\n",
      "    for d in doc.descriptors:\n",
      "        alld[d] = alld.get(d, 0) + 1\n",
      "119/70:\n",
      "alldlist = sorted([(d, alld[d]) for d in alld], key=lambda x: x[1], reverse=True)\n",
      "for x in range(25):\n",
      "    print(alldlist[x])\n",
      "119/71:\n",
      "all07 = []\n",
      "for i in range(1,6):\n",
      "    all07 += get_docs_by_month(2007, i)\n",
      "119/72: len(all07)\n",
      "119/73:\n",
      "alld07 = {}\n",
      "for doc in all07:\n",
      "    for d in doc.descriptors:\n",
      "        alld07[d] = alld07.get(d, 0) + 1\n",
      "119/74:\n",
      "alldlist07 = sorted([(d, alld07[d]) for d in alld07], key=lambda x: x[1], reverse=True)\n",
      "for x in range(25):\n",
      "    print(alldlist07[x])\n",
      "119/75:\n",
      "alldlist = sorted([(d, alld[d]) for d in alld], key=lambda x: x[1], reverse=True)\n",
      "for x in range(25):\n",
      "    print(alldlist[x])\n",
      "119/76:\n",
      "alldlist07 = sorted([(d, alld07[d][0], alld07[d][1]) for d in alld07], key=lambda x: x[1], reverse=True)\n",
      "for x in range(25):\n",
      "    print(alldlist07[x])\n",
      "119/77:\n",
      "alld07 = {}\n",
      "for doc in all07:\n",
      "    for d in doc.descriptors:\n",
      "        count, docs = alld07.get(d, (0, set()))\n",
      "        docs.add(doc)\n",
      "        alld07[d] = (count + 1, docs)\n",
      "119/78:\n",
      "alldlist07 = sorted([(d, alld07[d][0], alld07[d][1]) for d in alld07], key=lambda x: x[1], reverse=True)\n",
      "for x in range(25):\n",
      "    print(alldlist07[x])\n",
      "119/79:\n",
      "alldlist07 = sorted([(d, alld07[d][0], alld07[d][1]) for d in alld07], key=lambda x: x[1], reverse=True)\n",
      "for x in range(25):\n",
      "    print(alldlist07[x][0:1])\n",
      "119/80:\n",
      "alldlist07 = sorted([(d, alld07[d][0], alld07[d][1]) for d in alld07], key=lambda x: x[1], reverse=True)\n",
      "for x in range(25):\n",
      "    print(alldlist07[x][0:2])\n",
      "119/81: alld07['Computers and the Internet']\n",
      "119/82: alld07['Computers and the Internet'][1][0]\n",
      "119/83: alld07['Computers and the Internet'][1].peek()\n",
      "119/84: d1 = alld07['Computers and the Internet'][1].pop()\n",
      "119/85: d1\n",
      "119/86: d1.text\n",
      "119/87: d1 = alld07['Medicine and Health'][1].pop()\n",
      "119/88: d1.text\n",
      "119/89:\n",
      "for x in range(25):\n",
      "    print(alldlist07[x][0:2])\n",
      "119/90: len(alld07['Medicine and Health'][1])\n",
      "119/91:\n",
      "alld07 = {}\n",
      "for doc in all07:\n",
      "    for d in doc.descriptors:\n",
      "        count, docs = alld07.get(d, (0, set()))\n",
      "        docs.add(doc)\n",
      "        alld07[d] = (count + 1, docs)\n",
      "119/92:\n",
      "alldlist07 = sorted([(d, alld07[d][0], alld07[d][1]) for d in alld07], key=lambda x: x[1], reverse=True)\n",
      "for x in range(25):\n",
      "    print(alldlist07[x][0:2])\n",
      "119/93: len(alld07['Medicine and Health'][1])\n",
      "119/94: d1 = alld07['Computers and the Internet'][1].pop()\n",
      "119/95: d1.text\n",
      "119/96: computers = filter(lambda x: 'Politics and Government' not in all07[x].descriptors, alld07['Computers and the Internet'][1])\n",
      "119/97: len(computers)\n",
      "119/98:\n",
      "computers = list(filter(lambda x: 'Politics and Government' not in all07[x].descriptors, \\\n",
      "                        alld07['Computers and the Internet'][1]))\n",
      "119/99:\n",
      "computers = filter(lambda x: 'Politics and Government' not in all07[x].descriptors, \\\n",
      "                        alld07['Computers and the Internet'][1])\n",
      "119/100: len(computers)\n",
      "119/101: computers\n",
      "119/102:\n",
      "computers = filter(lambda x: x, \\\n",
      "                        alld07['Computers and the Internet'][1])\n",
      "119/103: computers\n",
      "119/104:\n",
      "computers = filter(lambda x: x, \\\n",
      "                        alld07['Computers and the Internet'])\n",
      "119/105: computers\n",
      "119/106: filter(lambda x: x > 1, set([1,2,3]))\n",
      "119/107: list(filter(lambda x: x > 1, set([1,2,3])))\n",
      "119/108:\n",
      "computers = list(filter(lambda x: 'Politics and Government' not in all07[x].descriptors, \\\n",
      "                        alld07['Computers and the Internet'][1]))\n",
      "119/109:\n",
      "[x for x in filter(lambda x: 'Politics and Government' not in all07[x].descriptors, \\\n",
      "                        alld07['Computers and the Internet'][1])]\n",
      "119/110: list(filter(lambda x: \"a\" not in x, set([\"abc\", \"def\"])))\n",
      "119/111: alld07['Computers and the Internet'][1]\n",
      "119/112: set(alld07['Computers and the Internet'][1])\n",
      "119/113: type(alld07['Computers and the Internet'][1])\n",
      "119/114:\n",
      "[x for x in filter(lambda x: 'Politics and Government' not in x.descriptors, \\\n",
      "                        alld07['Computers and the Internet'][1])]\n",
      "119/115:\n",
      "filter(lambda x: 'Politics and Government' not in x.descriptors, \\\n",
      "                        alld07['Computers and the Internet'][1])\n",
      "119/116:\n",
      "computers = list(filter(lambda x: 'Politics and Government' not in x.descriptors, \\\n",
      "                        alld07['Computers and the Internet'][1]))\n",
      "119/117: len(computers)\n",
      "119/118: d1 = computers[0]\n",
      "119/119: d1.text\n",
      "119/120:\n",
      "politics = list(filter(lambda x: 'Computers and the Internet' not in x.descriptors, \\\n",
      "                        alld07['Politics and Government'][1]))\n",
      "119/121: len(politics)\n",
      "119/122: politics = sample(politics, 1200)\n",
      "119/123: politics = random.sample(politics, 1200)\n",
      "119/124:\n",
      "import xml.etree.ElementTree as ET\n",
      "import os\n",
      "from typing import List\n",
      "from functools import reduce\n",
      "from calendar import monthrange\n",
      "import random\n",
      "119/125: politics = random.sample(politics, 1200)\n",
      "119/126: len(politics)\n",
      "119/127: computers = random.sample(computers, 1200)\n",
      "119/128: d1 = computers[0]\n",
      "119/129: d1.text\n",
      "119/130: len(d1.text.split(\" \"))\n",
      "119/131:\n",
      "class Doc:\n",
      "    def __init__(self, id: int, gdescriptors: List[str], descriptors: List[str], text: str) -> None:\n",
      "        self.id = id\n",
      "        self.gdescriptors = gdescriptors\n",
      "        self.descriptors = descriptors\n",
      "        self.text = text\n",
      "119/132:\n",
      "def parse_doc(file: str) -> Doc:\n",
      "    tree = ET.parse(file)\n",
      "    root = tree.getroot()\n",
      "    try:\n",
      "        fulltext = root.find('body').find('body.content').find(\"*[@class='full_text']\")\n",
      "        text = reduce(lambda x, y: x + y, [x.text for x in fulltext.findall('p')])\n",
      "        id = int(root.find('head').find('docdata').find('doc-id').attrib['id-string'])\n",
      "        classifiers = root.find('head').find('docdata').find('identified-content')\n",
      "        gclassifiers = classifiers.findall(\"*[@class='online_producer'][@type='general_descriptor']\")\n",
      "        dclassifiers = classifiers.findall(\"*[@class='online_producer'][@type='descriptor']\")\n",
      "        gdescriptors = [c.text for c in gclassifiers]\n",
      "        descriptors = [c.text for c in dclassifiers]\n",
      "        return Doc(id, gdescriptors, descriptors, text)\n",
      "    except:\n",
      "        return None\n",
      "119/133:\n",
      "def get_docs_by_month(year: int, month: int) -> List[Doc]:\n",
      "    days = monthrange(year, month)[1]\n",
      "    docs = []\n",
      "    for day in range(1, days+1):\n",
      "        for file in os.listdir(f'{year}/{month:02}/{day:02}/'):\n",
      "            doc = parse_doc(f'{year}/{month:02}/{day:02}/{file}')\n",
      "            if doc:\n",
      "                docs.append(doc)\n",
      "    return docs\n",
      "119/134: jan07 = get_docs_by_month(2007, 1)\n",
      "119/135: len(jan07)\n",
      "119/136:\n",
      "alld = {}\n",
      "for doc in docs:\n",
      "    for d in doc.descriptors:\n",
      "        alld[d] = alld.get(d, 0) + 1\n",
      "119/137:\n",
      "alldlist = sorted([(d, alld[d]) for d in alld], key=lambda x: x[1], reverse=True)\n",
      "for x in range(25):\n",
      "    print(alldlist[x])\n",
      "119/138:\n",
      "all07 = []\n",
      "for i in range(1,6):\n",
      "    all07 += get_docs_by_month(2007, i)\n",
      "119/139: len(all07)\n",
      "119/140:\n",
      "all06 = []\n",
      "for i in range(1,13):\n",
      "    all06 += get_docs_by_month(2006, i)\n",
      "119/141: len(all06)\n",
      "119/142:\n",
      "lens = []\n",
      "for c in computers:\n",
      "    lens.append(len(c.text.split(\" \")))\n",
      "119/143: hist(lens)\n",
      "119/144:\n",
      "hist(lens)\n",
      "plot.show()\n",
      "119/145: mean(lens)\n",
      "119/146: np.mean(lens)\n",
      "119/147:\n",
      "import xml.etree.ElementTree as ET\n",
      "import os\n",
      "from typing import List\n",
      "from functools import reduce\n",
      "from calendar import monthrange\n",
      "import random\n",
      "import matplotlib.pyplot as plt\n",
      "import numpy as np\n",
      "120/1:\n",
      "import xml.etree.ElementTree as ET\n",
      "import os\n",
      "from typing import List\n",
      "from functools import reduce\n",
      "from calendar import monthrange\n",
      "import random\n",
      "import matplotlib.pyplot as plt\n",
      "import numpy as np\n",
      "120/2:\n",
      "class Doc:\n",
      "    def __init__(self, id: int, gdescriptors: List[str], descriptors: List[str], text: str) -> None:\n",
      "        self.id = id\n",
      "        self.gdescriptors = gdescriptors\n",
      "        self.descriptors = descriptors\n",
      "        self.text = text\n",
      "120/3:\n",
      "def parse_doc(file: str) -> Doc:\n",
      "    tree = ET.parse(file)\n",
      "    root = tree.getroot()\n",
      "    try:\n",
      "        fulltext = root.find('body').find('body.content').find(\"*[@class='full_text']\")\n",
      "        text = reduce(lambda x, y: x + y, [x.text for x in fulltext.findall('p')])\n",
      "        id = int(root.find('head').find('docdata').find('doc-id').attrib['id-string'])\n",
      "        classifiers = root.find('head').find('docdata').find('identified-content')\n",
      "        gclassifiers = classifiers.findall(\"*[@class='online_producer'][@type='general_descriptor']\")\n",
      "        dclassifiers = classifiers.findall(\"*[@class='online_producer'][@type='descriptor']\")\n",
      "        gdescriptors = [c.text for c in gclassifiers]\n",
      "        descriptors = [c.text for c in dclassifiers]\n",
      "        return Doc(id, gdescriptors, descriptors, text)\n",
      "    except:\n",
      "        return None\n",
      "120/4:\n",
      "def get_docs_by_month(year: int, month: int) -> List[Doc]:\n",
      "    days = monthrange(year, month)[1]\n",
      "    docs = []\n",
      "    for day in range(1, days+1):\n",
      "        for file in os.listdir(f'{year}/{month:02}/{day:02}/'):\n",
      "            doc = parse_doc(f'{year}/{month:02}/{day:02}/{file}')\n",
      "            if doc:\n",
      "                docs.append(doc)\n",
      "    return docs\n",
      "120/5: jan07 = get_docs_by_month(2007, 1)\n",
      "120/6: len(jan07)\n",
      "120/7:\n",
      "alld = {}\n",
      "for doc in docs:\n",
      "    for d in doc.descriptors:\n",
      "        alld[d] = alld.get(d, 0) + 1\n",
      "120/8:\n",
      "computers = list(filter(lambda x: 'Politics and Government' not in x.descriptors, \\\n",
      "                        alld07['Computers and the Internet'][1]))\n",
      "120/9:\n",
      "alld = {}\n",
      "for doc in jan07:\n",
      "    for d in doc.descriptors:\n",
      "        alld[d] = alld.get(d, 0) + 1\n",
      "120/10:\n",
      "alldlist = sorted([(d, alld[d]) for d in alld], key=lambda x: x[1], reverse=True)\n",
      "for x in range(25):\n",
      "    print(alldlist[x])\n",
      "120/11:\n",
      "all07 = []\n",
      "for i in range(1,6):\n",
      "    all07 += get_docs_by_month(2007, i)\n",
      "120/12: len(all07)\n",
      "120/13:\n",
      "all06 = []\n",
      "for i in range(1,13):\n",
      "    all06 += get_docs_by_month(2006, i)\n",
      "120/14: len(all06)\n",
      "120/15:\n",
      "alld07 = {}\n",
      "for doc in all07:\n",
      "    for d in doc.descriptors:\n",
      "        count, docs = alld07.get(d, (0, set()))\n",
      "        docs.add(doc)\n",
      "        alld07[d] = (count + 1, docs)\n",
      "120/16:\n",
      "alldlist07 = sorted([(d, alld07[d][0], alld07[d][1]) for d in alld07], key=lambda x: x[1], reverse=True)\n",
      "for x in range(25):\n",
      "    print(alldlist07[x][0:2])\n",
      "120/17:\n",
      "computers = list(filter(lambda x: 'Politics and Government' not in x.descriptors, \\\n",
      "                        alld07['Computers and the Internet'][1]))\n",
      "120/18:\n",
      "lens = []\n",
      "for c in computers:\n",
      "    lens.append(len(c.text.split(\" \")))\n",
      "120/19: np.mean(lens)\n",
      "120/20: np.median(lens)\n",
      "120/21: np.max(lens), np.min(lens)\n",
      "120/22: plt.hist(lens)\n",
      "120/23:\n",
      "politics = list(filter(lambda x: 'Computers and the Internet' not in x.descriptors, \\\n",
      "                        alld07['Politics and Government'][1]))\n",
      "120/24:\n",
      "lens2 = []\n",
      "for c in politics:\n",
      "    lens2.append(len(c.text.split(\" \")))\n",
      "120/25:\n",
      "politics = list(filter(lambda x: 'Computers and the Internet' not in x.descriptors, \\\n",
      "                        alld07['Politics and Government'][1]))\n",
      "120/26:\n",
      "lens2 = []\n",
      "for c in politics:\n",
      "    lens2.append(len(c.text.split(\" \")))\n",
      "120/27: np.mean(lens2), np.median(lens2), np.max(lens2), np.min(lens2)\n",
      "120/28: plt.hist(lens2)\n",
      "120/29: len(politics)\n",
      "120/30: len(computers)\n",
      "120/31:\n",
      "politics = list(filter(lambda x: 'Computers and the Internet' not in x.descriptors, \\\n",
      "                        alld06['Politics and Government'][1]))\n",
      "120/32:\n",
      "alld06 = {}\n",
      "for doc in all06:\n",
      "    for d in doc.descriptors:\n",
      "        count, docs = alld06.get(d, (0, set()))\n",
      "        docs.add(doc)\n",
      "        alld06[d] = (count + 1, docs)\n",
      "120/33:\n",
      "politics = list(filter(lambda x: 'Computers and the Internet' not in x.descriptors, \\\n",
      "                        alld06['Politics and Government'][1]))\n",
      "120/34:\n",
      "lens2 = []\n",
      "for c in politics:\n",
      "    lens2.append(len(c.text.split(\" \")))\n",
      "120/35: np.mean(lens2), np.median(lens2), np.max(lens2), np.min(lens2)\n",
      "120/36: plt.hist(lens2)\n",
      "120/37:\n",
      "computers = list(filter(lambda x: 'Politics and Government' not in x.descriptors, \\\n",
      "                        alld06['Computers and the Internet'][1]))\n",
      "120/38:\n",
      "lens = []\n",
      "for c in computers:\n",
      "    lens.append(len(c.text.split(\" \")))\n",
      "120/39: np.mean(lens)\n",
      "120/40: np.median(lens)\n",
      "120/41: np.max(lens), np.min(lens)\n",
      "120/42: plt.hist(lens)\n",
      "120/43: len(computers)\n",
      "120/44: computers = random.sample(computers, 1500)\n",
      "120/45: len(politics)\n",
      "120/46: politics = random.sample(politics, 1500)\n",
      "120/47:\n",
      "politics = list(filter(lambda x: 'Computers and the Internet' not in x.descriptors, \\\n",
      "                        alld06['Politics and Government'][1]))\n",
      "120/48:\n",
      "computers = list(filter(lambda x: 'Politics and Government' not in x.descriptors, \\\n",
      "                        alld06['Computers and the Internet'][1]))\n",
      "120/49:\n",
      "def inrange(lo, hi):\n",
      "    return len(c.text.split(\" \")) >= lo and len(c.text.split(\" \")) <= hi\n",
      "120/50:\n",
      "def inrange(c, lo, hi):\n",
      "    return len(c.text.split(\" \")) >= lo and len(c.text.split(\" \")) <= hi\n",
      "120/51: computersr = [c for c in computers if inrange(c, 500, 1500)]\n",
      "120/52: len(computers4)\n",
      "120/53: len(computersr)\n",
      "120/54: politicsr = [p for p in politics if inrange(p, 500, 1500)]\n",
      "120/55: len(politicsr)\n",
      "120/56: np.mean(lens), np.median(lens), np.max(lens), np.min(lens)\n",
      "121/1:\n",
      "from __future__ import absolute_import, division, print_function, unicode_literals\n",
      "import tensorflow as tf\n",
      "121/2:\n",
      "from __future__ import absolute_import, division, print_function, unicode_literals\n",
      "import tensorflow as tf\n",
      "121/3:\n",
      "from __future__ import absolute_import, division, print_function, unicode_literals\n",
      "import tensorflow as tf\n",
      "120/57: politicss\n",
      "120/58: politics\n",
      "120/59: politics[0].text\n",
      "120/60:\n",
      "import xml.etree.ElementTree as ET\n",
      "import os\n",
      "from typing import List\n",
      "from functools import reduce\n",
      "from calendar import monthrange\n",
      "import random\n",
      "import matplotlib.pyplot as plt\n",
      "import numpy as np\n",
      "import json\n",
      "120/61: from gensim.summarization.summarizer import summarize\n",
      "120/62: from gensim.summarization.summarizer import summarize\n",
      "120/63: text = politics[0].text\n",
      "120/64: print(summarize(text))\n",
      "120/65: summaries_pol = [summarize(x.text) for x in politics]\n",
      "120/66: import timeit\n",
      "120/67:\n",
      "timer.timeit()\n",
      "summaries_pol = [summarize(x.text) for x in politics]\n",
      "120/68:\n",
      "timeit.timeit()\n",
      "summaries_pol = [summarize(x.text) for x in politics]\n",
      "120/69: import time\n",
      "120/70:\n",
      "t = time.clock()\n",
      "summaries_pol = [summarize(x.text) for x in politics]\n",
      "time.clock() - t\n",
      "120/71:\n",
      "t = time.process_time()\n",
      "summaries_pol = [summarize(x.text) for x in politics]\n",
      "time.process_time() - t\n",
      "120/72: len(politics)\n",
      "120/73: summaries_pol\n",
      "120/74:\n",
      "s = summarize(text)\n",
      "len(s.split(\" \"))\n",
      "120/75:\n",
      "s = summarize(politics[1].text)\n",
      "len(s.split(\" \"))\n",
      "120/76:\n",
      "s = summarize(politics[2].text)\n",
      "len(s.split(\" \"))\n",
      "120/77:\n",
      "s = summarize(politics[1].text)\n",
      "len(s.split(\" \"))\n",
      "120/78:\n",
      "s = summarize(politics[0].text)\n",
      "len(s.split(\" \"))\n",
      "120/79:\n",
      "s = summarize(politics[10].text)\n",
      "len(s.split(\" \"))\n",
      "120/80: politics[10].text\n",
      "120/81: summarize(politics[10].text)\n",
      "120/82: summarize(politics[10].text, word_count=100)\n",
      "120/83: summarize(politics[10].text, word_count=200)\n",
      "120/84:\n",
      "from gensim.summarization.summarizer import summarize\n",
      "from gensim.models import LsiModel\n",
      "from gensim.test.utils import common_dictionary, common_corpus\n",
      "120/85:\n",
      "model = LsiModel(common_corpus, id2word=common_dictionary)\n",
      "vectorized_corpus = model[common_corpus]\n",
      "120/86: vectorized_corpus\n",
      "120/87: common_corpus\n",
      "120/88: common_dictionary\n",
      "120/89: len(politics)\n",
      "120/90:\n",
      "from __future__ import absolute_import, division, print_function, unicode_literals\n",
      "import tensorflow as tf\n",
      "120/91:\n",
      "from tensorflow import keras\n",
      "from tensorflow.keras import layers\n",
      "120/92: from nltk.corpus import wordnet\n",
      "120/93: # use wordnet to determine if word is gendered or not?\n",
      "120/94: from gensim.models import Word2Vec\n",
      "120/95: politics[10].text\n",
      "120/96:\n",
      "from gensim.models import Word2Vec\n",
      "from gensim.utils import simple_preprocess\n",
      "120/97: simple_preprocessprocess(politics[10].text)\n",
      "120/98: simple_preprocess(politics[10].text)\n",
      "120/99: docs = [simple_preprocess(t.text) for t in politics]\n",
      "120/100: simple_preprocess(politics[10].text)[:10] # lowercasing, de-punctuation\n",
      "120/101:\n",
      "t = time.process_time()\n",
      "docs = [simple_preprocess(t.text) for t in politics]\n",
      "time.process_time() - t\n",
      "120/102: model = Word2Vec(docs)\n",
      "120/103:\n",
      "t = time.process_time()\n",
      "model = Word2Vec(docs)\n",
      "time.process_time() - t\n",
      "120/104: model.wv\n",
      "120/105: model.vocabulary\n",
      "120/106: model.kv.accuracy\n",
      "120/107: model.wv.most_similar(positive=['house'], topn=5)\n",
      "120/108: model.wv.most_similar(positive=['thursday'], topn=5)\n",
      "120/109: model.wv.most_similar(positive=['approve'], topn=5)\n",
      "120/110: model.wv.most_similar(positive=['Republican'], topn=5)\n",
      "120/111: model.wv.most_similar(positive=['republican'], topn=5)\n",
      "120/112: model.wv.most_similar(positive=['democrat'], topn=5)\n",
      "120/113: model.wv.most_similar(positive=['democrat'], topn=10)\n",
      "120/114: model.wv.most_similar(positive=['republican'], topn=10)\n",
      "120/115:\n",
      "t = time.process_time()\n",
      "sg = Word2Vec(docs,sg=1)\n",
      "time.process_time() - t\n",
      "120/116: model.wv.accuracy\n",
      "120/117: model.wv.accuracy(politics[0])\n",
      "120/118: model.wv.get_vector('democrat')\n",
      "120/119: model.wv.get_vector('democrat').shape\n",
      "120/120: sg.wv.most_similar(positive=['republican'], topn=10)\n",
      "120/121: sg.wv.most_similar(positive=['democrat'], topn=10)\n",
      "120/122: model.wv.most_similar(positive=['approve'], topn=5)\n",
      "120/123: model.wv.most_similar(positive=['thursday'], topn=5)\n",
      "120/124: model.wv.most_similar(positive=['republican'], topn=10)\n",
      "120/125: sg.wv.most_similar(positive=['approve'], topn=5)\n",
      "120/126: model.wv.most_similar(positive=['approve'], topn=5)\n",
      "120/127: model.wv.most_similar(positive=['democrat'], topn=10)\n",
      "120/128: wordnet.synset('female')\n",
      "120/129: wordnet.synsets('female')\n",
      "120/130: wordnet.synsets('female')[0]\n",
      "120/131:\n",
      "f = wordnet.synsets('female')[0]\n",
      "f = wordnet.synsets('male')[0]\n",
      "120/132:\n",
      "f = wordnet.synsets('female')[0]\n",
      "m = wordnet.synsets('male')[0]\n",
      "120/133: words = ['queen', 'actress', 'king', 'actor', 'math', 'humanities']\n",
      "120/134: words = ['queen', 'actress', 'king', 'actor', 'math', 'humanities', 'doctor']\n",
      "120/135:\n",
      "for w in words:\n",
      "    wn = wordnet.synsets(w)[0]\n",
      "    print(f.wup_similarity(wn), m.wup_similarity(wn))\n",
      "120/136: words = ['woman', 'man', 'queen', 'actress', 'king', 'actor', 'math', 'humanities', 'doctor']\n",
      "120/137:\n",
      "for w in words:\n",
      "    wn = wordnet.synsets(w)[0]\n",
      "    print(f.wup_similarity(wn), m.wup_similarity(wn))\n",
      "120/138:\n",
      "f = wordnet.synsets('female')[0]\n",
      "m = wordnet.synsets('male')[0]\n",
      "120/139: words = ['woman', 'man', 'queen', 'actress', 'king', 'actor', 'math', 'humanities', 'doctor']\n",
      "120/140:\n",
      "for w in words:\n",
      "    wn = wordnet.synsets(w)[0]\n",
      "    print(f.wup_similarity(wn), m.wup_similarity(wn))\n",
      "120/141:\n",
      "for w in words:\n",
      "    wn = wordnet.synsets(w)[0]\n",
      "    print(wn)\n",
      "    print(f.wup_similarity(wn), m.wup_similarity(wn))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "120/142: f.hyponyms(), f.hypernyms()\n",
      "120/143:\n",
      "f = wordnet.synsets('woman')[0]\n",
      "m = wordnet.synsets('man')[0]\n",
      "120/144: words = ['woman', 'man', 'queen', 'actress', 'king', 'actor', 'math', 'humanities', 'doctor']\n",
      "120/145:\n",
      "for w in words:\n",
      "    wn = wordnet.synsets(w)[0]\n",
      "    print(wn)\n",
      "    print(f.wup_similarity(wn), m.wup_similarity(wn))\n",
      "120/146: f.hyponyms(), f.hypernyms()\n",
      "120/147: m.hyponyms(), m.hypernyms()\n",
      "120/148: len(m.hyponyms())\n",
      "120/149:\n",
      "import xml.etree.ElementTree as ET\n",
      "import os\n",
      "from typing import List\n",
      "from functools import reduce\n",
      "from calendar import monthrange\n",
      "import random\n",
      "import matplotlib.pyplot as plt\n",
      "import numpy as np\n",
      "import json\n",
      "120/150: import time\n",
      "120/151:\n",
      "with open('debiaswe-master/data/gender_specific_seed.json') as f:\n",
      "    gender_specific = f.read()\n",
      "120/152: gender_specific\n",
      "120/153:\n",
      "with open('debiaswe-master/data/gender_specific_seed.json') as f:\n",
      "    gender_specific = json.loads(f.read())\n",
      "120/154: gender_specific\n",
      "120/155: len(gender_specific)\n",
      "120/156:\n",
      "with open('debiaswe-master/data/gender_specific_full.json') as f:\n",
      "    gender_specific_full = json.loads(f.read())\n",
      "120/157: len(gender_specific_all)\n",
      "120/158: len(gender_specific_full)\n",
      "120/159: gender_specific_full\n",
      "120/160: gender_specific\n",
      "120/161: gender_specific[:10]\n",
      "120/162: gender_specific_full[:10]\n",
      "120/163:\n",
      "with open('debiaswe-master/data/professions.json') as f:\n",
      "    profs = json.loads(f.read())\n",
      "120/164: profs\n",
      "120/165: len(profs)\n",
      "120/166: p = wordnet.synsets('person')[0]\n",
      "120/167: p.hyponyms()\n",
      "120/168: p = wordnet.synsets('doctor')[0]\n",
      "120/169: p.hypernyms()\n",
      "120/170: p.hyponyms()\n",
      "120/171: p.hypernyms()\n",
      "120/172: p = wordnet.synsets('actor')[0]\n",
      "120/173: p.hypernyms()\n",
      "120/174: p = wordnet.synsets('programmer')[0]\n",
      "120/175: p.hypernyms()\n",
      "120/176: p = wordnet.synsets('engineer')[0]\n",
      "120/177: p.hypernyms()\n",
      "120/178: p = wordnet.synsets('person')[0]\n",
      "120/179: p = wordnet.synsets('medical practitioner')[0]\n",
      "120/180: p = wordnet.synsets('doctor')[0]\n",
      "120/181: p.hypernyms()\n",
      "120/182: p = wordnet.synsets('medical_practitioner.n.01')[0]\n",
      "120/183: p = wordnet.synsets('medical_practitioner')[0]\n",
      "120/184: p.hypernyms()\n",
      "120/185: p = wordnet.synsets('medical_practitioner')[0]\n",
      "120/186: p.hypernyms()\n",
      "120/187: p = wordnet.synsets('health_professional')[0]\n",
      "120/188: p.hypernyms()\n",
      "120/189: p = wordnet.synsets('professional')[0]\n",
      "120/190: p.hypernyms()\n",
      "120/191: p = wordnet.synsets('adult')[0]\n",
      "120/192: p.hypernyms()\n",
      "120/193:\n",
      "# person -> performer -> actor\n",
      "# person -> adult -> professional -> health professional -> medical practitioner -> doctor\n",
      "120/194: # female - male vector\n",
      "120/195: model.wv.get_vector('he')\n",
      "120/196: gendervec = model.wv.get_vector('he') - model.wv.get_vector('she')\n",
      "120/197: gendervec.dot(gendervec)\n",
      "120/198: gendervec.dot(gendervec)/gendervec.dot(gendervec)\n",
      "120/199: model.wv.get_vector('him')\n",
      "120/200: him = model.wv.get_vector('him')\n",
      "120/201: gendervec.dot(him)/gendervec.dot(gendervec)\n",
      "120/202: it = model.wv.get_vector('it')\n",
      "120/203: gendervec.dot(it)/gendervec.dot(gendervec)\n",
      "120/204: she = model.wv.get_vector('she')\n",
      "120/205: gendervec.dot(she)/gendervec.dot(gendervec)\n",
      "120/206: him.dot(gendervec)/gendervec.dot(gendervec)\n",
      "120/207: gendervec.dot(him)/gendervec.dot(gendervec)\n",
      "120/208: gendervec.dot(him)/him.dot(him)\n",
      "120/209: gendervec.dot(him)\n",
      "120/210: gendervec.dot(it)\n",
      "120/211: gendervec.dot(him)/gendervec.dot(gendervec)\n",
      "120/212: gendervec.dot(it)/gendervec.dot(gendervec)\n",
      "120/213: he = model.wv.get_vector('he')\n",
      "120/214: gendervec.dot(he)/gendervec.dot(gendervec)\n",
      "120/215: gendervec.dot(he)/(gendervec.dot(gendervec)*he.dot(he))\n",
      "120/216: gendervec.dot(it)/(gendervec.dot(gendervec)*it.dot(it))\n",
      "120/217: gendervec.dot(him)/(gendervec.dot(gendervec)*him.dot(him))\n",
      "120/218: man = model.wv.get_vector('man')\n",
      "120/219: woman = model.wv.get_vector('woman')\n",
      "120/220:\n",
      "def get_proj(w1):\n",
      "    gendervec = model.wv.get_vector('he') - model.wv.get_vector('she')\n",
      "    wvec = model.wv.get_vector(w1)\n",
      "    p = np.dot(gendervec, wvec)/np.linalg.norm(gendervec)\n",
      "    print(p)\n",
      "120/221: get_proj('him')\n",
      "120/222: get_proj('it')\n",
      "120/223: get_proj('she')\n",
      "120/224: get_proj('he')\n",
      "120/225: get_proj('cat')\n",
      "120/226: get_proj('hat')\n",
      "120/227: get_proj('egg')\n",
      "120/228: get_proj('doctor')\n",
      "120/229: get_proj('actor')\n",
      "120/230: get_proj('actress')\n",
      "120/231: get_proj('woman')\n",
      "120/232: get_proj('woman'), get_proj('man')\n",
      "120/233: get_proj('him'), get_proj('it')\n",
      "120/234: get_proj('she'), get_proj('he')\n",
      "120/235: get_proj('him'), get_proj('it'), get_proj('she'), get_proj('he')\n",
      "120/236:\n",
      "def get_proj(w1):\n",
      "    gendervec = model.wv.get_vector('he') - model.wv.get_vector('she')\n",
      "    wvec = model.wv.get_vector(w1)\n",
      "    p = np.dot(gendervec, wvec)/np.linalg.norm(gendervec)\n",
      "    return p\n",
      "120/237: get_proj('him'), get_proj('it'), get_proj('she'), get_proj('he')\n",
      "120/238: get_proj('cat'), get_proj('hat'), get_proj('doctor')\n",
      "120/239: get_proj('woman'), get_proj('man'), get_proj('actor')\n",
      "120/240: get_proj('her')\n",
      "120/241: get_proj('her'), get_proj('dress')\n",
      "120/242: get_proj('her'), get_proj('dress'), get_proj('male')\n",
      "120/243: get_proj('her'), get_proj('female'), get_proj('male')\n",
      "120/244: get_proj('cat'), get_proj('hat'), get_proj('doctor'), get_proj('nurse')\n",
      "120/245: model.wv.get_vector('woman') + model.wv.get_vector('he') - model.wv.get_vector('she')\n",
      "120/246: mtest = model.wv.get_vector('woman') + model.wv.get_vector('he') - model.wv.get_vector('she')\n",
      "120/247: model.most_similar(positive=[mtest], topn=1)\n",
      "120/248: # f.hyponyms(), f.hypernyms()\n",
      "120/249: # m.hyponyms(), m.hypernyms()\n",
      "120/250:\n",
      "with open('debiaswe-master/data/equalize-pairs.json') as f:\n",
      "    pairs = json.loads(f.read())\n",
      "120/251:\n",
      "with open('debiaswe-master/data/equalize_pairs.json') as f:\n",
      "    pairs = json.loads(f.read())\n",
      "120/252: pairs\n",
      "120/253: pairs[:5]\n",
      "120/254: from sklearn.decomposition import PCA\n",
      "120/255:\n",
      "# borrowed from Bolukbasi et al\n",
      "matrix = []\n",
      "for a, b in pairs:\n",
      "    center = (model.wv.get_vector(a) + model.wv.get_vector(b))/2\n",
      "    matrix.append(model.wv.get_vector(a) - center)\n",
      "    matrix.append(model.wv.get_vector(b) - center)\n",
      "matrix = np.array(matrix)\n",
      "pca = PCA(n_components = 10)\n",
      "120/256: model.wv.vocabulary\n",
      "120/257: model.wv.vocab\n",
      "120/258:\n",
      "# borrowed from Bolukbasi et al\n",
      "matrix = []\n",
      "for a, b in pairs:\n",
      "    if a not in model.wv.vocab or b not in model.wv.vocab:\n",
      "        print(a,b)\n",
      "        continue\n",
      "    center = (model.wv.get_vector(a) + model.wv.get_vector(b))/2\n",
      "    matrix.append(model.wv.get_vector(a) - center)\n",
      "    matrix.append(model.wv.get_vector(b) - center)\n",
      "matrix = np.array(matrix)\n",
      "pca = PCA(n_components = 10)\n",
      "120/259:\n",
      "# borrowed from Bolukbasi et al\n",
      "matrix = []\n",
      "for a, b in pairs:\n",
      "    if a not in model.wv.vocab or b not in model.wv.vocab:\n",
      "        print(a,b)\n",
      "        continue\n",
      "    center = (model.wv.get_vector(a) + model.wv.get_vector(b))/2\n",
      "    matrix.append(model.wv.get_vector(a) - center)\n",
      "    matrix.append(model.wv.get_vector(b) - center)\n",
      "matrix = np.array(matrix)\n",
      "pca = PCA(n_components = 10)\n",
      "print(matrix.shape)\n",
      "120/260: pair\n",
      "120/261: pairs\n",
      "120/262: \"Hi\".lower()\n",
      "120/263: pairs = list(map(pairs, lambda x: x))\n",
      "120/264: pairs = list(map(lambda x: x, pairs))\n",
      "120/265: pairs = list(map(lambda x: [x[0].lower(), x[1].lower()], pairs))\n",
      "120/266: pairs\n",
      "120/267:\n",
      "# borrowed from Bolukbasi et al\n",
      "matrix = []\n",
      "for a, b in pairs:\n",
      "    if a not in model.wv.vocab or b not in model.wv.vocab:\n",
      "        print(a,b)\n",
      "        continue\n",
      "    center = (model.wv.get_vector(a) + model.wv.get_vector(b))/2\n",
      "    matrix.append(model.wv.get_vector(a) - center)\n",
      "    matrix.append(model.wv.get_vector(b) - center)\n",
      "matrix = np.array(matrix)\n",
      "pca = PCA(n_components = 10)\n",
      "print(matrix.shape)\n",
      "120/268:\n",
      "# borrowed from Bolukbasi et al\n",
      "matrix = []\n",
      "for a, b in pairs:\n",
      "    if a not in model.wv.vocab or b not in model.wv.vocab:\n",
      "        continue\n",
      "    center = (model.wv.get_vector(a) + model.wv.get_vector(b))/2\n",
      "    matrix.append(model.wv.get_vector(a) - center)\n",
      "    matrix.append(model.wv.get_vector(b) - center)\n",
      "matrix = np.array(matrix)\n",
      "pca = PCA(n_components = 10)\n",
      "print(matrix.shape)\n",
      "120/269: pca.components\n",
      "120/270: pca._components\n",
      "120/271: pca.components_\n",
      "120/272:\n",
      "# borrowed from Bolukbasi et al\n",
      "matrix = []\n",
      "for a, b in pairs:\n",
      "    if a not in model.wv.vocab or b not in model.wv.vocab:\n",
      "        continue\n",
      "    center = (model.wv.get_vector(a) + model.wv.get_vector(b))/2\n",
      "    matrix.append(model.wv.get_vector(a) - center)\n",
      "    matrix.append(model.wv.get_vector(b) - center)\n",
      "matrix = np.array(matrix)\n",
      "pca = PCA(n_components = 10)\n",
      "pca.fit(matrix)\n",
      "bar(range(num_components), pca.explained_variance_ratio_)\n",
      "print(matrix.shape)\n",
      "120/273:\n",
      "from sklearn.decomposition import PCA\n",
      "from matplotlib.pyplot import bar\n",
      "120/274:\n",
      "# borrowed from Bolukbasi et al\n",
      "matrix = []\n",
      "for a, b in pairs:\n",
      "    if a not in model.wv.vocab or b not in model.wv.vocab:\n",
      "        continue\n",
      "    center = (model.wv.get_vector(a) + model.wv.get_vector(b))/2\n",
      "    matrix.append(model.wv.get_vector(a) - center)\n",
      "    matrix.append(model.wv.get_vector(b) - center)\n",
      "matrix = np.array(matrix)\n",
      "pca = PCA(n_components = 10)\n",
      "pca.fit(matrix)\n",
      "bar(range(num_components), pca.explained_variance_ratio_)\n",
      "print(matrix.shape)\n",
      "120/275:\n",
      "from sklearn.decomposition import PCA\n",
      "from matplotlib.pyplot import bar\n",
      "120/276:\n",
      "# borrowed from Bolukbasi et al\n",
      "matrix = []\n",
      "for a, b in pairs:\n",
      "    if a not in model.wv.vocab or b not in model.wv.vocab:\n",
      "        continue\n",
      "    center = (model.wv.get_vector(a) + model.wv.get_vector(b))/2\n",
      "    matrix.append(model.wv.get_vector(a) - center)\n",
      "    matrix.append(model.wv.get_vector(b) - center)\n",
      "matrix = np.array(matrix)\n",
      "pca = PCA(n_components = 10)\n",
      "pca.fit(matrix)\n",
      "bar(range(10), pca.explained_variance_ratio_)\n",
      "print(matrix.shape)\n",
      "120/277:\n",
      "pca_r = PCA(n_components = 10)\n",
      "pca_r.fit(np.random.rand((54,100)))\n",
      "bar(range(10), pca_r.explained_variance_ratio_)\n",
      "120/278:\n",
      "pca_r = PCA(n_components = 10)\n",
      "pca_r.fit(np.random.rand(54,100))\n",
      "bar(range(10), pca_r.explained_variance_ratio_)\n",
      "120/279: pca.components_\n",
      "120/280: pca.components_[0]\n",
      "120/281: pca.components_[0].shape\n",
      "120/282: len(filter(lambda x: x in model.wv.vocab, profs))\n",
      "120/283: len(list(filter(lambda x: x in model.wv.vocab, profs)))\n",
      "120/284: list(filter(lambda x: x in model.wv.vocab, profs)).shape\n",
      "120/285: list(filter(lambda x: x in model.wv.vocab, profs))\n",
      "120/286: profs\n",
      "120/287: list(filter(lambda x: x in model.wv.vocab, [p[0] for p in profs]))\n",
      "120/288: proflist = list(filter(lambda x: x in model.wv.vocab, [p[0] for p in profs]))\n",
      "120/289: len(proflist)\n",
      "120/290: proflist = list(filter(lambda x: x in model.wv.vocab, [p[0] for p in profs if abs(p[1]) < 0.6]))\n",
      "120/291: len(proflist)\n",
      "120/292: proflist\n",
      "120/293: g = pca.components_[0]\n",
      "120/294:\n",
      "def gproj(w1):\n",
      "    wvec = model.wv.get_vector(w1)\n",
      "    p = np.dot(g, wvec)/np.linalg.norm(g)\n",
      "    return p\n",
      "120/295: gproj('he')\n",
      "120/296:\n",
      "for x in ['he', 'she', 'doctor', 'nurse', 'actress', 'actor', 'man', 'woman', 'male', 'female', 'math', 'dress']:\n",
      "    print(x, gproj(x))\n",
      "120/297:\n",
      "for x in ['he', 'she', 'doctor', 'nurse', 'man', 'woman', 'male', 'female', 'math', 'dress']:\n",
      "    print(x, gproj(x))\n",
      "120/298: profs[:5]\n",
      "120/299:\n",
      "all_ = []\n",
      "for y in range(1988, 2007):\n",
      "    for i in range(1,13):\n",
      "        all_ += get_docs_by_month(y, i)\n",
      "120/300:\n",
      "def get_docs_by_month(year: int, month: int) -> List[Doc]:\n",
      "    days = monthrange(year, month)[1]\n",
      "    docs = []\n",
      "    for day in range(1, days+1):\n",
      "        for file in os.listdir(f'data/{year}/{month:02}/{day:02}/'):\n",
      "            doc = parse_doc(f'data/{year}/{month:02}/{day:02}/{file}')\n",
      "            if doc:\n",
      "                docs.append(doc)\n",
      "    return docs\n",
      "120/301:\n",
      "all_ = []\n",
      "for y in range(1988, 2007):\n",
      "    for i in range(1,13):\n",
      "        all_ += get_docs_by_month(y, i)\n",
      "120/302:\n",
      "all07 = []\n",
      "for i in range(1,6):\n",
      "    all07 += get_docs_by_month(2007, i)\n",
      "120/303:\n",
      "all06 = []\n",
      "for i in range(1,13):\n",
      "    all06 += get_docs_by_month(2006, i)\n",
      "120/304: len(all06)\n",
      "120/305:\n",
      "t = time.process_time()\n",
      "all06 = []\n",
      "for i in range(1,13):\n",
      "    all06 += get_docs_by_month(2006, i)\n",
      "time.process_time() - t\n",
      "120/306:\n",
      "t = time.process_time()\n",
      "alld06 = {}\n",
      "for doc in all06:\n",
      "    for d in doc.descriptors:\n",
      "        count, docs = alld06.get(d, (0, set()))\n",
      "        docs.add(doc)\n",
      "        alld06[d] = (count + 1, docs)\n",
      "time.process_time() - t\n",
      "120/307: alld06[0]\n",
      "120/308: alld06\n",
      "120/309: alld06['Theater'][0]\n",
      "120/310: alld06['Theater']\n",
      "120/311: alld06['Theater'][1]\n",
      "120/312: alld06['Theater'][1][0]\n",
      "120/313: alld06['Theater'][1]\n",
      "120/314:\n",
      "t = time.process_time()\n",
      "all_ = []\n",
      "for y in range(1988, 2007):\n",
      "    for i in range(1,13):\n",
      "        all_ += get_docs_by_month(y, i)\n",
      "time.process_time() - t\n",
      "120/315:\n",
      "t = time.process_time()\n",
      "all_ = []\n",
      "for y in range(1988, 2007):\n",
      "    for i in range(1,13):\n",
      "        all_ += get_docs_by_month(y, i)\n",
      "time.process_time() - t\n",
      "120/316:\n",
      "t = time.process_time()\n",
      "all_ = []\n",
      "for y in range(1988, 2007):\n",
      "    print(y)\n",
      "    for i in range(1,13):\n",
      "        all_ += get_docs_by_month(y, i)\n",
      "time.process_time() - t\n",
      "120/317: json.dump(all_[:5])\n",
      "120/318:\n",
      "with open('alldocs.json', 'w') as outfile:\n",
      "    json.dump(all_[:5], outfile)\n",
      "120/319:\n",
      "with open('alldocs.json', 'w') as outfile:\n",
      "    json.dump(all_[:5].__dict__, outfile)\n",
      "120/320:\n",
      "with open('alldocs.json', 'w') as outfile:\n",
      "    json.dump([x.__dict__ for x in all_[:5]], outfile)\n",
      "120/321:\n",
      "with open('alldocs.json', 'w') as outfile:\n",
      "    json.dump([x.__dict__ for x in all_[:5], outfile)\n",
      "120/322:\n",
      "with open('alldocs.json', 'w') as outfile:\n",
      "    json.dump([x.__dict__ for x in all_[:5]], outfile)\n",
      "120/323:\n",
      "with open('alldocs.json', 'r') as infile:\n",
      "    l = json.load(infile)\n",
      "120/324: l\n",
      "120/325: l[0]\n",
      "120/326: l[0]['text']\n",
      "120/327:\n",
      "with open('alldocs.json', 'w') as outfile:\n",
      "    json.dump([x.__dict__ for x in all_, outfile)\n",
      "120/328:\n",
      "with open('alldocs.json', 'w') as outfile:\n",
      "    json.dump([x.__dict__ for x in all_], outfile)\n",
      "120/329:\n",
      "with open('alldocs.json', 'r') as infile:\n",
      "    l = json.load(infile)\n",
      "len(l)\n",
      "120/330:\n",
      "with open('alldocs.json', 'r') as infile:\n",
      "    l = json.load(infile)\n",
      "120/331:\n",
      "with open('alldocs.json', 'r') as infile:\n",
      "    l = json.load(infile)\n",
      "120/332: len(all_)\n",
      "120/333:\n",
      "with open('alldocs.json', 'r') as infile:\n",
      "    print(infile)\n",
      "120/334:\n",
      "with open('alldocs.json', 'r') as infile:\n",
      "    l = json.load(infile)\n",
      "120/335:\n",
      "with open('alldocs1.json', 'w') as outfile:\n",
      "    json.dump([x.__dict__ for x in all_[:10]], outfile)\n",
      "120/336:\n",
      "with open('alldocs1.json', 'r') as infile:\n",
      "    l = json.load(infile)\n",
      "120/337:\n",
      "with open('alldocs1.json', 'w') as outfile:\n",
      "    json.dump([x.__dict__ for x in all_[:10000]], outfile)\n",
      "120/338:\n",
      "with open('alldocs1.json', 'r') as infile:\n",
      "    l = json.load(infile)\n",
      "120/339:\n",
      "with open('alldocs1.json', 'w') as outfile:\n",
      "    json.dump([x.__dict__ for x in all_[:1685787]], outfile)\n",
      "120/340:\n",
      "t = time.process_time()\n",
      "with open('alldocs1.json', 'w') as outfile:\n",
      "    json.dump([x.__dict__ for x in all_[:10000]], outfile)\n",
      "time.process_time() - t\n",
      "120/341:\n",
      "t = time.process_time()\n",
      "with open('alldocs1.json', 'w') as outfile:\n",
      "    json.dump([x.__dict__ for x in all_[:100000]], outfile)\n",
      "time.process_time() - t\n",
      "120/342:\n",
      "with open('alldocs1.json', 'r') as infile:\n",
      "    l = json.load(infile)\n",
      "120/343:\n",
      "t = time.process_time()\n",
      "with open('alldocs1.json', 'w') as outfile:\n",
      "    json.dump([x.__dict__ for x in all_[:1000000]], outfile)\n",
      "time.process_time() - t\n",
      "120/344:\n",
      "with open('alldocs1.json', 'r') as infile:\n",
      "    l = json.load(infile)\n",
      "120/345:\n",
      "t = time.process_time()\n",
      "with open('alldocs1.json', 'w') as outfile:\n",
      "    json.dump([x.__dict__ for x in all_[:500000]], outfile)\n",
      "time.process_time() - t\n",
      "120/346:\n",
      "with open('alldocs1.json', 'r') as infile:\n",
      "    l = json.load(infile)\n",
      "120/347:\n",
      "t = time.process_time()\n",
      "with open('alldocs2.json', 'w') as outfile:\n",
      "    json.dump([x.__dict__ for x in all_[1000000:]], outfile)\n",
      "time.process_time() - t\n",
      "120/348:\n",
      "t = time.process_time()\n",
      "with open('alldocs2.json', 'r') as infile:\n",
      "    l = json.load(infile)\n",
      "time.process_time() - t\n",
      "120/349:\n",
      "t = time.process_time()\n",
      "with open('alldocs2.json', 'w') as outfile:\n",
      "    json.dump([x.__dict__ for x in all_[1680000:]], outfile)\n",
      "time.process_time() - t\n",
      "120/350:\n",
      "t = time.process_time()\n",
      "with open('alldocs2.json', 'r') as infile:\n",
      "    l = json.load(infile)\n",
      "time.process_time() - t\n",
      "120/351:\n",
      "t = time.process_time()\n",
      "with open('alldocs1.json', 'r') as infile:\n",
      "    l = json.load(infile)\n",
      "time.process_time() - t\n",
      "120/352:\n",
      "t = time.process_time()\n",
      "with open('alldocs2.json', 'w') as outfile:\n",
      "    json.dump([x.__dict__ for x in all_[1600000:1680000]], outfile)\n",
      "time.process_time() - t\n",
      "120/353:\n",
      "t = time.process_time()\n",
      "with open('alldocs2.json', 'r') as infile:\n",
      "    l = json.load(infile)\n",
      "time.process_time() - t\n",
      "120/354:\n",
      "t = time.process_time()\n",
      "with open('alldocs2.json', 'w') as outfile:\n",
      "    json.dump([x.__dict__ for x in all_[500000:600000]], outfile)\n",
      "time.process_time() - t\n",
      "120/355:\n",
      "t = time.process_time()\n",
      "with open('alldocs2.json', 'r') as infile:\n",
      "    l = json.load(infile)\n",
      "time.process_time() - t\n",
      "120/356:\n",
      "t = time.process_time()\n",
      "with open('alldocs2.json', 'w') as outfile:\n",
      "    json.dump([x.__dict__ for x in all_[600000:700000]], outfile)\n",
      "time.process_time() - t\n",
      "120/357:\n",
      "t = time.process_time()\n",
      "with open('alldocs2.json', 'r') as infile:\n",
      "    l = json.load(infile)\n",
      "time.process_time() - t\n",
      "120/358:\n",
      "t = time.process_time()\n",
      "with open('alldocs2.json', 'w') as outfile:\n",
      "    json.dump([x.__dict__ for x in all_[700000:1000000]], outfile)\n",
      "time.process_time() - t\n",
      "120/359:\n",
      "t = time.process_time()\n",
      "with open('alldocs2.json', 'r') as infile:\n",
      "    l = json.load(infile)\n",
      "time.process_time() - t\n",
      "120/360:\n",
      "t = time.process_time()\n",
      "with open('alldocs3.json', 'w') as outfile:\n",
      "    json.dump([x.__dict__ for x in all_[1000000:1300000]], outfile)\n",
      "time.process_time() - t\n",
      "120/361:\n",
      "t = time.process_time()\n",
      "with open('alldocs3.json', 'r') as infile:\n",
      "    l = json.load(infile)\n",
      "time.process_time() - t\n",
      "120/362:\n",
      "t = time.process_time()\n",
      "with open('alldocs4.json', 'w') as outfile:\n",
      "    json.dump([x.__dict__ for x in all_[1300000:]], outfile)\n",
      "time.process_time() - t\n",
      "120/363:\n",
      "t = time.process_time()\n",
      "with open('alldocs4.json', 'r') as infile:\n",
      "    l = json.load(infile)\n",
      "time.process_time() - t\n",
      "120/364:\n",
      "t = time.process_time()\n",
      "with open('alldocs5.json', 'w') as outfile:\n",
      "    json.dump([x.__dict__ for x in all_[1500000:1600000]], outfile)\n",
      "time.process_time() - t\n",
      "120/365:\n",
      "t = time.process_time()\n",
      "with open('alldocs2.json', 'w') as outfile:\n",
      "    json.dump([x.__dict__ for x in all_[500000:]], outfile)\n",
      "time.process_time() - t\n",
      "120/366:\n",
      "t = time.process_time()\n",
      "with open('alldocs2.json', 'w') as outfile:\n",
      "    json.dump([x.__dict__ for x in all_[1500000:1600000]], outfile)\n",
      "time.process_time() - t\n",
      "120/367:\n",
      "t = time.process_time()\n",
      "with open('alldocs2.json', 'r') as infile:\n",
      "    l = json.load(infile)\n",
      "time.process_time() - t\n",
      "120/368:\n",
      "t = time.process_time()\n",
      "with open('alldocs.json', 'r') as infile:\n",
      "    l = json.load(infile)\n",
      "time.process_time() - t\n",
      "120/369:\n",
      "t = time.process_time()\n",
      "with open('alldocs1.json', 'w') as outfile:\n",
      "    json.dump([x.__dict__ for x in all_[1000000:1300000]], outfile)\n",
      "time.process_time() - t\n",
      "120/370:\n",
      "t = time.process_time()\n",
      "with open('alldocs1.json', 'w') as outfile:\n",
      "    json.dump([x.__dict__ for x in all_[:500000]], outfile)\n",
      "time.process_time() - t\n",
      "120/371:\n",
      "t = time.process_time()\n",
      "with open('alldocs1.json', 'r') as infile:\n",
      "    l1 = json.load(infile)\n",
      "time.process_time() - t\n",
      "120/372: len(l + l1)\n",
      "120/373: len(l)\n",
      "120/374: len(l1)\n",
      "120/375:\n",
      "t = time.process_time()\n",
      "with open('alldocs2.json', 'w') as outfile:\n",
      "    json.dump([x.__dict__ for x in all_[500000:]], outfile)\n",
      "time.process_time() - t\n",
      "120/376:\n",
      "t = time.process_time()\n",
      "with open('alldocs2.json', 'r') as infile:\n",
      "    l2 = json.load(infile)\n",
      "time.process_time() - t\n",
      "120/377:\n",
      "t = time.process_time()\n",
      "with open('alldocs2.json', 'w') as outfile:\n",
      "    json.dump([x.__dict__ for x in all_[500000:1000000]], outfile)\n",
      "time.process_time() - t\n",
      "120/378:\n",
      "t = time.process_time()\n",
      "with open('alldocs2.json', 'r') as infile:\n",
      "    l2 = json.load(infile)\n",
      "time.process_time() - t\n",
      "120/379:\n",
      "t = time.process_time()\n",
      "with open('alldocs2.json', 'r') as infile:\n",
      "    l2 = json.load(infile)\n",
      "time.process_time() - t\n",
      "121/4:\n",
      "t = time.process_time()\n",
      "with open('alldocs2.json', 'r') as infile:\n",
      "    l2 = json.load(infile)\n",
      "time.process_time() - t\n",
      "120/380:\n",
      "t = time.process_time()\n",
      "with open('alldocs2.json', 'r') as infile:\n",
      "    l2 = json.load(infile)\n",
      "time.process_time() - t\n",
      "120/381:\n",
      "t = time.process_time()\n",
      "with open('alldocs3.json', 'w') as outfile:\n",
      "    json.dump([x.__dict__ for x in all_[1000000:]], outfile)\n",
      "time.process_time() - t\n",
      "120/382: len(l1 + l2)\n",
      "120/383:\n",
      "t = time.process_time()\n",
      "with open('alldocs3.json', 'r') as infile:\n",
      "    l3 = json.load(infile)\n",
      "time.process_time() - t\n",
      "120/384:\n",
      "t = time.process_time()\n",
      "with open('alldocs3.json', 'r') as infile:\n",
      "    l3 = json.load(infile)\n",
      "time.process_time() - t\n",
      "120/385:\n",
      "t = time.process_time()\n",
      "with open('alldocs3.json', 'r') as infile:\n",
      "    l3 = json.load(infile)\n",
      "time.process_time() - t\n",
      "120/386:\n",
      "t = time.process_time()\n",
      "with open('alldocs3.json', 'w') as outfile:\n",
      "    json.dump([x.__dict__ for x in all_[1000000:1200000]], outfile)\n",
      "time.process_time() - t\n",
      "120/387:\n",
      "t = time.process_time()\n",
      "with open('alldocs3.json', 'r') as infile:\n",
      "    l3 = json.load(infile)\n",
      "time.process_time() - t\n",
      "120/388:\n",
      "t = time.process_time()\n",
      "with open('alldocs4.json', 'w') as outfile:\n",
      "    json.dump([x.__dict__ for x in all_[1200000:1400000]], outfile)\n",
      "time.process_time() - t\n",
      "120/389:\n",
      "t = time.process_time()\n",
      "with open('alldocs4.json', 'r') as infile:\n",
      "    l3 = json.load(infile)\n",
      "time.process_time() - t\n",
      "120/390:\n",
      "t = time.process_time()\n",
      "with open('alldocs4.json', 'w') as outfile:\n",
      "    json.dump([x.__dict__ for x in all_[1400000:1500000]], outfile)\n",
      "time.process_time() - t\n",
      "120/391:\n",
      "t = time.process_time()\n",
      "with open('alldocs4.json', 'r') as infile:\n",
      "    l4 = json.load(infile)\n",
      "time.process_time() - t\n",
      "120/392:\n",
      "t = time.process_time()\n",
      "with open('alldocs4.json', 'w') as outfile:\n",
      "    json.dump([x.__dict__ for x in all_[1500000:1600000]], outfile)\n",
      "time.process_time() - t\n",
      "120/393:\n",
      "t = time.process_time()\n",
      "with open('alldocs4.json', 'r') as infile:\n",
      "    l4 = json.load(infile)\n",
      "time.process_time() - t\n",
      "120/394:\n",
      "t = time.process_time()\n",
      "with open('alldocs3.json', 'w') as outfile:\n",
      "    json.dump([x.__dict__ for x in all_[1000000:1400000]], outfile)\n",
      "time.process_time() - t\n",
      "120/395:\n",
      "t = time.process_time()\n",
      "with open('alldocs3.json', 'r') as infile:\n",
      "    l3 = json.load(infile)\n",
      "time.process_time() - t\n",
      "120/396:\n",
      "t = time.process_time()\n",
      "with open('alldocs4.json', 'w') as outfile:\n",
      "    json.dump([x.__dict__ for x in all_[1400000:]], outfile)\n",
      "time.process_time() - t\n",
      "120/397:\n",
      "t = time.process_time()\n",
      "with open('alldocs4.json', 'r') as infile:\n",
      "    l4 = json.load(infile)\n",
      "time.process_time() - t\n",
      "120/398: len(l1 + l2 + l3 + l4)\n",
      "120/399:\n",
      "t = time.process_time()\n",
      "with open('alldocs.json', 'r') as infile:\n",
      "    l = json.load(infile)\n",
      "time.process_time() - t\n",
      "120/400:\n",
      "t = time.process_time()\n",
      "with open('alldocs0.json', 'w') as outfile:\n",
      "    json.dump([x.__dict__ for x in all_[:1000000]], outfile)\n",
      "time.process_time() - t\n",
      "120/401:\n",
      "t = time.process_time()\n",
      "with open('alldocs0.json', 'r') as infile:\n",
      "    l0 = json.load(infile)\n",
      "time.process_time() - t\n",
      "120/402:\n",
      "t = time.process_time()\n",
      "with open('alldocs0.json', 'w') as outfile:\n",
      "    json.dump([x.__dict__ for x in all_[:800000]], outfile)\n",
      "time.process_time() - t\n",
      "120/403:\n",
      "t = time.process_time()\n",
      "with open('alldocs0.json', 'r') as infile:\n",
      "    l0 = json.load(infile)\n",
      "time.process_time() - t\n",
      "120/404:\n",
      "t = time.process_time()\n",
      "with open('alldocs0.json', 'r') as infile:\n",
      "    l0 = json.load(infile)\n",
      "time.process_time() - t\n",
      "120/405:\n",
      "t = time.process_time()\n",
      "with open('alldocs0.json', 'w') as outfile:\n",
      "    json.dump([x.__dict__ for x in all_[:600000]], outfile)\n",
      "time.process_time() - t\n",
      "120/406:\n",
      "t = time.process_time()\n",
      "with open('alldocs0.json', 'r') as infile:\n",
      "    l0 = json.load(infile)\n",
      "time.process_time() - t\n",
      "120/407:\n",
      "def save(i, arr):\n",
      "    t = time.process_time()\n",
      "    with open('alldocs' str(i) '.json', 'w') as outfile:\n",
      "        json.dump([x.__dict__ for x in arr], outfile)\n",
      "    print(time.process_time() - t)\n",
      "120/408: save(6, all_[:10])\n",
      "120/409:\n",
      "def save(i, arr):\n",
      "    t = time.process_time()\n",
      "    with open('alldocs' + str(i) + '.json', 'w') as outfile:\n",
      "        json.dump([x.__dict__ for x in arr], outfile)\n",
      "    print(time.process_time() - t)\n",
      "120/410:\n",
      "def load(i):\n",
      "    t = time.process_time()\n",
      "    with open('alldocs' + str(i) + '.json', 'r') as infile:\n",
      "        l = json.load(infile)\n",
      "    print(time.process_time() - t)\n",
      "    return l\n",
      "120/411: save(6, all_[:10])\n",
      "120/412: l6 = load(6)\n",
      "120/413: l6\n",
      "120/414: save(0, all_[:600000])\n",
      "120/415: l0 = load(0)\n",
      "120/416: save(1, all_[600000:1200000])\n",
      "120/417: l1 = load(1)\n",
      "120/418: save(2, all_[1200000:])\n",
      "120/419: l2 = load(2)\n",
      "120/420: len(l0 + l1 + l2)\n",
      "120/421:\n",
      "l = l0 + l1 + l2\n",
      "len(l)\n",
      "120/422:\n",
      "t = time.process_time()\n",
      "alld = {}\n",
      "for doc in l:\n",
      "    for d in doc.descriptors:\n",
      "        count, docs = alldd.get(d, (0, set()))\n",
      "        docs.add(doc)\n",
      "        alld[d] = (count + 1, docs)\n",
      "time.process_time() - t\n",
      "120/423:\n",
      "t = time.process_time()\n",
      "alld = {}\n",
      "for doc in l:\n",
      "    doc = Doc(doc)\n",
      "    for d in doc.descriptors:\n",
      "        count, docs = alldd.get(d, (0, set()))\n",
      "        docs.add(doc)\n",
      "        alld[d] = (count + 1, docs)\n",
      "time.process_time() - t\n",
      "120/424:\n",
      "t = time.process_time()\n",
      "alld = {}\n",
      "for doc in l:\n",
      "    for d in doc['descriptors']:\n",
      "        count, docs = alldd.get(d, (0, set()))\n",
      "        docs.add(doc)\n",
      "        alld[d] = (count + 1, docs)\n",
      "time.process_time() - t\n",
      "120/425:\n",
      "t = time.process_time()\n",
      "alld = {}\n",
      "for doc in l:\n",
      "    for d in doc['descriptors']:\n",
      "        count, docs = alld.get(d, (0, set()))\n",
      "        docs.add(doc)\n",
      "        alld[d] = (count + 1, docs)\n",
      "time.process_time() - t\n",
      "120/426:\n",
      "t = time.process_time()\n",
      "alld = {}\n",
      "for doc in l:\n",
      "    doc = Dict(doc['id'], doc['gdescriptors'], doc['descriptors'], doc['text'])\n",
      "    for d in doc.descriptors:\n",
      "        count, docs = alld.get(d, (0, set()))\n",
      "        docs.add(doc)\n",
      "        alld[d] = (count + 1, docs)\n",
      "time.process_time() - t\n",
      "120/427:\n",
      "t = time.process_time()\n",
      "alld = {}\n",
      "for doc in l:\n",
      "    doc = Doc(doc['id'], doc['gdescriptors'], doc['descriptors'], doc['text'])\n",
      "    for d in doc.descriptors:\n",
      "        count, docs = alld.get(d, (0, set()))\n",
      "        docs.add(doc)\n",
      "        alld[d] = (count + 1, docs)\n",
      "time.process_time() - t\n",
      "120/428:\n",
      "alldlist06 = sorted([(d, alld06[d][0], alld06[d][1]) for d in alld06], key=lambda x: x[1], reverse=True)\n",
      "for x in range(10):\n",
      "    print(alldlist06[x][0:2])\n",
      "120/429:\n",
      "alldlist = sorted([(d, alld[d][0], alld[d][1]) for d in alld], key=lambda x: x[1], reverse=True)\n",
      "for x in range(10):\n",
      "    print(alldlist[x][0:2])\n",
      "120/430:\n",
      "alldlist = sorted([(d, alld[d][0], alld[d][1]) for d in alld], key=lambda x: x[1], reverse=True)\n",
      "for x in range(15):\n",
      "    print(alldlist[x][0:2])\n",
      "120/431:\n",
      "politics = list(filter(lambda x: 'Computers and the Internet' not in x.descriptors, \\\n",
      "                        alld06['Politics and Government'][1]))\n",
      "120/432:\n",
      "politics = list(filter(lambda x: 'Computers and the Internet' not in x.descriptors, \\\n",
      "                        alld['Politics and Government'][1]))\n",
      "120/433:\n",
      "lens2 = []\n",
      "for c in politics:\n",
      "    lens2.append(len(c.text.split(\" \")))\n",
      "120/434: np.mean(lens2), np.median(lens2), np.max(lens2), np.min(lens2)\n",
      "120/435: plt.hist(lens2)\n",
      "120/436: politicsr = [p for p in politics if inrange(p, 500, 1500)]\n",
      "120/437: len(politicsr)\n",
      "120/438: politics[0].text\n",
      "120/439: simple_preprocess(politics[10].text)[:10] # lowercasing, de-punctuation\n",
      "120/440:\n",
      "t = time.process_time()\n",
      "docs = [simple_preprocess(t.text) for t in politics]\n",
      "time.process_time() - t\n",
      "120/441: len(politics)\n",
      "120/442:\n",
      "t = time.process_time()\n",
      "model = Word2Vec(docs)\n",
      "time.process_time() - t\n",
      "120/443: model.wv.most_similar(positive=['thursday'], topn=5)\n",
      "120/444: model.wv.most_similar(positive=['approve'], topn=5)\n",
      "120/445: model.wv.most_similar(positive=['republican'], topn=10)\n",
      "120/446: model.wv.most_similar(positive=['democrat'], topn=10)\n",
      "120/447:\n",
      "t = time.process_time()\n",
      "sg = Word2Vec(docs,sg=1)\n",
      "time.process_time() - t\n",
      "120/448: sg.wv.most_similar(positive=['approve'], topn=5)\n",
      "120/449: sg.wv.most_similar(positive=['republican'], topn=5)\n",
      "120/450: sg.wv.most_similar(positive=['democrat'], topn=5)\n",
      "120/451: sg.wv.most_similar(positive=['republican'], topn=10)\n",
      "120/452: sg.wv.most_similar(positive=['democrat'], topn=10)\n",
      "120/453: sg.wv.most_similar(positive=['approve'], topn=10)\n",
      "120/454: sg.wv.most_similar(positive=['approve'], topn=5)\n",
      "120/455:\n",
      "# borrowed from Bolukbasi et al\n",
      "matrix = []\n",
      "for a, b in pairs:\n",
      "    if a not in model.wv.vocab or b not in model.wv.vocab:\n",
      "        continue\n",
      "    center = (model.wv.get_vector(a) + model.wv.get_vector(b))/2\n",
      "    matrix.append(model.wv.get_vector(a) - center)\n",
      "    matrix.append(model.wv.get_vector(b) - center)\n",
      "matrix = np.array(matrix)\n",
      "pca = PCA(n_components = 10)\n",
      "pca.fit(matrix)\n",
      "bar(range(10), pca.explained_variance_ratio_)\n",
      "print(matrix.shape)\n",
      "120/456:\n",
      "# borrowed from Bolukbasi et al\n",
      "matrix = []\n",
      "for a, b in pairs:\n",
      "    if a not in model.wv.vocab or b not in model.wv.vocab:\n",
      "        continue\n",
      "    center = (model.wv.get_vector(a) + model.wv.get_vector(b))/2\n",
      "    matrix.append(model.wv.get_vector(a) - center)\n",
      "    matrix.append(model.wv.get_vector(b) - center)\n",
      "matrix = np.array(matrix)\n",
      "pca = PCA(n_components = 10)\n",
      "pca.fit(matrix)\n",
      "bar(range(10), pca.explained_variance_ratio_)\n",
      "print(matrix.shape)\n",
      "120/457: g = pca.components_[0]\n",
      "120/458:\n",
      "for x in ['he', 'she', 'doctor', 'nurse', 'man', 'woman', 'male', 'female', 'math', 'dress']:\n",
      "    print(x, gproj(x))\n",
      "120/459:\n",
      "for x in ['he', 'she', 'doctor', 'nurse', 'man', 'woman', 'male', 'female', 'politician', 'dress']:\n",
      "    print(x, gproj(x))\n",
      "120/460:\n",
      "for x in ['he', 'she', 'doctor', 'nurse', 'man', 'woman', 'male', 'female', 'politician', 'guy']:\n",
      "    print(x, gproj(x))\n",
      "120/461:\n",
      "for x in ['he', 'she', 'doctor', 'nurse', 'man', 'woman', 'male', 'female', 'politician', 'guy', 'scientist']:\n",
      "    print(x, gproj(x))\n",
      "120/462:\n",
      "for x in ['he', 'she', 'doctor', 'nurse', 'man', 'woman', 'male', 'female', 'politician', 'guy', 'scientist', 'teacher']:\n",
      "    print(x, gproj(x))\n",
      "120/463:\n",
      "wordlist = ['he', 'she', 'doctor', 'nurse', 'man', 'woman', 'male', 'female', 'politician', 'guy', \\\n",
      "            'scientist', 'teacher']\n",
      "120/464:\n",
      "for x in wordlist:\n",
      "    print(x, gproj(x))\n",
      "120/465:\n",
      "wordlist = ['he', 'she', 'doctor', 'nurse', 'man', 'woman', 'male', 'female', 'politician', 'guy', \\\n",
      "            'scientist', 'teacher', 'professional']\n",
      "120/466:\n",
      "wordlist = ['he', 'she', 'doctor', 'nurse', 'man', 'woman', 'male', 'female', 'politician', 'guy', \\\n",
      "            'scientist', 'teacher', 'professional', 'mother']\n",
      "120/467:\n",
      "for x in wordlist:\n",
      "    print(x, gproj(x))\n",
      "120/468:\n",
      "wordlist = ['he', 'she', 'doctor', 'nurse', 'man', 'woman', 'male', 'female', 'politician', 'guy', \\\n",
      "            'scientist', 'teacher', 'professional', 'mother', 'sweet']\n",
      "120/469:\n",
      "for x in wordlist:\n",
      "    print(x, gproj(x))\n",
      "120/470:\n",
      "wordlist = ['he', 'she', 'doctor', 'nurse', 'man', 'woman', 'male', 'female', 'politician', 'guy', \\\n",
      "            'scientist', 'teacher', 'professional', 'mother', 'pretty', 'handsome']\n",
      "120/471:\n",
      "for x in wordlist:\n",
      "    print(x, gproj(x))\n",
      "120/472:\n",
      "wordlist = ['he', 'she', 'doctor', 'nurse', 'man', 'woman', 'male', 'female', 'politician', 'guy', \\\n",
      "            'scientist', 'teacher', 'professional', 'mother', 'pretty', 'handsome', 'strong']\n",
      "120/473:\n",
      "for x in wordlist:\n",
      "    print(x, gproj(x))\n",
      "120/474:\n",
      "wordlist = ['he', 'she', 'doctor', 'nurse', 'man', 'woman', 'male', 'female', 'politician', 'guy', \\\n",
      "            'scientist', 'teacher', 'professional', 'mother', 'pretty', 'handsome', 'strong', 'witch']\n",
      "120/475:\n",
      "for x in wordlist:\n",
      "    print(x, gproj(x))\n",
      "120/476:\n",
      "wordlist = ['he', 'she', 'doctor', 'nurse', 'man', 'woman', 'male', 'female', 'politician', 'guy', \\\n",
      "            'scientist', 'teacher', 'professional', 'mother', 'pretty', 'handsome', 'strong', 'whore']\n",
      "120/477:\n",
      "for x in wordlist:\n",
      "    print(x, gproj(x))\n",
      "120/478:\n",
      "wordlist = ['he', 'she', 'doctor', 'nurse', 'man', 'woman', 'male', 'female', 'politician', 'guy', \\\n",
      "            'scientist', 'teacher', 'professional', 'mother', 'pretty', 'handsome', 'strong', 'coward']\n",
      "120/479:\n",
      "for x in wordlist:\n",
      "    print(x, gproj(x))\n",
      "120/480:\n",
      "wordlist = ['he', 'she', 'doctor', 'nurse', 'man', 'woman', 'male', 'female', 'politician', 'guy', \\\n",
      "            'scientist', 'teacher', 'professor', 'mother', 'pretty', 'handsome', 'strong', 'coward']\n",
      "120/481:\n",
      "for x in wordlist:\n",
      "    print(x, gproj(x))\n",
      "120/482:\n",
      "wordlist = ['he', 'she', 'doctor', 'nurse', 'man', 'woman', 'male', 'female', 'politician', 'guy', \\\n",
      "            'scientist', 'teacher', 'professor', 'mother', 'pretty', 'handsome', 'strong', 'coward', 'dress']\n",
      "120/483:\n",
      "for x in wordlist:\n",
      "    print(x, gproj(x))\n",
      "120/484:\n",
      "wordlist = ['he', 'she', 'doctor', 'nurse', 'man', 'woman', 'male', 'female', 'politician', 'guy', \\\n",
      "            'scientist', 'teacher', 'professor', 'mother', 'pretty', 'handsome', 'strong', 'coward', 'dress', 'pantsuit']\n",
      "120/485:\n",
      "for x in wordlist:\n",
      "    print(x, gproj(x))\n",
      "120/486:\n",
      "wordlist = ['he', 'she', 'doctor', 'nurse', 'man', 'woman', 'male', 'female', 'politician', 'guy', \\\n",
      "            'scientist', 'teacher', 'professor', 'mother', 'pretty', 'handsome', 'strong', 'coward', \\\n",
      "            'dress', 'pantsuit', 'suit']\n",
      "120/487:\n",
      "for x in wordlist:\n",
      "    print(x, gproj(x))\n",
      "120/488:\n",
      "wordlist = ['he', 'she', 'doctor', 'nurse', 'man', 'woman', 'male', 'female', 'politician', 'guy', \\\n",
      "            'scientist', 'teacher', 'professor', 'mother', 'pretty', 'handsome', 'strong', 'coward', \\\n",
      "            'dress', 'math']\n",
      "120/489:\n",
      "for x in wordlist:\n",
      "    print(x, gproj(x))\n",
      "120/490:\n",
      "wordlist = ['he', 'she', 'doctor', 'nurse', 'man', 'woman', 'male', 'female', 'politician', 'guy', \\\n",
      "            'scientist', 'teacher', 'professor', 'mother', 'pretty', 'handsome', 'strong', 'coward', \\\n",
      "            'dress', 'mathematics']\n",
      "120/491:\n",
      "for x in wordlist:\n",
      "    print(x, gproj(x))\n",
      "120/492:\n",
      "wordlist = ['he', 'she', 'doctor', 'nurse', 'man', 'woman', 'male', 'female', 'politician', 'guy', \\\n",
      "            'scientist', 'teacher', 'professor', 'mother', 'pretty', 'handsome', 'strong', 'coward', \\\n",
      "            'dress', 'sciences']\n",
      "120/493:\n",
      "for x in wordlist:\n",
      "    print(x, gproj(x))\n",
      "120/494:\n",
      "wordlist = ['he', 'she', 'doctor', 'nurse', 'man', 'woman', 'male', 'female', 'politician', 'guy', \\\n",
      "            'scientist', 'teacher', 'professor', 'mother', 'pretty', 'handsome', 'strong', 'coward', \\\n",
      "            'dress', 'fragile', 'leadership']\n",
      "120/495:\n",
      "for x in wordlist:\n",
      "    print(x, gproj(x))\n",
      "120/496:\n",
      "wordlist = ['he', 'she', 'doctor', 'nurse', 'man', 'woman', 'male', 'female', 'politician', 'guy', \\\n",
      "            'scientist', 'teacher', 'professor', 'mother', 'pretty', 'handsome', 'strong', 'coward', \\\n",
      "            'dress', 'weak', 'leadership']\n",
      "120/497:\n",
      "for x in wordlist:\n",
      "    print(x, gproj(x))\n",
      "120/498:\n",
      "wordlist = ['he', 'she', 'doctor', 'nurse', 'man', 'woman', 'male', 'female', 'politician', 'guy', \\\n",
      "            'scientist', 'teacher', 'professor', 'mother', 'pretty', 'handsome', 'strong', 'coward', \\\n",
      "            'dress', 'timid', 'leadership']\n",
      "120/499:\n",
      "for x in wordlist:\n",
      "    print(x, gproj(x))\n",
      "120/500:\n",
      "wordlist = ['he', 'she', 'doctor', 'nurse', 'man', 'woman', 'male', 'female', 'politician', 'guy', \\\n",
      "            'scientist', 'teacher', 'professor', 'mother', 'pretty', 'handsome', 'strong', 'coward', \\\n",
      "            'dress', 'timid', 'leadership', 'blond']\n",
      "120/501:\n",
      "for x in wordlist:\n",
      "    print(x, gproj(x))\n",
      "120/502:\n",
      "wordlist = ['he', 'she', 'doctor', 'nurse', 'man', 'woman', 'male', 'female', 'politician', 'guy', \\\n",
      "            'scientist', 'teacher', 'professor', 'mother', 'pretty', 'handsome', 'strong', 'coward', \\\n",
      "            'dress', 'timid', 'leadership', 'blond', 'lovely']\n",
      "120/503:\n",
      "for x in wordlist:\n",
      "    print(x, gproj(x))\n",
      "120/504:\n",
      "wordlist = ['he', 'she', 'doctor', 'nurse', 'man', 'woman', 'male', 'female', 'politician', 'guy', \\\n",
      "            'scientist', 'teacher', 'professor', 'mother', 'pretty', 'handsome', 'strong', 'coward', \\\n",
      "            'dress', 'timid', 'leadership', 'blond', 'lovely', 'petite']\n",
      "120/505:\n",
      "for x in wordlist:\n",
      "    print(x, gproj(x))\n",
      "120/506:\n",
      "wordlist = ['he', 'she', 'doctor', 'nurse', 'man', 'woman', 'male', 'female', 'politician', 'guy', \\\n",
      "            'scientist', 'teacher', 'professor', 'mother', 'pretty', 'handsome', 'strong', 'coward', \\\n",
      "            'dress', 'timid', 'leadership', 'blond', 'lovely', 'petite', 'surgeon']\n",
      "120/507:\n",
      "for x in wordlist:\n",
      "    print(x, gproj(x))\n",
      "120/508:\n",
      "wordlist = ['he', 'she', 'doctor', 'nurse', 'man', 'woman', 'male', 'female', 'politician', 'guy', \\\n",
      "            'scientist', 'teacher', 'professor', 'mother', 'pretty', 'handsome', 'strong', 'coward', \\\n",
      "            'dress', 'timid', 'leadership', 'blond', 'lovely', 'petite', 'surgeon', 'physician', 'shopkeeper']\n",
      "120/509:\n",
      "for x in wordlist:\n",
      "    print(x, gproj(x))\n",
      "120/510:\n",
      "wordlist = ['he', 'she', 'doctor', 'nurse', 'man', 'woman', 'male', 'female', 'politician', 'guy', \\\n",
      "            'scientist', 'teacher', 'professor', 'mother', 'pretty', 'handsome', 'strong', 'coward', \\\n",
      "            'dress', 'timid', 'leadership', 'blond', 'lovely', 'petite', 'surgeon', 'physician', 'shopkeeper' \\\n",
      "            'giggle', 'cooking', 'sewing', 'chuckle']\n",
      "120/511:\n",
      "for x in wordlist:\n",
      "    print(x, gproj(x))\n",
      "120/512:\n",
      "wordlist = ['he', 'she', 'doctor', 'nurse', 'man', 'woman', 'male', 'female', 'politician', 'guy', \\\n",
      "            'scientist', 'teacher', 'professor', 'mother', 'pretty', 'handsome', 'strong', 'coward', \\\n",
      "            'dress', 'timid', 'leadership', 'blond', 'lovely', 'petite', 'surgeon', 'physician', 'shopkeeper', \\\n",
      "            'giggle', 'cooking', 'sewing', 'chuckle']\n",
      "120/513:\n",
      "for x in wordlist:\n",
      "    print(x, gproj(x))\n",
      "120/514:\n",
      "wordlist = ['he', 'she', 'doctor', 'nurse', 'man', 'woman', 'male', 'female', 'politician', 'guy', \\\n",
      "            'scientist', 'teacher', 'professor', 'mother', 'pretty', 'handsome', 'strong', 'coward', \\\n",
      "            'dress', 'timid', 'leadership', 'blond', 'lovely', 'petite', 'surgeon', 'physician', 'shopkeeper', \\\n",
      "            'cooking', 'sewing', 'chuckle']\n",
      "120/515:\n",
      "for x in wordlist:\n",
      "    print(x, gproj(x))\n",
      "120/516:\n",
      "wordlist = ['he', 'she', 'doctor', 'nurse', 'man', 'woman', 'male', 'female', 'politician', 'guy', \\\n",
      "            'scientist', 'teacher', 'professor', 'mother', 'pretty', 'handsome', 'strong', 'coward', \\\n",
      "            'dress', 'timid', 'leadership', 'blond', 'lovely', 'petite', 'surgeon', 'physician', 'shopkeeper', \\\n",
      "            'cooking', 'sewing', 'chuckle', 'sassy']\n",
      "120/517:\n",
      "for x in wordlist:\n",
      "    print(x, gproj(x))\n",
      "120/518:\n",
      "wordlist = ['he', 'she', 'doctor', 'nurse', 'man', 'woman', 'male', 'female', 'politician', 'guy', \\\n",
      "            'scientist', 'teacher', 'professor', 'mother', 'pretty', 'handsome', 'strong', 'coward', \\\n",
      "            'dress', 'timid', 'leadership', 'blond', 'lovely', 'petite', 'surgeon', 'physician', 'shopkeeper', \\\n",
      "            'cooking', 'sewing', 'chuckle', 'sassy', 'diva', 'lanky']\n",
      "120/519:\n",
      "for x in wordlist:\n",
      "    print(x, gproj(x))\n",
      "120/520:\n",
      "wordlist = ['he', 'she', 'doctor', 'nurse', 'man', 'woman', 'male', 'female', 'politician', 'guy', \\\n",
      "            'scientist', 'teacher', 'professor', 'mother', 'pretty', 'handsome', 'strong', 'coward', \\\n",
      "            'dress', 'timid', 'leadership', 'blond', 'lovely', 'petite', 'surgeon', 'physician', 'shopkeeper', \\\n",
      "            'cooking', 'sewing', 'chuckle', 'sassy', lanky']\n",
      "120/521:\n",
      "wordlist = ['he', 'she', 'doctor', 'nurse', 'man', 'woman', 'male', 'female', 'politician', 'guy', \\\n",
      "            'scientist', 'teacher', 'professor', 'mother', 'pretty', 'handsome', 'strong', 'coward', \\\n",
      "            'dress', 'timid', 'leadership', 'blond', 'lovely', 'petite', 'surgeon', 'physician', 'shopkeeper', \\\n",
      "            'cooking', 'sewing', 'chuckle', 'sassy', 'lanky']\n",
      "120/522:\n",
      "for x in wordlist:\n",
      "    print(x, gproj(x))\n",
      "120/523:\n",
      "wordlist = ['he', 'she', 'doctor', 'nurse', 'man', 'woman', 'male', 'female', 'politician', 'guy', \\\n",
      "            'scientist', 'teacher', 'professor', 'mother', 'pretty', 'handsome', 'strong', 'coward', \\\n",
      "            'dress', 'timid', 'leadership', 'blond', 'lovely', 'petite', 'surgeon', 'physician', 'shopkeeper', \\\n",
      "            'cooking', 'sewing', 'chuckle', 'sassy', 'lanky', 'carpentry']\n",
      "120/524:\n",
      "for x in wordlist:\n",
      "    print(x, gproj(x))\n",
      "120/525:\n",
      "wordlist = ['he', 'she', 'doctor', 'nurse', 'man', 'woman', 'male', 'female', 'politician', 'guy', \\\n",
      "            'scientist', 'teacher', 'professor', 'mother', 'pretty', 'handsome', 'strong', 'coward', \\\n",
      "            'dress', 'timid', 'leadership', 'blond', 'lovely', 'petite', 'surgeon', 'physician', 'shopkeeper', \\\n",
      "            'cooking', 'sewing', 'chuckle', 'sassy', 'lanky', 'competent', 'brilliant']\n",
      "120/526:\n",
      "for x in wordlist:\n",
      "    print(x, gproj(x))\n",
      "120/527:\n",
      "wordlist = ['he', 'she', 'doctor', 'nurse', 'man', 'woman', 'male', 'female', 'politician', 'guy', \\\n",
      "            'scientist', 'teacher', 'professor', 'mother', 'pretty', 'handsome', 'strong', 'coward', \\\n",
      "            'dress', 'timid', 'leadership', 'blond', 'lovely', 'petite', 'surgeon', 'physician', 'shopkeeper', \\\n",
      "            'cooking', 'sewing', 'chuckle', 'sassy', 'lanky', 'competent', 'brilliant', 'capable']\n",
      "120/528:\n",
      "for x in wordlist:\n",
      "    print(x, gproj(x))\n",
      "120/529:\n",
      "wordlist = ['he', 'she', 'doctor', 'nurse', 'man', 'woman', 'male', 'female', 'politician', 'guy', \\\n",
      "            'scientist', 'teacher', 'professor', 'mother', 'pretty', 'handsome', 'strong', 'coward', \\\n",
      "            'dress', 'timid', 'leadership', 'blond', 'lovely', 'petite', 'surgeon', 'physician', 'shopkeeper', \\\n",
      "            'cooking', 'sewing', 'chuckle', 'sassy', 'lanky', 'competent', 'brilliant', 'capable', 'smart', \\\n",
      "            'intelligent', 'dumb', 'stupid']\n",
      "120/530:\n",
      "for x in wordlist:\n",
      "    print(x, gproj(x))\n",
      "120/531:\n",
      "wordlist = ['he', 'she', 'doctor', 'nurse', 'man', 'woman', 'male', 'female', 'politician', 'guy', \\\n",
      "            'scientist', 'teacher', 'professor', 'mother', 'pretty', 'handsome', 'strong', 'coward', \\\n",
      "            'dress', 'timid', 'leadership', 'blond', 'lovely', 'petite', 'surgeon', 'physician', 'shopkeeper', \\\n",
      "            'cooking', 'sewing', 'chuckle', 'sassy', 'lanky', 'competent', 'brilliant', 'capable', 'smart', \\\n",
      "            'intelligent', 'whiny']\n",
      "120/532:\n",
      "for x in wordlist:\n",
      "    print(x, gproj(x))\n",
      "120/533:\n",
      "wordlist = ['he', 'she', 'doctor', 'nurse', 'man', 'woman', 'male', 'female', 'politician', 'guy', \\\n",
      "            'scientist', 'teacher', 'professor', 'mother', 'pretty', 'handsome', 'strong', 'coward', \\\n",
      "            'dress', 'timid', 'leadership', 'blond', 'lovely', 'petite', 'surgeon', 'physician', 'shopkeeper', \\\n",
      "            'cooking', 'sewing', 'chuckle', 'sassy', 'lanky', 'competent', 'brilliant', 'capable', 'smart', \\\n",
      "            'intelligent', 'incompetent']\n",
      "120/534:\n",
      "for x in wordlist:\n",
      "    print(x, gproj(x))\n",
      "120/535:\n",
      "wordlist = ['he', 'she', 'doctor', 'nurse', 'man', 'woman', 'male', 'female', 'politician', 'guy', \\\n",
      "            'scientist', 'teacher', 'professor', 'mother', 'pretty', 'handsome', 'strong', 'coward', \\\n",
      "            'dress', 'timid', 'leadership', 'blond', 'lovely', 'petite', 'surgeon', 'physician', 'shopkeeper', \\\n",
      "            'cooking', 'sewing', 'chuckle', 'sassy', 'lanky', 'competent', 'brilliant', 'capable', 'smart', \\\n",
      "            'intelligent', 'incompetent', 'librarian', 'cosmetics']\n",
      "120/536:\n",
      "for x in wordlist:\n",
      "    print(x, gproj(x))\n",
      "120/537: alld['Television'][0]\n",
      "120/538: alld['Television'][1][0]\n",
      "120/539: pairs\n",
      "120/540: len(wordlist)\n",
      "120/541:\n",
      "wordlist = ['he', 'she', 'doctor', 'nurse', 'man', 'woman', 'male', 'female', 'politician', 'guy', \\\n",
      "            'scientist', 'teacher', 'professor', 'mother', 'pretty', 'handsome', 'strong', 'coward', \\\n",
      "            'dress', 'timid', 'leadership', 'blond', 'lovely', 'petite', 'surgeon', 'physician', 'shopkeeper', \\\n",
      "            'cooking', 'sewing', 'chuckle', 'sassy', 'lanky', 'competent', 'brilliant', 'capable', 'smart', \\\n",
      "            'intelligent', 'incompetent', 'librarian', 'cosmetics', 'maid']\n",
      "120/542:\n",
      "for x in wordlist:\n",
      "    print(x, gproj(x))\n",
      "120/543:\n",
      "wordlist = ['he', 'she', 'doctor', 'nurse', 'man', 'woman', 'male', 'female', 'politician', 'guy', \\\n",
      "            'scientist', 'teacher', 'professor', 'mother', 'pretty', 'handsome', 'strong', 'coward', \\\n",
      "            'dress', 'timid', 'leadership', 'blond', 'lovely', 'petite', 'surgeon', 'physician', 'shopkeeper', \\\n",
      "            'cooking', 'sewing', 'chuckle', 'sassy', 'lanky', 'competent', 'brilliant', 'capable', 'smart', \\\n",
      "            'intelligent', 'incompetent', 'librarian', 'cosmetics', 'maid', 'clever']\n",
      "120/544:\n",
      "for x in wordlist:\n",
      "    print(x, gproj(x))\n",
      "120/545:\n",
      "wordlist = ['he', 'she', 'doctor', 'nurse', 'man', 'woman', 'male', 'female', 'politician', 'guy', \\\n",
      "            'scientist', 'teacher', 'professor', 'mother', 'pretty', 'handsome', 'strong', 'coward', \\\n",
      "            'dress', 'timid', 'leadership', 'blond', 'lovely', 'petite', 'surgeon', 'physician', 'shopkeeper', \\\n",
      "            'cooking', 'sewing', 'chuckle', 'sassy', 'lanky', 'competent', 'brilliant', 'capable', 'smart', \\\n",
      "            'intelligent', 'incompetent', 'librarian', 'cosmetics', 'maid', 'clever', 'well-dressed']\n",
      "120/546:\n",
      "for x in wordlist:\n",
      "    print(x, gproj(x))\n",
      "120/547:\n",
      "wordlist = ['he', 'she', 'doctor', 'nurse', 'man', 'woman', 'male', 'female', 'politician', 'guy', \\\n",
      "            'scientist', 'teacher', 'professor', 'mother', 'pretty', 'handsome', 'strong', 'coward', \\\n",
      "            'dress', 'timid', 'leadership', 'blond', 'lovely', 'petite', 'surgeon', 'physician', 'shopkeeper', \\\n",
      "            'cooking', 'sewing', 'chuckle', 'sassy', 'lanky', 'competent', 'brilliant', 'capable', 'smart', \\\n",
      "            'intelligent', 'incompetent', 'librarian', 'cosmetics', 'maid', 'clever', 'dressed']\n",
      "120/548:\n",
      "for x in wordlist:\n",
      "    print(x, gproj(x))\n",
      "120/549:\n",
      "wordlist = ['he', 'she', 'doctor', 'nurse', 'man', 'woman', 'male', 'female', 'politician', 'guy', \\\n",
      "            'scientist', 'teacher', 'professor', 'mother', 'pretty', 'handsome', 'strong', 'coward', \\\n",
      "            'dress', 'timid', 'leadership', 'blond', 'lovely', 'petite', 'surgeon', 'physician', 'shopkeeper', \\\n",
      "            'cooking', 'sewing', 'chuckle', 'sassy', 'lanky', 'competent', 'brilliant', 'capable', 'smart', \\\n",
      "            'intelligent', 'incompetent', 'librarian', 'cosmetics', 'maid', 'clever', 'dressed', 'captain']\n",
      "120/550: len(wordlist)\n",
      "120/551:\n",
      "for x in wordlist:\n",
      "    print(x, gproj(x))\n",
      "120/552:\n",
      "wordlist = ['he', 'she', 'doctor', 'nurse', 'man', 'woman', 'male', 'female', 'politician', 'guy', \\\n",
      "            'scientist', 'teacher', 'professor', 'mother', 'pretty', 'handsome', 'strong', 'coward', \\\n",
      "            'dress', 'timid', 'leadership', 'blond', 'lovely', 'petite', 'surgeon', 'physician', 'shopkeeper', \\\n",
      "            'cooking', 'sewing', 'chuckle', 'sassy', 'lanky', 'competent', 'brilliant', 'capable', 'smart', \\\n",
      "            'intelligent', 'incompetent', 'librarian', 'cosmetics', 'maid', 'clever', 'dressed', 'captain', \\\n",
      "            'nanny', 'maestro', 'philosopher', 'architect', 'financier', 'warrior', 'magician', 'receptionist']\n",
      "120/553: len(wordlist)\n",
      "120/554:\n",
      "for x in wordlist:\n",
      "    print(x, gproj(x))\n",
      "120/555:\n",
      "wordlist = ['he', 'she', 'doctor', 'nurse', 'man', 'woman', 'male', 'female', 'politician', 'guy', \\\n",
      "            'scientist', 'teacher', 'professor', 'mother', 'pretty', 'handsome', 'strong', 'coward', \\\n",
      "            'dress', 'timid', 'leadership', 'blond', 'lovely', 'petite', 'surgeon', 'physician', 'shopkeeper', \\\n",
      "            'cooking', 'sewing', 'chuckle', 'sassy', 'lanky', 'competent', 'brilliant', 'capable', 'smart', \\\n",
      "            'intelligent', 'incompetent', 'librarian', 'cosmetics', 'maid', 'clever', 'dressed', 'captain', \\\n",
      "            'nanny', 'philosopher', 'architect', 'financier', 'warrior', 'magician', 'receptionist']\n",
      "120/556:\n",
      "for x in wordlist:\n",
      "    print(x, gproj(x))\n",
      "120/557:\n",
      "wordlist = ['he', 'she', 'doctor', 'nurse', 'man', 'woman', 'male', 'female', 'politician', 'guy', \\\n",
      "            'scientist', 'teacher', 'professor', 'mother', 'pretty', 'handsome', 'strong', 'coward', \\\n",
      "            'dress', 'timid', 'leadership', 'blond', 'lovely', 'petite', 'surgeon', 'physician', 'shopkeeper', \\\n",
      "            'cooking', 'sewing', 'chuckle', 'sassy', 'lanky', 'competent', 'brilliant', 'capable', 'smart', \\\n",
      "            'intelligent', 'incompetent', 'librarian', 'cosmetics', 'maid', 'clever', 'dressed', 'captain', \\\n",
      "            'nanny', 'philosopher', 'architect', 'financier', 'warrior', 'magician', 'housekeeper', 'homemaker']\n",
      "120/558:\n",
      "for x in wordlist:\n",
      "    print(x, gproj(x))\n",
      "122/1:\n",
      "import xml.etree.ElementTree as ET\n",
      "import os\n",
      "from typing import List\n",
      "from functools import reduce\n",
      "from calendar import monthrange\n",
      "import random\n",
      "import matplotlib.pyplot as plt\n",
      "import numpy as np\n",
      "import json\n",
      "122/2: import time\n",
      "122/3:\n",
      "class Doc:\n",
      "    def __init__(self, id: int, gdescriptors: List[str], descriptors: List[str], text: str) -> None:\n",
      "        self.id = id\n",
      "        self.gdescriptors = gdescriptors\n",
      "        self.descriptors = descriptors\n",
      "        self.text = text\n",
      "122/4:\n",
      "def parse_doc(file: str) -> Doc:\n",
      "    tree = ET.parse(file)\n",
      "    root = tree.getroot()\n",
      "    try:\n",
      "        fulltext = root.find('body').find('body.content').find(\"*[@class='full_text']\")\n",
      "        text = reduce(lambda x, y: x + y, [x.text for x in fulltext.findall('p')])\n",
      "        id = int(root.find('head').find('docdata').find('doc-id').attrib['id-string'])\n",
      "        classifiers = root.find('head').find('docdata').find('identified-content')\n",
      "        gclassifiers = classifiers.findall(\"*[@class='online_producer'][@type='general_descriptor']\")\n",
      "        dclassifiers = classifiers.findall(\"*[@class='online_producer'][@type='descriptor']\")\n",
      "        gdescriptors = [c.text for c in gclassifiers]\n",
      "        descriptors = [c.text for c in dclassifiers]\n",
      "        return Doc(id, gdescriptors, descriptors, text)\n",
      "    except:\n",
      "        return None\n",
      "122/5:\n",
      "def get_docs_by_month(year: int, month: int) -> List[Doc]:\n",
      "    days = monthrange(year, month)[1]\n",
      "    docs = []\n",
      "    for day in range(1, days+1):\n",
      "        for file in os.listdir(f'data/{year}/{month:02}/{day:02}/'):\n",
      "            doc = parse_doc(f'data/{year}/{month:02}/{day:02}/{file}')\n",
      "            if doc:\n",
      "                docs.append(doc)\n",
      "    return docs\n",
      "122/6:\n",
      "t = time.process_time()\n",
      "all_ = []\n",
      "for y in range(1988, 2007):\n",
      "    print(y)\n",
      "    for i in range(1,13):\n",
      "        all_ += get_docs_by_month(y, i)\n",
      "time.process_time() - t\n",
      "122/7: len(all_)\n",
      "122/8:\n",
      "with open('alldocs.json', 'w') as outfile:\n",
      "    json.dump([x.__dict__ for x in all_], outfile)\n",
      "122/9:\n",
      "def save(i, arr):\n",
      "    t = time.process_time()\n",
      "    with open('alldocs' + str(i) + '.json', 'w') as outfile:\n",
      "        json.dump([x.__dict__ for x in arr], outfile)\n",
      "    print(time.process_time() - t)\n",
      "122/10:\n",
      "def load(i):\n",
      "    t = time.process_time()\n",
      "    with open('alldocs' + str(i) + '.json', 'r') as infile:\n",
      "        l = json.load(infile)\n",
      "    print(time.process_time() - t)\n",
      "    return l\n",
      "122/11: save(0, all_[:600000])\n",
      "122/12: l0 = load(0)\n",
      "122/13: save(1, all_[600000:1200000])\n",
      "122/14: l1 = load(1)\n",
      "122/15: save(2, all_[1200000:])\n",
      "122/16: l2 = load(2)\n",
      "122/17:\n",
      "l = l0 + l1 + l2\n",
      "len(l)\n",
      "122/18:\n",
      "t = time.process_time()\n",
      "all06 = []\n",
      "for i in range(1,13):\n",
      "    all06 += get_docs_by_month(2006, i)\n",
      "time.process_time() - t\n",
      "122/19: len(all06)\n",
      "122/20:\n",
      "t = time.process_time()\n",
      "alld06 = {}\n",
      "for doc in all06:\n",
      "    for d in doc.descriptors:\n",
      "        count, docs = alld06.get(d, (0, set()))\n",
      "        docs.add(doc)\n",
      "        alld06[d] = (count + 1, docs)\n",
      "time.process_time() - t\n",
      "122/21:\n",
      "t = time.process_time()\n",
      "alld = {}\n",
      "for doc in l:\n",
      "    doc = Doc(doc['id'], doc['gdescriptors'], doc['descriptors'], doc['text'])\n",
      "    for d in doc.descriptors:\n",
      "        count, docs = alld.get(d, (0, set()))\n",
      "        docs.add(doc)\n",
      "        alld[d] = (count + 1, docs)\n",
      "time.process_time() - t\n",
      "122/22:\n",
      "alldlist06 = sorted([(d, alld06[d][0], alld06[d][1]) for d in alld06], key=lambda x: x[1], reverse=True)\n",
      "for x in range(10):\n",
      "    print(alldlist06[x][0:2])\n",
      "122/23:\n",
      "alldlist = sorted([(d, alld[d][0], alld[d][1]) for d in alld], key=lambda x: x[1], reverse=True)\n",
      "for x in range(15):\n",
      "    print(alldlist[x][0:2])\n",
      "122/24:\n",
      "computers = list(filter(lambda x: 'Politics and Government' not in x.descriptors, \\\n",
      "                        alld06['Computers and the Internet'][1]))\n",
      "122/25:\n",
      "def inrange(c, lo, hi):\n",
      "    return len(c.text.split(\" \")) >= lo and len(c.text.split(\" \")) <= hi\n",
      "122/26: computersr = [c for c in computers if inrange(c, 500, 1500)]\n",
      "122/27: len(computersr)\n",
      "122/28: len(computers)\n",
      "122/29:\n",
      "lens = []\n",
      "for c in computers:\n",
      "    lens.append(len(c.text.split(\" \")))\n",
      "122/30: np.mean(lens), np.median(lens), np.max(lens), np.min(lens)\n",
      "122/31: plt.hist(lens)\n",
      "122/32:\n",
      "politics = list(filter(lambda x: 'Computers and the Internet' not in x.descriptors, \\\n",
      "                        alld['Politics and Government'][1]))\n",
      "122/33: alld['Television'][1][0]\n",
      "122/34:\n",
      "lens2 = []\n",
      "for c in politics:\n",
      "    lens2.append(len(c.text.split(\" \")))\n",
      "122/35: np.mean(lens2), np.median(lens2), np.max(lens2), np.min(lens2)\n",
      "122/36: plt.hist(lens2)\n",
      "122/37: computerss = random.sample(computers, 1500)\n",
      "122/38: politicsr = [p for p in politics if inrange(p, 500, 1500)]\n",
      "122/39: len(politicsr)\n",
      "122/40: len(politics)\n",
      "122/41: politicss = random.sample(politics, 1500)\n",
      "122/42: politics[0].text\n",
      "122/43:\n",
      "with open(\"politics.txt\", \"w\") as file:\n",
      "    json.dump()\n",
      "122/44: test = json.loads(x)\n",
      "122/45:\n",
      "from gensim.models import Word2Vec\n",
      "from gensim.utils import simple_preprocess\n",
      "122/46: simple_preprocess(politics[10].text)[:10] # lowercasing, de-punctuation\n",
      "122/47: len(politics)\n",
      "122/48:\n",
      "t = time.process_time()\n",
      "docs = [simple_preprocess(t.text) for t in politics]\n",
      "time.process_time() - t\n",
      "122/49:\n",
      "t = time.process_time()\n",
      "model = Word2Vec(docs)\n",
      "time.process_time() - t\n",
      "122/50:\n",
      "t = time.process_time()\n",
      "sg = Word2Vec(docs,sg=1)\n",
      "time.process_time() - t\n",
      "122/51: model.wv.most_similar(positive=['thursday'], topn=5)\n",
      "122/52: model.wv.most_similar(positive=['approve'], topn=5)\n",
      "122/53: sg.wv.most_similar(positive=['approve'], topn=5)\n",
      "122/54: model.wv.most_similar(positive=['republican'], topn=10)\n",
      "122/55: sg.wv.most_similar(positive=['republican'], topn=10)\n",
      "122/56: model.wv.most_similar(positive=['democrat'], topn=10)\n",
      "122/57: sg.wv.most_similar(positive=['democrat'], topn=10)\n",
      "122/58: model.wv.get_vector('democrat').shape\n",
      "122/59:\n",
      "from gensim.summarization.summarizer import summarize\n",
      "from gensim.models import LsiModel\n",
      "from gensim.test.utils import common_dictionary, common_corpus\n",
      "122/60: politics[10].text\n",
      "122/61: summarize(politics[10].text, word_count=200)\n",
      "122/62:\n",
      "s = summarize(politics[10].text)\n",
      "len(s.split(\" \"))\n",
      "122/63: print(summarize(text))\n",
      "122/64: print(s)\n",
      "122/65:\n",
      "t = time.process_time()\n",
      "summaries_pol = [summarize(x.text) for x in politics]\n",
      "time.process_time() - t\n",
      "122/66: politics\n",
      "122/67: politics[0]\n",
      "122/68: politics[0].text\n",
      "122/69: len(politics[0].text)\n",
      "122/70:\n",
      "t = time.process_time()\n",
      "summaries_pol = [summarize(x.text) for x in politics if len(x.text) > 0]\n",
      "time.process_time() - t\n",
      "122/71:\n",
      "t = time.process_time()\n",
      "summaries_pol = [summarize(x.text) for x in politics if len(x.text) > 500]\n",
      "time.process_time() - t\n",
      "122/72:\n",
      "t = time.process_time()\n",
      "# summaries_pol = [summarize(x.text) for x in politics if len(x.text) > 500]\n",
      "for i in range(len(politics)):\n",
      "    try:\n",
      "        summarize(politics[i].text)\n",
      "    except:\n",
      "        print(i, len(x.text), x.text)\n",
      "time.process_time() - t\n",
      "122/73:\n",
      "t = time.process_time()\n",
      "# summaries_pol = [summarize(x.text) for x in politics if len(x.text) > 500]\n",
      "for i in range(len(politics)):\n",
      "    try:\n",
      "        summarize(politics[i].text)\n",
      "    except:\n",
      "        x = politics[i]\n",
      "        print(i, len(x.text), x.text)\n",
      "time.process_time() - t\n",
      "122/74: len(politics)\n",
      "122/75: politics[1646].text\n",
      "122/76: summarize(politics[1646].text()\n",
      "122/77: summarize(politics[1646].text)\n",
      "122/78: politics[1646].text\n",
      "122/79:\n",
      "t = time.process_time()\n",
      "# summaries_pol = [summarize(x.text) for x in politics if len(x.text) > 500]\n",
      "summaries_pol = []\n",
      "for i in range(len(politics)):\n",
      "    try:\n",
      "        summaries_pol.append(summarize(politics[i].text))\n",
      "    except:\n",
      "        x = politics[i]\n",
      "        print(i)\n",
      "time.process_time() - t\n",
      "122/80: len(summaries_pol)\n",
      "122/81:\n",
      "with open('debiaswe-master/data/equalize_pairs.json') as f:\n",
      "    pairs = json.loads(f.read())\n",
      "122/82:\n",
      "from sklearn.decomposition import PCA\n",
      "from matplotlib.pyplot import bar\n",
      "122/83: pairs = list(map(lambda x: [x[0].lower(), x[1].lower()], pairs))\n",
      "122/84: pairs\n",
      "122/85:\n",
      "# borrowed from Bolukbasi et al\n",
      "matrix = []\n",
      "for a, b in pairs:\n",
      "    if a not in model.wv.vocab or b not in model.wv.vocab:\n",
      "        continue\n",
      "    center = (model.wv.get_vector(a) + model.wv.get_vector(b))/2\n",
      "    matrix.append(model.wv.get_vector(a) - center)\n",
      "    matrix.append(model.wv.get_vector(b) - center)\n",
      "matrix = np.array(matrix)\n",
      "pca = PCA(n_components = 10)\n",
      "pca.fit(matrix)\n",
      "bar(range(10), pca.explained_variance_ratio_)\n",
      "print(matrix.shape)\n",
      "122/86:\n",
      "pca_r = PCA(n_components = 10)\n",
      "pca_r.fit(np.random.rand(54,100))\n",
      "bar(range(10), pca_r.explained_variance_ratio_)\n",
      "122/87: pca.components_[0].shape\n",
      "122/88: g = pca.components_[0]\n",
      "122/89:\n",
      "def gproj(w1):\n",
      "    wvec = model.wv.get_vector(w1)\n",
      "    p = np.dot(g, wvec)/np.linalg.norm(g)\n",
      "    return p\n",
      "122/90:\n",
      "wordlist = ['he', 'she', 'doctor', 'nurse', 'man', 'woman', 'male', 'female', 'politician', 'guy', \\\n",
      "            'scientist', 'teacher', 'professor', 'mother', 'pretty', 'handsome', 'strong', 'coward', \\\n",
      "            'dress', 'timid', 'leadership', 'blond', 'lovely', 'petite', 'surgeon', 'physician', 'shopkeeper', \\\n",
      "            'cooking', 'sewing', 'chuckle', 'sassy', 'lanky', 'competent', 'brilliant', 'capable', 'smart', \\\n",
      "            'intelligent', 'incompetent', 'librarian', 'cosmetics', 'maid', 'clever', 'dressed', 'captain', \\\n",
      "            'nanny', 'philosopher', 'architect', 'financier', 'warrior', 'magician', 'housekeeper', 'homemaker']\n",
      "122/91: len(wordlist)\n",
      "122/92:\n",
      "for x in wordlist:\n",
      "    print(x, gproj(x))\n",
      "122/93: len(pairs)\n",
      "122/94:\n",
      "pca_r = PCA(n_components = 10)\n",
      "pca_r.fit(np.random.rand(76,100))\n",
      "bar(range(10), pca_r.explained_variance_ratio_)\n",
      "122/95:\n",
      "pca_r = PCA(n_components = 10)\n",
      "pca_r.fit(np.random.rand(76,100))\n",
      "bar(range(10), pca_r.explained_variance_ratio_)\n",
      "122/96:\n",
      "pca_r = PCA(n_components = 10)\n",
      "pca_r.fit(np.random.rand(76,100))\n",
      "bar(range(10), pca_r.explained_variance_ratio_)\n",
      "122/97: pairs[:5]\n",
      "122/98: ## For Politics\n",
      "122/99:\n",
      "mps = list(filter(lambda x: 'Politics and Government' not in x.descriptors, \\\n",
      "                        alld['Motion Pictures'][1]))\n",
      "122/100:\n",
      "books = list(filter(lambda x: 'Politics and Government' not in x.descriptors, \\\n",
      "                        alld['Books and Literature'][1]))\n",
      "122/101:\n",
      "sports = list(filter(lambda x: 'Politics and Government' not in x.descriptors, \\\n",
      "                        alld['Football'][1]+alld['Baseball'][1]+alld['Baskettball'][1]))\n",
      "122/102:\n",
      "sports = list(filter(lambda x: 'Politics and Government' not in x.descriptors, \\\n",
      "                        alld['Football'][1]|alld['Baseball'][1]|alld['Basketball'][1]))\n",
      "122/103: len(mps), len(books), len(sports), len(computers)\n",
      "122/104:\n",
      "computers = list(filter(lambda x: 'Politics and Government' not in x.descriptors, \\\n",
      "                        alld['Computers and the Internet'][1]))\n",
      "122/105: len(mps), len(books), len(sports), len(computers)\n",
      "122/106:\n",
      "def getw2v(category):\n",
      "    docs = [simple_preprocess(t.text) for t in category]\n",
      "    return Word2Vec(docs)\n",
      "122/107: m_mps = getw2v(mps)\n",
      "122/108:\n",
      "t = time.process_time()\n",
      "m_mps = getw2v(mps)\n",
      "time.process_time() - t\n",
      "122/109:\n",
      "t = time.process_time()\n",
      "m_books = getw2v(books)\n",
      "time.process_time() - t\n",
      "122/110: m_books.wv.most_similar(positive=['thursday'], topn=5)\n",
      "122/111: m_mps.wv.most_similar(positive=['thursday'], topn=5)\n",
      "122/112: m_books.wv.most_similar(positive=['woman'], topn=5)\n",
      "122/113: m_books.wv.most_similar(positive=['good'], topn=5)\n",
      "122/114:\n",
      "def getw2v(category):\n",
      "    docs = [simple_preprocess(t.text) for t in category]\n",
      "    return Word2Vec(docs), Word2Vec(docs,sg=1)\n",
      "122/115:\n",
      "t = time.process_time()\n",
      "m_mps, sg_mps = getw2v(mps)\n",
      "time.process_time() - t\n",
      "122/116:\n",
      "t = time.process_time()\n",
      "m_books, sg_books = getw2v(books)\n",
      "time.process_time() - t\n",
      "122/117: sg_books.wv.most_similar(positive=['good'], topn=5)\n",
      "122/118: sg_mps.wv.most_similar(positive=['thursday'], topn=5)\n",
      "122/119: sg_books.wv.most_similar(positive=['thursday'], topn=5)\n",
      "122/120: sg_books.wv.most_similar(positive=['good'], topn=5)\n",
      "122/121: model.wv.most_similar(positive=['good'], topn=5)\n",
      "122/122: model.wv.most_similar(positive=['approve'], topn=5)\n",
      "122/123:\n",
      "def getg(m):\n",
      "    matrix = []\n",
      "    for a, b in pairs:\n",
      "        if a not in model.wv.vocab or b not in model.wv.vocab:\n",
      "            continue\n",
      "        center = (model.wv.get_vector(a) + model.wv.get_vector(b))/2\n",
      "        matrix.append(model.wv.get_vector(a) - center)\n",
      "        matrix.append(model.wv.get_vector(b) - center)\n",
      "    matrix = np.array(matrix)\n",
      "    p = PCA(n_components = 10)\n",
      "    p.fit(matrix)\n",
      "    bar(range(10), p.explained_variance_ratio_)\n",
      "    print(matrix.shape)\n",
      "    return p.components_[0]\n",
      "122/124: g_mps = getg(m_mps)\n",
      "122/125:\n",
      "def getg(m):\n",
      "    matrix = []\n",
      "    for a, b in pairs:\n",
      "        if a not in m.wv.vocab or b not in m.wv.vocab:\n",
      "            continue\n",
      "        center = (m.wv.get_vector(a) + m.wv.get_vector(b))/2\n",
      "        matrix.append(m.wv.get_vector(a) - center)\n",
      "        matrix.append(m.wv.get_vector(b) - center)\n",
      "    matrix = np.array(matrix)\n",
      "    p = PCA(n_components = 10)\n",
      "    p.fit(matrix)\n",
      "    bar(range(10), p.explained_variance_ratio_)\n",
      "    print(matrix.shape)\n",
      "    return p.components_[0]\n",
      "122/126: g_mps = getg(m_mps)\n",
      "122/127: g_books = getg(m_books)\n",
      "122/128:\n",
      "def gproj(g, w1):\n",
      "    wvec = model.wv.get_vector(w1)\n",
      "    p = np.dot(g, wvec)/np.linalg.norm(g)\n",
      "    return p\n",
      "122/129:\n",
      "for x in wordlist:\n",
      "    print(x, gproj(g, x))\n",
      "122/130:\n",
      "for x in wordlist:\n",
      "    print(x, gproj(g_mps, x))\n",
      "122/131:\n",
      "def gproj(g, m, w1):\n",
      "    wvec = m.wv.get_vector(w1)\n",
      "    p = np.dot(g, wvec)/np.linalg.norm(g)\n",
      "    return p\n",
      "122/132:\n",
      "for x in wordlist:\n",
      "    print(x, gproj(g, model, x))\n",
      "122/133:\n",
      "for x in wordlist:\n",
      "    print(x, gproj(g_mps, m_mps, x))\n",
      "122/134:\n",
      "for x in wordlist:\n",
      "    print(x, gproj(g_books, m_books, x))\n",
      "122/135: pairs[:15]\n",
      "122/136: pairs\n",
      "122/137:\n",
      "t = time.process_time()\n",
      "m_sports, sg_sports = getw2v(sports)\n",
      "time.process_time() - t\n",
      "122/138: g_sports = getg(m_sports)\n",
      "122/139:\n",
      "for x in wordlist:\n",
      "    print(x, gproj(g_sports, m_sports, x))\n",
      "122/140:\n",
      "with open('debiaswe-master/data/professions.json') as f:\n",
      "    profs = json.loads(f.read())\n",
      "122/141: profs[:5]\n",
      "122/142: proflist = list(filter(lambda x: x in model.wv.vocab, [p[0] for p in profs if abs(p[1]) < 0.6]))\n",
      "122/143: len(proflist)\n",
      "122/144: proflist\n",
      "122/145:\n",
      "def bias(g, m):\n",
      "    b = 0\n",
      "    for x in proflist:\n",
      "        b += gproj(g, m, x)**2\n",
      "    print(b)\n",
      "122/146: bias(g, model)\n",
      "122/147: bias(g_books, m_books)\n",
      "122/148:\n",
      "def bias(g, m):\n",
      "    b = 0\n",
      "    l = list(filter(lambda x: x in m.wv.vocab, proflist))\n",
      "    for x in l:\n",
      "        b += gproj(g, m, x)**2\n",
      "    print(b/len(l))\n",
      "122/149: bias(g, model)\n",
      "122/150: bias(g_books, m_books)\n",
      "122/151: bias(g_mps, g_mps)\n",
      "122/152: bias(g_mps, m_mps)\n",
      "122/153: bias(g_sports, m_sports)\n",
      "122/154:\n",
      "for p in proflist:\n",
      "    if p in m_sports.wv.vocab:\n",
      "        print(p, gproj(g_sports, m_sports, p))\n",
      "122/155:\n",
      "for p in proflist:\n",
      "    if p in m_sports.wv.vocab:\n",
      "        print(p, gproj(getg(sg_sports), sg_sports, p))\n",
      "122/156:\n",
      "for p in proflist:\n",
      "    if p in m_sports.wv.vocab:\n",
      "        print(p, gproj(g_sports, m_sports, p))\n",
      "122/157: # Bias measured on list of professions\n",
      "122/158:\n",
      "def bias(g, m, wordlist):\n",
      "    b = 0\n",
      "    l = list(filter(lambda x: x in m.wv.vocab, wordlist))\n",
      "    for x in l:\n",
      "        b += gproj(g, m, x)**2\n",
      "    print(b/len(l))\n",
      "122/159: bias(g, model, proflist)\n",
      "122/160: bias(g_books, m_books, proflist)\n",
      "122/161: bias(g_mps, m_mps, proflist)\n",
      "122/162: bias(g_sports, m_sports, proflist)\n",
      "122/163:\n",
      "count = 0\n",
      "for p in proflist:\n",
      "    count += 1\n",
      "    if count > 10:\n",
      "        continue\n",
      "    if p in m_sports.wv.vocab:\n",
      "        print(p, gproj(g_sports, m_sports, p))\n",
      "122/164:\n",
      "count = 0\n",
      "for p in proflist:\n",
      "    count += 1\n",
      "    if count > 20:\n",
      "        continue\n",
      "    if p in m_sports.wv.vocab:\n",
      "        print(p, gproj(g_sports, m_sports, p))\n",
      "122/165:\n",
      "with open('debiaswe-master/data/gender_specific_seed.json') as f:\n",
      "    gender_specific = json.loads(f.read())\n",
      "122/166: len(gender_specific)\n",
      "122/167: gender_specific\n",
      "122/168: gender_specific[:10]\n",
      "122/169: bias(g_sports, m_sports, gender_specific)\n",
      "122/170: bias(g, model, gender_specific)\n",
      "122/171: bias(g_books, m_books, gender_specific)\n",
      "122/172: bias(g_mps, m_mps, gender_specific)\n",
      "122/173: ## Gender-neutral adjectives describing people\n",
      "122/174:\n",
      "from bs4 import BeautifulSoup\n",
      "import urllib2\n",
      "122/175:\n",
      "from bs4 import BeautifulSoup\n",
      "import urllib2\n",
      "122/176:\n",
      "from bs4 import BeautifulSoup\n",
      "import urllib.request as urllib2\n",
      "122/177:\n",
      "url = 'https://www.enchantedlearning.com/wordlist/adjectivesforpeople.shtml'\n",
      "file = urllib2.urlopen(url)\n",
      "html = file.read()\n",
      "file.close()\n",
      "soup = BeautifulSoup(html, features='lxml')\n",
      "122/178:\n",
      "url = 'https://www.enchantedlearning.com/wordlist/adjectivesforpeople.shtml'\n",
      "file = urllib2.urlopen(url)\n",
      "html = file.read()\n",
      "file.close()\n",
      "soup = BeautifulSoup(html)\n",
      "122/179: names = soup.find_all(attrs={\"class\": \"wordlist-item\"})\n",
      "122/180: names\n",
      "122/181: names = [x.contents[0] for x in names]\n",
      "122/182: names\n",
      "122/183: names[:5]\n",
      "122/184:\n",
      "from bs4 import BeautifulSoup\n",
      "import urllib.request as urllib2\n",
      "import csv\n",
      "122/185: adj = soup.find_all(attrs={\"class\": \"wordlist-item\"})\n",
      "122/186: adjs = [x.contents[0] for x in names]\n",
      "122/187: adjs[:5]\n",
      "122/188: adjs = [x.contents[0] for x in adj]\n",
      "122/189: adjs[:5]\n",
      "122/190:\n",
      "with open('adjectives.csv', mode='a') as f:\n",
      "    writer = csv.writer(f, delimiter=',')\n",
      "\n",
      "    for i in range(len(adjs)):\n",
      "        writer.writerow(adjs[i])\n",
      "122/191:\n",
      "with open('adjectives.csv', mode='a') as f:\n",
      "    writer = csv.writer(f, delimiter=',')\n",
      "\n",
      "    for i in range(len(adjs)):\n",
      "        writer.write(adjs[i])\n",
      "122/192:\n",
      "with open('adjectives.csv', mode='a') as f:\n",
      "    writer = csv.writer(f, delimiter=',')\n",
      "\n",
      "    for i in range(len(adjs)):\n",
      "        writer.writerow([adjs[i]])\n",
      "122/193:\n",
      "with open('adjectives.csv', mode='a') as f:\n",
      "    writer = csv.writer(f, delimiter=',')\n",
      "    writer.writerow(adjs)\n",
      "122/194:\n",
      "with open('adjectives.csv', mode='a') as f:\n",
      "    writer = csv.writer(f, delimiter=',')\n",
      "    writer.writerow(adjs)\n",
      "122/195:\n",
      "with open('adjectives.csv', mode='w') as f:\n",
      "    writer = csv.writer(f, delimiter=',')\n",
      "    writer.writerow(adjs)\n",
      "122/196:\n",
      "with open('adjectives.csv') as csv_file:\n",
      "    csv_reader = csv.reader(csv_file, delimiter=',')\n",
      "    for row in csv_reader:\n",
      "        print(row)\n",
      "122/197:\n",
      "with open('adjectives.csv') as csv_file:\n",
      "    csv_reader = csv.reader(csv_file, delimiter=',')\n",
      "    for row in csv_reader:\n",
      "        print(row[0])\n",
      "122/198:\n",
      "with open('adjectives.csv') as csv_file:\n",
      "    csv_reader = csv.reader(csv_file, delimiter=',')\n",
      "    for row in csv_reader[0]:\n",
      "        print(row[0])\n",
      "122/199:\n",
      "with open('adjectives.csv') as csv_file:\n",
      "    csv_reader = csv.reader(csv_file, delimiter=',')\n",
      "    for row in csv_reader[0]:\n",
      "        print(row)\n",
      "122/200:\n",
      "with open('adjectives.csv') as csv_file:\n",
      "    r = read(csv_file)\n",
      "122/201:\n",
      "with open('adjectives.csv') as csv_file:\n",
      "    for x in csv_file:\n",
      "        print x\n",
      "122/202:\n",
      "with open('adjectives.csv') as csv_file:\n",
      "    for x in csv_file:\n",
      "        print(x)\n",
      "122/203:\n",
      "with open('adjectives.csv') as csv_file:\n",
      "    csv_file.split(\",\")\n",
      "122/204:\n",
      "with open('adjectives.csv') as csv_file:\n",
      "    csv_file.read().split(\",\")\n",
      "122/205:\n",
      "with open('adjectives.csv') as csv_file:\n",
      "    adjectives = csv_file.read().split(\",\")\n",
      "122/206: adjectives\n",
      "122/207:\n",
      "def testbias(wordlist):\n",
      "    print(\"sports:\", bias(g_sports, m_sports, wordlist))\n",
      "    print(bias(g, model, wordlist))\n",
      "    print(bias(g_mps, m_mps, wordlist))\n",
      "    print(bias(g_books, m_books, wordlist))\n",
      "122/208: testbias(gender_specific)\n",
      "122/209:\n",
      "def testbias(wordlist):\n",
      "    print(\"sports:\" + bias(g_sports, m_sports, wordlist))\n",
      "    print(\"politics: \" + bias(g, model, wordlist))\n",
      "    print(\"motion pictures: \" + bias(g_mps, m_mps, wordlist))\n",
      "    print(\"books: \" + bias(g_books, m_books, wordlist))\n",
      "122/210: testbias(gender_specific)\n",
      "122/211:\n",
      "def bias(g, m, wordlist):\n",
      "    b = 0\n",
      "    l = list(filter(lambda x: x in m.wv.vocab, wordlist))\n",
      "    for x in l:\n",
      "        b += gproj(g, m, x)**2\n",
      "    return b/len(l)\n",
      "122/212: bias(g, model, proflist)\n",
      "122/213:\n",
      "def testbias(wordlist):\n",
      "    print(\"sports:\" + bias(g_sports, m_sports, wordlist))\n",
      "    print(\"politics: \" + bias(g, model, wordlist))\n",
      "    print(\"motion pictures: \" + bias(g_mps, m_mps, wordlist))\n",
      "    print(\"books: \" + bias(g_books, m_books, wordlist))\n",
      "122/214: testbias(gender_specific)\n",
      "122/215:\n",
      "def testbias(wordlist):\n",
      "#     print(f\"sports:\" + bias(g_sports, m_sports, wordlist))\n",
      "#     print(f\"politics: \" + bias(g, model, wordlist))\n",
      "#     print(f\"motion pictures: \" + bias(g_mps, m_mps, wordlist))\n",
      "    print(f\"books: {bias(g_books, m_books, wordlist)}\")\n",
      "122/216: testbias(gender_specific)\n",
      "122/217:\n",
      "def testbias(wordlist):\n",
      "    print(f\"sports: {bias(g_sports, m_sports, wordlist)}\")\n",
      "    print(f\"politics: {bias(g, model, wordlist)}\")\n",
      "    print(f\"motion pictures: {bias(g_mps, m_mps, wordlist)}\")\n",
      "    print(f\"books: {bias(g_books, m_books, wordlist)}\")\n",
      "122/218: testbias(gender_specific)\n",
      "122/219: testbias(proflist)\n",
      "122/220: testbias(adjectives)\n",
      "122/221:\n",
      "m_comp = getw2v(computers)\n",
      "g_comp = getg(computers)\n",
      "for x in wordlist:\n",
      "    print(x, gproj(g_comp, m_comp, x))\n",
      "122/222:\n",
      "m_comp = getw2v(computers)\n",
      "g_comp = getg(computers)\n",
      "122/223:\n",
      "m_comp, sg_comp = getw2v(computers)\n",
      "g_comp = getg(computers)\n",
      "122/224: m_comp, sg_comp = getw2v(computers)\n",
      "122/225: g_comp = getg(m_comp)\n",
      "122/226:\n",
      "for x in wordlist:\n",
      "    print(x, gproj(g_comp, m_comp, x))\n",
      "122/227:\n",
      "for x in filter(lambda x: x in m_comp.wv.vocab, wordlist):\n",
      "    print(x, gproj(g_comp, m_comp, x))\n",
      "122/228:\n",
      "def testbias(wordlist):\n",
      "    print(f\"sports: {bias(g_sports, m_sports, wordlist)}\")\n",
      "    print(f\"politics: {bias(g, model, wordlist)}\")\n",
      "    print(f\"motion pictures: {bias(g_mps, m_mps, wordlist)}\")\n",
      "    print(f\"books: {bias(g_books, m_books, wordlist)}\")\n",
      "    print(f\"computers: {bias(g_comps, m_comps, wordlist)}\")\n",
      "122/229: testbias(proflist)\n",
      "122/230:\n",
      "def testbias(wordlist):\n",
      "    print(f\"sports: {bias(g_sports, m_sports, wordlist)}\")\n",
      "    print(f\"politics: {bias(g, model, wordlist)}\")\n",
      "    print(f\"motion pictures: {bias(g_mps, m_mps, wordlist)}\")\n",
      "    print(f\"books: {bias(g_books, m_books, wordlist)}\")\n",
      "    print(f\"computers: {bias(g_comp, m_comp, wordlist)}\")\n",
      "122/231: testbias(proflist)\n",
      "122/232: testbias(gender_specific)\n",
      "122/233: testbias(adjectives)\n",
      "122/234:\n",
      "with open('debiaswe-master/data/gender_specific_full.json') as f:\n",
      "    gender_specific_full = json.loads(f.read())\n",
      "122/235: testbias(gender_specific_full)\n",
      "122/236:\n",
      "def getw2v(category, s):\n",
      "    docs = [simple_preprocess(t.text) for t in category]\n",
      "    return Word2Vec(docs, size=s)\n",
      "122/237:\n",
      "t = time.process_time()\n",
      "mpol3 = getw2v(politics, 300)\n",
      "time.process_time() - t\n",
      "122/238: mpol3.wv.get('hi')\n",
      "122/239: mpol3.wv.get_vector('yellow')\n",
      "122/240: mpol3.wv.get_vector('yellow').shape\n",
      "122/241:\n",
      "t = time.process_time()\n",
      "msports3 = getw2v(sports, 300)\n",
      "time.process_time() - t\n",
      "122/242:\n",
      "t = time.process_time()\n",
      "mmps3 = getw2v(mps, 300)\n",
      "time.process_time() - t\n",
      "122/243:\n",
      "t = time.process_time()\n",
      "msports3 = getw2v(sports, 300)\n",
      "time.process_time() - t\n",
      "122/244:\n",
      "t = time.process_time()\n",
      "mbooks3 = getw2v(books, 300)\n",
      "time.process_time() - t\n",
      "122/245:\n",
      "t = time.process_time()\n",
      "mcomp3 = getw2v(computers, 300)\n",
      "time.process_time() - t\n",
      "122/246: gpol3 = getg(mpol3)\n",
      "122/247: gsports3 = getg(msports3)\n",
      "122/248: gcomp3 = getg(mcomp3)\n",
      "122/249: gbooks3 = getg(mbooks3)\n",
      "122/250: gmps3 = getg(mmps3)\n",
      "122/251:\n",
      "def testbias3(wordlist):\n",
      "    print(f\"sports: {bias(g_sports3, m_sports3, wordlist)}\")\n",
      "    print(f\"politics: {bias(gpol3, mpol3, wordlist)}\")\n",
      "    print(f\"motion pictures: {bias(g_mps3, m_mps3, wordlist)}\")\n",
      "    print(f\"books: {bias(g_books3, m_books3, wordlist)}\")\n",
      "    print(f\"computers: {bias(g_comp3, m_comp3, wordlist)}\")\n",
      "122/252: testbias3(proflist)\n",
      "122/253:\n",
      "def testbias3(wordlist):\n",
      "    print(f\"sports: {bias(gsports3, msports3, wordlist)}\")\n",
      "    print(f\"politics: {bias(gpol3, mpol3, wordlist)}\")\n",
      "    print(f\"motion pictures: {bias(gmps3, mmps3, wordlist)}\")\n",
      "    print(f\"books: {bias(gbooks3, mbooks3, wordlist)}\")\n",
      "    print(f\"computers: {bias(gcomp3, mcomp3, wordlist)}\")\n",
      "122/254: testbias3(proflist)\n",
      "122/255: testbias3(gender_specific)\n",
      "122/256: testbias3(adjectives)\n",
      "122/257:\n",
      "for x in wordlist:\n",
      "    print(x, gproj(gpol3, mpol3, x))\n",
      "122/258: testbias3(gender_specific_full)\n",
      "122/259: testbias3(gender_specific)\n",
      "122/260:\n",
      "t = time.process_time()\n",
      "summaries_comp = []\n",
      "for i in range(len(computers)):\n",
      "    try:\n",
      "        summaries_comp.append(summarize(computers[i].text))\n",
      "    except:\n",
      "        x = computers[i]\n",
      "        print(i)\n",
      "time.process_time() - t\n",
      "122/261:\n",
      "t = time.process_time()\n",
      "summaries_comp = []\n",
      "for i in range(len(computers)):\n",
      "    try:\n",
      "        summaries_comp.append(summarize(computers[i].text))\n",
      "    except:\n",
      "        print(i)\n",
      "time.process_time() - t\n",
      "122/262:\n",
      "t = time.process_time()\n",
      "summaries_books = []\n",
      "for i in range(len(books)):\n",
      "    try:\n",
      "        summaries_books.append(summarize(books[i].text))\n",
      "    except:\n",
      "        print(i)\n",
      "time.process_time() - t\n",
      "122/263:\n",
      "t = time.process_time()\n",
      "summaries_mps = []\n",
      "for i in range(len(mps)):\n",
      "    try:\n",
      "        summaries_mps.append(summarize(mps[i].text))\n",
      "    except:\n",
      "        print(i)\n",
      "time.process_time() - t\n",
      "122/264:\n",
      "t = time.process_time()\n",
      "summaries_sports = []\n",
      "for i in range(len(sports)):\n",
      "    try:\n",
      "        summaries_sports.append(summarize(sports[i].text))\n",
      "    except:\n",
      "        print(i)\n",
      "time.process_time() - t\n",
      "122/265: m_comp_s, sg_comp_s = getw2v(summaries_comp)\n",
      "122/266: m_comp_s, sg_comp_s = getw2v(summaries_comp, 100)\n",
      "122/267: simple_preprocess(politics[10].text) # lowercasing, de-punctuation\n",
      "122/268: simple_preprocess(politics[10].text)[:10] # lowercasing, de-punctuation\n",
      "122/269:\n",
      "def getw2v(category):\n",
      "    docs = [[simple_preprocess(x) for x in t.text.split(\".\")] for t in category].flatten()\n",
      "    return Word2Vec(docs), Word2Vec(docs,sg=1)\n",
      "122/270:\n",
      "t = time.process_time()\n",
      "m_pol, sg_pol = getw2v(politics)\n",
      "time.process_time() - t\n",
      "122/271:\n",
      "def getw2v(category):\n",
      "    docs = [simple_preprocess(x) for x in t.text.split(\".\") for t in category]\n",
      "    return Word2Vec(docs), Word2Vec(docs,sg=1)\n",
      "122/272:\n",
      "t = time.process_time()\n",
      "m_pol, sg_pol = getw2v(politics)\n",
      "time.process_time() - t\n",
      "122/273:\n",
      "def getw2v(category):\n",
      "    docs = []\n",
      "    for t in category:\n",
      "        for x in t.text.split(\".\"):\n",
      "            docs.append(simple_preprocess(x))\n",
      "    return Word2Vec(docs), Word2Vec(docs,sg=1)\n",
      "122/274:\n",
      "def getw2v(category):\n",
      "    docs = []\n",
      "    for t in category:\n",
      "        for x in t.text.split(\".\"):\n",
      "            docs.append(simple_preprocess(x))\n",
      "    return Word2Vec(docs)\n",
      "# , Word2Vec(docs,sg=1)\n",
      "122/275:\n",
      "t = time.process_time()\n",
      "m_pol, sg_pol = getw2v(politics)\n",
      "time.process_time() - t\n",
      "122/276:\n",
      "t = time.process_time()\n",
      "m_pol = getw2v(politics)\n",
      "time.process_time() - t\n",
      "122/277: m_pol.wv.most_similar(positive=['good'], topn=5)\n",
      "122/278: g_pol2 = getg(m_pol)\n",
      "122/279:\n",
      "for x in wordlist:\n",
      "    print(x, gproj(gpol2, m_pol, x))\n",
      "122/280:\n",
      "for x in wordlist:\n",
      "    print(x, gproj(g_pol2, m_pol, x))\n",
      "122/281:\n",
      "def getw2v(category):\n",
      "    docs = [simple_preprocess(t.text) for t in category]\n",
      "    return Word2Vec(docs)\n",
      "122/282:\n",
      "def getw2v_s(sums):\n",
      "    docs = [simple_preprocess(t) for t in sums]\n",
      "    return Word2Vec(docs)\n",
      "122/283: m_pol_s = getw2v_s(summaries_pol)\n",
      "122/284: gpol_s = getg(m_pol_s)\n",
      "122/285: bias(gpol_s, mpol_s, proflist)\n",
      "122/286: bias(g_pol_s, m_pol_s, proflist)\n",
      "122/287: bias(gpol_s, m_pol_s, proflist)\n",
      "122/288: bias(gpol_s, mpol_s, adjectives)\n",
      "122/289: bias(gpol_s, m_pol_s, adjectives)\n",
      "122/290: bias(gpol_s, m_pol_s, gender_specific)\n",
      "122/291: bias(gpol_s, m_pol_s, gender_specific_full)\n",
      "122/292: bias(gpol_s, m_pol_s, gender_specific)\n",
      "122/293:\n",
      "for x in wordlist:\n",
      "    print(x, gproj(gpol_s, m_pol_s, x))\n",
      "122/294:\n",
      "for x in filter(lambda x: x in m_pol_s.wv.vocab, wordlist):\n",
      "    print(x, gproj(gpol_s, m_pol_s, x))\n",
      "122/295:\n",
      "for x in filter(lambda x: x in m_pol_s.wv.vocab, wordlist):\n",
      "    print(x, gproj(gpol2, m_pol, x))\n",
      "122/296:\n",
      "for x in filter(lambda x: x in m_pol_s.wv.vocab, wordlist):\n",
      "    print(x, gproj(g_pol2, m_pol, x))\n",
      "122/297: print(1)\n",
      "122/298: len(summaries_pol)\n",
      "122/299:\n",
      "t = time.process_time()\n",
      "summaries_comp = []\n",
      "for i in range(len(computers)):\n",
      "    try:\n",
      "        summaries_comp.append(summarize(computers[i].text))\n",
      "    except:\n",
      "        print(i)\n",
      "time.process_time() - t\n",
      "122/300:\n",
      "t = time.process_time()\n",
      "summaries_books = []\n",
      "for i in range(len(books)):\n",
      "    try:\n",
      "        summaries_books.append(summarize(books[i].text))\n",
      "    except:\n",
      "        print(i)\n",
      "time.process_time() - t\n",
      "122/301:\n",
      "t = time.process_time()\n",
      "summaries_mps = []\n",
      "for i in range(len(mps)):\n",
      "    try:\n",
      "        summaries_mps.append(summarize(mps[i].text))\n",
      "    except:\n",
      "        print(i)\n",
      "time.process_time() - t\n",
      "122/302: testbias_s(proflist)\n",
      "122/303:\n",
      "def testbias_s(wordlist):\n",
      "    print(f\"sports: {bias(gsports_s, msports_s, wordlist)}\")\n",
      "    print(f\"politics: {bias(gpol_s, mpol_s, wordlist)}\")\n",
      "    print(f\"motion pictures: {bias(gmps_s, mmps_s, wordlist)}\")\n",
      "    print(f\"books: {bias(gbooks_s, mbooks_s, wordlist)}\")\n",
      "    print(f\"computers: {bias(gcomp_s, mcomp_s, wordlist)}\")\n",
      "122/304: testbias_s(proflist)\n",
      "122/305: sentences = nltk.sent_tokenize(politics[10])\n",
      "122/306: import nlkt\n",
      "122/307: import nltk\n",
      "122/308: sentences = nltk.sent_tokenize(politics[10])\n",
      "122/309: sentences = nltk.sent_tokenize(politics[10].text)\n",
      "122/310: sentences\n",
      "122/311: len(sentences)\n",
      "122/312: sentences\n",
      "122/313: stopwords.words('english')\n",
      "122/314: nltk.stopwords.words('english')\n",
      "122/315: nltk.corpus.stopwords.words('english')\n",
      "122/316:\n",
      "simple_preprocess(sentences[0]\n",
      "                 )\n",
      "122/317:\n",
      "def getw2v(category):\n",
      "    sentences = []\n",
      "    for t in category:\n",
      "        sentences += nltk.sent_tokenize(t.text)\n",
      "    docs = [simple_preprocess(s) for s in sentences]\n",
      "    return Word2Vec(docs)\n",
      "122/318:\n",
      "def getw2v(category, size):\n",
      "    sentences = []\n",
      "    for t in category:\n",
      "        sentences += nltk.sent_tokenize(t.text)\n",
      "    docs = [simple_preprocess(s) for s in sentences]\n",
      "    return Word2Vec(docs, size=s)\n",
      "122/319:\n",
      "t = time.process_time()\n",
      "mpol3 = getw2v(politics, 300)\n",
      "time.process_time() - t\n",
      "122/320:\n",
      "def getw2v(category, size):\n",
      "    sentences = []\n",
      "    for t in category:\n",
      "        sentences += nltk.sent_tokenize(t.text)\n",
      "    docs = [simple_preprocess(s) for s in sentences]\n",
      "    return Word2Vec(docs, size=size)\n",
      "122/321:\n",
      "t = time.process_time()\n",
      "mpol3 = getw2v(politics, 300)\n",
      "time.process_time() - t\n",
      "122/322:\n",
      "t = time.process_time()\n",
      "mpol = getw2v(politics, 100)\n",
      "time.process_time() - t\n",
      "122/323: mpol.wv.most_similar(positive=['good'], topn=5)\n",
      "122/324: mpol3.wv.most_similar(positive=['good'], topn=5)\n",
      "122/325: g_pol2 = getg(m_pol)\n",
      "122/326: g_pol2 = getg(mpol)\n",
      "122/327: g_pol = getg(mpol)\n",
      "122/328: gpol3 = getg(mpol3)\n",
      "122/329:\n",
      "for x in filter(lambda x: x in m_pol_s.wv.vocab, wordlist):\n",
      "    print(x, gproj(gpol, mpol, x))\n",
      "122/330: gpol = getg(mpol)\n",
      "122/331:\n",
      "for x in filter(lambda x: x in m_pol_s.wv.vocab, wordlist):\n",
      "    print(x, gproj(gpol, mpol, x))\n",
      "122/332:\n",
      "wordlist = ['he', 'she', 'doctor', 'nurse', 'man', 'woman', 'male', 'female', 'politician', 'guy', \\\n",
      "            'scientist', 'teacher', 'professor', 'mother', 'pretty', 'handsome', 'strong', 'coward', \\\n",
      "            'dress', 'timid', 'leadership', 'blond', 'lovely', 'petite', 'surgeon', 'physician', 'shopkeeper', \\\n",
      "            'cooking', 'sewing', 'chuckle', 'sassy', 'lanky', 'competent', 'brilliant', 'capable', 'smart', \\\n",
      "            'intelligent', 'incompetent', 'librarian', 'cosmetics', 'maid', 'clever', 'dressed', 'captain', \\\n",
      "            'nanny', 'philosopher', 'architect', 'financier', 'warrior', 'magician', 'housekeeper', 'homemaker', 'programmer']\n",
      "122/333:\n",
      "for x in filter(lambda x: x in m_pol_s.wv.vocab, wordlist):\n",
      "    print(x, gproj(gpol, mpol, x))\n",
      "122/334: bias(gpol, mpol, proflist)\n",
      "122/335: bias(gpol, mpol, adjectives)\n",
      "122/336: bias(gpol, mpol, gender_specific)\n",
      "122/337: bias(gpol, mpol, gender_specific_full)\n",
      "122/338:\n",
      "def getw2v_s(sums):\n",
      "    sentences = []\n",
      "    for t in sums:\n",
      "        sentences += nltk.sent_tokenize(t)\n",
      "    docs = [simple_preprocess(s) for s in sentences]\n",
      "    return Word2Vec(docs)\n",
      "122/339: mpols = getw2v_s(summaries_pol)\n",
      "122/340:\n",
      "t = time.process_time()\n",
      "mpols = getw2v_s(summaries_pol)\n",
      "time.process_time() - t\n",
      "122/341: mpols.wv.most_similar(positive=['approve'], topn=5)\n",
      "122/342:\n",
      "for x in filter(lambda x: x in mpols.wv.vocab, wordlist):\n",
      "    print(x, gproj(gpols, m_pols, x))\n",
      "122/343: gpols = getg(mpols)\n",
      "122/344:\n",
      "for x in filter(lambda x: x in mpols.wv.vocab, wordlist):\n",
      "    print(x, gproj(gpols, m_pols, x))\n",
      "122/345:\n",
      "for x in filter(lambda x: x in mpols.wv.vocab, wordlist):\n",
      "    print(x, gproj(gpols, mpols, x))\n",
      "122/346: testbias_s(proflist)\n",
      "122/347: bias(gpols, mpols, proflist)\n",
      "122/348: bias(gpols, mpols, adjectives)\n",
      "122/349: bias(gpols, mpols, gender_specific)\n",
      "122/350: bias(gpol, mpol, proflist), bias(gpols, mpols, proflist)\n",
      "122/351: bias(gpol, mpol, adjectives), bias(gpols, mpols, adjectives)\n",
      "122/352: bias(gpol, mpol, gender_specific), bias(gpols, mpols, gender_specific)\n",
      "122/353:\n",
      "t = time.process_time()\n",
      "msports = getw2v_s(summaries_sports)\n",
      "time.process_time() - t\n",
      "122/354:\n",
      "t = time.process_time()\n",
      "mcomp = getw2v_s(summaries_comp)\n",
      "time.process_time() - t\n",
      "122/355:\n",
      "def getw2v(category):\n",
      "    sentences = []\n",
      "    for t in category:\n",
      "        sentences += nltk.sent_tokenize(t.text)\n",
      "    docs = [simple_preprocess(s) for s in sentences]\n",
      "    return Word2Vec(docs)\n",
      "122/356:\n",
      "t = time.process_time()\n",
      "msports = getw2v(sports)\n",
      "time.process_time() - t\n",
      "122/357:\n",
      "t = time.process_time()\n",
      "mcomp = getw2v(computers)\n",
      "time.process_time() - t\n",
      "122/358:\n",
      "t = time.process_time()\n",
      "msportss = getw2v_s(summaries_sports)\n",
      "time.process_time() - t\n",
      "122/359: gcomp = getg(mcomp)\n",
      "122/360: gsports = getg(msports)\n",
      "122/361: gsportss = getg(msportss)\n",
      "122/362: gsports = getg(msports)\n",
      "122/363: gcomps = getg(mcomps)\n",
      "122/364:\n",
      "t = time.process_time()\n",
      "mcomps = getw2v_s(summaries_comp)\n",
      "time.process_time() - t\n",
      "122/365:\n",
      "t = time.process_time()\n",
      "mcomp = getw2v(computers)\n",
      "time.process_time() - t\n",
      "122/366: bias(gcomp, mcomp, proflist), bias(gcomps, mcomps, proflist)\n",
      "122/367:\n",
      "t = time.process_time()\n",
      "mcomp = getw2v(computers)\n",
      "time.process_time() - t\n",
      "122/368: gcomp = getg(mcomp)\n",
      "122/369: gcomps = getg(mcomps)\n",
      "122/370: gsportss = getg(msportss)\n",
      "122/371: gsports = getg(msports)\n",
      "122/372:\n",
      "for x in filter(lambda x: x in msportss.wv.vocab, wordlist):\n",
      "    print(x, gproj(gsportss, msportss, x))\n",
      "122/373: bias(gcomp, mcomp, proflist), bias(gcomps, mcomps, proflist)\n",
      "122/374: bias(gcomp, mcomp, adjectives), bias(gcomps, mcomps, adjectives)\n",
      "122/375: bias(gcomp, mcomp, gender_specific), bias(gcomps, mcomps, gender_specific)\n",
      "122/376: bias(gsports, msports, proflist), bias(gsportss, msportss, proflist)\n",
      "122/377: bias(gsports, msports, adjectives), bias(gsportss, msportss, adjectives)\n",
      "122/378: bias(gsports, msports, gender_specific), bias(gsportss, msportss, gender_specific)\n",
      "122/379: len(proflist)\n",
      "122/380: from sklearn.linear_model import LinearRegression\n",
      "122/381:\n",
      "x = np.array([gproj(gpol, mpol, w) for w in proflist])\n",
      "y = np.array([gproj(gpols, mpols, w) for w in proflist])\n",
      "122/382:\n",
      "x = np.array([gproj(gpol, mpol, w) for w in \\\n",
      "              filter(lambda x: x in mpol.wv.vocab, proflist)])\n",
      "y = np.array([gproj(gpols, mpols, w) for w in \\\n",
      "              filter(lambda x: x in mpol.wv.vocab, proflist)]\n",
      "122/383:\n",
      "x = np.array([gproj(gpol, mpol, w) for w in \\\n",
      "              filter(lambda x: x in mpol.wv.vocab, proflist)])\n",
      "y = np.array([gproj(gpols, mpols, w) for w in \\\n",
      "              filter(lambda x: x in mpol.wv.vocab, proflist)])\n",
      "122/384:\n",
      "x = np.array([gproj(gpol, mpol, w) for w in \\\n",
      "              filter(lambda x: x in mpol.wv.vocab, proflist)])\n",
      "y = np.array([gproj(gpols, mpols, w) for w in \\\n",
      "              filter(lambda x: x in mpols.wv.vocab, proflist)])\n",
      "122/385:\n",
      "x = np.array([gproj(gpol, mpol, w) for w in \\\n",
      "              filter(lambda x: x in mpols.wv.vocab, proflist)])\n",
      "y = np.array([gproj(gpols, mpols, w) for w in \\\n",
      "              filter(lambda x: x in mpols.wv.vocab, proflist)])\n",
      "122/386: slope, intercept, r_value, p_value, std_err = stats.linregress(x,y)\n",
      "122/387: from scipy import stats\n",
      "122/388: slope, intercept, r_value, p_value, std_err = stats.linregress(x,y)\n",
      "122/389: r_value\n",
      "122/390: p_value\n",
      "122/391: x.shape\n",
      "122/392: r_value**2\n",
      "122/393: r_value\n",
      "122/394: slope\n",
      "122/395:\n",
      "for x in filter(lambda x: x in mcomp.wv.vocab, wordlist):\n",
      "    print(x, gproj(gcomp, mcomp, x))\n",
      "122/396:\n",
      "x = np.array([gproj(gcomp, mcomp, w) for w in \\\n",
      "              filter(lambda x: x in mcomps.wv.vocab, proflist)])\n",
      "y = np.array([gproj(gcomps, mcomps, w) for w in \\\n",
      "              filter(lambda x: x in mcomps.wv.vocab, proflist)])\n",
      "122/397:\n",
      "xc = np.array([gproj(gcomp, mcomp, w) for w in \\\n",
      "              filter(lambda x: x in mcomps.wv.vocab, proflist)])\n",
      "yc = np.array([gproj(gcomps, mcomps, w) for w in \\\n",
      "              filter(lambda x: x in mcomps.wv.vocab, proflist)])\n",
      "122/398:\n",
      "x = np.array([gproj(gpol, mpol, w) for w in \\\n",
      "              filter(lambda x: x in mpols.wv.vocab, proflist)])\n",
      "y = np.array([gproj(gpols, mpols, w) for w in \\\n",
      "              filter(lambda x: x in mpols.wv.vocab, proflist)])\n",
      "122/399: slopec, interceptc, r_valuec, p_valuec, std_errc = stats.linregress(xc,yc)\n",
      "122/400: slopec, p_valuec\n",
      "122/401: slope, p_value\n",
      "122/402: gproj(gcomps, mcomps, \"he\")\n",
      "122/403: gproj(gpols, mpols, \"he\")\n",
      "122/404: gproj(gpols, mpols, \"he\"), gproj(gpol, mpol, \"he\")\n",
      "122/405: gproj(gcomps, mcomps, \"he\"), gproj(gcomps, mcomps, \"he\")\n",
      "122/406: gproj(gcomps, mcomps, \"he\"), gproj(gcomp, mcomp, \"he\")\n",
      "122/407: gproj(gpols, mpols, \"he\"), gproj(gpol, mpol, \"he\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "122/408:\n",
      "xc = np.array([gproj(gcomp, mcomp, w) for w in \\\n",
      "              filter(lambda x: x in mcomps.wv.vocab, gender_specific)])\n",
      "yc = np.array([gproj(gcomps, mcomps, w) for w in \\\n",
      "              filter(lambda x: x in mcomps.wv.vocab, gender_specific)])\n",
      "122/409: slopec, interceptc, r_valuec, p_valuec, std_errc = stats.linregress(xc,yc)\n",
      "122/410: slopec, p_valuec\n",
      "122/411:\n",
      "x = np.array([gproj(gpol, mpol, w) for w in \\\n",
      "              filter(lambda x: x in mpols.wv.vocab, gender_specific)])\n",
      "y = np.array([gproj(gpols, mpols, w) for w in \\\n",
      "              filter(lambda x: x in mpols.wv.vocab, gender_specific)])\n",
      "122/412: slope, intercept, r_value, p_value, std_err = stats.linregress(x,y)\n",
      "122/413: slope, p_value\n",
      "122/414:\n",
      "# generate a random subset of text for each text with length \n",
      "# equal to number of sentences in summary\n",
      "122/415: from random import sample\n",
      "122/416:\n",
      "controls_pol = []\n",
      "count = 0\n",
      "for i in politics:\n",
      "    if i in [1646, 2145, 2331, 5396, 5569, 6193, 8035, 8425]:\n",
      "        continue\n",
      "    n = len(nltk.sent_tokenize(summaries_pol[count]))\n",
      "    count += 1\n",
      "    sents_pol = nltk.sent_tokenize(politics[i])\n",
      "    controls_pol += sample(sents_pol, n)\n",
      "122/417:\n",
      "controls_pol = []\n",
      "count = 0\n",
      "for i in range(len(politics)):\n",
      "    if i in [1646, 2145, 2331, 5396, 5569, 6193, 8035, 8425]:\n",
      "        continue\n",
      "    n = len(nltk.sent_tokenize(summaries_pol[count]))\n",
      "    count += 1\n",
      "    sents_pol = nltk.sent_tokenize(politics[i].text)\n",
      "    controls_pol += sample(sents_pol, n)\n",
      "122/418:\n",
      "t = time.process_time()\n",
      "controls_pol = []\n",
      "count = 0\n",
      "for i in range(len(politics)):\n",
      "    if i in [1646, 2145, 2331, 5396, 5569, 6193, 8035, 8425]:\n",
      "        continue\n",
      "    n = len(nltk.sent_tokenize(summaries_pol[count]))\n",
      "    count += 1\n",
      "    sents_pol = nltk.sent_tokenize(politics[i].text)\n",
      "    controls_pol += sample(sents_pol, n)\n",
      "time.process_time() - t\n",
      "122/419: controls_pol\n",
      "122/420: len(controls_pol)\n",
      "122/421:\n",
      "sums_pol = []\n",
      "for s in summaries_pol:\n",
      "    sums_pol += nltk.sent_tokenize(s)\n",
      "len(sums_pol)\n",
      "122/422: sums_pol\n",
      "122/423: controls_pol\n",
      "122/424:\n",
      "t = time.process_time()\n",
      "docs = [simple_preprocess(s) for s in controls_pol]\n",
      "mcpol = Word2Vec(docs)\n",
      "time.process_time() - t\n",
      "122/425: gcpol = getg(mcpol)\n",
      "122/426: bias(gcpol, mcpol, gender_specific)\n",
      "122/427: bias(gpol, mpol, proflist)\n",
      "122/428: bias(gcpol, mcpol, proflist)\n",
      "122/429: bias(gpols, mpols, proflist)\n",
      "122/430:\n",
      "x = np.array([gproj(gpols, mpols, w) for w in \\\n",
      "              filter(lambda x: x in mpols.wv.vocab, gender_specific)])\n",
      "y = np.array([gproj(gcpol, mcpol, w) for w in \\\n",
      "              filter(lambda x: x in mpols.wv.vocab, gender_specific)])\n",
      "slope, intercept, r_value, p_value, std_err = stats.linregress(x,y)\n",
      "slope, p_value\n",
      "122/431:\n",
      "x = np.array([gproj(gpols, mpols, w) for w in \\\n",
      "              filter(lambda x: x in mpols.wv.vocab and x in mcpol.wv.vocab, gender_specific)])\n",
      "y = np.array([gproj(gcpol, mcpol, w) for w in \\\n",
      "              filter(lambda x: x in mpols.wv.vocab and x in mcpol.wv.vocab, gender_specific)])\n",
      "slope, intercept, r_value, p_value, std_err = stats.linregress(x,y)\n",
      "slope, p_value\n",
      "122/432:\n",
      "x = np.array([gproj(gpols, mpols, w) for w in \\\n",
      "              filter(lambda x: x in mpols.wv.vocab and x in mcpol.wv.vocab, proflist)])\n",
      "y = np.array([gproj(gcpol, mcpol, w) for w in \\\n",
      "              filter(lambda x: x in mpols.wv.vocab and x in mcpol.wv.vocab, proflist)])\n",
      "slope, intercept, r_value, p_value, std_err = stats.linregress(x,y)\n",
      "slope, p_value\n",
      "122/433:\n",
      "words_cpol = []\n",
      "for s in controls_pol:\n",
      "    words_cpol += nltk.word_tokenize(s)\n",
      "len(words_cpol)\n",
      "122/434:\n",
      "words_spol = []\n",
      "for s in sums_pol:\n",
      "    words_spol += nltk.word_tokenize(s)\n",
      "len(words_spol)\n",
      "122/435:\n",
      "words_pol = []\n",
      "for p in politics:\n",
      "    words_pol += nltk.word_tokenize(p.text)\n",
      "len(words_pol)\n",
      "122/436: words_pol[:10]\n",
      "122/437: # Controlling for number of words\n",
      "122/438: bias(gpol, mpol, proflist)\n",
      "122/439: len(politics)\n",
      "122/440: from random import sample, randint\n",
      "122/441: randint(10)\n",
      "122/442: randint(1, 2)\n",
      "122/443: randint(1, 2)\n",
      "122/444: randint(1, 2)\n",
      "122/445: nltk.word_tokenize(politics[0].text)\n",
      "122/446:\n",
      "t = time.process_time()\n",
      "wcontrols_pol = []\n",
      "count = 0\n",
      "for i in range(len(politics)):\n",
      "    if i > 2:\n",
      "        break\n",
      "    if i in [1646, 2145, 2331, 5396, 5569, 6193, 8035, 8425]:\n",
      "        continue\n",
      "    # get number of words in the summary\n",
      "    n = len(nltk.word_tokenize(summaries_pol[count]))\n",
      "    sents = nltk.sent_tokenize(politics[i].text)\n",
      "    wcount = 0\n",
      "    wtot = len(nltk.word_tokenize(politics[i].text)\n",
      "    start = rand(0, wtot-n)\n",
      "    end = start + n\n",
      "    started = False\n",
      "    for s in sents:\n",
      "        w = nltk.word_tokenize(s)\n",
      "        wcount += len(w)\n",
      "        if wcount < start:\n",
      "            continue\n",
      "        elif started is False:\n",
      "            wcontrols_pol.append(s[start-wcount:])\n",
      "            started = True\n",
      "        elif wcount >= end:\n",
      "            wcontrols_pol.append(s[:wcount-end])\n",
      "            break\n",
      "        else:\n",
      "            wcontrols_pol.append(s)\n",
      "    # get n-word consecutive subset of original text\n",
      "    count += 1\n",
      "time.process_time() - t\n",
      "122/447:\n",
      "t = time.process_time()\n",
      "wcontrols_pol = []\n",
      "count = 0\n",
      "for i in range(len(politics)):\n",
      "    if i > 2:\n",
      "        break\n",
      "    if i in [1646, 2145, 2331, 5396, 5569, 6193, 8035, 8425]:\n",
      "        continue\n",
      "    # get number of words in the summary\n",
      "    n = len(nltk.word_tokenize(summaries_pol[count]))\n",
      "    sents = nltk.sent_tokenize(politics[i].text)\n",
      "    wcount = 0\n",
      "    wtot = len(nltk.word_tokenize(politics[i].text)\n",
      "    start = randint(0, wtot-n)\n",
      "    end = start + n\n",
      "    started = False\n",
      "    for s in sents:\n",
      "        w = nltk.word_tokenize(s)\n",
      "        wcount += len(w)\n",
      "        if wcount < start:\n",
      "            continue\n",
      "        elif started is False:\n",
      "            wcontrols_pol.append(s[start-wcount:])\n",
      "            started = True\n",
      "        elif wcount >= end:\n",
      "            wcontrols_pol.append(s[:wcount-end])\n",
      "            break\n",
      "        else:\n",
      "            wcontrols_pol.append(s)\n",
      "    # get n-word consecutive subset of original text\n",
      "    count += 1\n",
      "time.process_time() - t\n",
      "122/448:\n",
      "t = time.process_time()\n",
      "wcontrols_pol = []\n",
      "count = 0\n",
      "for i in range(len(politics)):\n",
      "    if i > 2:\n",
      "        break\n",
      "    if i in [1646, 2145, 2331, 5396, 5569, 6193, 8035, 8425]:\n",
      "        continue\n",
      "    # get number of words in the summary\n",
      "    n = len(nltk.word_tokenize(summaries_pol[count]))\n",
      "    sents = nltk.sent_tokenize(politics[i].text)\n",
      "    wcount = 0\n",
      "    wtot = len(nltk.word_tokenize(politics[i].text))\n",
      "    start = randint(0, wtot-n)\n",
      "    end = start + n\n",
      "    started = False\n",
      "    for s in sents:\n",
      "        w = nltk.word_tokenize(s)\n",
      "        wcount += len(w)\n",
      "        if wcount < start:\n",
      "            continue\n",
      "        elif started is False:\n",
      "            wcontrols_pol.append(s[start-wcount:])\n",
      "            started = True\n",
      "        elif wcount >= end:\n",
      "            wcontrols_pol.append(s[:wcount-end])\n",
      "            break\n",
      "        else:\n",
      "            wcontrols_pol.append(s)\n",
      "    # get n-word consecutive subset of original text\n",
      "    count += 1\n",
      "time.process_time() - t\n",
      "122/449: wcontrols_pol\n",
      "122/450:\n",
      "t = time.process_time()\n",
      "wcontrols_pol = []\n",
      "count = 0\n",
      "for i in range(len(politics)):\n",
      "    print(\"i\", i)\n",
      "    if i > 1:\n",
      "        break\n",
      "    if i in [1646, 2145, 2331, 5396, 5569, 6193, 8035, 8425]:\n",
      "        continue\n",
      "    # get number of words in the summary\n",
      "    n = len(nltk.word_tokenize(summaries_pol[count]))\n",
      "    print(\"n\", n)\n",
      "    sents = nltk.sent_tokenize(politics[i].text)\n",
      "    wcount = 0\n",
      "    wtot = len(nltk.word_tokenize(politics[i].text))\n",
      "    start = randint(0, wtot-n)\n",
      "    end = start + n\n",
      "    started = False\n",
      "    for s in sents:\n",
      "        w = nltk.word_tokenize(s)\n",
      "        wcount += len(w)\n",
      "        if wcount < start:\n",
      "            continue\n",
      "        elif started is False:\n",
      "            wcontrols_pol.append(s[start-wcount:])\n",
      "            started = True\n",
      "        elif wcount >= end:\n",
      "            wcontrols_pol.append(s[:wcount-end])\n",
      "            break\n",
      "        else:\n",
      "            wcontrols_pol.append(s)\n",
      "    # get n-word consecutive subset of original text\n",
      "    count += 1\n",
      "    print(sum([len(x) for x in wcontrols_pol]))\n",
      "time.process_time() - t\n",
      "122/451:\n",
      "t = time.process_time()\n",
      "wcontrols_pol = []\n",
      "count = 0\n",
      "for i in range(len(politics)):\n",
      "    print(\"i\", i)\n",
      "    if i > 1:\n",
      "        break\n",
      "    if i in [1646, 2145, 2331, 5396, 5569, 6193, 8035, 8425]:\n",
      "        continue\n",
      "    # get number of words in the summary\n",
      "    n = len(nltk.word_tokenize(summaries_pol[count]))\n",
      "    print(\"n\", n)\n",
      "    sents = nltk.sent_tokenize(politics[i].text)\n",
      "    wcount = 0\n",
      "    wtot = len(nltk.word_tokenize(politics[i].text))\n",
      "    start = randint(0, wtot-n)\n",
      "    end = start + n\n",
      "    started = False\n",
      "    for s in sents:\n",
      "        w = nltk.word_tokenize(s)\n",
      "        wcount += len(w)\n",
      "        if wcount < start:\n",
      "            continue\n",
      "        elif started is False:\n",
      "            wcontrols_pol.append(s[start-wcount:])\n",
      "            started = True\n",
      "        elif wcount >= end:\n",
      "            wcontrols_pol.append(s[:wcount-end])\n",
      "            break\n",
      "        else:\n",
      "            wcontrols_pol.append(s)\n",
      "    # get n-word consecutive subset of original text\n",
      "    count += 1\n",
      "    print(start, end)\n",
      "    print(sum([len(x) for x in wcontrols_pol]))\n",
      "time.process_time() - t\n",
      "122/452:\n",
      "t = time.process_time()\n",
      "wcontrols_pol = []\n",
      "count = 0\n",
      "for i in range(len(politics)):\n",
      "    print(\"i\", i)\n",
      "    if i > 1:\n",
      "        break\n",
      "    if i in [1646, 2145, 2331, 5396, 5569, 6193, 8035, 8425]:\n",
      "        continue\n",
      "    # get number of words in the summary\n",
      "    n = len(nltk.word_tokenize(summaries_pol[count]))\n",
      "    print(\"n\", n)\n",
      "    sents = nltk.sent_tokenize(politics[i].text)\n",
      "    wcount = 0\n",
      "    wtot = len(nltk.word_tokenize(politics[i].text))\n",
      "    start = randint(0, wtot-n)\n",
      "    end = start + n\n",
      "    started = False\n",
      "    for s in sents:\n",
      "        w = nltk.word_tokenize(s)\n",
      "        wcount += len(w)\n",
      "        if wcount < start:\n",
      "            continue\n",
      "        elif started is False:\n",
      "            print(\"s-w\", start-wcount)\n",
      "            wcontrols_pol.append(s[start-wcount:])\n",
      "            started = True\n",
      "        elif wcount >= end:\n",
      "            print(\"w-e\", wcount-end)\n",
      "            wcontrols_pol.append(s[:wcount-end])\n",
      "            break\n",
      "        else:\n",
      "            wcontrols_pol.append(s)\n",
      "    # get n-word consecutive subset of original text\n",
      "    count += 1\n",
      "    print(start, end)\n",
      "    print(sum([len(x) for x in wcontrols_pol]))\n",
      "time.process_time() - t\n",
      "122/453:\n",
      "t = time.process_time()\n",
      "wcontrols_pol = []\n",
      "count = 0\n",
      "for i in range(len(politics)):\n",
      "    print(\"i\", i)\n",
      "    if i > 1:\n",
      "        break\n",
      "    if i in [1646, 2145, 2331, 5396, 5569, 6193, 8035, 8425]:\n",
      "        continue\n",
      "    # get number of words in the summary\n",
      "    n = len(nltk.word_tokenize(summaries_pol[count]))\n",
      "    print(\"n\", n)\n",
      "    sents = nltk.sent_tokenize(politics[i].text)\n",
      "    wcount = 0\n",
      "    wtot = len(nltk.word_tokenize(politics[i].text))\n",
      "    start = randint(0, wtot-n)\n",
      "    end = start + n\n",
      "    started = False\n",
      "    for s in sents:\n",
      "        w = nltk.word_tokenize(s)\n",
      "        wcount += len(w)\n",
      "        if wcount < start:\n",
      "            continue\n",
      "        elif started is False:\n",
      "            print(\"s-w\", wcount-start)\n",
      "            wcontrols_pol.append(s[wcount-start:])\n",
      "            started = True\n",
      "        elif wcount >= end:\n",
      "            print(\"w-e\", wcount-end)\n",
      "            wcontrols_pol.append(s[:wcount-end])\n",
      "            break\n",
      "        else:\n",
      "            wcontrols_pol.append(s)\n",
      "    # get n-word consecutive subset of original text\n",
      "    count += 1\n",
      "    print(start, end)\n",
      "    print(sum([len(x) for x in wcontrols_pol]))\n",
      "time.process_time() - t\n",
      "122/454:\n",
      "t = time.process_time()\n",
      "wcontrols_pol = []\n",
      "count = 0\n",
      "for i in range(len(politics)):\n",
      "    print(\"i\", i)\n",
      "    if i > 1:\n",
      "        break\n",
      "    if i in [1646, 2145, 2331, 5396, 5569, 6193, 8035, 8425]:\n",
      "        continue\n",
      "    # get number of words in the summary\n",
      "    n = len(nltk.word_tokenize(summaries_pol[count]))\n",
      "    print(\"n\", n)\n",
      "    sents = nltk.sent_tokenize(politics[i].text)\n",
      "    wcount = 0\n",
      "    wtot = len(nltk.word_tokenize(politics[i].text))\n",
      "    start = randint(0, wtot-n)\n",
      "    end = start + n\n",
      "    started = False\n",
      "    for s in sents:\n",
      "        w = nltk.word_tokenize(s)\n",
      "        wcount += len(w)\n",
      "        if wcount < start:\n",
      "            continue\n",
      "        elif started is False:\n",
      "            print(\"s-w\", wcount-start)\n",
      "            wcontrols_pol.append(s[wcount-start:])\n",
      "            started = True\n",
      "        elif wcount > end:\n",
      "            print(\"w-e\", wcount-end)\n",
      "            wcontrols_pol.append(s[:wcount-end])\n",
      "            break\n",
      "        else:\n",
      "            wcontrols_pol.append(s)\n",
      "    # get n-word consecutive subset of original text\n",
      "    count += 1\n",
      "    print(start, end)\n",
      "    print(sum([len(x) for x in wcontrols_pol]))\n",
      "time.process_time() - t\n",
      "122/455:\n",
      "t = time.process_time()\n",
      "wcontrols_pol = []\n",
      "count = 0\n",
      "for i in range(len(politics)):\n",
      "    print(\"i\", i)\n",
      "    if i > 1:\n",
      "        break\n",
      "    if i in [1646, 2145, 2331, 5396, 5569, 6193, 8035, 8425]:\n",
      "        continue\n",
      "    # get number of words in the summary\n",
      "    n = len(nltk.word_tokenize(summaries_pol[count]))\n",
      "    print(\"n\", n)\n",
      "    sents = nltk.sent_tokenize(politics[i].text)\n",
      "    wcount = 0\n",
      "    wtot = len(nltk.word_tokenize(politics[i].text))\n",
      "    print(\"wtot\", wtot)\n",
      "    start = randint(0, wtot-n)\n",
      "    end = start + n\n",
      "    started = False\n",
      "    for s in sents:\n",
      "        w = nltk.word_tokenize(s)\n",
      "        wcount += len(w)\n",
      "        print(\"wcount\", wcount)\n",
      "        if wcount < start:\n",
      "            continue\n",
      "        elif started is False:\n",
      "            print(\"s-w\", wcount-start)\n",
      "            wcontrols_pol.append(s[wcount-start:])\n",
      "            started = True\n",
      "        elif wcount > end:\n",
      "            print(\"w-e\", wcount-end)\n",
      "            wcontrols_pol.append(s[:wcount-end])\n",
      "            break\n",
      "        else:\n",
      "            wcontrols_pol.append(s)\n",
      "    # get n-word consecutive subset of original text\n",
      "    count += 1\n",
      "    print(start, end)\n",
      "    print(sum([len(x) for x in wcontrols_pol]))\n",
      "time.process_time() - t\n",
      "122/456:\n",
      "t = time.process_time()\n",
      "wcontrols_pol = []\n",
      "count = 0\n",
      "for i in range(len(politics)):\n",
      "    print(\"i\", i)\n",
      "    if i > 1:\n",
      "        break\n",
      "    if i in [1646, 2145, 2331, 5396, 5569, 6193, 8035, 8425]:\n",
      "        continue\n",
      "    # get number of words in the summary\n",
      "    n = len(nltk.word_tokenize(summaries_pol[count]))\n",
      "    print(\"n\", n)\n",
      "    sents = nltk.sent_tokenize(politics[i].text)\n",
      "    wcount = 0\n",
      "    wtot = len(nltk.word_tokenize(politics[i].text))\n",
      "    print(\"wtot\", wtot)\n",
      "    start = randint(0, wtot-n)\n",
      "    end = start + n\n",
      "    started = False\n",
      "    for s in sents:\n",
      "        w = nltk.word_tokenize(s)\n",
      "        wcount += len(w)\n",
      "        print(\"wcount\", wcount)\n",
      "        if wcount < start:\n",
      "            continue\n",
      "        elif started is False:\n",
      "            print(\"s-w\", wcount-start)\n",
      "            wcontrols_pol.append(s[wcount-start:])\n",
      "            started = True\n",
      "        elif wcount > end:\n",
      "            print(\"w-e\", wcount-end)\n",
      "            wcontrols_pol.append(s[:len(w)+wcount-end])\n",
      "            break\n",
      "        else:\n",
      "            wcontrols_pol.append(s)\n",
      "    # get n-word consecutive subset of original text\n",
      "    count += 1\n",
      "    print(start, end)\n",
      "    print([len(x) for x in wcontrols_pol])\n",
      "time.process_time() - t\n",
      "122/457:\n",
      "t = time.process_time()\n",
      "wcontrols_pol = []\n",
      "count = 0\n",
      "for i in range(len(politics)):\n",
      "    print(\"i\", i)\n",
      "    if i > 1:\n",
      "        break\n",
      "    if i in [1646, 2145, 2331, 5396, 5569, 6193, 8035, 8425]:\n",
      "        continue\n",
      "    # get number of words in the summary\n",
      "    n = len(nltk.word_tokenize(summaries_pol[count]))\n",
      "    print(\"n\", n)\n",
      "    sents = nltk.sent_tokenize(politics[i].text)\n",
      "    wcount = 0\n",
      "    wtot = len(nltk.word_tokenize(politics[i].text))\n",
      "    print(\"wtot\", wtot)\n",
      "    start = randint(0, wtot-n)\n",
      "    end = start + n\n",
      "    started = False\n",
      "    for s in sents:\n",
      "        w = nltk.word_tokenize(s)\n",
      "        wcount += len(w)\n",
      "        print(\"wcount\", wcount)\n",
      "        if wcount < start:\n",
      "            continue\n",
      "        elif started is False:\n",
      "            print(\"s-w\", wcount-start)\n",
      "            wcontrols_pol.append(w[wcount-start:])\n",
      "            started = True\n",
      "        elif wcount > end:\n",
      "            print(\"w-e\", wcount-end)\n",
      "            wcontrols_pol.append(w[:len(w)+wcount-end])\n",
      "            break\n",
      "        else:\n",
      "            wcontrols_pol.append(w)\n",
      "    # get n-word consecutive subset of original text\n",
      "    count += 1\n",
      "    print(start, end)\n",
      "    print([len(x) for x in wcontrols_pol])\n",
      "time.process_time() - t\n",
      "122/458:\n",
      "t = time.process_time()\n",
      "wcontrols_pol = []\n",
      "count = 0\n",
      "for i in range(len(politics)):\n",
      "    print(\"i\", i)\n",
      "    if i > 1:\n",
      "        break\n",
      "    if i in [1646, 2145, 2331, 5396, 5569, 6193, 8035, 8425]:\n",
      "        continue\n",
      "    # get number of words in the summary\n",
      "    n = len(nltk.word_tokenize(summaries_pol[count]))\n",
      "    print(\"n\", n)\n",
      "    sents = nltk.sent_tokenize(politics[i].text)\n",
      "    wcount = 0\n",
      "    wtot = len(nltk.word_tokenize(politics[i].text))\n",
      "    print(\"wtot\", wtot)\n",
      "    start = randint(0, wtot-n)\n",
      "    end = start + n\n",
      "    started = False\n",
      "    for s in sents:\n",
      "        w = nltk.word_tokenize(s)\n",
      "        wcount += len(w)\n",
      "        print(\"wcount\", wcount)\n",
      "        if wcount < start:\n",
      "            continue\n",
      "        elif started is False:\n",
      "            print(\"s-w\", wcount-start)\n",
      "            wcontrols_pol.append(w[wcount-start:])\n",
      "            started = True\n",
      "        elif wcount > end:\n",
      "            print(\"w-e\", wcount-end)\n",
      "            wcontrols_pol.append(w[:len(w)+wcount-end])\n",
      "            break\n",
      "        else:\n",
      "            wcontrols_pol.append(w)\n",
      "    # get n-word consecutive subset of original text\n",
      "    count += 1\n",
      "    print(start, end)\n",
      "    print(sum([len(x) for x in wcontrols_pol]))\n",
      "time.process_time() - t\n",
      "122/459:\n",
      "t = time.process_time()\n",
      "wcontrols_pol = []\n",
      "count = 0\n",
      "for i in range(len(politics)):\n",
      "    print(\"i\", i)\n",
      "    if i > 1:\n",
      "        break\n",
      "    if i in [1646, 2145, 2331, 5396, 5569, 6193, 8035, 8425]:\n",
      "        continue\n",
      "    # get number of words in the summary\n",
      "    n = len(nltk.word_tokenize(summaries_pol[count]))\n",
      "    print(\"n\", n)\n",
      "    sents = nltk.sent_tokenize(politics[i].text)\n",
      "    wcount = 0\n",
      "    wtot = len(nltk.word_tokenize(politics[i].text))\n",
      "    print(\"wtot\", wtot)\n",
      "    start = randint(0, wtot-n)\n",
      "    end = start + n\n",
      "    started = False\n",
      "    for s in sents:\n",
      "        w = nltk.word_tokenize(s)\n",
      "        wcount += len(w)\n",
      "        print(\"wcount\", wcount)\n",
      "        if wcount < start:\n",
      "            continue\n",
      "        elif started is False:\n",
      "            print(\"s-w\", wcount-start)\n",
      "            wcontrols_pol.append(w[wcount-start:])\n",
      "            started = True\n",
      "        elif wcount > end:\n",
      "            print(\"w-e\", wcount-end)\n",
      "            wcontrols_pol.append(w[:len(w)+wcount-end])\n",
      "            break\n",
      "        else:\n",
      "            wcontrols_pol.append(w)\n",
      "    # get n-word consecutive subset of original text\n",
      "    count += 1\n",
      "    print(start, end)\n",
      "    print([len(x) for x in wcontrols_pol], sum([len(x) for x in wcontrols_pol]))\n",
      "time.process_time() - t\n",
      "122/460:\n",
      "t = time.process_time()\n",
      "wcontrols_pol = []\n",
      "count = 0\n",
      "for i in range(len(politics)):\n",
      "    print(\"i\", i)\n",
      "    if i > 1:\n",
      "        break\n",
      "    if i in [1646, 2145, 2331, 5396, 5569, 6193, 8035, 8425]:\n",
      "        continue\n",
      "    # get number of words in the summary\n",
      "    n = len(nltk.word_tokenize(summaries_pol[count]))\n",
      "    print(\"n\", n)\n",
      "    sents = nltk.sent_tokenize(politics[i].text)\n",
      "    wcount = 0\n",
      "    wtot = len(nltk.word_tokenize(politics[i].text))\n",
      "    print(\"wtot\", wtot)\n",
      "    start = randint(0, wtot-n)\n",
      "    end = start + n\n",
      "    started = False\n",
      "    for s in sents:\n",
      "        w = nltk.word_tokenize(s)\n",
      "        wcount += len(w)\n",
      "        print(\"wcount\", wcount)\n",
      "        if wcount < start:\n",
      "            continue\n",
      "        elif started is False:\n",
      "            print(\"s-w\", len(w)-start)\n",
      "            wcontrols_pol.append(w[len(w)-start:])\n",
      "            started = True\n",
      "        elif wcount > end:\n",
      "            print(\"w-e\", wcount-end)\n",
      "            wcontrols_pol.append(w[:len(w)+wcount-end])\n",
      "            break\n",
      "        else:\n",
      "            wcontrols_pol.append(w)\n",
      "    # get n-word consecutive subset of original text\n",
      "    count += 1\n",
      "    print(start, end)\n",
      "    print([len(x) for x in wcontrols_pol], sum([len(x) for x in wcontrols_pol]))\n",
      "time.process_time() - t\n",
      "122/461:\n",
      "t = time.process_time()\n",
      "wcontrols_pol = []\n",
      "count = 0\n",
      "for i in range(len(politics)):\n",
      "    print(\"i\", i)\n",
      "    if i > 1:\n",
      "        break\n",
      "    if i in [1646, 2145, 2331, 5396, 5569, 6193, 8035, 8425]:\n",
      "        continue\n",
      "    # get number of words in the summary\n",
      "    n = len(nltk.word_tokenize(summaries_pol[count]))\n",
      "    print(\"n\", n)\n",
      "    sents = nltk.sent_tokenize(politics[i].text)\n",
      "    wcount = 0\n",
      "    wtot = len(nltk.word_tokenize(politics[i].text))\n",
      "    print(\"wtot\", wtot)\n",
      "    start = randint(0, wtot-n)\n",
      "    end = start + n\n",
      "    started = False\n",
      "    for s in sents:\n",
      "        w = nltk.word_tokenize(s)\n",
      "        wcount += len(w)\n",
      "        print(\"wcount\", wcount)\n",
      "        if wcount < start:\n",
      "            continue\n",
      "        elif started is False:\n",
      "            print(\"s-w\", start-wcount+len(w))\n",
      "            wcontrols_pol.append(w[start-wcount+len(w):])\n",
      "            started = True\n",
      "        elif wcount > end:\n",
      "            print(\"w-e\", wcount-end)\n",
      "            wcontrols_pol.append(w[:len(w)+wcount-end])\n",
      "            break\n",
      "        else:\n",
      "            wcontrols_pol.append(w)\n",
      "    # get n-word consecutive subset of original text\n",
      "    count += 1\n",
      "    print(start, end)\n",
      "    print([len(x) for x in wcontrols_pol], sum([len(x) for x in wcontrols_pol]))\n",
      "time.process_time() - t\n",
      "122/462:\n",
      "t = time.process_time()\n",
      "wcontrols_pol = []\n",
      "count = 0\n",
      "for i in range(len(politics)):\n",
      "    print(\"i\", i)\n",
      "    if i > 1:\n",
      "        break\n",
      "    if i in [1646, 2145, 2331, 5396, 5569, 6193, 8035, 8425]:\n",
      "        continue\n",
      "    # get number of words in the summary\n",
      "    n = len(nltk.word_tokenize(summaries_pol[count]))\n",
      "    print(\"n\", n)\n",
      "    sents = nltk.sent_tokenize(politics[i].text)\n",
      "    wcount = 0\n",
      "    wtot = len(nltk.word_tokenize(politics[i].text))\n",
      "    print(\"wtot\", wtot)\n",
      "    start = randint(0, wtot-n)\n",
      "    end = start + n\n",
      "    started = False\n",
      "    for s in sents:\n",
      "        w = nltk.word_tokenize(s)\n",
      "        wcount += len(w)\n",
      "        print(\"wcount\", wcount)\n",
      "        if wcount < start:\n",
      "            continue\n",
      "        elif started is False:\n",
      "            print(\"s-w\", start-wcount+len(w))\n",
      "            wcontrols_pol.append(w[start-wcount+len(w):])\n",
      "            started = True\n",
      "        elif wcount > end:\n",
      "            print(\"w-e\", wcount-end)\n",
      "            wcontrols_pol.append(w[:len(w)-wcount+end])\n",
      "            break\n",
      "        else:\n",
      "            wcontrols_pol.append(w)\n",
      "    # get n-word consecutive subset of original text\n",
      "    count += 1\n",
      "    print(start, end)\n",
      "    print([len(x) for x in wcontrols_pol], sum([len(x) for x in wcontrols_pol]))\n",
      "time.process_time() - t\n",
      "122/463:\n",
      "t = time.process_time()\n",
      "wcontrols_pol = []\n",
      "count = 0\n",
      "tot = 0\n",
      "for i in range(len(politics)):\n",
      "    if i in [1646, 2145, 2331, 5396, 5569, 6193, 8035, 8425]:\n",
      "        continue\n",
      "    # get number of words in the summary\n",
      "    n = len(nltk.word_tokenize(summaries_pol[count]))\n",
      "    tot += n\n",
      "    sents = nltk.sent_tokenize(politics[i].text)\n",
      "    wcount = 0\n",
      "    wtot = len(nltk.word_tokenize(politics[i].text))\n",
      "    start = randint(0, wtot-n)\n",
      "    end = start + n\n",
      "    started = False\n",
      "    for s in sents:\n",
      "        w = nltk.word_tokenize(s)\n",
      "        wcount += len(w)\n",
      "        if wcount < start:\n",
      "            continue\n",
      "        elif started is False:\n",
      "            wcontrols_pol.append(w[start-wcount+len(w):])\n",
      "            started = True\n",
      "        elif wcount > end:\n",
      "            wcontrols_pol.append(w[:len(w)-wcount+end])\n",
      "            break\n",
      "        else:\n",
      "            wcontrols_pol.append(w)\n",
      "    # get n-word consecutive subset of original text\n",
      "    count += 1\n",
      "time.process_time() - t\n",
      "122/464: print(n, sum([len(x) for x in wcontrols_pol]))\n",
      "122/465: print(tot, sum([len(x) for x in wcontrols_pol]))\n",
      "122/466:\n",
      "t = time.process_time()\n",
      "docs = [simple_preprocess(s) for s in wcontrols_pol]\n",
      "mwcpol = Word2Vec(docs)\n",
      "time.process_time() - t\n",
      "122/467: wcontrols_pol\n",
      "122/468: controls_pol[:10]\n",
      "122/469: simple_preprocess(controls_pol[:10])\n",
      "122/470: swcontrols_pol = [\" \".join(x) for x in wcontrols_pol]\n",
      "122/471:\n",
      "t = time.process_time()\n",
      "docs = [simple_preprocess(s) for s in swcontrols_pol]\n",
      "mwcpol = Word2Vec(docs)\n",
      "time.process_time() - t\n",
      "122/472: swcontrols_pol[:10]\n",
      "122/473: gwcpol = getg(mwcpol)\n",
      "122/474: wcontrols_pol[:20]\n",
      "122/475: docs[0]\n",
      "122/476: docs[1]\n",
      "122/477: sum(len(x) for x in docs)\n",
      "122/478:\n",
      "words_spol = []\n",
      "for s in sums_pol:\n",
      "    words_spol += nltk.simple_preprocess(s)\n",
      "len(words_spol)\n",
      "122/479:\n",
      "words_spol = []\n",
      "for s in sums_pol:\n",
      "    words_spol += simple_preprocess(s)\n",
      "len(words_spol)\n",
      "122/480: bias(gwcpol, mwcpol, proflist)\n",
      "122/481: bias(gwcpol, mwcpol, gender_specific)\n",
      "122/482: bias(gpols, mpols, proflist), bias(gpols, mpols, gender_specific)\n",
      "122/483:\n",
      "t = time.process_time()\n",
      "wcontrols_pol = []\n",
      "count = 0\n",
      "tot = 0\n",
      "for i in range(len(politics)):\n",
      "    if i in [1646, 2145, 2331, 5396, 5569, 6193, 8035, 8425]:\n",
      "        continue\n",
      "    # get number of words in the summary\n",
      "    n = len(nltk.word_tokenize(summaries_pol[count]))\n",
      "    tot += n\n",
      "    sents = nltk.sent_tokenize(politics[i].text)\n",
      "    wcount = 0\n",
      "    wtot = len(nltk.word_tokenize(politics[i].text))\n",
      "    start = randint(0, wtot-n)\n",
      "    end = start + n\n",
      "    started = False\n",
      "    for s in sents:\n",
      "        w = nltk.word_tokenize(s)\n",
      "        wcount += len(w)\n",
      "        if wcount < start:\n",
      "            continue\n",
      "        elif started is False:\n",
      "            wcontrols_pol.append(w[start-wcount+len(w)-1:])\n",
      "            started = True\n",
      "        elif wcount > end:\n",
      "            wcontrols_pol.append(w[:len(w)-wcount+end+1])\n",
      "            break\n",
      "        else:\n",
      "            wcontrols_pol.append(w)\n",
      "    # get n-word consecutive subset of original text\n",
      "    count += 1\n",
      "time.process_time() - t\n",
      "122/484: swcontrols_pol = [\" \".join(x) for x in wcontrols_pol]\n",
      "122/485: print(tot, sum([len(x) for x in wcontrols_pol]))\n",
      "122/486: sum(len(x) for x in docs)\n",
      "122/487:\n",
      "t = time.process_time()\n",
      "docs = [simple_preprocess(s) for s in swcontrols_pol]\n",
      "mwcpol = Word2Vec(docs)\n",
      "time.process_time() - t\n",
      "122/488: sum(len(x) for x in docs)\n",
      "122/489: gwcpol = getg(mwcpol)\n",
      "122/490: bias(gwcpol, mwcpol, proflist)\n",
      "122/491: bias(gwcpol, mwcpol, gender_specific)\n",
      "122/492: bias(gpols, mpols, proflist)\n",
      "122/493:\n",
      "t = time.process_time()\n",
      "wcontrols_pol = []\n",
      "count = 0\n",
      "tot = 0\n",
      "for i in range(len(politics)):\n",
      "    if i in [1646, 2145, 2331, 5396, 5569, 6193, 8035, 8425]:\n",
      "        continue\n",
      "    # get number of words in the summary\n",
      "    n = len(nltk.word_tokenize(summaries_pol[count]))\n",
      "    tot += n\n",
      "    sents = nltk.sent_tokenize(politics[i].text)\n",
      "    wcount = 0\n",
      "    wtot = len(nltk.word_tokenize(politics[i].text))\n",
      "    start = randint(0, wtot-n)\n",
      "    end = start + n\n",
      "    started = False\n",
      "    for s in sents:\n",
      "        w = nltk.word_tokenize(s)\n",
      "        wcount += len(w)\n",
      "        if wcount < start:\n",
      "            continue\n",
      "        elif started is False:\n",
      "            wcontrols_pol.append(w[start-wcount+len(w)-1:])\n",
      "            started = True\n",
      "        elif wcount > end:\n",
      "            wcontrols_pol.append(w[:len(w)-wcount+end+2])\n",
      "            break\n",
      "        else:\n",
      "            wcontrols_pol.append(w)\n",
      "    # get n-word consecutive subset of original text\n",
      "    count += 1\n",
      "time.process_time() - t\n",
      "122/494: swcontrols_pol = [\" \".join(x) for x in wcontrols_pol]\n",
      "122/495: print(tot, sum([len(x) for x in wcontrols_pol]))\n",
      "122/496:\n",
      "t = time.process_time()\n",
      "docs = [simple_preprocess(s) for s in swcontrols_pol]\n",
      "mwcpol = Word2Vec(docs)\n",
      "time.process_time() - t\n",
      "122/497: sum(len(x) for x in docs)\n",
      "122/498: gwcpol = getg(mwcpol)\n",
      "122/499: bias(gwcpol, mwcpol, proflist)\n",
      "122/500: bias(gwcpol, mwcpol, gender_specific)\n",
      "122/501: bias(gpols, mpols, proflist)\n",
      "122/502:\n",
      "t = time.process_time()\n",
      "wcontrols_pol = []\n",
      "count = 0\n",
      "tot = 0\n",
      "for i in range(len(politics)):\n",
      "    if i in [1646, 2145, 2331, 5396, 5569, 6193, 8035, 8425]:\n",
      "        continue\n",
      "    # get number of words in the summary\n",
      "    n = len(nltk.word_tokenize(summaries_pol[count]))\n",
      "    tot += n\n",
      "    sents = nltk.sent_tokenize(politics[i].text)\n",
      "    wcount = 0\n",
      "    wtot = len(nltk.word_tokenize(politics[i].text))\n",
      "    start = randint(0, wtot-n)\n",
      "    end = start + n\n",
      "    started = False\n",
      "    for s in sents:\n",
      "        w = nltk.word_tokenize(s)\n",
      "        wcount += len(w)\n",
      "        if wcount < start:\n",
      "            continue\n",
      "        elif started is False:\n",
      "            wcontrols_pol.append(w[max(0, start-wcount+len(w)-2):])\n",
      "            started = True\n",
      "        elif wcount > end:\n",
      "            wcontrols_pol.append(w[:min(len(w), len(w)-wcount+end+2)])\n",
      "            break\n",
      "        else:\n",
      "            wcontrols_pol.append(w)\n",
      "    # get n-word consecutive subset of original text\n",
      "    count += 1\n",
      "time.process_time() - t\n",
      "122/503: swcontrols_pol = [\" \".join(x) for x in wcontrols_pol]\n",
      "122/504: print(tot, sum([len(x) for x in wcontrols_pol]))\n",
      "122/505:\n",
      "t = time.process_time()\n",
      "docs = [simple_preprocess(s) for s in swcontrols_pol]\n",
      "mwcpol = Word2Vec(docs)\n",
      "time.process_time() - t\n",
      "122/506: sum(len(x) for x in docs)\n",
      "122/507: gwcpol = getg(mwcpol)\n",
      "122/508: bias(gwcpol, mwcpol, proflist)\n",
      "122/509: bias(gwcpol, mwcpol, gender_specific)\n",
      "122/510: bias(gpols, mpols, proflist)\n",
      "122/511: np.random()\n",
      "122/512: random.random()\n",
      "122/513: random.random()\n",
      "122/514: random.random()\n",
      "122/515: random.random()\n",
      "122/516: random.random()\n",
      "122/517: random.random()\n",
      "122/518: random.randint(1, 2)\n",
      "122/519: random.randint(1, 2)\n",
      "122/520: random.randint(1, 2)\n",
      "122/521: random.randint(1, 2)\n",
      "122/522: random.randint(1, 2)\n",
      "122/523:\n",
      "t = time.process_time()\n",
      "wcontrols_pol = []\n",
      "count = 0\n",
      "tot = 0\n",
      "for i in range(len(politics)):\n",
      "    if i in [1646, 2145, 2331, 5396, 5569, 6193, 8035, 8425]:\n",
      "        continue\n",
      "    # get number of words in the summary\n",
      "    n = len(nltk.word_tokenize(summaries_pol[count]))\n",
      "    tot += n\n",
      "    sents = nltk.sent_tokenize(politics[i].text)\n",
      "    wcount = 0\n",
      "    wtot = len(nltk.word_tokenize(politics[i].text))\n",
      "    start = randint(0, wtot-n)\n",
      "    end = start + n\n",
      "    started = False\n",
      "    for s in sents:\n",
      "        w = nltk.word_tokenize(s)\n",
      "        wcount += len(w)\n",
      "        if wcount < start:\n",
      "            continue\n",
      "        elif started is False:  \n",
      "            wcontrols_pol.append(w[max(0, start-wcount+len(w)-random.randint(1,2)):])\n",
      "            started = True\n",
      "        elif wcount > end:\n",
      "            wcontrols_pol.append(w[:min(len(w), len(w)-wcount+end+2)])\n",
      "            break\n",
      "        else:\n",
      "            wcontrols_pol.append(w)\n",
      "    # get n-word consecutive subset of original text\n",
      "    count += 1\n",
      "time.process_time() - t\n",
      "122/524: swcontrols_pol = [\" \".join(x) for x in wcontrols_pol]\n",
      "122/525: print(tot, sum([len(x) for x in wcontrols_pol]))\n",
      "122/526:\n",
      "t = time.process_time()\n",
      "docs = [simple_preprocess(s) for s in swcontrols_pol]\n",
      "mwcpol = Word2Vec(docs)\n",
      "time.process_time() - t\n",
      "122/527: sum(len(x) for x in docs)\n",
      "122/528: gwcpol = getg(mwcpol)\n",
      "122/529: bias(gwcpol, mwcpol, proflist)\n",
      "122/530: bias(gwcpol, mwcpol, gender_specific)\n",
      "122/531: bias(gpols, mpols, proflist)\n",
      "122/532:\n",
      "x = np.array([gproj(gpols, mpols, w) for w in \\\n",
      "              filter(lambda x: x in mpols.wv.vocab, gender_specific)])\n",
      "y = np.array([gproj(gwcpol, mwcpol, w) for w in \\\n",
      "              filter(lambda x: x in mpols.wv.vocab, gender_specific)])\n",
      "slope, intercept, r_value, p_value, std_err = stats.linregress(x,y)\n",
      "slope, p_value\n",
      "122/533:\n",
      "x = np.array([gproj(gpols, mpols, w) for w in \\\n",
      "              filter(lambda x: x in mpols.wv.vocab and mwcpol.wv.vocab, gender_specific)])\n",
      "y = np.array([gproj(gwcpol, mwcpol, w) for w in \\\n",
      "              filter(lambda x: x in mpols.wv.vocab and mwcpol.wv.vocab, gender_specific)])\n",
      "slope, intercept, r_value, p_value, std_err = stats.linregress(x,y)\n",
      "slope, p_value\n",
      "122/534:\n",
      "x = np.array([gproj(gpols, mpols, w) for w in \\\n",
      "              filter(lambda x: x in mpols.wv.vocab and x in mwcpol.wv.vocab, gender_specific)])\n",
      "y = np.array([gproj(gwcpol, mwcpol, w) for w in \\\n",
      "              filter(lambda x: x in mpols.wv.vocab and x in mwcpol.wv.vocab, gender_specific)])\n",
      "slope, intercept, r_value, p_value, std_err = stats.linregress(x,y)\n",
      "slope, p_value\n",
      "122/535:\n",
      "travel = list(filter(lambda x: 'Politics and Government' not in x.descriptors, \\\n",
      "                        alld['Travel and Vacations'][1]))\n",
      "122/536:\n",
      "len(travel)\n",
      "t = time.process_time()\n",
      "mtrav = getw2v(travel)\n",
      "time.process_time() - t\n",
      "122/537: len(travel)\n",
      "122/538: len(politics)\n",
      "122/539:\n",
      "all_list = sorted([(d, all_[d][0], all_[d][1]) for d in all_], key=lambda x: x[1], reverse=True)\n",
      "for x in range(10):\n",
      "    print(all_list[x][0:2])\n",
      "122/540:\n",
      "t = time.process_time()\n",
      "alld_ = {}\n",
      "for doc in all_:\n",
      "    doc = Doc(doc['id'], doc['gdescriptors'], doc['descriptors'], doc['text'])\n",
      "    for d in doc.descriptors:\n",
      "        count, docs = alld_.get(d, (0, set()))\n",
      "        docs.add(doc)\n",
      "        alld_[d] = (count + 1, docs)\n",
      "time.process_time() - t\n",
      "122/541:\n",
      "t = time.process_time()\n",
      "alld_ = {}\n",
      "for doc in l:\n",
      "    doc = Doc(doc['id'], doc['gdescriptors'], doc['descriptors'], doc['text'])\n",
      "    for d in doc.descriptors:\n",
      "        count, docs = alld_.get(d, (0, set()))\n",
      "        docs.add(doc)\n",
      "        alld_[d] = (count + 1, docs)\n",
      "time.process_time() - t\n",
      "123/1:\n",
      "import xml.etree.ElementTree as ET\n",
      "import os\n",
      "from typing import List\n",
      "from functools import reduce\n",
      "from calendar import monthrange\n",
      "import random\n",
      "import matplotlib.pyplot as plt\n",
      "import numpy as np\n",
      "import json\n",
      "123/2: %history -g\n",
      "123/3: %history -g -f Thesis_recover\n",
      "123/4: import time\n",
      "123/5:\n",
      "from bs4 import BeautifulSoup\n",
      "import urllib.request as urllib2\n",
      "import csv\n",
      "123/6:\n",
      "url = 'https://www.enchantedlearning.com/wordlist/adjectivesforpeople.shtml'\n",
      "file = urllib2.urlopen(url)\n",
      "html = file.read()\n",
      "file.close()\n",
      "soup = BeautifulSoup(html)\n",
      "123/7: adj = soup.find_all(attrs={\"class\": \"wordlist-item\"})\n",
      "123/8: adjs = [x.contents[0] for x in adj]\n",
      "123/9: adjs[:5]\n",
      "123/10:\n",
      "with open('adjectives.csv', mode='w') as f:\n",
      "    writer = csv.writer(f, delimiter=',')\n",
      "    writer.writerow(adjs)\n",
      "123/11:\n",
      "with open('adjectives.csv') as csv_file:\n",
      "    adjectives = csv_file.read().split(\",\")\n",
      "123/12:\n",
      "class Doc:\n",
      "    def __init__(self, id: int, gdescriptors: List[str], descriptors: List[str], text: str) -> None:\n",
      "        self.id = id\n",
      "        self.gdescriptors = gdescriptors\n",
      "        self.descriptors = descriptors\n",
      "        self.text = text\n",
      "123/13:\n",
      "def parse_doc(file: str) -> Doc:\n",
      "    tree = ET.parse(file)\n",
      "    root = tree.getroot()\n",
      "    try:\n",
      "        fulltext = root.find('body').find('body.content').find(\"*[@class='full_text']\")\n",
      "        text = reduce(lambda x, y: x + y, [x.text for x in fulltext.findall('p')])\n",
      "        id = int(root.find('head').find('docdata').find('doc-id').attrib['id-string'])\n",
      "        classifiers = root.find('head').find('docdata').find('identified-content')\n",
      "        gclassifiers = classifiers.findall(\"*[@class='online_producer'][@type='general_descriptor']\")\n",
      "        dclassifiers = classifiers.findall(\"*[@class='online_producer'][@type='descriptor']\")\n",
      "        gdescriptors = [c.text for c in gclassifiers]\n",
      "        descriptors = [c.text for c in dclassifiers]\n",
      "        return Doc(id, gdescriptors, descriptors, text)\n",
      "    except:\n",
      "        return None\n",
      "123/14:\n",
      "def get_docs_by_month(year: int, month: int) -> List[Doc]:\n",
      "    days = monthrange(year, month)[1]\n",
      "    docs = []\n",
      "    for day in range(1, days+1):\n",
      "        for file in os.listdir(f'data/{year}/{month:02}/{day:02}/'):\n",
      "            doc = parse_doc(f'data/{year}/{month:02}/{day:02}/{file}')\n",
      "            if doc:\n",
      "                docs.append(doc)\n",
      "    return docs\n",
      "123/15:\n",
      "t = time.process_time()\n",
      "all_ = []\n",
      "for y in range(1988, 2007):\n",
      "    print(y)\n",
      "    for i in range(1,13):\n",
      "        all_ += get_docs_by_month(y, i)\n",
      "time.process_time() - t\n",
      "123/16:\n",
      "t = time.process_time()\n",
      "all_ = []\n",
      "for y in range(2000, 2007):\n",
      "    print(y)\n",
      "    for i in range(1,13):\n",
      "        all_ += get_docs_by_month(y, i)\n",
      "time.process_time() - t\n",
      "123/17:\n",
      "t = time.process_time()\n",
      "all_ = []\n",
      "for y in range(1988, 2007): #1988 is earliest but takes a while to run\n",
      "    print(y)\n",
      "    for i in range(1,13):\n",
      "        all_ += get_docs_by_month(y, i)\n",
      "time.process_time() - t\n",
      "123/18: len(all_)\n",
      "123/19:\n",
      "t = time.process_time()\n",
      "alld = {}\n",
      "for doc in all_:\n",
      "    doc = Doc(doc['id'], doc['gdescriptors'], doc['descriptors'], doc['text'])\n",
      "    for d in doc.descriptors:\n",
      "        count, docs = alld.get(d, (0, set()))\n",
      "        docs.add(doc)\n",
      "        alld[d] = (count + 1, docs)\n",
      "time.process_time() - t\n",
      "123/20: len(all_)\n",
      "123/21: all_[0]\n",
      "123/22:\n",
      "t = time.process_time()\n",
      "alld = {}\n",
      "for doc in all_:\n",
      "    doc = Doc(doc['id'], doc['gdescriptors'], doc['descriptors'], doc['text'])\n",
      "    for d in doc.descriptors:\n",
      "        count, docs = alld.get(d, (0, set()))\n",
      "        docs.add(doc)\n",
      "        alld[d] = (count + 1, docs)\n",
      "time.process_time() - t\n",
      "123/23: all_[0]['id']\n",
      "123/24: all_[0]\n",
      "123/25: all_[0].id\n",
      "123/26:\n",
      "t = time.process_time()\n",
      "alld = {}\n",
      "for doc in all_:\n",
      "    doc = Doc(doc.id, doc.gdescriptors, doc.descriptors, doc.text)\n",
      "    for d in doc.descriptors:\n",
      "        count, docs = alld.get(d, (0, set()))\n",
      "        docs.add(doc)\n",
      "        alld[d] = (count + 1, docs)\n",
      "time.process_time() - t\n",
      "123/27:\n",
      "alldlist = sorted([(d, alld[d][0], alld[d][1]) for d in alld], key=lambda x: x[1], reverse=True)\n",
      "for x in range(15):\n",
      "    print(alldlist[x][0:2])\n",
      "123/28:\n",
      "politics = list(filter(lambda x: 'Computers and the Internet' not in x.descriptors, \\\n",
      "                        alld['Politics and Government'][1]))\n",
      "123/29:\n",
      "from gensim.models import Word2Vec\n",
      "from gensim.utils import simple_preprocess\n",
      "124/1:\n",
      "import xml.etree.ElementTree as ET\n",
      "import os\n",
      "from typing import List\n",
      "from functools import reduce\n",
      "from calendar import monthrange\n",
      "import random\n",
      "import matplotlib.pyplot as plt\n",
      "import numpy as np\n",
      "import json\n",
      "124/2: import time\n",
      "124/3:\n",
      "from bs4 import BeautifulSoup\n",
      "import urllib.request as urllib2\n",
      "import csv\n",
      "125/1:\n",
      "import xml.etree.ElementTree as ET\n",
      "import os\n",
      "from typing import List\n",
      "from functools import reduce\n",
      "from calendar import monthrange\n",
      "import random\n",
      "import matplotlib.pyplot as plt\n",
      "import numpy as np\n",
      "import json\n",
      "123/30: adjs[:5]\n",
      "125/2:\n",
      "import xml.etree.ElementTree as ET\n",
      "import os\n",
      "from typing import List\n",
      "from functools import reduce\n",
      "from calendar import monthrange\n",
      "import random\n",
      "import matplotlib.pyplot as plt\n",
      "import numpy as np\n",
      "import json\n",
      "125/3: import time\n",
      "125/4:\n",
      "from bs4 import BeautifulSoup\n",
      "import urllib.request as urllib2\n",
      "import csv\n",
      "125/5:\n",
      "url = 'https://www.enchantedlearning.com/wordlist/adjectivesforpeople.shtml'\n",
      "file = urllib2.urlopen(url)\n",
      "html = file.read()\n",
      "file.close()\n",
      "soup = BeautifulSoup(html)\n",
      "125/6: adj = soup.find_all(attrs={\"class\": \"wordlist-item\"})\n",
      "125/7: adjs = [x.contents[0] for x in adj]\n",
      "125/8: adjs[:5]\n",
      "125/9:\n",
      "with open('adjectives.csv', mode='w') as f:\n",
      "    writer = csv.writer(f, delimiter=',')\n",
      "    writer.writerow(adjs)\n",
      "125/10:\n",
      "with open('adjectives.csv') as csv_file:\n",
      "    adjectives = csv_file.read().split(\",\")\n",
      "125/11:\n",
      "class Doc:\n",
      "    def __init__(self, id: int, gdescriptors: List[str], descriptors: List[str], text: str) -> None:\n",
      "        self.id = id\n",
      "        self.gdescriptors = gdescriptors\n",
      "        self.descriptors = descriptors\n",
      "        self.text = text\n",
      "125/12:\n",
      "def parse_doc(file: str) -> Doc:\n",
      "    tree = ET.parse(file)\n",
      "    root = tree.getroot()\n",
      "    try:\n",
      "        fulltext = root.find('body').find('body.content').find(\"*[@class='full_text']\")\n",
      "        text = reduce(lambda x, y: x + y, [x.text for x in fulltext.findall('p')])\n",
      "        id = int(root.find('head').find('docdata').find('doc-id').attrib['id-string'])\n",
      "        classifiers = root.find('head').find('docdata').find('identified-content')\n",
      "        gclassifiers = classifiers.findall(\"*[@class='online_producer'][@type='general_descriptor']\")\n",
      "        dclassifiers = classifiers.findall(\"*[@class='online_producer'][@type='descriptor']\")\n",
      "        gdescriptors = [c.text for c in gclassifiers]\n",
      "        descriptors = [c.text for c in dclassifiers]\n",
      "        return Doc(id, gdescriptors, descriptors, text)\n",
      "    except:\n",
      "        return None\n",
      "125/13:\n",
      "def get_docs_by_month(year: int, month: int) -> List[Doc]:\n",
      "    days = monthrange(year, month)[1]\n",
      "    docs = []\n",
      "    for day in range(1, days+1):\n",
      "        for file in os.listdir(f'data/{year}/{month:02}/{day:02}/'):\n",
      "            doc = parse_doc(f'data/{year}/{month:02}/{day:02}/{file}')\n",
      "            if doc:\n",
      "                docs.append(doc)\n",
      "    return docs\n",
      "125/14:\n",
      "t = time.process_time()\n",
      "all_ = []\n",
      "for y in range(1988, 2007):\n",
      "    print(y)\n",
      "    for i in range(1,13):\n",
      "        all_ += get_docs_by_month(y, i)\n",
      "time.process_time() - t\n",
      "125/15: len(all_)\n",
      "125/16:\n",
      "t = time.process_time()\n",
      "alld = {}\n",
      "for doc in all_:\n",
      "    doc = Doc(doc.id, doc.gdescriptors, doc.descriptors, doc.text)\n",
      "    for d in doc.descriptors:\n",
      "        count, docs = alld.get(d, (0, set()))\n",
      "        docs.add(doc)\n",
      "        alld[d] = (count + 1, docs)\n",
      "time.process_time() - t\n",
      "125/17:\n",
      "alldlist = sorted([(d, alld[d][0], alld[d][1]) for d in alld], key=lambda x: x[1], reverse=True)\n",
      "for x in range(15):\n",
      "    print(alldlist[x][0:2])\n",
      "125/18:\n",
      "with open('alldocs.json', 'w') as outfile:\n",
      "    json.dump([x.__dict__ for x in all_], outfile)\n",
      "125/19:\n",
      "def save(i, arr):\n",
      "    t = time.process_time()\n",
      "    with open('alldocs' + str(i) + '.json', 'w') as outfile:\n",
      "        json.dump([x.__dict__ for x in arr], outfile)\n",
      "    print(time.process_time() - t)\n",
      "125/20:\n",
      "def load(i):\n",
      "    t = time.process_time()\n",
      "    with open('alldocs' + str(i) + '.json', 'r') as infile:\n",
      "        l = json.load(infile)\n",
      "    print(time.process_time() - t)\n",
      "    return l\n",
      "125/21:\n",
      "computers = list(filter(lambda x: 'Politics and Government' not in x.descriptors, \\\n",
      "                        alld['Computers and the Internet'][1]))\n",
      "125/22:\n",
      "politics = list(filter(lambda x: 'Computers and the Internet' not in x.descriptors, \\\n",
      "                        alld['Politics and Government'][1]))\n",
      "125/23:\n",
      "politics = list(filter(lambda x: 'Computers and the Internet' not in x.descriptors, \\\n",
      "                        alld['Politics and Government'][1]))\n",
      "125/24:\n",
      "trsvel = list(filter(lambda x: 'Politics and Government' not in x.descriptors, \\\n",
      "                        alld['Travel and Vacations'][1]))\n",
      "125/25:\n",
      "travel = list(filter(lambda x: 'Politics and Government' not in x.descriptors, \\\n",
      "                        alld['Travel and Vacations'][1]))\n",
      "125/26: len(computers), len(politics), len(travel)\n",
      "125/27:\n",
      "from gensim.models import Word2Vec\n",
      "from gensim.utils import simple_preprocess\n",
      "125/28:\n",
      "def getw2v(category):\n",
      "    docs = [simple_preprocess(t.text) for t in category]\n",
      "    return Word2Vec(docs)\n",
      "125/29:\n",
      "t = time.process_time()\n",
      "mpol = getw2v(politics)\n",
      "time.process_time() - t\n",
      "125/30: mpol.wv.most_similar(positive=['good'], topn=5)\n",
      "125/31: mpol.wv.most_similar(positive=['thursday'], topn=5)\n",
      "125/32: mpol.wv.most_similar(positive=['approve'], topn=5)\n",
      "125/33:\n",
      "t = time.process_time()\n",
      "mcomp = getw2v(computers)\n",
      "time.process_time() - t\n",
      "125/34:\n",
      "t = time.process_time()\n",
      "mtrav = getw2v(travel)\n",
      "time.process_time() - t\n",
      "125/35: mtrav.wv.most_similar(positive=['approve'], topn=5)\n",
      "125/36: mtrav.wv.most_similar(positive=['good'], topn=5)\n",
      "125/37: mcomp.wv.most_similar(positive=['good'], topn=5)\n",
      "125/38:\n",
      "from gensim.summarization.summarizer import summarize\n",
      "from gensim.models import LsiModel\n",
      "from gensim.test.utils import common_dictionary, common_corpus\n",
      "125/39:\n",
      "def getsummaries(category):\n",
      "    s = []\n",
      "    for i in range(len(category)):\n",
      "        try:\n",
      "            s.append(summarize(category[i].text))\n",
      "        except:\n",
      "            print(i)\n",
      "    return s\n",
      "125/40:\n",
      "t = time.process_time()\n",
      "summaries_pol = getsummaries(politics)\n",
      "time.process_time() - t\n",
      "125/41:\n",
      "t = time.process_time()\n",
      "summaries_comp = getsummaries(computers)\n",
      "time.process_time() - t\n",
      "125/42:\n",
      "t = time.process_time()\n",
      "summaries_trav = getsummaries(travel)\n",
      "time.process_time() - t\n",
      "125/43:\n",
      "def getw2v_s(sums):\n",
      "    docs = [simple_preprocess(t) for t in sums]\n",
      "    return Word2Vec(docs)\n",
      "125/44: mcomps = getw2v_s(summaries_comp)\n",
      "125/45: mpols = getw2v_s(summaries_pol)\n",
      "125/46: mtravs = getw2v_s(summaries_trav)\n",
      "125/47:\n",
      "with open('debiaswe-master/data/equalize_pairs.json') as f:\n",
      "    pairs = json.loads(f.read())\n",
      "125/48:\n",
      "from sklearn.decomposition import PCA\n",
      "from matplotlib.pyplot import bar\n",
      "125/49: pairs = list(map(lambda x: [x[0].lower(), x[1].lower()], pairs))\n",
      "125/50: pairs\n",
      "125/51: pairs[:5]\n",
      "125/52:\n",
      "def getg(m):\n",
      "    matrix = []\n",
      "    for a, b in pairs:\n",
      "        if a not in m.wv.vocab or b not in m.wv.vocab:\n",
      "            continue\n",
      "        center = (m.wv.get_vector(a) + m.wv.get_vector(b))/2\n",
      "        matrix.append(m.wv.get_vector(a) - center)\n",
      "        matrix.append(m.wv.get_vector(b) - center)\n",
      "    matrix = np.array(matrix)\n",
      "    p = PCA(n_components = 10)\n",
      "    p.fit(matrix)\n",
      "    bar(range(10), p.explained_variance_ratio_)\n",
      "    print(matrix.shape)\n",
      "    return p.components_[0]\n",
      "125/53: gpol = getg(mpol)\n",
      "125/54: gtrav = getg(mtrav)\n",
      "125/55: gcomp = getg(mcomp)\n",
      "125/56:\n",
      "def getw2v(category):\n",
      "    docs = []\n",
      "    for t in category:\n",
      "        for x in t.text.split(\".\"):\n",
      "            docs.append(simple_preprocess(x))\n",
      "    return Word2Vec(docs)\n",
      "125/57:\n",
      "t = time.process_time()\n",
      "mpol = getw2v(politics)\n",
      "time.process_time() - t\n",
      "125/58:\n",
      "def getw2v(category):\n",
      "    sentences = []\n",
      "    for t in category:\n",
      "        sentences += nltk.sent_tokenize(t.text)\n",
      "    docs = [simple_preprocess(s) for s in sentences]\n",
      "    return Word2Vec(docs, size=size)\n",
      "125/59:\n",
      "t = time.process_time()\n",
      "mpol = getw2v(politics)\n",
      "time.process_time() - t\n",
      "125/60: import nltk\n",
      "125/61:\n",
      "def getw2v(category):\n",
      "    sentences = []\n",
      "    for t in category:\n",
      "        sentences += nltk.sent_tokenize(t.text)\n",
      "    docs = [simple_preprocess(s) for s in sentences]\n",
      "    return Word2Vec(docs, size=size)\n",
      "125/62:\n",
      "t = time.process_time()\n",
      "mpol = getw2v(politics)\n",
      "time.process_time() - t\n",
      "125/63:\n",
      "def getw2v(category):\n",
      "    sentences = []\n",
      "    for t in category:\n",
      "        sentences += nltk.sent_tokenize(t.text)\n",
      "    docs = [simple_preprocess(s) for s in sentences]\n",
      "    return Word2Vec(docs)\n",
      "125/64:\n",
      "t = time.process_time()\n",
      "mpol = getw2v(politics)\n",
      "time.process_time() - t\n",
      "125/65:\n",
      "t = time.process_time()\n",
      "mcomp = getw2v(computers)\n",
      "time.process_time() - t\n",
      "125/66:\n",
      "t = time.process_time()\n",
      "mtrav = getw2v(travel)\n",
      "time.process_time() - t\n",
      "125/67:\n",
      "mpol.wv.most_similar(positive=['approve'], topn=3), \n",
      "mcomp.wv.most_similar(positive=['approve'], topn=3),\n",
      "mtrav.wv.most_similar(positive=['approve'], topn=3)\n",
      "125/68:\n",
      "print(mpol.wv.most_similar(positive=['approve'], topn=3), \n",
      "mcomp.wv.most_similar(positive=['approve'], topn=3),\n",
      "mtrav.wv.most_similar(positive=['approve'], topn=3))\n",
      "125/69: mpol.wv.most_similar(positive=['approve'], topn=3)\n",
      "125/70: mcomp.wv.most_similar(positive=['approve'], topn=3)\n",
      "125/71: mtrav.wv.most_similar(positive=['approve'], topn=3)\n",
      "125/72:\n",
      "def getw2v_s(sums):\n",
      "    sentences = []\n",
      "    for t in sums:\n",
      "        sentences += nltk.sent_tokenize(t)\n",
      "    docs = [simple_preprocess(s) for s in sentences]\n",
      "    return Word2Vec(docs)\n",
      "125/73: mpols = getw2v_s(summaries_pol)\n",
      "125/74: mcomps = getw2v_s(summaries_comp)\n",
      "125/75: mtravs = getw2v_s(summaries_trav)\n",
      "125/76:\n",
      "def getg(m):\n",
      "    matrix = []\n",
      "    for a, b in pairs:\n",
      "        if a not in m.wv.vocab or b not in m.wv.vocab:\n",
      "            continue\n",
      "        center = (m.wv.get_vector(a) + m.wv.get_vector(b))/2\n",
      "        matrix.append(m.wv.get_vector(a) - center)\n",
      "        matrix.append(m.wv.get_vector(b) - center)\n",
      "    matrix = np.array(matrix)\n",
      "    p = PCA(n_components = 10)\n",
      "    p.fit(matrix)\n",
      "    bar(range(10), p.explained_variance_ratio_)\n",
      "    print(matrix.shape)\n",
      "    return p.components_[0]\n",
      "125/77: gpol = getg(mpol)\n",
      "125/78: gcomp = getg(mcomp)\n",
      "125/79: gtrav = getg(mtrav)\n",
      "125/80:\n",
      "def gproj(g, m, w1):\n",
      "    wvec = m.wv.get_vector(w1)\n",
      "    p = np.dot(g, wvec)/np.linalg.norm(g)\n",
      "    return p\n",
      "125/81:\n",
      "wordlist = ['he', 'she', 'doctor', 'nurse', 'man', 'woman', 'male', 'female', 'politician', 'guy', \\\n",
      "            'scientist', 'teacher', 'professor', 'mother', 'pretty', 'handsome', 'strong', 'coward', \\\n",
      "            'dress', 'timid', 'leadership', 'blond', 'lovely', 'petite', 'surgeon', 'physician', 'shopkeeper', \\\n",
      "            'cooking', 'sewing', 'chuckle', 'sassy', 'lanky', 'competent', 'brilliant', 'capable', 'smart', \\\n",
      "            'intelligent', 'incompetent', 'librarian', 'cosmetics', 'maid', 'clever', 'dressed', 'captain', \\\n",
      "            'nanny', 'philosopher', 'architect', 'financier', 'warrior', 'magician', 'housekeeper', 'homemaker']\n",
      "125/82: len(wordlist)\n",
      "125/83:\n",
      "for x in filter(lambda x: x in mpol.wv.vocab, wordlist):\n",
      "    print(x, gproj(gpol, mpol, x))\n",
      "125/84:\n",
      "with open('debiaswe-master/data/professions.json') as f:\n",
      "    profs = json.loads(f.read())\n",
      "125/85: proflist = list(filter(lambda x: x in model.wv.vocab, [p[0] for p in profs if abs(p[1]) < 0.6]))\n",
      "125/86: proflist = list(filter(lambda x: x in mpol.wv.vocab, [p[0] for p in profs if abs(p[1]) < 0.6]))\n",
      "125/87: len(proflist)\n",
      "125/88: proflist\n",
      "125/89:\n",
      "def bias(g, m, wordlist):\n",
      "    b = 0\n",
      "    l = list(filter(lambda x: x in m.wv.vocab, wordlist))\n",
      "    for x in l:\n",
      "        b += gproj(g, m, x)**2\n",
      "    return sqrt(b)/len(l)\n",
      "125/90: bias(gpol, mpol, proflist)\n",
      "125/91:\n",
      "def bias(g, m, wordlist):\n",
      "    b = 0\n",
      "    l = list(filter(lambda x: x in m.wv.vocab, wordlist))\n",
      "    for x in l:\n",
      "        b += gproj(g, m, x)**2\n",
      "    return np.sqrt(b)/len(l)\n",
      "125/92: bias(gpol, mpol, proflist)\n",
      "125/93:\n",
      "def bias(g, m, wordlist):\n",
      "    b = 0\n",
      "    l = list(filter(lambda x: x in m.wv.vocab, wordlist))\n",
      "    for x in l:\n",
      "        b += gproj(g, m, x)**2\n",
      "    return b/len(l)\n",
      "125/94: bias(gpol, mpol, proflist)\n",
      "125/95:\n",
      "def bias(g, m, wordlist):\n",
      "    b = 0\n",
      "    l = list(filter(lambda x: x in m.wv.vocab, wordlist))\n",
      "    for x in l:\n",
      "        b += gproj(g, m, x)**2\n",
      "    return np.sqrt(b/len(l))\n",
      "125/96: bias(gpol, mpol, proflist)\n",
      "125/97: bias(gpols, mpols, adjectives)\n",
      "125/98:\n",
      "res = sorted([(x, gproj(gpol, mpol, x)) for x in filter(lambda x: x in mpol.wv.vocab, wordlist)], key=lambda x: x[1])\n",
      "res\n",
      "125/99: pairs\n",
      "125/100: proflist[:5]\n",
      "125/101: proflist[:10]\n",
      "125/102: proflist[:5]\n",
      "125/103:\n",
      "def testbias(wordlist):\n",
      "    print(f\"politics: {bias(gpol, mpol, wordlist)}\")\n",
      "    print(f\"politics summary: {bias(gpols, mpols, wordlist)}\")\n",
      "    print(f\"computers: {bias(gcomp, mcomp, wordlist)}\")\n",
      "    print(f\"computers summary: {bias(gcomps, mcomps, wordlist)}\")\n",
      "    print(f\"travel: {bias(gtrav, mtrav, wordlist)}\")\n",
      "    print(f\"travel summary: {bias(gtravs, mtravs, wordlist)}\")\n",
      "125/104:\n",
      "wordlist = ['he', 'she', 'doctor', 'nurse', 'man', 'woman', 'male', 'female', 'politician', 'guy', \\\n",
      "            'scientist', 'teacher', 'professor', 'mother', 'pretty', 'handsome', 'strong', 'coward', \\\n",
      "            'timid', 'leadership', 'blond', 'lovely', 'petite', 'surgeon', 'physician', 'shopkeeper', \\\n",
      "            'cooking', 'sewing', 'chuckle', 'sassy', 'lanky', 'competent', 'brilliant', 'capable', 'smart', \\\n",
      "            'intelligent', 'incompetent', 'librarian', 'cosmetics', 'maid', 'clever', 'dressed', 'captain', \\\n",
      "            'nanny', 'philosopher', 'architect', 'financier', 'warrior', 'magician', 'housekeeper', 'homemaker']\n",
      "125/105: res = sorted([(x, gproj(gpol, mpol, x)) for x in filter(lambda x: x in mpol.wv.vocab, wordlist)], key=lambda x: x[1])\n",
      "125/106:\n",
      "res = sorted([(x, gproj(gpol, mpol, x)) for x in filter(lambda x: x in mpol.wv.vocab, wordlist)], key=lambda x: x[1])\n",
      "for i in range(20):\n",
      "    print(res[i] + \"\\t\" + res[-i])\n",
      "125/107:\n",
      "res = sorted([(x, gproj(gpol, mpol, x)) for x in filter(lambda x: x in mpol.wv.vocab, wordlist)], key=lambda x: x[1])\n",
      "for i in range(20):\n",
      "    print(res[i][0] + res[i][1] + \"\\t\" + res[-i][0] + res[-i][1])\n",
      "125/108:\n",
      "res = sorted([(x, gproj(gpol, mpol, x)) for x in filter(lambda x: x in mpol.wv.vocab, wordlist)], key=lambda x: x[1])\n",
      "for i in range(20):\n",
      "    print(f\"{res[i][0]} {str(res[i][1])} \\t {res[-i][0]} {res[-i][1]}\")\n",
      "125/109:\n",
      "res = sorted([(x, gproj(gpol, mpol, x)) for x in filter(lambda x: x in mpol.wv.vocab, wordlist)], key=lambda x: x[1])\n",
      "for i in range(20):\n",
      "    print(f\"{res[i][0]} {res[i][1]} \\t {res[-i][0]} {res[-i][1]}\")\n",
      "125/110:\n",
      "res = sorted([(x, gproj(gpol, mpol, x)) for x in filter(lambda x: x in mpol.wv.vocab, wordlist)], key=lambda x: x[1])\n",
      "for i in range(20):\n",
      "    print(f\"{res[i][0]} {res[i][1]} \\t\\t {res[-i][0]} {res[-i][1]}\")\n",
      "125/111:\n",
      "res = sorted([(x, gproj(gpol, mpol, x)) for x in filter(lambda x: x in mpol.wv.vocab, wordlist)], key=lambda x: x[1])\n",
      "for i in range(20):\n",
      "    print(f\"{res[i][0]} {res[i][1]} \\t\\t {res[-i-1][0]} {res[-i][1]}\")\n",
      "125/112:\n",
      "res = sorted([(x, gproj(gpol, mpol, x)) for x in filter(lambda x: x in mpol.wv.vocab, wordlist)], key=lambda x: x[1])\n",
      "for i in range(20):\n",
      "    print(f\"{res[i][0]} \\t {res[i][1]} \\t\\t {res[-i-1][0]} \\t {res[-i][1]}\")\n",
      "125/113:\n",
      "res = sorted([(x, gproj(gpol, mpol, x)) for x in filter(lambda x: x in mpol.wv.vocab, wordlist)], key=lambda x: x[1])\n",
      "for i in range(20):\n",
      "    print(f\"{res[i][0]} \\t\\t {res[i][1]} \\t\\t {res[-i-1][0]} \\t\\t {res[-i][1]}\")\n",
      "125/114:\n",
      "res = sorted([(x, gproj(gpol, mpol, x)) for x in filter(lambda x: x in mpol.wv.vocab, wordlist)], key=lambda x: x[1])\n",
      "for i in range(20):\n",
      "    print(f\"{res[i][0].ljust(20)} \\t\\t {res[i][1]} \\t\\t {res[-i-1][0]} \\t\\t {res[-i][1]}\")\n",
      "125/115:\n",
      "res = sorted([(x, gproj(gpol, mpol, x)) for x in filter(lambda x: x in mpol.wv.vocab, wordlist)], key=lambda x: x[1])\n",
      "for i in range(20):\n",
      "    print(f\"{res[i][0].ljust(20)} \\t\\t {res[i][1]} \\t\\t {res[-i-1][0].ljust(20)} \\t\\t {res[-i][1]}\")\n",
      "125/116:\n",
      "res = sorted([(x, gproj(gpol, mpol, x)) for x in filter(lambda x: x in mpol.wv.vocab, wordlist)], key=lambda x: x[1])\n",
      "for i in range(20):\n",
      "    print(f\"{res[i][0].ljust(20)} {res[i][1]} \\t\\t {res[-i-1][0].ljust(20)} {res[-i][1]}\")\n",
      "125/117:\n",
      "res = sorted([(x, gproj(gpol, mpol, x)) for x in filter(lambda x: x in mpol.wv.vocab, wordlist)], key=lambda x: x[1])\n",
      "for i in range(20):\n",
      "    print(f\"{res[i][0].ljust(20)} {res[i][1]} \\t\\t {res[-i-1][0].ljust(20)} {res[-i-1][1]}\")\n",
      "125/118: bias(gpols, mpols, adjectives)\n",
      "125/119: gpols = getg(mpol)\n",
      "125/120: gcomps = getg(mcomps)\n",
      "125/121: gtravs = getg(mtravs)\n",
      "125/122: testbias(proflist)\n",
      "125/123:\n",
      "res = sorted([(x, gproj(gtrav, mtrav, x)) for x in filter(lambda x: x in mtrav.wv.vocab, wordlist)], key=lambda x: x[1])\n",
      "for i in range(20):\n",
      "    print(f\"{res[i][0].ljust(20)} {res[i][1]} \\t\\t {res[-i-1][0].ljust(20)} {res[-i-1][1]}\")\n",
      "125/124: testbias(gender_specific)\n",
      "125/125:\n",
      "with open('debiaswe-master/data/gender_specific_full.json') as f:\n",
      "    gender_specific_full = json.loads(f.read())\n",
      "125/126: bias(gpol, mpol, proflist), bias(gpols, mpols, proflist)\n",
      "125/127: bias(gpol, mpol, gender_specific), bias(gpols, mpols, gender_specific)\n",
      "125/128:\n",
      "with open('debiaswe-master/data/gender_specific_full.json') as f:\n",
      "    gender_specific_full = json.loads(f.read())\n",
      "125/129:\n",
      "with open('debiaswe-master/data/gender_specific_full.json') as f:\n",
      "    gender_specific = json.loads(f.read())\n",
      "125/130: bias(gpol, mpol, gender_specific), bias(gpols, mpols, gender_specific)\n",
      "125/131: testbias(gender_specific)\n",
      "125/132:\n",
      "with open('debiaswe-master/data/gender_specific_seed.json') as f:\n",
      "    gender_specific = json.loads(f.read())\n",
      "125/133: testbias(gender_specific)\n",
      "125/134: testbias(adjectives)\n",
      "125/135: testbias(gender_specific_full)\n",
      "125/136:\n",
      "with open('debiaswe-master/data/gender_specific_full.json') as f:\n",
      "    gender_specific_full = json.loads(f.read())\n",
      "125/137: from sklearn.linear_model import LinearRegression\n",
      "125/138:\n",
      "def regress(g1, m1, g2, m2, wordlist):\n",
      "    x = np.array([gproj(g1, m1, w) for w in wordlist])\n",
      "    y = np.array([gproj(g2, m2, w) for w in wordlist])\n",
      "125/139: from scipy import stats\n",
      "125/140:\n",
      "def regress(g1, m1, g2, m2, wordlist):\n",
      "    x = np.array([gproj(g1, m1, w) for w in wordlist if w in m1.wv.vocab and w in m2.wv.vocab])\n",
      "    y = np.array([gproj(g2, m2, w) for w in wordlist if w in m1.wv.vocab and w in m2.wv.vocab])\n",
      "    slope, intercept, r_value, p_value, std_err = stats.linregress(x,y)\n",
      "    return slope, p_value\n",
      "125/141: regress(gpol, mpol, gpols, mpols, proflist)\n",
      "125/142: regress(gcomp, mcomp, gcomps, mcomps, proflist)\n",
      "125/143: regress(gtrav, mtrav, gtravs, mtravs, proflist)\n",
      "125/144:\n",
      "t = time.process_time()\n",
      "controls_pol = []\n",
      "count = 0\n",
      "for i in range(len(politics)):\n",
      "    if i in [1646, 2145, 2331, 5396, 5569, 6193, 8035, 8425]:\n",
      "        continue\n",
      "    n = len(nltk.sent_tokenize(summaries_pol[count]))\n",
      "    count += 1\n",
      "    sents_pol = nltk.sent_tokenize(politics[i].text)\n",
      "    controls_pol += sample(sents_pol, n)\n",
      "time.process_time() - t\n",
      "125/145: from random import sample, randint\n",
      "125/146:\n",
      "t = time.process_time()\n",
      "controls_pol = []\n",
      "count = 0\n",
      "for i in range(len(politics)):\n",
      "    if i in [1646, 2145, 2331, 5396, 5569, 6193, 8035, 8425]:\n",
      "        continue\n",
      "    n = len(nltk.sent_tokenize(summaries_pol[count]))\n",
      "    count += 1\n",
      "    sents_pol = nltk.sent_tokenize(politics[i].text)\n",
      "    controls_pol += sample(sents_pol, n)\n",
      "time.process_time() - t\n",
      "125/147:\n",
      "t = time.process_time()\n",
      "controls_pol = []\n",
      "count = 0\n",
      "for i in range(len(politics)):\n",
      "    if i in [1008, 1938, 4466, 6292, 7783, 8379, 9505, 10035]:\n",
      "        continue\n",
      "    n = len(nltk.sent_tokenize(summaries_pol[count]))\n",
      "    count += 1\n",
      "    sents_pol = nltk.sent_tokenize(politics[i].text)\n",
      "    controls_pol += sample(sents_pol, n)\n",
      "time.process_time() - t\n",
      "125/148:\n",
      "t = time.process_time()\n",
      "docs = [simple_preprocess(s) for s in controls_pol]\n",
      "mcpol = Word2Vec(docs)\n",
      "time.process_time() - t\n",
      "125/149: gcpol = getg(mcpol)\n",
      "125/150: bias(gcpol, mcpol, proflist)\n",
      "125/151:\n",
      "words_cpol = []\n",
      "for s in controls_pol:\n",
      "    words_cpol += nltk.word_tokenize(s)\n",
      "len(words_cpol)\n",
      "125/152:\n",
      "sums_pol = []\n",
      "for s in summaries_pol:\n",
      "    sums_pol += nltk.sent_tokenize(s)\n",
      "len(sums_pol)\n",
      "125/153:\n",
      "words_spol = []\n",
      "for s in sums_pol:\n",
      "    words_spol += nltk.word_tokenize(s)\n",
      "len(words_spol)\n",
      "125/154:\n",
      "words_pol = []\n",
      "for p in politics:\n",
      "    words_pol += nltk.word_tokenize(p.text)\n",
      "len(words_pol)\n",
      "125/155:\n",
      "t = time.process_time()\n",
      "wcontrols_pol = []\n",
      "count = 0\n",
      "tot = 0\n",
      "for i in range(len(politics)):\n",
      "    if i in [1646, 2145, 2331, 5396, 5569, 6193, 8035, 8425]:\n",
      "        continue\n",
      "    # get number of words in the summary\n",
      "    n = len(nltk.word_tokenize(summaries_pol[count]))\n",
      "    tot += n\n",
      "    sents = nltk.sent_tokenize(politics[i].text)\n",
      "    wcount = 0\n",
      "    wtot = len(nltk.word_tokenize(politics[i].text))\n",
      "    start = randint(0, wtot-n)\n",
      "    end = start + n\n",
      "    started = False\n",
      "    for s in sents:\n",
      "        w = nltk.word_tokenize(s)\n",
      "        wcount += len(w)\n",
      "        if wcount < start:\n",
      "            continue\n",
      "        elif started is False:  \n",
      "            wcontrols_pol.append(w[max(0, start-wcount+len(w)-random.randint(1,2)):])\n",
      "            started = True\n",
      "        elif wcount > end:\n",
      "            wcontrols_pol.append(w[:min(len(w), len(w)-wcount+end+2)])\n",
      "            break\n",
      "        else:\n",
      "            wcontrols_pol.append(w)\n",
      "    # get n-word consecutive subset of original text\n",
      "    count += 1\n",
      "time.process_time() - t\n",
      "125/156:\n",
      "t = time.process_time()\n",
      "wcontrols_pol = []\n",
      "count = 0\n",
      "tot = 0\n",
      "for i in range(len(politics)):\n",
      "    if i in [1008, 1938, 4466, 6292, 7783, 8379, 9505, 10035]:\n",
      "        continue\n",
      "    # get number of words in the summary\n",
      "    n = len(nltk.word_tokenize(summaries_pol[count]))\n",
      "    tot += n\n",
      "    sents = nltk.sent_tokenize(politics[i].text)\n",
      "    wcount = 0\n",
      "    wtot = len(nltk.word_tokenize(politics[i].text))\n",
      "    start = randint(0, wtot-n)\n",
      "    end = start + n\n",
      "    started = False\n",
      "    for s in sents:\n",
      "        w = nltk.word_tokenize(s)\n",
      "        wcount += len(w)\n",
      "        if wcount < start:\n",
      "            continue\n",
      "        elif started is False:  \n",
      "            wcontrols_pol.append(w[max(0, start-wcount+len(w)-random.randint(1,2)):])\n",
      "            started = True\n",
      "        elif wcount > end:\n",
      "            wcontrols_pol.append(w[:min(len(w), len(w)-wcount+end+2)])\n",
      "            break\n",
      "        else:\n",
      "            wcontrols_pol.append(w)\n",
      "    # get n-word consecutive subset of original text\n",
      "    count += 1\n",
      "time.process_time() - t\n",
      "125/157:\n",
      "swcontrols_pol = [\" \".join(x) for x in wcontrols_pol]\n",
      "print(tot, sum([len(x) for x in wcontrols_pol]))\n",
      "125/158:\n",
      "t = time.process_time()\n",
      "docs = [simple_preprocess(s) for s in swcontrols_pol]\n",
      "mwcpol = Word2Vec(docs)\n",
      "time.process_time() - t\n",
      "125/159: gwcpol = getg(mwcpol)\n",
      "125/160: bias(gwcpol, mwcpol, proflist)\n",
      "125/161: bias(gwcpol, mwcpol, proflist), bias(gpols, mpols, proflist)\n",
      "125/162: regress(gpols, mpols, gwcpol, mwcpol, proflist)\n",
      "125/163: regress(gpol, mpol, gcomp, mcomp, proflist)\n",
      "125/164:\n",
      "t = time.process_time()\n",
      "wcontrols_pol = []\n",
      "count = 0\n",
      "tot = 0\n",
      "for i in range(len(politics)):\n",
      "    if i in [1008, 1938, 4466, 6292, 7783, 8379, 9505, 10035]:\n",
      "        continue\n",
      "    # get number of words in the summary\n",
      "    n = len(nltk.word_tokenize(summaries_pol[count]))\n",
      "    tot += n\n",
      "    sents = nltk.sent_tokenize(politics[i].text)\n",
      "    wcount = 0\n",
      "    wtot = len(nltk.word_tokenize(politics[i].text))\n",
      "    start = randint(0, wtot-n)\n",
      "    end = start + n\n",
      "    started = False\n",
      "    for s in sents:\n",
      "        w = nltk.word_tokenize(s)\n",
      "        wcount += len(w)\n",
      "        if wcount < start:\n",
      "            continue\n",
      "        elif started is False:  \n",
      "            wcontrols_pol.append(w[max(0, start-wcount+len(w)-1):])\n",
      "            started = True\n",
      "        elif wcount > end:\n",
      "            wcontrols_pol.append(w[:min(len(w), len(w)-wcount+end+1)])\n",
      "            break\n",
      "        else:\n",
      "            wcontrols_pol.append(w)\n",
      "    # get n-word consecutive subset of original text\n",
      "    count += 1\n",
      "time.process_time() - t\n",
      "125/165:\n",
      "swcontrols_pol = [\" \".join(x) for x in wcontrols_pol]\n",
      "print(tot, sum([len(x) for x in wcontrols_pol]))\n",
      "125/166:\n",
      "t = time.process_time()\n",
      "docs = [simple_preprocess(s) for s in swcontrols_pol]\n",
      "mwcpol = Word2Vec(docs)\n",
      "time.process_time() - t\n",
      "129/1:\n",
      "%%javascript\n",
      "MathJax.Hub.Config({\n",
      "    TeX: { equationNumbers: { autoNumber: \"AMS\" } }\n",
      "});\n",
      "129/2:\n",
      "%load_ext autoreload\n",
      "%autoreload 2\n",
      "%matplotlib inline\n",
      "127/1:\n",
      "import xml.etree.ElementTree as ET\n",
      "import os\n",
      "from typing import List\n",
      "from functools import reduce\n",
      "from calendar import monthrange\n",
      "import random\n",
      "import matplotlib.pyplot as plt\n",
      "import numpy as np\n",
      "import json\n",
      "129/3:\n",
      "import lab\n",
      "from matplotlib import pyplot as plt\n",
      "\n",
      "# Read an image as a PyTorch tensor\n",
      "x = lab.imread('data/peppers.png')\n",
      "\n",
      "# Visualize the input x\n",
      "plt.figure(1, figsize=(12,12))\n",
      "lab.imarraysc(x)\n",
      "\n",
      "# Show the shape of the tensor\n",
      "print(f\"The image tensor shape is {list(x.shape)}\")\n",
      "\n",
      "# Show the data type of the tensor\n",
      "print(f\"The image tensor type is {x.dtype}\")\n",
      "127/2:\n",
      "import xml.etree.ElementTree as ET\n",
      "import os\n",
      "from typing import List\n",
      "from functools import reduce\n",
      "from calendar import monthrange\n",
      "import random\n",
      "import matplotlib.pyplot as plt\n",
      "import numpy as np\n",
      "import json\n",
      "127/3: import time\n",
      "127/4:\n",
      "from bs4 import BeautifulSoup\n",
      "import urllib.request as urllib2\n",
      "import csv\n",
      "127/5:\n",
      "url = 'https://www.enchantedlearning.com/wordlist/adjectivesforpeople.shtml'\n",
      "file = urllib2.urlopen(url)\n",
      "html = file.read()\n",
      "file.close()\n",
      "soup = BeautifulSoup(html)\n",
      "127/6: adj = soup.find_all(attrs={\"class\": \"wordlist-item\"})\n",
      "127/7: adjs = [x.contents[0] for x in adj]\n",
      "127/8: adjs[:5]\n",
      "127/9:\n",
      "with open('adjectives.csv', mode='w') as f:\n",
      "    writer = csv.writer(f, delimiter=',')\n",
      "    writer.writerow(adjs)\n",
      "127/10:\n",
      "with open('adjectives.csv') as csv_file:\n",
      "    adjectives = csv_file.read().split(\",\")\n",
      "127/11:\n",
      "class Doc:\n",
      "    def __init__(self, id: int, gdescriptors: List[str], descriptors: List[str], text: str) -> None:\n",
      "        self.id = id\n",
      "        self.gdescriptors = gdescriptors\n",
      "        self.descriptors = descriptors\n",
      "        self.text = text\n",
      "127/12:\n",
      "def parse_doc(file: str) -> Doc:\n",
      "    tree = ET.parse(file)\n",
      "    root = tree.getroot()\n",
      "    try:\n",
      "        fulltext = root.find('body').find('body.content').find(\"*[@class='full_text']\")\n",
      "        text = reduce(lambda x, y: x + y, [x.text for x in fulltext.findall('p')])\n",
      "        id = int(root.find('head').find('docdata').find('doc-id').attrib['id-string'])\n",
      "        classifiers = root.find('head').find('docdata').find('identified-content')\n",
      "        gclassifiers = classifiers.findall(\"*[@class='online_producer'][@type='general_descriptor']\")\n",
      "        dclassifiers = classifiers.findall(\"*[@class='online_producer'][@type='descriptor']\")\n",
      "        gdescriptors = [c.text for c in gclassifiers]\n",
      "        descriptors = [c.text for c in dclassifiers]\n",
      "        return Doc(id, gdescriptors, descriptors, text)\n",
      "    except:\n",
      "        return None\n",
      "127/13:\n",
      "def get_docs_by_month(year: int, month: int) -> List[Doc]:\n",
      "    days = monthrange(year, month)[1]\n",
      "    docs = []\n",
      "    for day in range(1, days+1):\n",
      "        for file in os.listdir(f'data/{year}/{month:02}/{day:02}/'):\n",
      "            doc = parse_doc(f'data/{year}/{month:02}/{day:02}/{file}')\n",
      "            if doc:\n",
      "                docs.append(doc)\n",
      "    return docs\n",
      "127/14:\n",
      "t = time.process_time()\n",
      "all_ = []\n",
      "for y in range(1988, 2007):\n",
      "    print(y)\n",
      "    for i in range(1,13):\n",
      "        all_ += get_docs_by_month(y, i)\n",
      "time.process_time() - t\n",
      "127/15: len(all_)\n",
      "127/16:\n",
      "t = time.process_time()\n",
      "alld = {}\n",
      "for doc in all_:\n",
      "    doc = Doc(doc.id, doc.gdescriptors, doc.descriptors, doc.text)\n",
      "    for d in doc.descriptors:\n",
      "        count, docs = alld.get(d, (0, set()))\n",
      "        docs.add(doc)\n",
      "        alld[d] = (count + 1, docs)\n",
      "time.process_time() - t\n",
      "127/17:\n",
      "computers = list(filter(lambda x: 'Politics and Government' not in x.descriptors, \\\n",
      "                        alld['Computers and the Internet'][1]))\n",
      "127/18:\n",
      "politics = list(filter(lambda x: 'Computers and the Internet' not in x.descriptors, \\\n",
      "                        alld['Politics and Government'][1]))\n",
      "127/19:\n",
      "travel = list(filter(lambda x: 'Politics and Government' not in x.descriptors, \\\n",
      "                        alld['Travel and Vacations'][1]))\n",
      "127/20: len(computers), len(politics), len(travel)\n",
      "127/21:\n",
      "from gensim.models import Word2Vec\n",
      "from gensim.utils import simple_preprocess\n",
      "127/22: import nltk\n",
      "127/23:\n",
      "def getw2v(category):\n",
      "    sentences = []\n",
      "    for t in category:\n",
      "        sentences += nltk.sent_tokenize(t.text)\n",
      "    docs = [simple_preprocess(s) for s in sentences]\n",
      "    return Word2Vec(docs)\n",
      "127/24:\n",
      "t = time.process_time()\n",
      "mpol = getw2v(politics)\n",
      "time.process_time() - t\n",
      "127/25:\n",
      "t = time.process_time()\n",
      "mcomp = getw2v(computers)\n",
      "time.process_time() - t\n",
      "127/26:\n",
      "t = time.process_time()\n",
      "mtrav = getw2v(travel)\n",
      "time.process_time() - t\n",
      "127/27:\n",
      "from gensim.summarization.summarizer import summarize\n",
      "from gensim.models import LsiModel\n",
      "from gensim.test.utils import common_dictionary, common_corpus\n",
      "127/28:\n",
      "def getsummaries(category):\n",
      "    s = []\n",
      "    for i in range(len(category)):\n",
      "        try:\n",
      "            s.append(summarize(category[i].text))\n",
      "        except:\n",
      "            print(i)\n",
      "    return s\n",
      "127/29:\n",
      "t = time.process_time()\n",
      "summaries_pol = getsummaries(politics)\n",
      "time.process_time() - t\n",
      "127/30:\n",
      "t = time.process_time()\n",
      "summaries_comp = getsummaries(computers)\n",
      "time.process_time() - t\n",
      "127/31:\n",
      "t = time.process_time()\n",
      "summaries_trav = getsummaries(travel)\n",
      "time.process_time() - t\n",
      "127/32: mpols = getw2v_s(summaries_pol)\n",
      "129/4:\n",
      "import torch\n",
      "\n",
      "# Create a bank of linear filters\n",
      "w = torch.randn(10,3,5,5)\n",
      "\n",
      "# Visualize the filters\n",
      "plt.figure(1, figsize=(12,12))\n",
      "lab.imarraysc(w, spacing=1) ;\n",
      "129/5:\n",
      "import torch.nn.functional as F\n",
      "\n",
      "# Apply the convolution operator\n",
      "y = F.conv2d(x, w)\n",
      "\n",
      "# Visualize the convolution result\n",
      "print(f\"The output shape is {y.shape}\")\n",
      "129/6:\n",
      "# Visualize the output y, one channel per image\n",
      "fig = plt.figure(figsize=(15, 10))\n",
      "lab.imarraysc(lab.t2im(y)) ;\n",
      "129/7:\n",
      "# Try again, downsampling the output\n",
      "y_ds = F.conv2d(x, w, stride=16)\n",
      "\n",
      "plt.figure(figsize=(15, 10))\n",
      "lab.imarraysc(lab.t2im(y_ds)) ;\n",
      "129/8:\n",
      "# Try again, downsampling the output\n",
      "y_ds = F.conv2d(x, w, padding=4)\n",
      "\n",
      "plt.figure(figsize=(15, 10))\n",
      "lab.imarraysc(lab.t2im(y_ds)) ;\n",
      "129/9:\n",
      "# Initialize a filter\n",
      "w = torch.Tensor([\n",
      "    [0,  1, 0 ],\n",
      "    [1, -4, 1 ],\n",
      "    [0,  1, 0 ]\n",
      "])[None,None,:,:].expand(1,3,3,3)\n",
      "\n",
      "print(f\"The shape of the filter w is {list(w.shape)}\")\n",
      "\n",
      "# Apply convolution\n",
      "y_lap = F.conv2d(x, w) ;\n",
      "\n",
      "# Show the output\n",
      "plt.figure(1,figsize=(12,12))\n",
      "lab.imsc(y_lap[0])\n",
      "\n",
      "# Show the output absolute value\n",
      "plt.figure(2,figsize=(12,12))\n",
      "lab.imsc(abs(y_lap)[0]) ;\n",
      "129/10:\n",
      "# Initialize a filter\n",
      "w = torch.Tensor([1,  0, -1])[None,None,:].expand(1,3,3,3)\n",
      "\n",
      "# Convolution\n",
      "y = F.conv2d(x, w) ;\n",
      "\n",
      "# ReLU\n",
      "z = F.relu(y) ;\n",
      "\n",
      "plt.figure(1,figsize=(12,12))\n",
      "lab.imsc(y[0])\n",
      "\n",
      "plt.figure(2,figsize=(12,12))\n",
      "lab.imsc(z[0]) ;\n",
      "129/11:\n",
      "y = F.max_pool2d(x, 15)\n",
      "\n",
      "plt.figure(1, figsize=(12,12))\n",
      "lab.imarraysc(lab.t2im(y)) ;\n",
      "129/12:\n",
      "# LRN with some specially-chosen parameters\n",
      "y_nrm = F.local_response_norm(x, 5, alpha=5, beta=0.5, k=0)\n",
      "\n",
      "plt.figure(1,figsize=(12,12))\n",
      "lab.imarraysc(lab.t2im(y_nrm)) ;\n",
      "129/13:\n",
      "import math\n",
      "\n",
      "# Another implementation of the same\n",
      "y_nrm_alt = x / torch.sqrt((x**2).sum(1))\n",
      "\n",
      "plt.figure(1,figsize=(12,12))\n",
      "lab.imarraysc(lab.t2im(y_nrm_alt))\n",
      "\n",
      "# Check that they indeed match\n",
      "def compare(x,y):\n",
      "    with torch.no_grad():\n",
      "        a2 = torch.mean((x - y)**2)\n",
      "        b2 = torch.mean(x**2)\n",
      "        c2 = torch.mean(y**2)\n",
      "        return 200 * math.sqrt(a2.item()) / math.sqrt(b2.item() + c2.item())\n",
      "\n",
      "print(f\"The difference between y_nrm and y_nrm_alt is {compare(y_nrm,y_nrm_alt):.1f}%\")\n",
      "129/14: y\n",
      "129/15: y<0\n",
      "129/16: sum(y<0)\n",
      "129/17: count(y<0)\n",
      "129/18: np,sum(y<0)\n",
      "129/19: sum(y<0)\n",
      "129/20: True in (y<0)\n",
      "129/21:\n",
      "# Read an example image\n",
      "x1 = lab.imread('data/peppers.png')\n",
      "x1.requires_grad_(True)\n",
      "\n",
      "# Create a bank of linear filters\n",
      "w1 = torch.randn(10,3,5,5)\n",
      "w1.requires_grad_(True)\n",
      "\n",
      "# Other CNN parameters\n",
      "pool2_size = 10\n",
      "\n",
      "# Run the CNN forward\n",
      "x2 = F.conv2d(x1, w1)\n",
      "x3 = F.max_pool2d(x2, pool2_size)\n",
      "\n",
      "# Create the derivative dz/dx3\n",
      "dzdx3 = torch.randn(x3.shape)\n",
      "\n",
      "# Run the CNN backward\n",
      "x3.backward(dzdx3)\n",
      "dzdx1 = x1.grad\n",
      "dzdw1 = w1.grad\n",
      "\n",
      "print(f\"Size of dzdx1 = {list(dzdx1.shape)}\")\n",
      "print(f\"Size of dxdw1 = {list(dzdw1.shape)}\")\n",
      "129/22:\n",
      "y = F.conv2d(x,w) # forward mode (get output y)\n",
      "p = torch.randn(y.shape) # get a random tensor with the same size as y\n",
      "\n",
      "# Directly call backward functions for demonstration\n",
      "dx = torch.nn.grad.conv2d_input(x.shape, w, p)\n",
      "dw = torch.nn.grad.conv2d_weight(x, w.shape, p)\n",
      "\n",
      "print(f\"The shape of x is {list(x.shape)} and that of dx is {list(dx.shape)}\")\n",
      "print(f\"The shape of w is {list(x.shape)} and that of dw is {list(dx.shape)}\")\n",
      "print(f\"The shape of y is {list(y.shape)} and that of  p is {list(p.shape)}\")\n",
      "129/23:\n",
      "x.requires_grad_(True)\n",
      "w.requires_grad_(True)\n",
      "if x.grad is not None:\n",
      "    x.grad.zero_()\n",
      "if w.grad is not None:\n",
      "    w.grad.zero_()\n",
      "y = F.conv2d(x,w) \n",
      "y.backward(p)\n",
      "dx_ = x.grad\n",
      "dw_ = w.grad\n",
      "\n",
      "print(f\"The difference between dx and dx_ is {compare(dx,dx_):.1f}%\")\n",
      "print(f\"The difference between dw and dw_ is {compare(dw,dw_):.1f}%\")\n",
      "129/24:\n",
      "# Read an example image\n",
      "x = lab.imread('data/peppers.png')\n",
      "x.requires_grad_(True)\n",
      "\n",
      "# Create a bank of linear filters\n",
      "w = torch.randn(10,3,5,5)\n",
      "w.requires_grad_(True)\n",
      "\n",
      "# Forward\n",
      "y = F.conv2d(x, w)\n",
      "\n",
      "# Set the derivative dz/dy to a randmo value\n",
      "dzdy = torch.randn(y.shape)\n",
      "\n",
      "# Backward\n",
      "y.backward(dzdy)\n",
      "dzdx = x.grad\n",
      "dzdw = w.grad\n",
      "\n",
      "print(f\"Size of dzdx = {list(dzdx.shape)}\")\n",
      "print(f\"Size of dzdw = {list(dzdw.shape)}\")\n",
      "129/25:\n",
      "# Check the derivative numerically\n",
      "with torch.no_grad():\n",
      "    delta = torch.randn(x.shape)\n",
      "    step = 0.0001\n",
      "    xp = x + step * delta\n",
      "    yp = F.conv2d(xp, w)\n",
      "    dzdx_numerical = torch.sum(dzdy * (yp - y) / step)\n",
      "    dzdx_analytical = torch.sum(dzdx * delta)\n",
      "    err = compare(dzdx_numerical,dzdx_analytical)\n",
      "\n",
      "print(f\"dzdx_numerical: {dzdx_numerical}\")\n",
      "print(f\"dzdx_analytical: {dzdx_analytical}\")\n",
      "print(f\"numerical vs analytical rel. error: {err:.3f}%\")\n",
      "129/26:\n",
      "# Read an example image\n",
      "x1 = lab.imread('data/peppers.png')\n",
      "x1.requires_grad_(True)\n",
      "\n",
      "# Create a bank of linear filters\n",
      "w1 = torch.randn(10,3,5,5)\n",
      "w1.requires_grad_(True)\n",
      "\n",
      "# Other CNN parameters\n",
      "pool2_size = 10\n",
      "\n",
      "# Run the CNN forward\n",
      "x2 = F.conv2d(x1, w1)\n",
      "x3 = F.max_pool2d(x2, pool2_size)\n",
      "\n",
      "# Create the derivative dz/dx3\n",
      "dzdx3 = torch.randn(x3.shape)\n",
      "\n",
      "# Run the CNN backward\n",
      "x3.backward(dzdx3)\n",
      "dzdx1 = x1.grad\n",
      "dzdw1 = w1.grad\n",
      "\n",
      "print(f\"Size of dzdx1 = {list(dzdx1.shape)}\")\n",
      "print(f\"Size of dxdw1 = {list(dzdw1.shape)}\")\n",
      "129/27:\n",
      "# Check the derivative numerically\n",
      "with torch.no_grad():\n",
      "    delta = torch.randn(w1.shape)\n",
      "    step = 0.00001\n",
      "    w1p = w1 + step * delta\n",
      "    x1p = x1\n",
      "    x2p = F.conv2d(x1p, w1p)\n",
      "    x3p = F.max_pool2d(x2p, pool2_size)\n",
      "    \n",
      "    dzdw1_numerical = torch.sum(dzdx3 * (x3p - x3) / step)\n",
      "    dzdw1_analytical = torch.sum(dzdw1 * delta)\n",
      "    err = compare(dzdw1_numerical, dzdw1_analytical)\n",
      "\n",
      "print(f\"dzdw1_numerical: {dzdw1_numerical}\")\n",
      "print(f\"dzdw1_analytical: {dzdw1_analytical}\")\n",
      "print(f\"numerical vs analytical rel. error: {err:.3f}%\")\n",
      "129/28:\n",
      "def tinycnn(x, w, b):\n",
      "    pad1 = (w.shape[2] - 1) // 2\n",
      "    rho2 = 3\n",
      "    pad2 = (rho2 - 1) // 2\n",
      "    x = F.conv2d(x, w, b, padding=pad1)\n",
      "    x = F.avg_pool2d(x, rho2, padding=pad2, stride=1)\n",
      "    return x\n",
      "129/29:\n",
      "# Load a training image and convert to gray-scale\n",
      "im0 = lab.imread('data/dots.jpg')\n",
      "im0 = im0.mean(dim=1)[None,:,:,:]\n",
      "\n",
      "# Compute the location of black blobs in the image\n",
      "pos, neg, indices = lab.extract_black_blobs(im0)\n",
      "pos = pos / pos.sum()\n",
      "neg = neg / neg.sum()\n",
      "129/30:\n",
      "# Display the training data\n",
      "plt.figure(1, figsize=(8,8))\n",
      "lab.imsc(im0[0])\n",
      "plt.plot(indices[1], indices[0],'go', markersize=8, mfc='none')\n",
      "\n",
      "plt.figure(2, figsize=(8,8))\n",
      "lab.imsc(pos[0])\n",
      "\n",
      "plt.figure(3, figsize=(8,8))\n",
      "lab.imsc(neg[0]) ;\n",
      "129/31:\n",
      "# Preprocess the image by subtracting its mean\n",
      "im = im0 - im0.mean()\n",
      "im = lab.imsmooth(im, 3)\n",
      "129/32:\n",
      "num_iterations = 501\n",
      "rate = 10\n",
      "momentum = 0.9\n",
      "shrinkage = 0.0001\n",
      "plot_period = 200\n",
      "\n",
      "with torch.no_grad():\n",
      "    w = torch.randn(1,1,3,3)\n",
      "    w = w - w.mean()\n",
      "    b = torch.Tensor(1)\n",
      "    b.zero_()\n",
      "\n",
      "E = []\n",
      "w.requires_grad_(True)\n",
      "b.requires_grad_(True)\n",
      "w_momentum = torch.zeros(w.shape)\n",
      "b_momentum = torch.zeros(b.shape)\n",
      "\n",
      "for t in range(num_iterations):\n",
      "\n",
      "    # Evaluate the CNN and the loss\n",
      "    y = tinycnn(im, w, b)\n",
      "    z = (pos * (1 - y).relu() + neg * y.relu()).sum()\n",
      "\n",
      "    # Track energy\n",
      "    E.append(z.item() + 0.5 * shrinkage * (w**2).sum().item())\n",
      "\n",
      "    # Backpropagation\n",
      "    z.backward()\n",
      "\n",
      "    # Gradient descent\n",
      "    with torch.no_grad():\n",
      "        w_momentum = momentum * w_momentum + (1 - momentum) * (w.grad + shrinkage * w)\n",
      "        b_momentum = momentum * b_momentum + (1 - momentum) * b.grad\n",
      "        w -= rate * w_momentum\n",
      "        b -= 0.1 * rate * b_momentum        \n",
      "        w.grad.zero_()\n",
      "        b.grad.zero_()\n",
      "\n",
      "    # Plotting\n",
      "    if t % plot_period == 0:\n",
      "        plt.figure(1,figsize=(12,4))\n",
      "        plt.clf()\n",
      "        fig = plt.gcf()\n",
      "        ax1 = fig.add_subplot(1, 3, 1)\n",
      "        plt.plot(E)\n",
      "        ax2 = fig.add_subplot(1, 3, 2)\n",
      "        lab.imsc(w.detach()[0])\n",
      "        ax3 = fig.add_subplot(1, 3, 3)\n",
      "        lab.imsc(y.detach()[0])\n",
      "129/33:\n",
      "# Load data\n",
      "imdb = torch.load('data/charsdb.pth')\n",
      "print(f\"imdb['images'] has shape {list(imdb['images'].shape)}\")\n",
      "print(f\"imdb['labels'] has shape {list(imdb['labels'].shape)}\")\n",
      "print(f\"imdb['sets'] has shape {list(imdb['sets'].shape)}\")\n",
      "129/34:\n",
      "# Load a pre-trained model. Do this in a pinch, otherwise train your own.\n",
      "# model = new_model()\n",
      "# model.load_state_dict(torch.load('data/charscnn.pth'))\n",
      "# model.load_state_dict(torch.load('data/charscnn_jitter.pth'))\n",
      "\n",
      "# Load sentence\n",
      "im = lab.imread('data/sentence-lato.png')\n",
      "im.sub_(im_mean)\n",
      "\n",
      "# Apply the CNN to the larger image\n",
      "y = model(im)\n",
      "\n",
      "# Show the string\n",
      "chars = lab.decode_predicted_string(y)\n",
      "print(f\"Predicted string '{''.join(chars)}'\")\n",
      "129/35:\n",
      "# Initialize the model\n",
      "model = new_model()\n",
      "\n",
      "# Run SGD to train the model\n",
      "model = lab.train_model(model, imdb, num_epochs=2, use_gpu=False)\n",
      "129/36:\n",
      "import torch.nn as nn\n",
      "\n",
      "def new_model():\n",
      "    return nn.Sequential(\n",
      "        nn.Conv2d(1,20,5),\n",
      "        nn.MaxPool2d(2,stride=2),\n",
      "        nn.Conv2d(20,50,5),\n",
      "        nn.MaxPool2d(2,stride=2),\n",
      "        nn.Conv2d(50,500,4),\n",
      "        nn.ReLU(),\n",
      "        nn.Conv2d(500,26,2),\n",
      "    )\n",
      "\n",
      "model = new_model()\n",
      "129/37:\n",
      "# Evaluate the network on three images\n",
      "y = model(imdb['images'].narrow(0,0,3))\n",
      "print(f\"The size of the network output is {list(y.shape)}\")\n",
      "129/38:\n",
      "# Load data\n",
      "imdb = torch.load('charsdb.pth')\n",
      "print(f\"imdb['images'] has shape {list(imdb['images'].shape)}\")\n",
      "print(f\"imdb['labels'] has shape {list(imdb['labels'].shape)}\")\n",
      "print(f\"imdb['sets'] has shape {list(imdb['sets'].shape)}\")\n",
      "129/39: %cd \"~/Downloads\"\n",
      "129/40: %cd \"~/Downloads/429/cos429a4/a4_part3_practical-cnn-pytorch\"\n",
      "129/41:\n",
      "# Load data\n",
      "imdb = torch.load('data/charsdb.pth')\n",
      "print(f\"imdb['images'] has shape {list(imdb['images'].shape)}\")\n",
      "print(f\"imdb['labels'] has shape {list(imdb['labels'].shape)}\")\n",
      "print(f\"imdb['sets'] has shape {list(imdb['sets'].shape)}\")\n",
      "129/42:\n",
      "%cd \"~/Downloads/429/cos429a4/a4_part3_practical-cnn-pytorch\"\n",
      "%ls\n",
      "129/43:\n",
      "%cd \"~/Downloads/429/cos429a4/a4_part3_practical-cnn-pytorch\"\n",
      "%ls \"data/\"\n",
      "129/44:\n",
      "# Load data\n",
      "imdb = torch.load('data/charsdb.pth')\n",
      "print(f\"imdb['images'] has shape {list(imdb['images'].shape)}\")\n",
      "print(f\"imdb['labels'] has shape {list(imdb['labels'].shape)}\")\n",
      "print(f\"imdb['sets'] has shape {list(imdb['sets'].shape)}\")\n",
      "129/45:\n",
      "# Plot the training data for 'a'\n",
      "plt.figure(1,figsize=(15,15))\n",
      "plt.clf()\n",
      "plt.title('Training data')\n",
      "sel = (imdb['sets'] == 0) & (imdb['labels'] == 0)\n",
      "lab.imarraysc(imdb['images'][sel,:,:,:])\n",
      "\n",
      "# Plot the validation data for 'a'\n",
      "plt.figure(2,figsize=(12,12))\n",
      "plt.clf()\n",
      "plt.title('Validation data')\n",
      "sel = (imdb['sets'] == 1) & (imdb['labels'] == 0)\n",
      "lab.imarraysc(imdb['images'][sel,:,:,:]) ;\n",
      "129/46:\n",
      "import torch.nn as nn\n",
      "\n",
      "def new_model():\n",
      "    return nn.Sequential(\n",
      "        nn.Conv2d(1,20,5),\n",
      "        nn.MaxPool2d(2,stride=2),\n",
      "        nn.Conv2d(20,50,5),\n",
      "        nn.MaxPool2d(2,stride=2),\n",
      "        nn.Conv2d(50,500,4),\n",
      "        nn.ReLU(),\n",
      "        nn.Conv2d(500,26,2),\n",
      "    )\n",
      "\n",
      "model = new_model()\n",
      "129/47:\n",
      "# Evaluate the network on three images\n",
      "y = model(imdb['images'].narrow(0,0,3))\n",
      "print(f\"The size of the network output is {list(y.shape)}\")\n",
      "129/48:\n",
      "# Preserves only the first two dimensions\n",
      "y = y.reshape(*y.shape[:2])\n",
      "print(f\"The size of the network output is now {list(y.shape)}\")\n",
      "129/49:\n",
      "# Load a pre-trained model. Do this in a pinch, otherwise train your own.\n",
      "# model = new_model()\n",
      "# model.load_state_dict(torch.load('data/charscnn.pth'))\n",
      "# model.load_state_dict(torch.load('data/charscnn_jitter.pth'))\n",
      "\n",
      "# Load sentence\n",
      "im = lab.imread('data/sentence-lato.png')\n",
      "im.sub_(im_mean)\n",
      "\n",
      "# Apply the CNN to the larger image\n",
      "y = model(im)\n",
      "\n",
      "# Show the string\n",
      "chars = lab.decode_predicted_string(y)\n",
      "print(f\"Predicted string '{''.join(chars)}'\")\n",
      "129/50:\n",
      "# Initialize the model\n",
      "model = new_model()\n",
      "\n",
      "# Run SGD to train the model\n",
      "model = lab.train_model(model, imdb, num_epochs=2, use_gpu=False)\n",
      "129/51:\n",
      "# Remove average intensity from input images\n",
      "im_mean = imdb['images'].mean()\n",
      "imdb['images'].sub_(im_mean) ;\n",
      "129/52:\n",
      "# Visualize the filters in the first layer\n",
      "plt.figure(1, figsize=(12,12))\n",
      "plt.title('filters in the first layer')\n",
      "lab.imarraysc(model[0].weight, spacing=1) ;\n",
      "129/53:\n",
      "# Load a pre-trained model. Do this in a pinch, otherwise train your own.\n",
      "# model = new_model()\n",
      "# model.load_state_dict(torch.load('data/charscnn.pth'))\n",
      "# model.load_state_dict(torch.load('data/charscnn_jitter.pth'))\n",
      "\n",
      "# Load sentence\n",
      "im = lab.imread('data/sentence-lato.png')\n",
      "im.sub_(im_mean)\n",
      "\n",
      "# Apply the CNN to the larger image\n",
      "y = model(im)\n",
      "\n",
      "# Show the string\n",
      "chars = lab.decode_predicted_string(y)\n",
      "print(f\"Predicted string '{''.join(chars)}'\")\n",
      "129/54:\n",
      "# Visualize the predicted string\n",
      "plt.figure(1, figsize=(12,8))\n",
      "lab.plot_predicted_string(im, y) ;\n",
      "129/55: y.shape\n",
      "129/56:\n",
      "im.shape\n",
      "y.shape\n",
      "129/57: im.shape, y.shape\n",
      "129/58:\n",
      "# Load a pre-trained model. Do this in a pinch, otherwise train your own.\n",
      "model = new_model()\n",
      "model.load_state_dict(torch.load('data/charscnn.pth'))\n",
      "model.load_state_dict(torch.load('data/charscnn_jitter.pth'))\n",
      "\n",
      "# Load sentence\n",
      "im = lab.imread('data/sentence-lato.png')\n",
      "im.sub_(im_mean)\n",
      "\n",
      "# Apply the CNN to the larger image\n",
      "y = model(im)\n",
      "\n",
      "# Show the string\n",
      "chars = lab.decode_predicted_string(y)\n",
      "print(f\"Predicted string '{''.join(chars)}'\")\n",
      "129/59:\n",
      "# Load a pre-trained model. Do this in a pinch, otherwise train your own.\n",
      "model = new_model()\n",
      "model.load_state_dict(torch.load('data/charscnn.pth'))\n",
      "model.load_state_dict(torch.load('data/charscnn_jitter.pth'))\n",
      "\n",
      "# Load sentence\n",
      "im = lab.imread('data/sentence-lato.png')\n",
      "im.sub_(im_mean)\n",
      "\n",
      "# Apply the CNN to the larger image\n",
      "y = model(im)\n",
      "\n",
      "# Show the string\n",
      "chars = lab.decode_predicted_string(y)\n",
      "print(f\"Predicted string '{''.join(chars)}'\")\n",
      "129/60:\n",
      "# Load a pre-trained model. Do this in a pinch, otherwise train your own.\n",
      "model = new_model()\n",
      "model.load_state_dict(torch.load('data/charscnn.pth'))\n",
      "model.load_state_dict(torch.load('data/charscnn_jitter.pth'))\n",
      "\n",
      "# Load sentence\n",
      "im = lab.imread('data/sentence-lato.png')\n",
      "im.sub_(im_mean)\n",
      "\n",
      "# Apply the CNN to the larger image\n",
      "y = model(im)\n",
      "\n",
      "# Show the string\n",
      "chars = lab.decode_predicted_string(y)\n",
      "print(f\"Predicted string '{''.join(chars)}'\")\n",
      "129/61:\n",
      "# Load a pre-trained model. Do this in a pinch, otherwise train your own.\n",
      "# model = new_model()\n",
      "# model.load_state_dict(torch.load('data/charscnn.pth'))\n",
      "# model.load_state_dict(torch.load('data/charscnn_jitter.pth'))\n",
      "\n",
      "# Load sentence\n",
      "im = lab.imread('data/sentence-lato.png')\n",
      "im.sub_(im_mean)\n",
      "\n",
      "# Apply the CNN to the larger image\n",
      "y = model(im)\n",
      "\n",
      "# Show the string\n",
      "chars = lab.decode_predicted_string(y)\n",
      "print(f\"Predicted string '{''.join(chars)}'\")\n",
      "129/62:\n",
      "# Visualize the filters in the first layer\n",
      "plt.figure(1, figsize=(12,12))\n",
      "plt.title('filters in the first layer')\n",
      "lab.imarraysc(model[0].weight, spacing=1) ;\n",
      "129/63:\n",
      "# Initialize the model\n",
      "model = new_model()\n",
      "\n",
      "# Run SGD to train the model\n",
      "model = lab.train_model(model, imdb, num_epochs=2, use_gpu=False)\n",
      "132/1: %matplotlib inline\n",
      "132/2:\n",
      "import torch\n",
      "import torchvision\n",
      "import torchvision.transforms as transforms\n",
      "133/1:\n",
      "import torch\n",
      "import torchvision\n",
      "import torchvision.transforms as transforms\n",
      "133/2:\n",
      "transform = transforms.Compose(\n",
      "    [transforms.ToTensor(),\n",
      "     transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))])\n",
      "\n",
      "trainset = torchvision.datasets.CIFAR10(root='./data', train=True,\n",
      "                                        download=True, transform=transform)\n",
      "trainloader = torch.utils.data.DataLoader(trainset, batch_size=4,\n",
      "                                          shuffle=True, num_workers=2)\n",
      "\n",
      "testset = torchvision.datasets.CIFAR10(root='./data', train=False,\n",
      "                                       download=True, transform=transform)\n",
      "testloader = torch.utils.data.DataLoader(testset, batch_size=4,\n",
      "                                         shuffle=False, num_workers=2)\n",
      "\n",
      "classes = ('plane', 'car', 'bird', 'cat',\n",
      "           'deer', 'dog', 'frog', 'horse', 'ship', 'truck')\n",
      "133/3: %matplotlib inline\n",
      "133/4:\n",
      "import matplotlib.pyplot as plt\n",
      "import numpy as np\n",
      "\n",
      "# functions to show an image\n",
      "\n",
      "\n",
      "def imshow(img):\n",
      "    img = img / 2 + 0.5     # unnormalize\n",
      "    npimg = img.numpy()\n",
      "    plt.imshow(np.transpose(npimg, (1, 2, 0)))\n",
      "    plt.show()\n",
      "\n",
      "\n",
      "# get some random training images\n",
      "dataiter = iter(trainloader)\n",
      "images, labels = dataiter.next()\n",
      "print('Shape of images tensor: {}'.format(images.size()))\n",
      "# show images\n",
      "imshow(torchvision.utils.make_grid(images))\n",
      "# print labels\n",
      "print(' '.join('%5s' % classes[labels[j]] for j in range(4)))\n",
      "133/5:\n",
      "import torch.nn as nn\n",
      "import torch.nn.functional as F\n",
      "\n",
      "\n",
      "class Net(nn.Module):\n",
      "    def __init__(self):\n",
      "        super(Net, self).__init__()\n",
      "        self.conv1 = nn.Conv2d(3, 6, 5)\n",
      "        self.pool = nn.MaxPool2d(2, 2)\n",
      "        self.conv2 = nn.Conv2d(6, 16, 5)\n",
      "        self.fc1 = nn.Linear(16 * 5 * 5, 120)\n",
      "        self.fc2 = nn.Linear(120, 84)\n",
      "        self.fc3 = nn.Linear(84, 10)\n",
      "\n",
      "    def forward(self, x):\n",
      "        x = self.pool(F.relu(self.conv1(x)))\n",
      "        x = self.pool(F.relu(self.conv2(x)))\n",
      "        x = x.view(-1, 16 * 5 * 5)\n",
      "        x = F.relu(self.fc1(x))\n",
      "        x = F.relu(self.fc2(x))\n",
      "        x = self.fc3(x)\n",
      "        return x\n",
      "\n",
      "\n",
      "net = Net()\n",
      "133/6:\n",
      "import torch.optim as optim\n",
      "\n",
      "criterion = nn.CrossEntropyLoss()\n",
      "optimizer = optim.SGD(net.parameters(), lr=0.001, momentum=0.9)\n",
      "133/7:\n",
      "for epoch in range(2):  # loop over the dataset multiple times\n",
      "\n",
      "    running_loss = 0.0\n",
      "    for i, data in enumerate(trainloader, 0):\n",
      "        # get the inputs; data is a list of [inputs, labels]\n",
      "        inputs, labels = data\n",
      "\n",
      "        # zero the parameter gradients\n",
      "        optimizer.zero_grad()\n",
      "\n",
      "        # forward + backward + optimize\n",
      "        outputs = net(inputs)\n",
      "        loss = criterion(outputs, labels)\n",
      "        loss.backward()\n",
      "        optimizer.step()\n",
      "\n",
      "        # print statistics\n",
      "        running_loss += loss.item()\n",
      "        if i % 2000 == 1999:    # print every 2000 mini-batches\n",
      "            print('[%d, %5d] loss: %.3f' %\n",
      "                  (epoch + 1, i + 1, running_loss / 2000))\n",
      "            running_loss = 0.0\n",
      "\n",
      "print('Finished Training')\n",
      "133/8:\n",
      "transform = transforms.Compose(\n",
      "    [transforms.ToTensor(),\n",
      "     transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))])\n",
      "\n",
      "trainset = torchvision.datasets.CIFAR10(root='./data', train=True,\n",
      "                                        download=True, transform=transform)\n",
      "trainloader = torch.utils.data.DataLoader(trainset, batch_size=4,\n",
      "                                          shuffle=True, num_workers=2)\n",
      "\n",
      "testset = torchvision.datasets.CIFAR10(root='./data', train=False,\n",
      "                                       download=True, transform=transform)\n",
      "testloader = torch.utils.data.DataLoader(testset, batch_size=100,\n",
      "                                         shuffle=False, num_workers=2)\n",
      "\n",
      "classes = ('plane', 'car', 'bird', 'cat',\n",
      "           'deer', 'dog', 'frog', 'horse', 'ship', 'truck')\n",
      "133/9:\n",
      "dataiter = iter(testloader)\n",
      "images, labels = dataiter.next()\n",
      "\n",
      "# print images\n",
      "imshow(torchvision.utils.make_grid(images))\n",
      "print('GroundTruth: ', ' '.join('%5s' % classes[labels[j]] for j in range(4)))\n",
      "133/10:\n",
      "dataiter = iter(testloader)\n",
      "images, labels = dataiter.next()\n",
      "\n",
      "# print images\n",
      "imshow(torchvision.utils.make_grid(images))\n",
      "print('GroundTruth: ', ' '.join('%5s' % classes[labels[j]] for j in range(100)))\n",
      "133/11:\n",
      "outputs = net(images)\n",
      "print(outputs)\n",
      "133/12:\n",
      "_, predicted = torch.max(outputs, 1)\n",
      "\n",
      "print('Predicted: ', ' '.join('%5s' % classes[predicted[j]]\n",
      "                              for j in range(100)))\n",
      "133/13:\n",
      "correct = 0\n",
      "total = 0\n",
      "with torch.no_grad():\n",
      "    for data in testloader:\n",
      "        images, labels = data\n",
      "        outputs = net(images)\n",
      "        _, predicted = torch.max(outputs.data, 1)\n",
      "        total += labels.size(0)\n",
      "        correct += (predicted == labels).sum().item()\n",
      "\n",
      "print('Accuracy of the network on the 10000 test images: %d %%' % (\n",
      "    100 * correct / total))\n",
      "133/14:\n",
      "correct = 0\n",
      "total = 0\n",
      "with torch.no_grad():\n",
      "    for data in testloader:\n",
      "        images, labels = data\n",
      "        outputs = net(images)\n",
      "        _, predicted = torch.max(outputs.data, 1)\n",
      "        total += labels.size(0)\n",
      "        correct += (predicted == labels).sum().item()\n",
      "print(total)\n",
      "print('Accuracy of the network on the 10000 test images: %d %%' % (\n",
      "    100 * correct / total))\n",
      "133/15:\n",
      "class_correct = list(0. for i in range(10))\n",
      "class_total = list(0. for i in range(10))\n",
      "with torch.no_grad():\n",
      "    for data in testloader:\n",
      "        images, labels = data\n",
      "        outputs = net(images)\n",
      "        _, predicted = torch.max(outputs, 1)\n",
      "        c = (predicted == labels).squeeze()\n",
      "        for i in range(4):\n",
      "            label = labels[i]\n",
      "            class_correct[label] += c[i].item()\n",
      "            class_total[label] += 1\n",
      "\n",
      "\n",
      "for i in range(10):\n",
      "    print('Accuracy of %5s : %2d %%' % (\n",
      "        classes[i], 100 * class_correct[i] / class_total[i]))\n",
      "133/16:\n",
      "transform = transforms.Compose(\n",
      "    [transforms.ToTensor(),\n",
      "     transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))])\n",
      "\n",
      "trainset = torchvision.datasets.CIFAR10(root='./data', train=True,\n",
      "                                        download=True, transform=transform)\n",
      "trainloader = torch.utils.data.DataLoader(trainset, batch_size=4,\n",
      "                                          shuffle=True, num_workers=2)\n",
      "\n",
      "testset = torchvision.datasets.CIFAR10(root='./data', train=False,\n",
      "                                       download=True, transform=transform)\n",
      "testloader = torch.utils.data.DataLoader(testset, batch_size=1,\n",
      "                                         shuffle=False, num_workers=2)\n",
      "\n",
      "classes = ('plane', 'car', 'bird', 'cat',\n",
      "           'deer', 'dog', 'frog', 'horse', 'ship', 'truck')\n",
      "133/17:\n",
      "dataiter = iter(testloader)\n",
      "images, labels = dataiter.next()\n",
      "\n",
      "# print images\n",
      "imshow(torchvision.utils.make_grid(images))\n",
      "print('GroundTruth: ', ' '.join('%5s' % classes[labels[j]] for j in range(100)))\n",
      "133/18:\n",
      "dataiter = iter(testloader)\n",
      "images, labels = dataiter.next()\n",
      "\n",
      "# print images\n",
      "imshow(torchvision.utils.make_grid(images))\n",
      "print('GroundTruth: ', ' '.join('%5s' % classes[labels[j]] for j in range(1)))\n",
      "133/19:\n",
      "_, predicted = torch.max(outputs, 1)\n",
      "\n",
      "print('Predicted: ', ' '.join('%5s' % classes[predicted[j]]\n",
      "                              for j in range(100)))\n",
      "133/20:\n",
      "dataiter = iter(testloader)\n",
      "images, labels = dataiter.next()\n",
      "\n",
      "# print images\n",
      "imshow(torchvision.utils.make_grid(images))\n",
      "print('GroundTruth: ', ' '.join('%5s' % classes[labels[j]] for j in range(100)))\n",
      "133/21:\n",
      "correct = 0\n",
      "total = 0\n",
      "with torch.no_grad():\n",
      "    for data in testloader:\n",
      "        images, labels = data\n",
      "        outputs = net(images)\n",
      "        _, predicted = torch.max(outputs.data, 1)\n",
      "        total += labels.size(0)\n",
      "        correct += (predicted == labels).sum().item()\n",
      "print(total)\n",
      "print('Accuracy of the network on the 10000 test images: %d %%' % (\n",
      "    100 * correct / total))\n",
      "133/22:\n",
      "transform = transforms.Compose(\n",
      "    [transforms.ToTensor(),\n",
      "     transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))])\n",
      "\n",
      "trainset = torchvision.datasets.CIFAR10(root='./data', train=True,\n",
      "                                        download=True, transform=transform)\n",
      "trainloader = torch.utils.data.DataLoader(trainset, batch_size=4,\n",
      "                                          shuffle=True, num_workers=2)\n",
      "\n",
      "testset = torchvision.datasets.CIFAR10(root='./data', train=False,\n",
      "                                       download=True, transform=transform)\n",
      "testloader = torch.utils.data.DataLoader(testset, batch_size=100,\n",
      "                                         shuffle=False, num_workers=2)\n",
      "\n",
      "classes = ('plane', 'car', 'bird', 'cat',\n",
      "           'deer', 'dog', 'frog', 'horse', 'ship', 'truck')\n",
      "133/23:\n",
      "dataiter = iter(testloader)\n",
      "images, labels = dataiter.next()\n",
      "\n",
      "# print images\n",
      "imshow(torchvision.utils.make_grid(images))\n",
      "print('GroundTruth: ', ' '.join('%5s' % classes[labels[j]] for j in range(100)))\n",
      "133/24:\n",
      "correct = 0\n",
      "total = 0\n",
      "with torch.no_grad():\n",
      "    for data in testloader:\n",
      "        images, labels = data\n",
      "        outputs = net(images)\n",
      "        _, predicted = torch.max(outputs.data, 1)\n",
      "        total += labels.size(0)\n",
      "        correct += (predicted == labels).sum().item()\n",
      "print(total)\n",
      "print('Accuracy of the network on the 10000 test images: %d %%' % (\n",
      "    100 * correct / total))\n",
      "133/25:\n",
      "class_correct = list(0. for i in range(10))\n",
      "class_total = list(0. for i in range(10))\n",
      "with torch.no_grad():\n",
      "    for data in testloader:\n",
      "        images, labels = data\n",
      "        outputs = net(images)\n",
      "        _, predicted = torch.max(outputs, 1)\n",
      "        c = (predicted == labels).squeeze()\n",
      "        for i in range(4):\n",
      "            label = labels[i]\n",
      "            class_correct[label] += c[i].item()\n",
      "            class_total[label] += 1\n",
      "\n",
      "\n",
      "for i in range(10):\n",
      "    print('Accuracy of %5s : %2d %%' % (\n",
      "        classes[i], 100 * class_correct[i] / class_total[i]))\n",
      "133/26:\n",
      "class_correct = list(0. for i in range(10))\n",
      "class_total = list(0. for i in range(10))\n",
      "with torch.no_grad():\n",
      "    for data in testloader:\n",
      "        images, labels = data\n",
      "        outputs = net(images)\n",
      "        _, predicted = torch.max(outputs, 1)\n",
      "        c = (predicted == labels).squeeze()\n",
      "        for i in range(100):\n",
      "            label = labels[i]\n",
      "            class_correct[label] += c[i].item()\n",
      "            class_total[label] += 1\n",
      "\n",
      "\n",
      "for i in range(10):\n",
      "    print('Accuracy of %5s : %2d %%' % (\n",
      "        classes[i], 100 * class_correct[i] / class_total[i]))\n",
      "133/27:\n",
      "class_correct = list(0. for i in range(10))\n",
      "class_total = list(0. for i in range(10))\n",
      "with torch.no_grad():\n",
      "    for data in testloader:\n",
      "        print(\"hi\")\n",
      "        images, labels = data\n",
      "        outputs = net(images)\n",
      "        _, predicted = torch.max(outputs, 1)\n",
      "        c = (predicted == labels).squeeze()\n",
      "        for i in range(100):\n",
      "            label = labels[i]\n",
      "            class_correct[label] += c[i].item()\n",
      "            class_total[label] += 1\n",
      "\n",
      "\n",
      "for i in range(10):\n",
      "    print('Accuracy of %5s : %2d %%' % (\n",
      "        classes[i], 100 * class_correct[i] / class_total[i]))\n",
      "133/28:\n",
      "class_correct = list(0. for i in range(10))\n",
      "class_total = list(0. for i in range(10))\n",
      "with torch.no_grad():\n",
      "    i = 0\n",
      "    for data in testloader:\n",
      "        print(i)\n",
      "        i += 1\n",
      "        images, labels = data\n",
      "        outputs = net(images)\n",
      "        _, predicted = torch.max(outputs, 1)\n",
      "        c = (predicted == labels).squeeze()\n",
      "        for i in range(100):\n",
      "            label = labels[i]\n",
      "            class_correct[label] += c[i].item()\n",
      "            class_total[label] += 1\n",
      "\n",
      "\n",
      "for i in range(10):\n",
      "    print('Accuracy of %5s : %2d %%' % (\n",
      "        classes[i], 100 * class_correct[i] / class_total[i]))\n",
      "133/29:\n",
      "class_correct = list(0. for i in range(10))\n",
      "class_total = list(0. for i in range(10))\n",
      "with torch.no_grad():\n",
      "    j = 0\n",
      "    for data in testloader:\n",
      "        print(j)\n",
      "        j += 1\n",
      "        images, labels = data\n",
      "        outputs = net(images)\n",
      "        _, predicted = torch.max(outputs, 1)\n",
      "        c = (predicted == labels).squeeze()\n",
      "        for i in range(100):\n",
      "            label = labels[i]\n",
      "            class_correct[label] += c[i].item()\n",
      "            class_total[label] += 1\n",
      "\n",
      "\n",
      "for i in range(10):\n",
      "    print('Accuracy of %5s : %2d %%' % (\n",
      "        classes[i], 100 * class_correct[i] / class_total[i]))\n",
      "133/30:\n",
      "class_correct = list(0. for i in range(10))\n",
      "class_total = list(0. for i in range(10))\n",
      "with torch.no_grad():\n",
      "    j = 0\n",
      "    for data in testloader:\n",
      "        print(j)\n",
      "        j += 1\n",
      "        images, labels = data\n",
      "        print(\"im \", len(images))\n",
      "        outputs = net(images)\n",
      "        _, predicted = torch.max(outputs, 1)\n",
      "        c = (predicted == labels).squeeze()\n",
      "        for i in range(100):\n",
      "            label = labels[i]\n",
      "            class_correct[label] += c[i].item()\n",
      "            class_total[label] += 1\n",
      "\n",
      "\n",
      "for i in range(10):\n",
      "    print('Accuracy of %5s : %2d %%' % (\n",
      "        classes[i], 100 * class_correct[i] / class_total[i]))\n",
      "133/31:\n",
      "class_correct = list(0. for i in range(10))\n",
      "class_total = list(0. for i in range(10))\n",
      "with torch.no_grad():\n",
      "    for data in testloader:\n",
      "        images, labels = data\n",
      "        outputs = net(images)\n",
      "        loss = criterion(outputs, labels)\n",
      "        _, predicted = torch.max(outputs, 1)\n",
      "        c = (predicted == labels).squeeze()\n",
      "        for i in range(4):\n",
      "            label = labels[i]\n",
      "            class_correct[label] += c[i].item()\n",
      "            class_total[label] += 1\n",
      "\n",
      "\n",
      "for i in range(10):\n",
      "    print('Accuracy of %5s : %2d %%' % (\n",
      "        classes[i], 100 * class_correct[i] / class_total[i]))\n",
      "133/32: labels\n",
      "133/33:\n",
      "class_correct = list(0. for i in range(10))\n",
      "class_total = list(0. for i in range(10))\n",
      "softmaxes = {k:[] for k in range(10)}\n",
      "with torch.no_grad():\n",
      "    for data in testloader:\n",
      "        images, labels = data\n",
      "        outputs = net(images)\n",
      "        _, predicted = torch.max(outputs, 1)\n",
      "        c = (predicted == labels).squeeze()\n",
      "        for i in range(100):\n",
      "            label = labels[i]\n",
      "            class_correct[label] += c[i].item()\n",
      "            class_total[label] += 1\n",
      "\n",
      "\n",
      "for i in range(10):\n",
      "    print('Accuracy of %5s : %2d %%' % (\n",
      "        classes[i], 100 * class_correct[i] / class_total[i]))\n",
      "133/34:\n",
      "class_correct = list(0. for i in range(10))\n",
      "class_total = list(0. for i in range(10))\n",
      "softmaxes = {k:[] for k in range(10)}\n",
      "with torch.no_grad():\n",
      "    for data in testloader:\n",
      "        images, labels = data\n",
      "        outputs = net(images)\n",
      "        _, predicted = torch.max(outputs, 1)\n",
      "        c = (predicted == labels).squeeze()\n",
      "        for i in range(100):\n",
      "            label = labels[i]\n",
      "            class_correct[label] += c[i].item()\n",
      "            class_total[label] += 1\n",
      "            softmaxes[label].append(outputs[i])\n",
      "\n",
      "for i in range(10):\n",
      "    print('Softmax loss of %5s : %2d %%' % (\n",
      "        classes[i], criterion(softmaxes[classes[i]], len(softmaxes[classes[i]])))\n",
      "\n",
      "print()\n",
      "\n",
      "for i in range(10):\n",
      "    print('Accuracy of %5s : %2d %%' % (\n",
      "        classes[i], 100 * class_correct[i] / class_total[i]))\n",
      "133/35:\n",
      "class_correct = list(0. for i in range(10))\n",
      "class_total = list(0. for i in range(10))\n",
      "softmaxes = {k:[] for k in range(10)}\n",
      "with torch.no_grad():\n",
      "    for data in testloader:\n",
      "        images, labels = data\n",
      "        outputs = net(images)\n",
      "        _, predicted = torch.max(outputs, 1)\n",
      "        c = (predicted == labels).squeeze()\n",
      "        for i in range(100):\n",
      "            label = labels[i]\n",
      "            class_correct[label] += c[i].item()\n",
      "            class_total[label] += 1\n",
      "            softmaxes[label].append(outputs[i])\n",
      "\n",
      "for i in range(10):\n",
      "    print('Softmax loss of %5s : %2d %%' % (\n",
      "        classes[i], criterion(softmaxes[classes[i]], len(softmaxes[classes[i]])))\n",
      "\n",
      "for i in range(10):\n",
      "    print('Accuracy of %5s : %2d %%' % (\n",
      "        classes[i], 100 * class_correct[i] / class_total[i]))\n",
      "133/36:\n",
      "class_correct = list(0. for i in range(10))\n",
      "class_total = list(0. for i in range(10))\n",
      "softmaxes = {k:[] for k in range(10)}\n",
      "with torch.no_grad():\n",
      "    for data in testloader:\n",
      "        images, labels = data\n",
      "        outputs = net(images)\n",
      "        _, predicted = torch.max(outputs, 1)\n",
      "        c = (predicted == labels).squeeze()\n",
      "        for i in range(100):\n",
      "            label = labels[i]\n",
      "            class_correct[label] += c[i].item()\n",
      "            class_total[label] += 1\n",
      "            softmaxes[label].append(outputs[i])\n",
      "\n",
      "for i in range(10):\n",
      "    print('Softmax loss of %5s : %2d %%' % (\n",
      "        classes[i], criterion(softmaxes[classes[i]], len(softmaxes[classes[i]]))))\n",
      "\n",
      "for i in range(10):\n",
      "    print('Accuracy of %5s : %2d %%' % (\n",
      "        classes[i], 100 * class_correct[i] / class_total[i]))\n",
      "133/37:\n",
      "class_correct = list(0. for i in range(10))\n",
      "class_total = list(0. for i in range(10))\n",
      "softmaxes = {k:[] for k in range(10)}\n",
      "with torch.no_grad():\n",
      "    for data in testloader:\n",
      "        images, labels = data\n",
      "        outputs = net(images)\n",
      "        _, predicted = torch.max(outputs, 1)\n",
      "        c = (predicted == labels).squeeze()\n",
      "        for i in range(100):\n",
      "            label = labels[i]\n",
      "            class_correct[label] += c[i].item()\n",
      "            class_total[label] += 1\n",
      "            softmaxes[label].append(outputs[i])\n",
      "print(softmaxes)\n",
      "for i in range(10):\n",
      "    print('Softmax loss of %5s : %2d %%' % (\n",
      "        classes[i], criterion(softmaxes[classes[i]], len(softmaxes[classes[i]]))))\n",
      "\n",
      "for i in range(10):\n",
      "    print('Accuracy of %5s : %2d %%' % (\n",
      "        classes[i], 100 * class_correct[i] / class_total[i]))\n",
      "133/38:\n",
      "class_correct = list(0. for i in range(10))\n",
      "class_total = list(0. for i in range(10))\n",
      "softmaxes = {k:[] for k in range(10)}\n",
      "print(softmaxes)\n",
      "with torch.no_grad():\n",
      "    for data in testloader:\n",
      "        images, labels = data\n",
      "        outputs = net(images)\n",
      "        _, predicted = torch.max(outputs, 1)\n",
      "        c = (predicted == labels).squeeze()\n",
      "        for i in range(100):\n",
      "            label = labels[i]\n",
      "            class_correct[label] += c[i].item()\n",
      "            class_total[label] += 1\n",
      "            softmaxes[label].append(outputs[i])\n",
      "\n",
      "for i in range(10):\n",
      "    print('Softmax loss of %5s : %2d %%' % (\n",
      "        classes[i], criterion(softmaxes[classes[i]], len(softmaxes[classes[i]]))))\n",
      "\n",
      "for i in range(10):\n",
      "    print('Accuracy of %5s : %2d %%' % (\n",
      "        classes[i], 100 * class_correct[i] / class_total[i]))\n",
      "133/39:\n",
      "class_correct = list(0. for i in range(10))\n",
      "class_total = list(0. for i in range(10))\n",
      "softmaxes = {k:[] for k in range(10)}\n",
      "softmaxes[1]\n",
      "with torch.no_grad():\n",
      "    for data in testloader:\n",
      "        images, labels = data\n",
      "        outputs = net(images)\n",
      "        _, predicted = torch.max(outputs, 1)\n",
      "        c = (predicted == labels).squeeze()\n",
      "        for i in range(100):\n",
      "            label = labels[i]\n",
      "            class_correct[label] += c[i].item()\n",
      "            class_total[label] += 1\n",
      "            softmaxes[label].append(outputs[i])\n",
      "\n",
      "for i in range(10):\n",
      "    print('Softmax loss of %5s : %2d %%' % (\n",
      "        classes[i], criterion(softmaxes[classes[i]], len(softmaxes[classes[i]]))))\n",
      "\n",
      "for i in range(10):\n",
      "    print('Accuracy of %5s : %2d %%' % (\n",
      "        classes[i], 100 * class_correct[i] / class_total[i]))\n",
      "133/40:\n",
      "class_correct = list(0. for i in range(10))\n",
      "class_total = list(0. for i in range(10))\n",
      "softmaxes = {k:[] for k in range(10)}\n",
      "softmaxes[0]\n",
      "with torch.no_grad():\n",
      "    for data in testloader:\n",
      "        images, labels = data\n",
      "        outputs = net(images)\n",
      "        _, predicted = torch.max(outputs, 1)\n",
      "        c = (predicted == labels).squeeze()\n",
      "        for i in range(100):\n",
      "            label = labels[i]\n",
      "            class_correct[label] += c[i].item()\n",
      "            class_total[label] += 1\n",
      "            softmaxes[label].append(outputs[i])\n",
      "\n",
      "for i in range(10):\n",
      "    print('Softmax loss of %5s : %2d %%' % (\n",
      "        classes[i], criterion(softmaxes[classes[i]], len(softmaxes[classes[i]]))))\n",
      "\n",
      "for i in range(10):\n",
      "    print('Accuracy of %5s : %2d %%' % (\n",
      "        classes[i], 100 * class_correct[i] / class_total[i]))\n",
      "133/41:\n",
      "class_correct = list(0. for i in range(10))\n",
      "class_total = list(0. for i in range(10))\n",
      "softmaxes = {k:[] for k in range(10)}\n",
      "softmaxes['0']\n",
      "with torch.no_grad():\n",
      "    for data in testloader:\n",
      "        images, labels = data\n",
      "        outputs = net(images)\n",
      "        _, predicted = torch.max(outputs, 1)\n",
      "        c = (predicted == labels).squeeze()\n",
      "        for i in range(100):\n",
      "            label = labels[i]\n",
      "            class_correct[label] += c[i].item()\n",
      "            class_total[label] += 1\n",
      "            softmaxes[label].append(outputs[i])\n",
      "\n",
      "for i in range(10):\n",
      "    print('Softmax loss of %5s : %2d %%' % (\n",
      "        classes[i], criterion(softmaxes[classes[i]], len(softmaxes[classes[i]]))))\n",
      "\n",
      "for i in range(10):\n",
      "    print('Accuracy of %5s : %2d %%' % (\n",
      "        classes[i], 100 * class_correct[i] / class_total[i]))\n",
      "133/42:\n",
      "class_correct = list(0. for i in range(10))\n",
      "class_total = list(0. for i in range(10))\n",
      "softmaxes = {k:[] for k in range(10)}\n",
      "softmaxes[0]\n",
      "with torch.no_grad():\n",
      "    for data in testloader:\n",
      "        images, labels = data\n",
      "        outputs = net(images)\n",
      "        _, predicted = torch.max(outputs, 1)\n",
      "        c = (predicted == labels).squeeze()\n",
      "        for i in range(100):\n",
      "            label = labels[i]\n",
      "            class_correct[label] += c[i].item()\n",
      "            class_total[label] += 1\n",
      "            softmaxes[label].append(outputs[i])\n",
      "\n",
      "for i in range(10):\n",
      "    print('Softmax loss of %5s : %2d %%' % (\n",
      "        classes[i], criterion(softmaxes[classes[i]], len(softmaxes[classes[i]]))))\n",
      "\n",
      "for i in range(10):\n",
      "    print('Accuracy of %5s : %2d %%' % (\n",
      "        classes[i], 100 * class_correct[i] / class_total[i]))\n",
      "133/43:\n",
      "class_correct = list(0. for i in range(10))\n",
      "class_total = list(0. for i in range(10))\n",
      "softmaxes = {k:[] for k in range(10)}\n",
      "softmaxes.get(0)\n",
      "with torch.no_grad():\n",
      "    for data in testloader:\n",
      "        images, labels = data\n",
      "        outputs = net(images)\n",
      "        _, predicted = torch.max(outputs, 1)\n",
      "        c = (predicted == labels).squeeze()\n",
      "        for i in range(100):\n",
      "            label = labels[i]\n",
      "            class_correct[label] += c[i].item()\n",
      "            class_total[label] += 1\n",
      "            softmaxes[label].append(outputs[i])\n",
      "\n",
      "for i in range(10):\n",
      "    print('Softmax loss of %5s : %2d %%' % (\n",
      "        classes[i], criterion(softmaxes[classes[i]], len(softmaxes[classes[i]]))))\n",
      "\n",
      "for i in range(10):\n",
      "    print('Accuracy of %5s : %2d %%' % (\n",
      "        classes[i], 100 * class_correct[i] / class_total[i]))\n",
      "133/44:\n",
      "class_correct = list(0. for i in range(10))\n",
      "class_total = list(0. for i in range(10))\n",
      "softmaxes = {k:[] for k in range(10)}\n",
      "softmaxes.0\n",
      "with torch.no_grad():\n",
      "    for data in testloader:\n",
      "        images, labels = data\n",
      "        outputs = net(images)\n",
      "        _, predicted = torch.max(outputs, 1)\n",
      "        c = (predicted == labels).squeeze()\n",
      "        for i in range(100):\n",
      "            label = labels[i]\n",
      "            class_correct[label] += c[i].item()\n",
      "            class_total[label] += 1\n",
      "            softmaxes[label].append(outputs[i])\n",
      "\n",
      "for i in range(10):\n",
      "    print('Softmax loss of %5s : %2d %%' % (\n",
      "        classes[i], criterion(softmaxes[classes[i]], len(softmaxes[classes[i]]))))\n",
      "\n",
      "for i in range(10):\n",
      "    print('Accuracy of %5s : %2d %%' % (\n",
      "        classes[i], 100 * class_correct[i] / class_total[i]))\n",
      "133/45:\n",
      "class_correct = list(0. for i in range(10))\n",
      "class_total = list(0. for i in range(10))\n",
      "softmaxes = {k:[] for k in range(10)}\n",
      "with torch.no_grad():\n",
      "    for data in testloader:\n",
      "        images, labels = data\n",
      "        outputs = net(images)\n",
      "        _, predicted = torch.max(outputs, 1)\n",
      "        c = (predicted == labels).squeeze()\n",
      "        for i in range(100):\n",
      "            label = labels[i]\n",
      "            class_correct[label] += c[i].item()\n",
      "            class_total[label] += 1\n",
      "            softmaxes.get(label).append(outputs[i])\n",
      "\n",
      "for i in range(10):\n",
      "    print('Softmax loss of %5s : %2d %%' % (\n",
      "        classes[i], criterion(softmaxes.get(classes[i]), len(softmaxes[classes[i]]))))\n",
      "\n",
      "for i in range(10):\n",
      "    print('Accuracy of %5s : %2d %%' % (\n",
      "        classes[i], 100 * class_correct[i] / class_total[i]))\n",
      "133/46:\n",
      "class_correct = list(0. for i in range(10))\n",
      "class_total = list(0. for i in range(10))\n",
      "softmaxes = {k:[] for k in range(10)}\n",
      "print(softmaxes.get(0))\n",
      "with torch.no_grad():\n",
      "    for data in testloader:\n",
      "        images, labels = data\n",
      "        outputs = net(images)\n",
      "        _, predicted = torch.max(outputs, 1)\n",
      "        c = (predicted == labels).squeeze()\n",
      "        for i in range(100):\n",
      "            label = labels[i]\n",
      "            class_correct[label] += c[i].item()\n",
      "            class_total[label] += 1\n",
      "            softmaxes.get(label).append(outputs[i])\n",
      "\n",
      "for i in range(10):\n",
      "    print('Softmax loss of %5s : %2d %%' % (\n",
      "        classes[i], criterion(softmaxes.get(classes[i]), len(softmaxes[classes[i]]))))\n",
      "\n",
      "for i in range(10):\n",
      "    print('Accuracy of %5s : %2d %%' % (\n",
      "        classes[i], 100 * class_correct[i] / class_total[i]))\n",
      "133/47:\n",
      "class_correct = list(0. for i in range(10))\n",
      "class_total = list(0. for i in range(10))\n",
      "softmaxes = {k:[] for k in range(10)}\n",
      "print(softmaxes.get(0).append(1))\n",
      "with torch.no_grad():\n",
      "    for data in testloader:\n",
      "        images, labels = data\n",
      "        outputs = net(images)\n",
      "        _, predicted = torch.max(outputs, 1)\n",
      "        c = (predicted == labels).squeeze()\n",
      "        for i in range(100):\n",
      "            label = labels[i]\n",
      "            class_correct[label] += c[i].item()\n",
      "            class_total[label] += 1\n",
      "            softmaxes.get(label).append(outputs[i])\n",
      "\n",
      "for i in range(10):\n",
      "    print('Softmax loss of %5s : %2d %%' % (\n",
      "        classes[i], criterion(softmaxes.get(classes[i]), len(softmaxes[classes[i]]))))\n",
      "\n",
      "for i in range(10):\n",
      "    print('Accuracy of %5s : %2d %%' % (\n",
      "        classes[i], 100 * class_correct[i] / class_total[i]))\n",
      "133/48:\n",
      "class_correct = list(0. for i in range(10))\n",
      "class_total = list(0. for i in range(10))\n",
      "softmaxes = {k:[] for k in range(10)}\n",
      "print(softmaxes.get(0))\n",
      "with torch.no_grad():\n",
      "    for data in testloader:\n",
      "        images, labels = data\n",
      "        outputs = net(images)\n",
      "        _, predicted = torch.max(outputs, 1)\n",
      "        c = (predicted == labels).squeeze()\n",
      "        for i in range(100):\n",
      "            label = labels[i]\n",
      "            class_correct[label] += c[i].item()\n",
      "            class_total[label] += 1\n",
      "            softmaxes.get(label).append(outputs[i])\n",
      "\n",
      "for i in range(10):\n",
      "    print('Softmax loss of %5s : %2d %%' % (\n",
      "        classes[i], criterion(softmaxes.get(classes[i]), len(softmaxes[classes[i]]))))\n",
      "\n",
      "for i in range(10):\n",
      "    print('Accuracy of %5s : %2d %%' % (\n",
      "        classes[i], 100 * class_correct[i] / class_total[i]))\n",
      "133/49:\n",
      "class_correct = list(0. for i in range(10))\n",
      "class_total = list(0. for i in range(10))\n",
      "softmaxes = {k:[] for k in range(10)}\n",
      "x = softmaxes.get(0)\n",
      "print(len(x), x.append(1))\n",
      "with torch.no_grad():\n",
      "    for data in testloader:\n",
      "        images, labels = data\n",
      "        outputs = net(images)\n",
      "        _, predicted = torch.max(outputs, 1)\n",
      "        c = (predicted == labels).squeeze()\n",
      "        for i in range(100):\n",
      "            label = labels[i]\n",
      "            class_correct[label] += c[i].item()\n",
      "            class_total[label] += 1\n",
      "            softmaxes.get(label).append(outputs[i])\n",
      "\n",
      "for i in range(10):\n",
      "    print('Softmax loss of %5s : %2d %%' % (\n",
      "        classes[i], criterion(softmaxes.get(classes[i]), len(softmaxes[classes[i]]))))\n",
      "\n",
      "for i in range(10):\n",
      "    print('Accuracy of %5s : %2d %%' % (\n",
      "        classes[i], 100 * class_correct[i] / class_total[i]))\n",
      "133/50:\n",
      "class_correct = list(0. for i in range(10))\n",
      "class_total = list(0. for i in range(10))\n",
      "softmaxes = {k:[] for k in range(10)}\n",
      "x = softmaxes.get(0)\n",
      "print(len(x), x.append(1))\n",
      "with torch.no_grad():\n",
      "    for data in testloader:\n",
      "        images, labels = data\n",
      "        outputs = net(images)\n",
      "        _, predicted = torch.max(outputs, 1)\n",
      "        c = (predicted == labels).squeeze()\n",
      "        for i in range(100):\n",
      "            label = labels[i]\n",
      "            class_correct[label] += c[i].item()\n",
      "            class_total[label] += 1\n",
      "            a = softmaxes.get(label)\n",
      "            a.append(outputs[i])\n",
      "\n",
      "for i in range(10):\n",
      "    print('Softmax loss of %5s : %2d %%' % (\n",
      "        classes[i], criterion(softmaxes.get(classes[i]), len(softmaxes[classes[i]]))))\n",
      "\n",
      "for i in range(10):\n",
      "    print('Accuracy of %5s : %2d %%' % (\n",
      "        classes[i], 100 * class_correct[i] / class_total[i]))\n",
      "133/51:\n",
      "class_correct = list(0. for i in range(10))\n",
      "class_total = list(0. for i in range(10))\n",
      "softmaxes = {k:[] for k in range(10)}\n",
      "x = softmaxes.get(0)\n",
      "print(len(x), x.append(1))\n",
      "with torch.no_grad():\n",
      "    for data in testloader:\n",
      "        images, labels = data\n",
      "        outputs = net(images)\n",
      "        _, predicted = torch.max(outputs, 1)\n",
      "        c = (predicted == labels).squeeze()\n",
      "        for i in range(100):\n",
      "            label = labels[i]\n",
      "            class_correct[label] += c[i].item()\n",
      "            class_total[label] += 1\n",
      "            a = softmaxes.get(label)\n",
      "            print(a)\n",
      "            a.append(outputs[i])\n",
      "\n",
      "for i in range(10):\n",
      "    print('Softmax loss of %5s : %2d %%' % (\n",
      "        classes[i], criterion(softmaxes.get(classes[i]), len(softmaxes[classes[i]]))))\n",
      "\n",
      "for i in range(10):\n",
      "    print('Accuracy of %5s : %2d %%' % (\n",
      "        classes[i], 100 * class_correct[i] / class_total[i]))\n",
      "133/52:\n",
      "class_correct = list(0. for i in range(10))\n",
      "class_total = list(0. for i in range(10))\n",
      "softmaxes = {k:np.array([]) for k in range(10)}\n",
      "x = softmaxes.get(0)\n",
      "print(len(x), x.append(1))\n",
      "with torch.no_grad():\n",
      "    for data in testloader:\n",
      "        images, labels = data\n",
      "        outputs = net(images)\n",
      "        _, predicted = torch.max(outputs, 1)\n",
      "        c = (predicted == labels).squeeze()\n",
      "        for i in range(100):\n",
      "            label = labels[i]\n",
      "            class_correct[label] += c[i].item()\n",
      "            class_total[label] += 1\n",
      "            a = softmaxes.get(label)\n",
      "            print(a)\n",
      "            a.append(outputs[i])\n",
      "\n",
      "for i in range(10):\n",
      "    print('Softmax loss of %5s : %2d %%' % (\n",
      "        classes[i], criterion(softmaxes.get(classes[i]), len(softmaxes[classes[i]]))))\n",
      "\n",
      "for i in range(10):\n",
      "    print('Accuracy of %5s : %2d %%' % (\n",
      "        classes[i], 100 * class_correct[i] / class_total[i]))\n",
      "133/53:\n",
      "class_correct = list(0. for i in range(10))\n",
      "class_total = list(0. for i in range(10))\n",
      "softmaxes = {k:list() for k in range(10)}\n",
      "x = softmaxes.get(0)\n",
      "print(len(x), x.append(1))\n",
      "with torch.no_grad():\n",
      "    for data in testloader:\n",
      "        images, labels = data\n",
      "        outputs = net(images)\n",
      "        _, predicted = torch.max(outputs, 1)\n",
      "        c = (predicted == labels).squeeze()\n",
      "        for i in range(100):\n",
      "            label = labels[i]\n",
      "            class_correct[label] += c[i].item()\n",
      "            class_total[label] += 1\n",
      "            a = softmaxes.get(label)\n",
      "            print(a)\n",
      "            a.append(outputs[i])\n",
      "\n",
      "for i in range(10):\n",
      "    print('Softmax loss of %5s : %2d %%' % (\n",
      "        classes[i], criterion(softmaxes.get(classes[i]), len(softmaxes[classes[i]]))))\n",
      "\n",
      "for i in range(10):\n",
      "    print('Accuracy of %5s : %2d %%' % (\n",
      "        classes[i], 100 * class_correct[i] / class_total[i]))\n",
      "133/54:\n",
      "class_correct = list(0. for i in range(10))\n",
      "class_total = list(0. for i in range(10))\n",
      "softmaxes = {k:np.array([]) for k in range(10)}\n",
      "x = softmaxes.get(0)\n",
      "print(len(x), x.append(1))\n",
      "with torch.no_grad():\n",
      "    for data in testloader:\n",
      "        images, labels = data\n",
      "        outputs = net(images)\n",
      "        _, predicted = torch.max(outputs, 1)\n",
      "        c = (predicted == labels).squeeze()\n",
      "        for i in range(100):\n",
      "            label = labels[i]\n",
      "            class_correct[label] += c[i].item()\n",
      "            class_total[label] += 1\n",
      "            a = softmaxes.get(label)\n",
      "            np.insert(a, outputs[i])\n",
      "\n",
      "for i in range(10):\n",
      "    print('Softmax loss of %5s : %2d %%' % (\n",
      "        classes[i], criterion(softmaxes.get(classes[i]), len(softmaxes[classes[i]]))))\n",
      "\n",
      "for i in range(10):\n",
      "    print('Accuracy of %5s : %2d %%' % (\n",
      "        classes[i], 100 * class_correct[i] / class_total[i]))\n",
      "133/55:\n",
      "class_correct = list(0. for i in range(10))\n",
      "class_total = list(0. for i in range(10))\n",
      "softmaxes = {k:np.array([]) for k in range(10)}\n",
      "x = softmaxes.get(0)\n",
      "with torch.no_grad():\n",
      "    for data in testloader:\n",
      "        images, labels = data\n",
      "        outputs = net(images)\n",
      "        _, predicted = torch.max(outputs, 1)\n",
      "        c = (predicted == labels).squeeze()\n",
      "        for i in range(100):\n",
      "            label = labels[i]\n",
      "            class_correct[label] += c[i].item()\n",
      "            class_total[label] += 1\n",
      "            a = softmaxes.get(label)\n",
      "            np.insert(a, outputs[i])\n",
      "\n",
      "for i in range(10):\n",
      "    print('Softmax loss of %5s : %2d %%' % (\n",
      "        classes[i], criterion(softmaxes.get(classes[i]), len(softmaxes[classes[i]]))))\n",
      "\n",
      "for i in range(10):\n",
      "    print('Accuracy of %5s : %2d %%' % (\n",
      "        classes[i], 100 * class_correct[i] / class_total[i]))\n",
      "133/56:\n",
      "class_correct = list(0. for i in range(10))\n",
      "class_total = list(0. for i in range(10))\n",
      "softmaxes = {k:np.array([]) for k in range(10)}\n",
      "x = softmaxes.get(0)\n",
      "with torch.no_grad():\n",
      "    for data in testloader:\n",
      "        images, labels = data\n",
      "        outputs = net(images)\n",
      "        _, predicted = torch.max(outputs, 1)\n",
      "        c = (predicted == labels).squeeze()\n",
      "        for i in range(100):\n",
      "            label = labels[i]\n",
      "            class_correct[label] += c[i].item()\n",
      "            class_total[label] += 1\n",
      "            a = softmaxes.get(label)\n",
      "            np.insert(a, outputs[i], 0)\n",
      "\n",
      "for i in range(10):\n",
      "    print('Softmax loss of %5s : %2d %%' % (\n",
      "        classes[i], criterion(softmaxes.get(classes[i]), len(softmaxes[classes[i]]))))\n",
      "\n",
      "for i in range(10):\n",
      "    print('Accuracy of %5s : %2d %%' % (\n",
      "        classes[i], 100 * class_correct[i] / class_total[i]))\n",
      "133/57:\n",
      "class_correct = list(0. for i in range(10))\n",
      "class_total = list(0. for i in range(10))\n",
      "softmaxes = {k: for k in range(10)}\n",
      "x = softmaxes.get(0)\n",
      "with torch.no_grad():\n",
      "    for data in testloader:\n",
      "        images, labels = data\n",
      "        outputs = net(images)\n",
      "        _, predicted = torch.max(outputs, 1)\n",
      "        c = (predicted == labels).squeeze()\n",
      "        for i in range(100):\n",
      "            label = labels[i]\n",
      "            class_correct[label] += c[i].item()\n",
      "            class_total[label] += 1\n",
      "            a = softmaxes.get(label)\n",
      "            np.insert(a, outputs[i], 0)\n",
      "\n",
      "for i in range(10):\n",
      "    print('Softmax loss of %5s : %2d %%' % (\n",
      "        classes[i], criterion(softmaxes.get(classes[i]), len(softmaxes[classes[i]]))))\n",
      "\n",
      "for i in range(10):\n",
      "    print('Accuracy of %5s : %2d %%' % (\n",
      "        classes[i], 100 * class_correct[i] / class_total[i]))\n",
      "133/58:\n",
      "class_correct = list(0. for i in range(10))\n",
      "class_total = list(0. for i in range(10))\n",
      "softmaxes = {k:[] for k in range(10)}\n",
      "x = softmaxes.get(0)\n",
      "with torch.no_grad():\n",
      "    for data in testloader:\n",
      "        images, labels = data\n",
      "        outputs = net(images)\n",
      "        _, predicted = torch.max(outputs, 1)\n",
      "        c = (predicted == labels).squeeze()\n",
      "        for i in range(100):\n",
      "            label = labels[i]\n",
      "            class_correct[label] += c[i].item()\n",
      "            class_total[label] += 1\n",
      "            a = softmaxes.get(label)\n",
      "            np.insert(a, outputs[i], 0)\n",
      "\n",
      "for i in range(10):\n",
      "    print('Softmax loss of %5s : %2d %%' % (\n",
      "        classes[i], criterion(softmaxes.get(classes[i]), len(softmaxes[classes[i]]))))\n",
      "\n",
      "for i in range(10):\n",
      "    print('Accuracy of %5s : %2d %%' % (\n",
      "        classes[i], 100 * class_correct[i] / class_total[i]))\n",
      "133/59:\n",
      "class_correct = list(0. for i in range(10))\n",
      "class_total = list(0. for i in range(10))\n",
      "softmaxes = {k:[] for k in range(10)}\n",
      "x = softmaxes.get(0)\n",
      "with torch.no_grad():\n",
      "    for data in testloader:\n",
      "        images, labels = data\n",
      "        outputs = net(images)\n",
      "        _, predicted = torch.max(outputs, 1)\n",
      "        c = (predicted == labels).squeeze()\n",
      "        for i in range(100):\n",
      "            label = labels[i]\n",
      "            class_correct[label] += c[i].item()\n",
      "            class_total[label] += 1\n",
      "            a = softmaxes.get(label)\n",
      "            print(a)\n",
      "\n",
      "for i in range(10):\n",
      "    print('Softmax loss of %5s : %2d %%' % (\n",
      "        classes[i], criterion(softmaxes.get(classes[i]), len(softmaxes[classes[i]]))))\n",
      "\n",
      "for i in range(10):\n",
      "    print('Accuracy of %5s : %2d %%' % (\n",
      "        classes[i], 100 * class_correct[i] / class_total[i]))\n",
      "133/60:\n",
      "class_correct = list(0. for i in range(10))\n",
      "class_total = list(0. for i in range(10))\n",
      "softmaxes = {k:1 for k in range(10)}\n",
      "x = softmaxes.get(0)\n",
      "with torch.no_grad():\n",
      "    for data in testloader:\n",
      "        images, labels = data\n",
      "        outputs = net(images)\n",
      "        _, predicted = torch.max(outputs, 1)\n",
      "        c = (predicted == labels).squeeze()\n",
      "        for i in range(100):\n",
      "            label = labels[i]\n",
      "            class_correct[label] += c[i].item()\n",
      "            class_total[label] += 1\n",
      "            a = softmaxes.get(label)\n",
      "            print(a)\n",
      "\n",
      "for i in range(10):\n",
      "    print('Softmax loss of %5s : %2d %%' % (\n",
      "        classes[i], criterion(softmaxes.get(classes[i]), len(softmaxes[classes[i]]))))\n",
      "\n",
      "for i in range(10):\n",
      "    print('Accuracy of %5s : %2d %%' % (\n",
      "        classes[i], 100 * class_correct[i] / class_total[i]))\n",
      "133/61:\n",
      "class_correct = list(0. for i in range(10))\n",
      "class_total = list(0. for i in range(10))\n",
      "softmaxes = [[] for i in range(10)]\n",
      "with torch.no_grad():\n",
      "    for data in testloader:\n",
      "        images, labels = data\n",
      "        outputs = net(images)\n",
      "        _, predicted = torch.max(outputs, 1)\n",
      "        c = (predicted == labels).squeeze()\n",
      "        for i in range(100):\n",
      "            label = labels[i]\n",
      "            class_correct[label] += c[i].item()\n",
      "            class_total[label] += 1\n",
      "            softmaxes[label]\n",
      "\n",
      "for i in range(10):\n",
      "    print('Softmax loss of %5s : %2d %%' % (\n",
      "        classes[i], criterion(softmaxes[classes[i]], len(softmaxes[classes[i]]))))\n",
      "\n",
      "for i in range(10):\n",
      "    print('Accuracy of %5s : %2d %%' % (\n",
      "        classes[i], 100 * class_correct[i] / class_total[i]))\n",
      "133/62:\n",
      "class_correct = list(0. for i in range(10))\n",
      "class_total = list(0. for i in range(10))\n",
      "softmaxes = [[] for i in range(10)]\n",
      "with torch.no_grad():\n",
      "    for data in testloader:\n",
      "        images, labels = data\n",
      "        outputs = net(images)\n",
      "        _, predicted = torch.max(outputs, 1)\n",
      "        c = (predicted == labels).squeeze()\n",
      "        for i in range(100):\n",
      "            label = labels[i]\n",
      "            class_correct[label] += c[i].item()\n",
      "            class_total[label] += 1\n",
      "            softmaxes[label]\n",
      "\n",
      "for i in range(10):\n",
      "    print('Softmax loss of %5s : %2d %%' % (\n",
      "        classes[i], criterion(softmaxes[i], len(softmaxes[i]))))\n",
      "\n",
      "for i in range(10):\n",
      "    print('Accuracy of %5s : %2d %%' % (\n",
      "        classes[i], 100 * class_correct[i] / class_total[i]))\n",
      "133/63:\n",
      "class_correct = list(0. for i in range(10))\n",
      "class_total = list(0. for i in range(10))\n",
      "softmaxes = [[] for i in range(10)]\n",
      "with torch.no_grad():\n",
      "    for data in testloader:\n",
      "        images, labels = data\n",
      "        outputs = net(images)\n",
      "        _, predicted = torch.max(outputs, 1)\n",
      "        c = (predicted == labels).squeeze()\n",
      "        for i in range(100):\n",
      "            label = labels[i]\n",
      "            class_correct[label] += c[i].item()\n",
      "            class_total[label] += 1\n",
      "            softmaxes[label] = outputs[i]\n",
      "\n",
      "for i in range(10):\n",
      "    print('Softmax loss of %5s : %2d %%' % (\n",
      "        classes[i], criterion(softmaxes[i], len(softmaxes[i]))))\n",
      "\n",
      "for i in range(10):\n",
      "    print('Accuracy of %5s : %2d %%' % (\n",
      "        classes[i], 100 * class_correct[i] / class_total[i]))\n",
      "127/33:\n",
      "def getw2v_s(sums):\n",
      "    sentences = []\n",
      "    for t in sums:\n",
      "        sentences += nltk.sent_tokenize(t)\n",
      "    docs = [simple_preprocess(s) for s in sentences]\n",
      "    return Word2Vec(docs)\n",
      "127/34: mpols = getw2v_s(summaries_pol)\n",
      "127/35: mcomps = getw2v_s(summaries_comp)\n",
      "127/36: mtravs = getw2v_s(summaries_trav)\n",
      "133/64:\n",
      "class_correct = list(0. for i in range(10))\n",
      "class_total = list(0. for i in range(10))\n",
      "softmaxes = [[] for i in range(10)]\n",
      "softmaxes2 = [[] for i in range(10)]\n",
      "with torch.no_grad():\n",
      "    for data in testloader:\n",
      "        images, labels = data\n",
      "        outputs = net(images)\n",
      "        _, predicted = torch.max(outputs, 1)\n",
      "        c = (predicted == labels).squeeze()\n",
      "        for i in range(100):\n",
      "            label = labels[i]\n",
      "            class_correct[label] += c[i].item()\n",
      "            class_total[label] += 1\n",
      "            softmaxes[label] = outputs[i]\n",
      "            softmaxes2[label]\n",
      "\n",
      "for i in range(10):\n",
      "    print('Softmax loss of %5s : %2d %%' % (\n",
      "        classes[i], criterion(softmaxes[i], [i for j in range(len(softmaxes[i]))])))\n",
      "\n",
      "for i in range(10):\n",
      "    print('Accuracy of %5s : %2d %%' % (\n",
      "        classes[i], 100 * class_correct[i] / class_total[i]))\n",
      "133/65:\n",
      "class_correct = list(0. for i in range(10))\n",
      "class_total = list(0. for i in range(10))\n",
      "softmaxes = [[] for i in range(10)]\n",
      "with torch.no_grad():\n",
      "    for data in testloader:\n",
      "        images, labels = data\n",
      "        outputs = net(images)\n",
      "        _, predicted = torch.max(outputs, 1)\n",
      "        c = (predicted == labels).squeeze()\n",
      "        for i in range(100):\n",
      "            label = labels[i]\n",
      "            class_correct[label] += c[i].item()\n",
      "            class_total[label] += 1\n",
      "            softmaxes[label] = outputs[i]\n",
      "\n",
      "for i in range(10):\n",
      "    print('Softmax loss of %5s : %2d %%' % (\n",
      "        classes[i], criterion(softmaxes[i], [i for j in range(len(softmaxes[i]))])))\n",
      "\n",
      "for i in range(10):\n",
      "    print('Accuracy of %5s : %2d %%' % (\n",
      "        classes[i], 100 * class_correct[i] / class_total[i]))\n",
      "133/66: outputs.shape\n",
      "133/67:\n",
      "class_correct = list(0. for i in range(10))\n",
      "class_total = list(0. for i in range(10))\n",
      "with torch.no_grad():\n",
      "    for data in testloader:\n",
      "        images, labels = data\n",
      "        outputs = net(images)\n",
      "        _, predicted = torch.max(outputs, 1)\n",
      "        c = (predicted == labels).squeeze()\n",
      "        for i in range(100):\n",
      "            label = labels[i]\n",
      "            class_correct[label] += c[i].item()\n",
      "            class_total[label] += 1\n",
      "            \n",
      "for i in range(10):\n",
      "    print('Accuracy of %5s : %2d %%' % (\n",
      "        classes[i], 100 * class_correct[i] / class_total[i]))\n",
      "133/68: outputs\n",
      "133/69: outputs.shap\n",
      "133/70: outputs.shape\n",
      "133/71: labels\n",
      "133/72: criterion(outputs, labels)\n",
      "133/73:\n",
      "print(outputs.shape, labels.shape)\n",
      "criterion(outputs, labels)\n",
      "133/74:\n",
      "softmaxes = [[] for i in range(10)]\n",
      "for i in range(100):\n",
      "    softmaxes[labels[i]] = outputs[i]\n",
      "print(outputs.shape, labels.shape)\n",
      "print(softmaxes[0].shape)\n",
      "criterion(outputs, labels)\n",
      "133/75:\n",
      "softmaxes = [[] for i in range(10)]\n",
      "for i in range(100):\n",
      "    softmaxes[labels[i]] = outputs[i,:]\n",
      "print(outputs.shape, labels.shape)\n",
      "print(softmaxes[0].shape)\n",
      "criterion(outputs, labels)\n",
      "133/76:\n",
      "softmaxes = [[] for i in range(10)]\n",
      "for i in range(100):\n",
      "    softmaxes[labels[i]] = np.stack(softmaxes[labels[i]], outputs[i])\n",
      "print(outputs.shape, labels.shape)\n",
      "print(softmaxes[0].shape)\n",
      "criterion(outputs, labels)\n",
      "133/77: np.stack([], [1,2,3])\n",
      "133/78:\n",
      "softmaxes = [[] for i in range(10)]\n",
      "for i in range(100):\n",
      "    cur = softmaxes[labels[i]]\n",
      "    if cur:\n",
      "        softmaxes[labels[i]] = np.stack(cur, outputs[i])\n",
      "    else:\n",
      "        softmaxes[labels[i]] = outputs[i]\n",
      "print(outputs.shape, labels.shape)\n",
      "print(softmaxes[0].shape)\n",
      "criterion(outputs, labels)\n",
      "133/79:\n",
      "softmaxes = [[] for i in range(10)]\n",
      "for i in range(100):\n",
      "    cur = softmaxes[labels[i]]\n",
      "    if len(cur):\n",
      "        softmaxes[labels[i]] = outputs[i]\n",
      "    else:\n",
      "        softmaxes[labels[i]] = np.stack(cur, outputs[i])\n",
      "        \n",
      "print(outputs.shape, labels.shape)\n",
      "print(softmaxes[0].shape)\n",
      "criterion(outputs, labels)\n",
      "133/80:\n",
      "softmaxes = [[] for i in range(10)]\n",
      "for i in range(100):\n",
      "    cur = softmaxes[labels[i]]\n",
      "    if len(cur):\n",
      "        softmaxes[labels[i]] = np.stack(cur, outputs[i])\n",
      "    else:\n",
      "        softmaxes[labels[i]] = outputs[i]\n",
      "print(outputs.shape, labels.shape)\n",
      "print(softmaxes[0].shape)\n",
      "criterion(outputs, labels)\n",
      "133/81:\n",
      "softmaxes = [[] for i in range(10)]\n",
      "for i in range(100):\n",
      "    cur = softmaxes[labels[i]]\n",
      "    print(outputs[i])\n",
      "    if len(cur):\n",
      "        softmaxes[labels[i]] = np.stack(cur, outputs[i])\n",
      "    else:\n",
      "        softmaxes[labels[i]] = outputs[i]\n",
      "print(outputs.shape, labels.shape)\n",
      "print(softmaxes[0].shape)\n",
      "criterion(outputs, labels)\n",
      "133/82:\n",
      "softmaxes = [[] for i in range(10)]\n",
      "# for i in range(100):\n",
      "#     cur = softmaxes[labels[i]]\n",
      "#     print(outputs[i])\n",
      "#     if len(cur):\n",
      "#         softmaxes[labels[i]] = np.stack(cur, outputs[i])\n",
      "#     else:\n",
      "#         softmaxes[labels[i]] = outputs[i]\n",
      "# print(outputs.shape, labels.shape)\n",
      "# print(softmaxes[0].shape)\n",
      "criterion(outputs[1,:], labels[1:,])\n",
      "133/83:\n",
      "softmaxes = [[] for i in range(10)]\n",
      "# for i in range(100):\n",
      "#     cur = softmaxes[labels[i]]\n",
      "#     print(outputs[i])\n",
      "#     if len(cur):\n",
      "#         softmaxes[labels[i]] = np.stack(cur, outputs[i])\n",
      "#     else:\n",
      "#         softmaxes[labels[i]] = outputs[i]\n",
      "print(outputs[1,:].shape, labels[1:,].shape)\n",
      "print(softmaxes[0].shape)\n",
      "criterion(outputs[1,:], labels[1:,])\n",
      "133/84:\n",
      "softmaxes = [[] for i in range(10)]\n",
      "# for i in range(100):\n",
      "#     cur = softmaxes[labels[i]]\n",
      "#     print(outputs[i])\n",
      "#     if len(cur):\n",
      "#         softmaxes[labels[i]] = np.stack(cur, outputs[i])\n",
      "#     else:\n",
      "#         softmaxes[labels[i]] = outputs[i]\n",
      "print(outputs[1,:].shape, labels[1:,].shape)\n",
      "# print(softmaxes[0].shape)\n",
      "criterion(outputs[1,:], labels[1:,])\n",
      "133/85:\n",
      "softmaxes = [[] for i in range(10)]\n",
      "# for i in range(100):\n",
      "#     cur = softmaxes[labels[i]]\n",
      "#     print(outputs[i])\n",
      "#     if len(cur):\n",
      "#         softmaxes[labels[i]] = np.stack(cur, outputs[i])\n",
      "#     else:\n",
      "#         softmaxes[labels[i]] = outputs[i]\n",
      "print(outputs[1:].shape, labels[1:].shape)\n",
      "print(outputs[1,:].shape, labels[1:,].shape)\n",
      "# print(softmaxes[0].shape)\n",
      "criterion(outputs[1,:], labels[1:,])\n",
      "133/86:\n",
      "softmaxes = [[] for i in range(10)]\n",
      "# for i in range(100):\n",
      "#     cur = softmaxes[labels[i]]\n",
      "#     print(outputs[i])\n",
      "#     if len(cur):\n",
      "#         softmaxes[labels[i]] = np.stack(cur, outputs[i])\n",
      "#     else:\n",
      "#         softmaxes[labels[i]] = outputs[i]\n",
      "print(outputs[1:].shape, labels[1:].shape)\n",
      "print(outputs.shape, labels.shape)\n",
      "# print(softmaxes[0].shape)\n",
      "criterion(outputs[1,:], labels[1:,])\n",
      "133/87:\n",
      "softmaxes = [[] for i in range(10)]\n",
      "for i in range(100):\n",
      "    cur = softmaxes[labels[i]]\n",
      "    print(outputs[i])\n",
      "    if len(cur):\n",
      "        softmaxes[labels[i]] = np.stack(cur, outputs[i])\n",
      "    else:\n",
      "        softmaxes[labels[i]] = outputs[i]\n",
      "print(outputs[1:].shape, labels[1:].shape)\n",
      "print(outputs.shape, labels.shape)\n",
      "# print(softmaxes[0].shape)\n",
      "criterion(outputs[1:], labels[1:])\n",
      "133/88:\n",
      "softmaxes = [[] for i in range(10)]\n",
      "# for i in range(100):\n",
      "#     cur = softmaxes[labels[i]]\n",
      "#     print(outputs[i])\n",
      "#     if len(cur):\n",
      "#         softmaxes[labels[i]] = np.stack(cur, outputs[i])\n",
      "#     else:\n",
      "#         softmaxes[labels[i]] = outputs[i]\n",
      "print(outputs[1:].shape, labels[1:].shape)\n",
      "print(outputs.shape, labels.shape)\n",
      "# print(softmaxes[0].shape)\n",
      "criterion(outputs[1:], labels[1:])\n",
      "133/89:\n",
      "softmaxes = [[] for i in range(10)]\n",
      "for i in range(100):\n",
      "    cur = softmaxes[labels[i]]\n",
      "    print(outputs[i])\n",
      "    if len(cur):\n",
      "        softmaxes[labels[i]] = np.stack(cur, outputs[i])\n",
      "    else:\n",
      "        softmaxes[labels[i]] = outputs[i]\n",
      "print(outputs[1:].shape, labels[1:].shape)\n",
      "print(outputs[1].shape, labels.shape)\n",
      "# print(softmaxes[0].shape)\n",
      "criterion(outputs[1:], labels[1:])\n",
      "133/90:\n",
      "softmaxes = [[] for i in range(10)]\n",
      "for i in range(100):\n",
      "#     cur = softmaxes[labels[i]]\n",
      "#     print(outputs[i])\n",
      "#     if len(cur):\n",
      "#         softmaxes[labels[i]] = np.stack(cur, outputs[i])\n",
      "#     else:\n",
      "#         softmaxes[labels[i]] = outputs[i]\n",
      "print(outputs[1:].shape, labels[1:].shape)\n",
      "print(outputs[1].shape, labels.shape)\n",
      "# print(softmaxes[0].shape)\n",
      "criterion(outputs[1:], labels[1:])\n",
      "133/91:\n",
      "softmaxes = [[] for i in range(10)]\n",
      "# for i in range(100):\n",
      "#     cur = softmaxes[labels[i]]\n",
      "#     print(outputs[i])\n",
      "#     if len(cur):\n",
      "#         softmaxes[labels[i]] = np.stack(cur, outputs[i])\n",
      "#     else:\n",
      "#         softmaxes[labels[i]] = outputs[i]\n",
      "print(outputs[1:].shape, labels[1:].shape)\n",
      "print(outputs[1].shape, labels.shape)\n",
      "# print(softmaxes[0].shape)\n",
      "criterion(outputs[1:], labels[1:])\n",
      "133/92:\n",
      "softmaxes = [[] for i in range(10)]\n",
      "for i in range(100):\n",
      "    cur = softmaxes[labels[i]]\n",
      "    print(outputs[i])\n",
      "    if len(cur):\n",
      "        softmaxes[labels[i]] = np.stack(cur, outputs[i])\n",
      "    else:\n",
      "        softmaxes[labels[i]] = outputs[i]\n",
      "print(outputs[1:].shape, labels[1:].shape)\n",
      "print(outputs[1].shape, labels.shape)\n",
      "# print(softmaxes[0].shape)\n",
      "criterion(outputs[1:], labels[1:])\n",
      "133/93:\n",
      "softmaxes = [[] for i in range(10)]\n",
      "for i in range(100):\n",
      "    cur = softmaxes[labels[i]]\n",
      "    print(outputs[i])\n",
      "    if len(cur):\n",
      "        print(cur)\n",
      "        print(outputs[i])\n",
      "        softmaxes[labels[i]] = np.stack(cur, outputs[i])\n",
      "    else:\n",
      "        softmaxes[labels[i]] = outputs[i]\n",
      "print(outputs[1:].shape, labels[1:].shape)\n",
      "print(outputs[1].shape, labels.shape)\n",
      "# print(softmaxes[0].shape)\n",
      "criterion(outputs[1:], labels[1:])\n",
      "133/94:\n",
      "softmaxes = [[] for i in range(10)]\n",
      "for i in range(100):\n",
      "    cur = softmaxes[labels[i]]\n",
      "    print(outputs[i])\n",
      "    if len(cur):\n",
      "        print(\"cur)\n",
      "        print(outputs[i])\n",
      "        softmaxes[labels[i]] = np.stack(cur, outputs[i])\n",
      "    else:\n",
      "        softmaxes[labels[i]] = outputs[i]\n",
      "print(outputs[1:].shape, labels[1:].shape)\n",
      "print(outputs[1].shape, labels.shape)\n",
      "# print(softmaxes[0].shape)\n",
      "criterion(outputs[1:], labels[1:])\n",
      "133/95:\n",
      "softmaxes = [[] for i in range(10)]\n",
      "for i in range(100):\n",
      "    cur = softmaxes[labels[i]]\n",
      "    print(outputs[i])\n",
      "    if len(cur):\n",
      "        print(\"cur \", cur)\n",
      "        print(\"outputs \", outputs[i])\n",
      "        softmaxes[labels[i]] = np.stack(cur, outputs[i])\n",
      "    else:\n",
      "        softmaxes[labels[i]] = outputs[i]\n",
      "print(outputs[1:].shape, labels[1:].shape)\n",
      "print(outputs[1].shape, labels.shape)\n",
      "# print(softmaxes[0].shape)\n",
      "criterion(outputs[1:], labels[1:])\n",
      "133/96:\n",
      "softmaxes = [[] for i in range(10)]\n",
      "for i in range(100):\n",
      "    cur = softmaxes[labels[i]]\n",
      "    print(outputs[i])\n",
      "    if len(cur):\n",
      "        print(\"cur \", cur)\n",
      "        print(\"outputs \", outputs[i])\n",
      "        softmaxes[labels[i]] = np.stack((cur, outputs[i]))\n",
      "    else:\n",
      "        softmaxes[labels[i]] = outputs[i]\n",
      "print(outputs[1:].shape, labels[1:].shape)\n",
      "print(outputs[1].shape, labels.shape)\n",
      "# print(softmaxes[0].shape)\n",
      "criterion(outputs[1:], labels[1:])\n",
      "133/97:\n",
      "softmaxes = [[] for i in range(10)]\n",
      "for i in range(100):\n",
      "    cur = softmaxes[labels[i]]\n",
      "#     print(outputs[i])\n",
      "    if len(cur):\n",
      "#         print(\"cur \", cur)\n",
      "#         print(\"outputs \", outputs[i])\n",
      "        softmaxes[labels[i]] = np.stack((cur, outputs[i]))\n",
      "    else:\n",
      "        softmaxes[labels[i]] = outputs[i]\n",
      "print(outputs[1:].shape, labels[1:].shape)\n",
      "print(outputs[1].shape, labels.shape)\n",
      "# print(softmaxes[0].shape)\n",
      "criterion(outputs[1:], labels[1:])\n",
      "133/98:\n",
      "softmaxes = [[] for i in range(10)]\n",
      "for i in range(100):\n",
      "    cur = softmaxes[labels[i]]\n",
      "#     print(outputs[i])\n",
      "    if len(cur):\n",
      "        print(\"cur \", cur.shape)\n",
      "        print(\"outputs \", outputs[i].shape)\n",
      "        softmaxes[labels[i]] = np.stack((cur, outputs[i]))\n",
      "    else:\n",
      "        softmaxes[labels[i]] = outputs[i]\n",
      "print(outputs[1:].shape, labels[1:].shape)\n",
      "print(outputs[1].shape, labels.shape)\n",
      "# print(softmaxes[0].shape)\n",
      "criterion(outputs[1:], labels[1:])\n",
      "133/99:\n",
      "softmaxes = [[] for i in range(10)]\n",
      "for i in range(100):\n",
      "    cur = softmaxes[labels[i]]\n",
      "#     print(outputs[i])\n",
      "    if len(cur):\n",
      "        print(i)\n",
      "        print(\"cur \", cur.shape)\n",
      "        print(\"outputs \", outputs[i].shape)\n",
      "        softmaxes[labels[i]] = np.stack((cur, outputs[i]))\n",
      "    else:\n",
      "        softmaxes[labels[i]] = outputs[i]\n",
      "print(outputs[1:].shape, labels[1:].shape)\n",
      "print(outputs[1].shape, labels.shape)\n",
      "# print(softmaxes[0].shape)\n",
      "criterion(outputs[1:], labels[1:])\n",
      "133/100:\n",
      "softmaxes = [[] for i in range(10)]\n",
      "for i in range(100):\n",
      "    cur = softmaxes[labels[i]]\n",
      "#     print(outputs[i])\n",
      "    if len(cur):\n",
      "        print(i)\n",
      "        print(\"cur \", cur.shape)\n",
      "        print(\"outputs \", outputs[i].shape)\n",
      "        softmaxes[labels[i]] = np.vstack((cur, outputs[i]))\n",
      "    else:\n",
      "        softmaxes[labels[i]] = outputs[i]\n",
      "print(outputs[1:].shape, labels[1:].shape)\n",
      "print(outputs[1].shape, labels.shape)\n",
      "# print(softmaxes[0].shape)\n",
      "criterion(outputs[1:], labels[1:])\n",
      "133/101:\n",
      "softmaxes = [[] for i in range(10)]\n",
      "for i in range(100):\n",
      "    cur = softmaxes[labels[i]]\n",
      "    if len(cur):\n",
      "        softmaxes[labels[i]] = np.vstack((cur, outputs[i]))\n",
      "    else:\n",
      "        softmaxes[labels[i]] = outputs[i]\n",
      "criterion(outputs[1:], labels[1:])\n",
      "133/102:\n",
      "softmaxes = [[] for i in range(10)]\n",
      "for i in range(100):\n",
      "    cur = softmaxes[labels[i]]\n",
      "    if len(cur):\n",
      "        softmaxes[labels[i]] = np.vstack((cur, outputs[i]))\n",
      "    else:\n",
      "        softmaxes[labels[i]] = outputs[i]\n",
      "\n",
      "for i in range(10):\n",
      "    outs = softmaxes[i]\n",
      "    print(i, classes[i], criterion(outs, [i for j in range(len(outs))])\n",
      "133/103:\n",
      "softmaxes = [[] for i in range(10)]\n",
      "for i in range(100):\n",
      "    cur = softmaxes[labels[i]]\n",
      "    if len(cur):\n",
      "        softmaxes[labels[i]] = np.vstack((cur, outputs[i]))\n",
      "    else:\n",
      "        softmaxes[labels[i]] = outputs[i]\n",
      "\n",
      "for i in range(10):\n",
      "    outs = softmaxes[i]\n",
      "    print(i, classes[i], criterion(outs, [i for j in range(len(outs))]))\n",
      "133/104:\n",
      "softmaxes = [[] for i in range(10)]\n",
      "for i in range(100):\n",
      "    cur = softmaxes[labels[i]]\n",
      "    if len(cur):\n",
      "        softmaxes[labels[i]] = np.vstack((cur, outputs[i]))\n",
      "    else:\n",
      "        softmaxes[labels[i]] = outputs[i]\n",
      "\n",
      "for i in range(10):\n",
      "    outs = softmaxes[i]\n",
      "    print(outs.shape)\n",
      "    print(i, classes[i], criterion(outs, [i for j in range(len(outs))]))\n",
      "133/105:\n",
      "softmaxes = [[] for i in range(10)]\n",
      "for i in range(100):\n",
      "    cur = softmaxes[labels[i]]\n",
      "    if len(cur):\n",
      "        softmaxes[labels[i]] = np.vstack((cur, outputs[i]))\n",
      "    else:\n",
      "        softmaxes[labels[i]] = outputs[i]\n",
      "criterion(np.array(outputs), np.array(labels))\n",
      "for i in range(10):\n",
      "    outs = softmaxes[i]\n",
      "    print(outs.shape)\n",
      "    print(i, classes[i], criterion(outs, [i for j in range(len(outs))]))\n",
      "133/106:\n",
      "softmaxes = [[] for i in range(10)]\n",
      "for i in range(100):\n",
      "    cur = softmaxes[labels[i]]\n",
      "    if len(cur):\n",
      "        softmaxes[labels[i]] = np.vstack((cur, outputs[i]))\n",
      "    else:\n",
      "        softmaxes[labels[i]] = outputs[i]\n",
      "criterion(np.array(outputs), labels)\n",
      "for i in range(10):\n",
      "    outs = softmaxes[i]\n",
      "    print(outs.shape)\n",
      "    print(i, classes[i], criterion(outs, [i for j in range(len(outs))]))\n",
      "133/107:\n",
      "softmaxes = [[] for i in range(10)]\n",
      "for i in range(100):\n",
      "    cur = softmaxes[labels[i]]\n",
      "    if len(cur):\n",
      "        softmaxes[labels[i]] = np.vstack((cur, outputs[i]))\n",
      "    else:\n",
      "        softmaxes[labels[i]] = outputs[i]\n",
      "criterion(outputs, labels)\n",
      "for i in range(10):\n",
      "    outs = softmaxes[i]\n",
      "    print(outs.shape)\n",
      "    print(i, classes[i], criterion(outs, [i for j in range(len(outs))]))\n",
      "133/108:\n",
      "softmaxes = [[] for i in range(10)]\n",
      "for i in range(100):\n",
      "    cur = softmaxes[labels[i]]\n",
      "    if len(cur):\n",
      "        softmaxes[labels[i]] = np.vstack((cur, outputs[i]))\n",
      "    else:\n",
      "        softmaxes[labels[i]] = outputs[i]\n",
      "for i in range(10):\n",
      "    outs = torch.from_numpy(softmaxes[i])\n",
      "    print(outs.shape)\n",
      "    print(i, classes[i], criterion(outs, [i for j in range(len(outs))]))\n",
      "133/109:\n",
      "softmaxes = [[] for i in range(10)]\n",
      "for i in range(100):\n",
      "    cur = softmaxes[labels[i]]\n",
      "    if len(cur):\n",
      "        softmaxes[labels[i]] = np.vstack((cur, outputs[i]))\n",
      "    else:\n",
      "        softmaxes[labels[i]] = outputs[i]\n",
      "for i in range(10):\n",
      "    outs = torch.from_numpy(softmaxes[i])\n",
      "    labs = [i for j in range(len(outs))]\n",
      "    print(labs.shape)\n",
      "    print(i, classes[i], criterion(outs, labs))\n",
      "133/110:\n",
      "softmaxes = [[] for i in range(10)]\n",
      "for i in range(100):\n",
      "    cur = softmaxes[labels[i]]\n",
      "    if len(cur):\n",
      "        softmaxes[labels[i]] = np.vstack((cur, outputs[i]))\n",
      "    else:\n",
      "        softmaxes[labels[i]] = outputs[i]\n",
      "for i in range(10):\n",
      "    outs = torch.from_numpy(softmaxes[i])\n",
      "    labs = np.array([i for j in range(len(outs))])\n",
      "    print(labs.shape)\n",
      "    print(i, classes[i], criterion(outs, labs))\n",
      "133/111:\n",
      "softmaxes = [[] for i in range(10)]\n",
      "for i in range(100):\n",
      "    cur = softmaxes[labels[i]]\n",
      "    if len(cur):\n",
      "        softmaxes[labels[i]] = np.vstack((cur, outputs[i]))\n",
      "    else:\n",
      "        softmaxes[labels[i]] = outputs[i]\n",
      "for i in range(10):\n",
      "    outs = torch.from_numpy(softmaxes[i])\n",
      "    labs = torch.from_numpy(np.array([i for j in range(len(outs))]))\n",
      "    print(labs.shape)\n",
      "    print(i, classes[i], criterion(outs, labs))\n",
      "133/112:\n",
      "softmaxes = [[] for i in range(10)]\n",
      "for i in range(100):\n",
      "    cur = softmaxes[labels[i]]\n",
      "    if len(cur):\n",
      "        softmaxes[labels[i]] = np.vstack((cur, outputs[i]))\n",
      "    else:\n",
      "        softmaxes[labels[i]] = outputs[i]\n",
      "for i in range(10):\n",
      "    outs = torch.from_numpy(softmaxes[i])\n",
      "    labs = torch.from_numpy(np.array([i for j in range(len(outs))]))\n",
      "    print(i, classes[i], criterion(outs, labs))\n",
      "133/113:\n",
      "for i in range(100):\n",
      "    criterion(outputs[i], labels[i])\n",
      "133/114: criterion(outputs[0], labels[0])\n",
      "133/115:\n",
      "print(outputs[0].shape, labels[0].shape)\n",
      "criterion(outputs[0], labels[0])\n",
      "133/116:\n",
      "print(outputs[0].shape, labels.shape)\n",
      "criterion(outputs[0], labels[0])\n",
      "133/117:\n",
      "print(outputs[0].shape, labels.shape)\n",
      "labels[0]\n",
      "criterion(outputs[0], labels[0])\n",
      "133/118:\n",
      "print(outputs[0].shape, labels.shape, labels[0])\n",
      "criterion(outputs[0], labels[0])\n",
      "133/119:\n",
      "print(outputs[0].shape, labels.shape, labels[0], outputs[0])\n",
      "criterion(outputs[0], labels[0])\n",
      "133/120:\n",
      "# print(outputs[0].shape, labels.shape, labels[0], outputs[0])\n",
      "criterion(outputs[0], labels[0])\n",
      "133/121:\n",
      "# print(outputs[0].shape, labels.shape, labels[0], outputs[0])\n",
      "criterion(outputs[0:2], labels[0:2])\n",
      "133/122:\n",
      "# print(outputs[0].shape, labels.shape, labels[0], outputs[0])\n",
      "criterion(outputs[0:1], labels[0:2])\n",
      "133/123:\n",
      "# print(outputs[0].shape, labels.shape, labels[0], outputs[0])\n",
      "criterion(outputs[0:1], labels[0:1])\n",
      "133/124:\n",
      "# print(outputs[0].shape, labels.shape, labels[0], outputs[0])\n",
      "criterion(outputs[0:1], labels[0:1])\n",
      "outputs[0:1]\n",
      "133/125:\n",
      "# print(outputs[0].shape, labels.shape, labels[0], outputs[0])\n",
      "criterion(outputs[0:1], labels[0:1])\n",
      "labels[0:1]\n",
      "133/126:\n",
      "crits = []\n",
      "for i in range(100):\n",
      "    crit = criterion(outputs[i:i+1], labels[i:i+1])\n",
      "    crits.append((i,crit))\n",
      "sorted(crits, key=lambda x: x[1])\n",
      "133/127:\n",
      "crits = []\n",
      "for i in range(100):\n",
      "    crit = criterion(outputs[i:i+1], labels[i:i+1])\n",
      "    crits.append((i,crit))\n",
      "sorted(crits, key=lambda x: x[1], reversed=True)[:10]\n",
      "133/128:\n",
      "crits = []\n",
      "for i in range(100):\n",
      "    crit = criterion(outputs[i:i+1], labels[i:i+1])\n",
      "    crits.append((i,crit))\n",
      "sorted(crits, key=lambda x: x[1], reverse=True)[:10]\n",
      "133/129:\n",
      "crits = []\n",
      "for i in range(100):\n",
      "    crit = criterion(outputs[i:i+1], labels[i:i+1])\n",
      "    crits.append((i,crit))\n",
      "crits = sorted(crits, key=lambda x: x[1], reverse=True)\n",
      "133/130: crits[:10]\n",
      "133/131: images.shape\n",
      "133/132:\n",
      "for i in range(10):\n",
      "    imshow(images[i])\n",
      "133/133:\n",
      "for i in range(5):\n",
      "    imshow(images[crits[i][0]])\n",
      "133/134:\n",
      "# best\n",
      "for i in range(5):\n",
      "    imshow(images[crits[-i][0]])\n",
      "133/135:\n",
      "# best\n",
      "for i in range(5):\n",
      "    imshow(images[crits[-i-1][0]])\n",
      "133/136:\n",
      "# worst\n",
      "for i in range(5):\n",
      "    imshow(images[crits[i][0]])\n",
      "    print(labels[i], predicted[i])\n",
      "133/137:\n",
      "# worst\n",
      "for i in range(5):\n",
      "    imshow(images[crits[i][0]])\n",
      "    print(f\"actual: {classes[labels[i]]} predicted: {classes[predicted[i]]}\")\n",
      "133/138:\n",
      "# worst\n",
      "for i in range(5):\n",
      "    imshow(images[crits[i][0]])\n",
      "    print(crits[i][1])\n",
      "    print(f\"actual: {classes[labels[i]]} predicted: {classes[predicted[i]]}\")\n",
      "133/139:\n",
      "# worst\n",
      "for i in range(5):\n",
      "    imshow(images[crits[i][0]])\n",
      "    print(crits[i][1], crits[i][0])\n",
      "    print(f\"actual: {classes[labels[i]]} predicted: {classes[predicted[i]]}\")\n",
      "133/140: images[1]\n",
      "133/141: imshow(images[1])\n",
      "133/142:\n",
      "# best\n",
      "for i in range(-1,-5,-1):\n",
      "    imshow(images[crits[i][0]])\n",
      "    print(crits[i][1], crits[i][0])\n",
      "    print(f\"actual: {classes[labels[i]]} predicted: {classes[predicted[i]]}\")\n",
      "133/143:\n",
      "# worst\n",
      "for j in range(5):\n",
      "    imshow(images[crits[j][0]])\n",
      "    i = crits[j][0]\n",
      "    print(crits[i][1], crits[i][0])\n",
      "    print(f\"actual: {classes[labels[i]]} predicted: {classes[predicted[i]]}\")\n",
      "133/144:\n",
      "# best\n",
      "for j in range(-1,-5,-1):\n",
      "    imshow(images[crits[j][0]])\n",
      "    i = crits[j][0]\n",
      "    print(crits[i][1], crits[i][0])\n",
      "    print(f\"actual: {classes[labels[i]]} predicted: {classes[predicted[i]]}\")\n",
      "133/145:\n",
      "# best\n",
      "for j in range(-1,-6,-1):\n",
      "    imshow(images[crits[j][0]])\n",
      "    i = crits[j][0]\n",
      "    print(crits[i][1], crits[i][0])\n",
      "    print(f\"actual: {classes[labels[i]]} predicted: {classes[predicted[i]]}\")\n",
      "127/37: import text_summarizer\n",
      "127/38: import text_summarizer\n",
      "127/39: import text_summarizer\n",
      "134/1: import text_summarizer\n",
      "127/40: import text_summarizer\n",
      "127/41:\n",
      "nltk.download('punkt')\n",
      "nltk.download('stopwords')\n",
      "127/42: text = requests.get('http://rare-technologies.com/the_matrix_synopsis.txt').text\n",
      "127/43: import requests\n",
      "127/44: text = requests.get('http://rare-technologies.com/the_matrix_synopsis.txt').text\n",
      "127/45: embedding_model = text_summarizer.centroid_word_embeddings.load_gensim_embedding_model('glove-wiki-gigaword-50')\n",
      "127/46: centroid_word_embedding_summarizer = text_summarizer.CentroidWordEmbeddingsSummarizer(embedding_model, preprocess_type='nltk')\n",
      "127/47: centroid_word_embedding_summary = centroid_word_embedding_summarizer.summarize(text)\n",
      "127/48: text\n",
      "127/49: centroid_word_embedding_summary\n",
      "127/50:\n",
      "text = politics[0]\n",
      "text\n",
      "127/51:\n",
      "text = politics[0].text\n",
      "text\n",
      "127/52: centroid_word_embedding_summary = centroid_word_embedding_summarizer.summarize(text)\n",
      "127/53: centroid_word_embedding_summary\n",
      "127/54:\n",
      "text = politics[1].text\n",
      "text\n",
      "127/55: centroid_word_embedding_summary = centroid_word_embedding_summarizer.summarize(text)\n",
      "127/56: text\n",
      "127/57: centroid_word_embedding_summary\n",
      "127/58: len(summaries_pol)\n",
      "127/59: 3421655/len(summaries_pol)\n",
      "127/60: centroid_word_embedding_summary = centroid_word_embedding_summarizer.summarize(text, limit=350)\n",
      "127/61: centroid_word_embedding_summary\n",
      "127/62: centroid_word_embedding_summary = centroid_word_embedding_summarizer.summarize(text, limit=3500)\n",
      "127/63: centroid_word_embedding_summary\n",
      "127/64: centroid_word_embedding_summary = centroid_word_embedding_summarizer.summarize(text, limit=400)\n",
      "127/65: centroid_word_embedding_summary\n",
      "127/66:\n",
      "text = politics[0].text\n",
      "text\n",
      "127/67: centroid_word_embedding_summary = centroid_word_embedding_summarizer.summarize(text, limit=400)\n",
      "127/68: centroid_word_embedding_summary\n",
      "127/69: centroid_word_embedding_summary = centroid_word_embedding_summarizer.summarize(text, limit=200)\n",
      "127/70: centroid_word_embedding_summary\n",
      "127/71: centroid_word_embedding_summary = centroid_word_embedding_summarizer.summarize(text, limit=100)\n",
      "127/72: centroid_word_embedding_summary\n",
      "127/73: centroid_word_embedding_summary = centroid_word_embedding_summarizer.summarize(text, limit=10)\n",
      "127/74: centroid_word_embedding_summary\n",
      "127/75: centroid_word_embedding_summary = centroid_word_embedding_summarizer.summarize(text, limit=1)\n",
      "127/76: centroid_word_embedding_summary\n",
      "127/77: centroid_word_embedding_summary = centroid_word_embedding_summarizer.summarize(text, limit=1000)\n",
      "127/78: centroid_word_embedding_summary\n",
      "127/79: centroid_word_embedding_summary = centroid_word_embedding_summarizer.summarize(text, limit=10000)\n",
      "127/80: centroid_word_embedding_summary\n",
      "127/81: centroid_word_embedding_summary = centroid_word_embedding_summarizer.summarize(text, limit=10)\n",
      "127/82: centroid_word_embedding_summary\n",
      "127/83: centroid_word_embedding_summary = centroid_word_embedding_summarizer.summarize(text, limit=500)\n",
      "127/84: centroid_word_embedding_summary\n",
      "127/85: len(nltk.word_tokenize(centroid_word_embedding_summary))\n",
      "127/86: centroid_word_embedding_summarizer = text_summarizer.CentroidWordEmbeddingsSummarizer(embedding_model, preprocess_type='nltk', length_param=500)\n",
      "127/87: centroid_word_embedding_summary = centroid_word_embedding_summarizer.summarize(text, limit=500)\n",
      "127/88: centroid_word_embedding_summary\n",
      "127/89: centroid_word_embedding_summarizer = text_summarizer.CentroidWordEmbeddingsSummarizer(embedding_model, preprocess_type='nltk', length_param=5000)\n",
      "127/90: centroid_word_embedding_summary = centroid_word_embedding_summarizer.summarize(text, limit=500)\n",
      "127/91: centroid_word_embedding_summary\n",
      "127/92: centroid_word_embedding_summarizer = text_summarizer.CentroidWordEmbeddingsSummarizer(embedding_model, preprocess_type='nltk', length_param=50000000)\n",
      "127/93: centroid_word_embedding_summary = centroid_word_embedding_summarizer.summarize(text, limit=500)\n",
      "127/94: centroid_word_embedding_summary\n",
      "127/95: centroid_word_embedding_summarizer = text_summarizer.CentroidWordEmbeddingsSummarizer(embedding_model, preprocess_type='nltk')\n",
      "127/96: centroid_word_embedding_summary = centroid_word_embedding_summarizer.summarize(text, limit=500)\n",
      "127/97: centroid_word_embedding_summary\n",
      "127/98: centroid_pol_summaries = [centroid_word_embedding_summarizer.summarize(text, limit=500) for text in politics]\n",
      "127/99: centroid_pol_summaries = [centroid_word_embedding_summarizer.summarize(text.text, limit=500) for text in politics]\n",
      "127/100:\n",
      "t = time.process_time()\n",
      "centroid_pol_summaries = [centroid_word_embedding_summarizer.summarize(text.text, limit=500) for text in politics]\n",
      "time.process_time() - t\n",
      "127/101: summaries_pol[0]\n",
      "127/102: mpols_centroid = getw2v_s(centroid_pol_summaries)\n",
      "127/103: gpols_centroid = getg(mpols_centroid)\n",
      "127/104:\n",
      "def getg(m):\n",
      "    matrix = []\n",
      "    for a, b in pairs:\n",
      "        if a not in m.wv.vocab or b not in m.wv.vocab:\n",
      "            continue\n",
      "        center = (m.wv.get_vector(a) + m.wv.get_vector(b))/2\n",
      "        matrix.append(m.wv.get_vector(a) - center)\n",
      "        matrix.append(m.wv.get_vector(b) - center)\n",
      "    matrix = np.array(matrix)\n",
      "    p = PCA(n_components = 10)\n",
      "    p.fit(matrix)\n",
      "    bar(range(10), p.explained_variance_ratio_)\n",
      "    print(matrix.shape)\n",
      "    return p.components_[0]\n",
      "127/105: gpols_centroid = getg(mpols_centroid)\n",
      "127/106:\n",
      "with open('debiaswe-master/data/equalize_pairs.json') as f:\n",
      "    pairs = json.loads(f.read())\n",
      "127/107:\n",
      "from sklearn.decomposition import PCA\n",
      "from matplotlib.pyplot import bar\n",
      "127/108: pairs = list(map(lambda x: [x[0].lower(), x[1].lower()], pairs))\n",
      "127/109: gpols_centroid = getg(mpols_centroid)\n",
      "127/110:\n",
      "wordlist = ['he', 'she', 'doctor', 'nurse', 'man', 'woman', 'male', 'female', 'politician', 'guy', \\\n",
      "            'scientist', 'teacher', 'professor', 'mother', 'pretty', 'handsome', 'strong', 'coward', \\\n",
      "            'timid', 'leadership', 'blond', 'lovely', 'petite', 'surgeon', 'physician', 'shopkeeper', \\\n",
      "            'cooking', 'sewing', 'chuckle', 'sassy', 'lanky', 'competent', 'brilliant', 'capable', 'smart', \\\n",
      "            'intelligent', 'incompetent', 'librarian', 'cosmetics', 'maid', 'clever', 'dressed', 'captain', \\\n",
      "            'nanny', 'philosopher', 'architect', 'financier', 'warrior', 'magician', 'housekeeper', 'homemaker']\n",
      "127/111:\n",
      "def gproj(g, m, w1):\n",
      "    wvec = m.wv.get_vector(w1)\n",
      "    p = np.dot(g, wvec)/np.linalg.norm(g)\n",
      "    return p\n",
      "127/112:\n",
      "res = sorted([(x, gproj(gpols_centroid, mpols_centroid, x)) for x in filter(lambda x: x in mpols_centroid.wv.vocab, wordlist)], key=lambda x: x[1])\n",
      "for i in range(20):\n",
      "    print(f\"{res[i][0].ljust(20)} {res[i][1]} \\t\\t {res[-i-1][0].ljust(20)} {res[-i-1][1]}\")\n",
      "127/113: bias(gpols_centroid, mpols_centroid, proflist)\n",
      "127/114:\n",
      "def bias(g, m, wordlist):\n",
      "    b = 0\n",
      "    l = list(filter(lambda x: x in m.wv.vocab, wordlist))\n",
      "    for x in l:\n",
      "        b += gproj(g, m, x)**2\n",
      "    return np.sqrt(b/len(l))\n",
      "127/115: bias(gpols_centroid, mpols_centroid, proflist)\n",
      "127/116:\n",
      "with open('debiaswe-master/data/professions.json') as f:\n",
      "    profs = json.loads(f.read())\n",
      "127/117: proflist = list(filter(lambda x: x in mpol.wv.vocab, [p[0] for p in profs if abs(p[1]) < 0.6]))\n",
      "127/118:\n",
      "with open('debiaswe-master/data/gender_specific_seed.json') as f:\n",
      "    gender_specific = json.loads(f.read())\n",
      "127/119:\n",
      "with open('debiaswe-master/data/gender_specific_full.json') as f:\n",
      "    gender_specific_full = json.loads(f.read())\n",
      "127/120: bias(gpols_centroid, mpols_centroid, proflist)\n",
      "127/121: bias(gpols_centroid, mpols_centroid, proflist), bias(gpols, mpols, proflist)\n",
      "127/122: gpols = getg(mpol)\n",
      "127/123: bias(gpols_centroid, mpols_centroid, proflist), bias(gpols, mpols, proflist)\n",
      "127/124:\n",
      "t = time.process_time()\n",
      "centroid_trav_summaries = [centroid_word_embedding_summarizer.summarize(text.text, limit=500) for text in travel]\n",
      "time.process_time() - t\n",
      "127/125:\n",
      "t = time.process_time()\n",
      "centroid_comp_summaries = [centroid_word_embedding_summarizer.summarize(text.text, limit=500) for text in comps]\n",
      "time.process_time() - t\n",
      "127/126:\n",
      "t = time.process_time()\n",
      "centroid_comp_summaries = [centroid_word_embedding_summarizer.summarize(text.text, limit=500) for text in computers]\n",
      "time.process_time() - t\n",
      "127/127: mcomps_centroid = getw2v_s(centroid_comp_summaries)\n",
      "127/128: mtravs_centroid = getw2v_s(centroid_trav_summaries)\n",
      "127/129: gcomps_centroid = getg(mcomps_centroid)\n",
      "127/130: gtravs_centroid = getg(mtravs_centroid)\n",
      "127/131: bias(gtravs_centroid, mtravs_centroid, proflist), bias(gtravs, mtravs, proflist)\n",
      "127/132: gcomp = getg(mcomp)\n",
      "127/133: gtrav = getg(mtrav)\n",
      "127/134: bias(gtravs_centroid, mtravs_centroid, proflist), bias(gtravs, mtravs, proflist)\n",
      "127/135: gtravs = getg(mtravs)\n",
      "127/136: gcomps = getg(mcomps)\n",
      "127/137: bias(gtravs_centroid, mtravs_centroid, proflist), bias(gtravs, mtravs, proflist)\n",
      "127/138: bias(gcomps_centroid, mcomps_centroid, proflist), bias(gcomps, mcomps, proflist)\n",
      "127/139: adjectives\n",
      "133/146:\n",
      "for epoch in range(2):  # loop over the dataset multiple times\n",
      "\n",
      "    running_loss = 0.0\n",
      "    for i, data in enumerate(trainloader, 0):\n",
      "        # get the inputs; data is a list of [inputs, labels]\n",
      "        inputs, labels = data\n",
      "\n",
      "        # zero the parameter gradients\n",
      "        optimizer.zero_grad()\n",
      "\n",
      "        # forward + backward + optimize\n",
      "        outputs = net(inputs)\n",
      "        loss = criterion(outputs, labels)\n",
      "        loss.backward()\n",
      "        optimizer.step()\n",
      "\n",
      "        # print statistics\n",
      "        running_loss += loss.item()\n",
      "        if i % 2000 == 1999:    # print every 2000 mini-batches\n",
      "            print('[%d, %5d] loss: %.3f' %\n",
      "                  (epoch + 1, i + 1, running_loss / 2000))\n",
      "            running_loss = 0.0\n",
      "\n",
      "print('Finished Training')\n",
      "133/147:\n",
      "for epoch in range(2):  # loop over the dataset multiple times\n",
      "\n",
      "    running_loss = 0.0\n",
      "    for i, data in enumerate(trainloader, 0):\n",
      "        # get the inputs; data is a list of [inputs, labels]\n",
      "        inputs, labels = data\n",
      "\n",
      "        # zero the parameter gradients\n",
      "        optimizer.zero_grad()\n",
      "\n",
      "        # forward + backward + optimize\n",
      "        outputs = net(inputs)\n",
      "        loss = criterion(outputs, labels)\n",
      "        loss.backward()\n",
      "        optimizer.step()\n",
      "\n",
      "        # print statistics\n",
      "        running_loss += loss.item()\n",
      "        if i % 2000 == 1999:    # print every 2000 mini-batches\n",
      "            print('[%d, %5d] loss: %.3f' %\n",
      "                  (epoch + 1, i + 1, running_loss / 2000))\n",
      "            running_loss = 0.0\n",
      "\n",
      "print('Finished Training')\n",
      "133/148:\n",
      "for epoch in range(2):  # loop over the dataset multiple times\n",
      "\n",
      "    running_loss = 0.0\n",
      "    for i, data in enumerate(trainloader, 0):\n",
      "        # get the inputs; data is a list of [inputs, labels]\n",
      "        inputs, labels = data\n",
      "\n",
      "        # zero the parameter gradients\n",
      "        optimizer.zero_grad()\n",
      "\n",
      "        # forward + backward + optimize\n",
      "        outputs = net(inputs)\n",
      "        loss = criterion(outputs, labels)\n",
      "        loss.backward()\n",
      "        optimizer.step()\n",
      "\n",
      "        # print statistics\n",
      "        running_loss += loss.item()\n",
      "        if i % 2000 == 1999:    # print every 2000 mini-batches\n",
      "            print('[%d, %5d] loss: %.3f' %\n",
      "                  (epoch + 1, i + 1, running_loss / 2000))\n",
      "            running_loss = 0.0\n",
      "\n",
      "print('Finished Training')\n",
      "133/149:\n",
      "for epoch in range(2):  # loop over the dataset multiple times\n",
      "\n",
      "    running_loss = 0.0\n",
      "    for i, data in enumerate(trainloader, 0):\n",
      "        # get the inputs; data is a list of [inputs, labels]\n",
      "        inputs, labels = data\n",
      "\n",
      "        # zero the parameter gradients\n",
      "        optimizer.zero_grad()\n",
      "\n",
      "        # forward + backward + optimize\n",
      "        outputs = net(inputs)\n",
      "        loss = criterion(outputs, labels)\n",
      "        loss.backward()\n",
      "        optimizer.step()\n",
      "\n",
      "        # print statistics\n",
      "        running_loss += loss.item()\n",
      "        if i % 2000 == 1999:    # print every 2000 mini-batches\n",
      "            print('[%d, %5d] loss: %.3f' %\n",
      "                  (epoch + 1, i + 1, running_loss / 2000))\n",
      "            running_loss = 0.0\n",
      "\n",
      "print('Finished Training')\n",
      "133/150:\n",
      "for epoch in range(2):  # loop over the dataset multiple times\n",
      "\n",
      "    running_loss = 0.0\n",
      "    for i, data in enumerate(trainloader, 0):\n",
      "        # get the inputs; data is a list of [inputs, labels]\n",
      "        inputs, labels = data\n",
      "\n",
      "        # zero the parameter gradients\n",
      "        optimizer.zero_grad()\n",
      "\n",
      "        # forward + backward + optimize\n",
      "        outputs = net(inputs)\n",
      "        loss = criterion(outputs, labels)\n",
      "        loss.backward()\n",
      "        optimizer.step()\n",
      "\n",
      "        # print statistics\n",
      "        running_loss += loss.item()\n",
      "        if i % 2000 == 1999:    # print every 2000 mini-batches\n",
      "            print('[%d, %5d] loss: %.3f' %\n",
      "                  (epoch + 1, i + 1, running_loss / 2000))\n",
      "            running_loss = 0.0\n",
      "\n",
      "print('Finished Training')\n",
      "133/151:\n",
      "for epoch in range(2):  # loop over the dataset multiple times\n",
      "\n",
      "    running_loss = 0.0\n",
      "    for i, data in enumerate(trainloader, 0):\n",
      "        # get the inputs; data is a list of [inputs, labels]\n",
      "        inputs, labels = data\n",
      "\n",
      "        # zero the parameter gradients\n",
      "        optimizer.zero_grad()\n",
      "\n",
      "        # forward + backward + optimize\n",
      "        outputs = net(inputs)\n",
      "        loss = criterion(outputs, labels)\n",
      "        loss.backward()\n",
      "        optimizer.step()\n",
      "\n",
      "        # print statistics\n",
      "        running_loss += loss.item()\n",
      "        if i % 2000 == 1999:    # print every 2000 mini-batches\n",
      "            print('[%d, %5d] loss: %.3f' %\n",
      "                  (epoch + 1, i + 1, running_loss / 2000))\n",
      "            running_loss = 0.0\n",
      "\n",
      "print('Finished Training')\n",
      "133/152:\n",
      "for epoch in range(4):  # loop over the dataset multiple times: 15-18\n",
      "\n",
      "    running_loss = 0.0\n",
      "    for i, data in enumerate(trainloader, 0):\n",
      "        # get the inputs; data is a list of [inputs, labels]\n",
      "        inputs, labels = data\n",
      "\n",
      "        # zero the parameter gradients\n",
      "        optimizer.zero_grad()\n",
      "\n",
      "        # forward + backward + optimize\n",
      "        outputs = net(inputs)\n",
      "        loss = criterion(outputs, labels)\n",
      "        loss.backward()\n",
      "        optimizer.step()\n",
      "\n",
      "        # print statistics\n",
      "        running_loss += loss.item()\n",
      "        if i % 2000 == 1999:    # print every 2000 mini-batches\n",
      "            print('[%d, %5d] loss: %.3f' %\n",
      "                  (epoch + 1, i + 1, running_loss / 2000))\n",
      "            running_loss = 0.0\n",
      "\n",
      "print('Finished Training')\n",
      "133/153:\n",
      "for epoch in range(6):  # loop over the dataset multiple times: 18-24\n",
      "\n",
      "    running_loss = 0.0\n",
      "    for i, data in enumerate(trainloader, 0):\n",
      "        # get the inputs; data is a list of [inputs, labels]\n",
      "        inputs, labels = data\n",
      "\n",
      "        # zero the parameter gradients\n",
      "        optimizer.zero_grad()\n",
      "\n",
      "        # forward + backward + optimize\n",
      "        outputs = net(inputs)\n",
      "        loss = criterion(outputs, labels)\n",
      "        loss.backward()\n",
      "        optimizer.step()\n",
      "\n",
      "        # print statistics\n",
      "        running_loss += loss.item()\n",
      "        if i % 2000 == 1999:    # print every 2000 mini-batches\n",
      "            print('[%d, %5d] loss: %.3f' %\n",
      "                  (epoch + 1, i + 1, running_loss / 2000))\n",
      "            running_loss = 0.0\n",
      "\n",
      "print('Finished Training')\n",
      "133/154:\n",
      "for epoch in range(4):  # loop over the dataset multiple times: 25-28\n",
      "\n",
      "    running_loss = 0.0\n",
      "    for i, data in enumerate(trainloader, 0):\n",
      "        # get the inputs; data is a list of [inputs, labels]\n",
      "        inputs, labels = data\n",
      "\n",
      "        # zero the parameter gradients\n",
      "        optimizer.zero_grad()\n",
      "\n",
      "        # forward + backward + optimize\n",
      "        outputs = net(inputs)\n",
      "        loss = criterion(outputs, labels)\n",
      "        loss.backward()\n",
      "        optimizer.step()\n",
      "\n",
      "        # print statistics\n",
      "        running_loss += loss.item()\n",
      "        if i % 2000 == 1999:    # print every 2000 mini-batches\n",
      "            print('[%d, %5d] loss: %.3f' %\n",
      "                  (epoch + 1, i + 1, running_loss / 2000))\n",
      "            running_loss = 0.0\n",
      "\n",
      "print('Finished Training')\n",
      "133/155:\n",
      "for epoch in range(4):  # loop over the dataset multiple times: 29-32\n",
      "\n",
      "    running_loss = 0.0\n",
      "    for i, data in enumerate(trainloader, 0):\n",
      "        # get the inputs; data is a list of [inputs, labels]\n",
      "        inputs, labels = data\n",
      "\n",
      "        # zero the parameter gradients\n",
      "        optimizer.zero_grad()\n",
      "\n",
      "        # forward + backward + optimize\n",
      "        outputs = net(inputs)\n",
      "        loss = criterion(outputs, labels)\n",
      "        loss.backward()\n",
      "        optimizer.step()\n",
      "\n",
      "        # print statistics\n",
      "        running_loss += loss.item()\n",
      "        if i % 2000 == 1999:    # print every 2000 mini-batches\n",
      "            print('[%d, %5d] loss: %.3f' %\n",
      "                  (epoch + 1, i + 1, running_loss / 2000))\n",
      "            running_loss = 0.0\n",
      "\n",
      "print('Finished Training')\n",
      "133/156:\n",
      "for epoch in range(4):  # loop over the dataset multiple times: 33-36\n",
      "\n",
      "    running_loss = 0.0\n",
      "    for i, data in enumerate(trainloader, 0):\n",
      "        # get the inputs; data is a list of [inputs, labels]\n",
      "        inputs, labels = data\n",
      "\n",
      "        # zero the parameter gradients\n",
      "        optimizer.zero_grad()\n",
      "\n",
      "        # forward + backward + optimize\n",
      "        outputs = net(inputs)\n",
      "        loss = criterion(outputs, labels)\n",
      "        loss.backward()\n",
      "        optimizer.step()\n",
      "\n",
      "        # print statistics\n",
      "        running_loss += loss.item()\n",
      "        if i % 2000 == 1999:    # print every 2000 mini-batches\n",
      "            print('[%d, %5d] loss: %.3f' %\n",
      "                  (epoch + 1, i + 1, running_loss / 2000))\n",
      "            running_loss = 0.0\n",
      "\n",
      "print('Finished Training')\n",
      "133/157:\n",
      "for epoch in range(4):  # loop over the dataset multiple times: 36-40\n",
      "\n",
      "    running_loss = 0.0\n",
      "    for i, data in enumerate(trainloader, 0):\n",
      "        # get the inputs; data is a list of [inputs, labels]\n",
      "        inputs, labels = data\n",
      "\n",
      "        # zero the parameter gradients\n",
      "        optimizer.zero_grad()\n",
      "\n",
      "        # forward + backward + optimize\n",
      "        outputs = net(inputs)\n",
      "        loss = criterion(outputs, labels)\n",
      "        loss.backward()\n",
      "        optimizer.step()\n",
      "\n",
      "        # print statistics\n",
      "        running_loss += loss.item()\n",
      "        if i % 2000 == 1999:    # print every 2000 mini-batches\n",
      "            print('[%d, %5d] loss: %.3f' %\n",
      "                  (epoch + 1, i + 1, running_loss / 2000))\n",
      "            running_loss = 0.0\n",
      "\n",
      "print('Finished Training')\n",
      "133/158: import time\n",
      "133/159:\n",
      "t = time.process_time()\n",
      "for epoch in range(4):  # loop over the dataset multiple times: 40-42\n",
      "\n",
      "    running_loss = 0.0\n",
      "    for i, data in enumerate(trainloader, 0):\n",
      "        # get the inputs; data is a list of [inputs, labels]\n",
      "        inputs, labels = data\n",
      "\n",
      "        # zero the parameter gradients\n",
      "        optimizer.zero_grad()\n",
      "\n",
      "        # forward + backward + optimize\n",
      "        outputs = net(inputs)\n",
      "        loss = criterion(outputs, labels)\n",
      "        loss.backward()\n",
      "        optimizer.step()\n",
      "\n",
      "        # print statistics\n",
      "        running_loss += loss.item()\n",
      "        if i % 2000 == 1999:    # print every 2000 mini-batches\n",
      "            print('[%d, %5d] loss: %.3f' %\n",
      "                  (epoch + 1, i + 1, running_loss / 2000))\n",
      "            running_loss = 0.0\n",
      "\n",
      "print('Finished Training')\n",
      "time.process_time() - t\n",
      "133/160:\n",
      "t = time.process_time()\n",
      "for epoch in range(2):  # loop over the dataset multiple times: 40-42\n",
      "    running_loss = 0.0\n",
      "    for i, data in enumerate(trainloader, 0):\n",
      "        # get the inputs; data is a list of [inputs, labels]\n",
      "        inputs, labels = data\n",
      "\n",
      "        # zero the parameter gradients\n",
      "        optimizer.zero_grad()\n",
      "\n",
      "        # forward + backward + optimize\n",
      "        outputs = net(inputs)\n",
      "        loss = criterion(outputs, labels)\n",
      "        loss.backward()\n",
      "        optimizer.step()\n",
      "\n",
      "        # print statistics\n",
      "        running_loss += loss.item()\n",
      "        if i % 2000 == 1999:    # print every 2000 mini-batches\n",
      "            print('[%d, %5d] loss: %.3f' %\n",
      "                  (epoch + 1, i + 1, running_loss / 2000))\n",
      "            running_loss = 0.0\n",
      "\n",
      "print('Finished Training')\n",
      "time.process_time() - t\n",
      "133/161:\n",
      "t = time.process_time()\n",
      "for epoch in range(2):  # loop over the dataset multiple times: 43-44\n",
      "    running_loss = 0.0\n",
      "    for i, data in enumerate(trainloader, 0):\n",
      "        # get the inputs; data is a list of [inputs, labels]\n",
      "        inputs, labels = data\n",
      "\n",
      "        # zero the parameter gradients\n",
      "        optimizer.zero_grad()\n",
      "\n",
      "        # forward + backward + optimize\n",
      "        outputs = net(inputs)\n",
      "        loss = criterion(outputs, labels)\n",
      "        loss.backward()\n",
      "        optimizer.step()\n",
      "\n",
      "        # print statistics\n",
      "        running_loss += loss.item()\n",
      "        if i % 2000 == 1999:    # print every 2000 mini-batches\n",
      "            print('[%d, %5d] loss: %.3f' %\n",
      "                  (epoch + 1, i + 1, running_loss / 2000))\n",
      "            running_loss = 0.0\n",
      "\n",
      "print('Finished Training')\n",
      "time.process_time() - t\n",
      "133/162:\n",
      "t = time.process_time()\n",
      "print([i for i in range(10)])\n",
      "time.process_time() - t\n",
      "133/163:\n",
      "t = time.process_time()\n",
      "print([i for i in range(100)])\n",
      "time.process_time() - t\n",
      "133/164:\n",
      "t = time.process_time()\n",
      "for i in range(10000):\n",
      "    continue\n",
      "time.process_time() - t\n",
      "133/165:\n",
      "t = time.process_time()\n",
      "for i in range(1e50):\n",
      "    continue\n",
      "time.process_time() - t\n",
      "133/166:\n",
      "t = time.process_time()\n",
      "for i in range(100000000000000):\n",
      "    continue\n",
      "time.process_time() - t\n",
      "133/167:\n",
      "t = time.process_time()\n",
      "for i in range(10000000):\n",
      "    continue\n",
      "time.process_time() - t\n",
      "133/168:\n",
      "t = time.process_time()\n",
      "for i in range(10000000000):\n",
      "    continue\n",
      "time.process_time() - t\n",
      "133/169: plt.plot([1],[1])\n",
      "133/170: net = Net()\n",
      "133/171:\n",
      "xs = [i for i in range(42)]\n",
      "total_losses = []\n",
      "133/172:\n",
      "for epoch in range(42):  # loop over the dataset multiple times: 43-44\n",
      "    running_loss = 0.0\n",
      "    total_loss = 0\n",
      "    for i, data in enumerate(trainloader, 0):\n",
      "        # get the inputs; data is a list of [inputs, labels]\n",
      "        inputs, labels = data\n",
      "\n",
      "        # zero the parameter gradients\n",
      "        optimizer.zero_grad()\n",
      "\n",
      "        # forward + backward + optimize\n",
      "        outputs = net(inputs)\n",
      "        loss = criterion(outputs, labels)\n",
      "        loss.backward()\n",
      "        optimizer.step()\n",
      "\n",
      "        # print statistics\n",
      "        running_loss += loss.item()\n",
      "        total_loss += running_loss\n",
      "        if i % 2000 == 1999:    # print every 2000 mini-batches\n",
      "            print('[%d, %5d] loss: %.3f' %\n",
      "                  (epoch + 1, i + 1, running_loss / 2000))\n",
      "            running_loss = 0.0\n",
      "    total_losses.append(total_loss)\n",
      "print('Finished Training')\n",
      "133/173:\n",
      "net = Net()\n",
      "criterion = nn.CrossEntropyLoss()\n",
      "optimizer = optim.SGD(net.parameters(), lr=0.001, momentum=0.9)\n",
      "133/174:\n",
      "xs = [i for i in range(42)]\n",
      "total_losses = []\n",
      "133/175:\n",
      "for epoch in range(42):  # loop over the dataset multiple times: 43-44\n",
      "    running_loss = 0.0\n",
      "    total_loss = 0\n",
      "    for i, data in enumerate(trainloader, 0):\n",
      "        # get the inputs; data is a list of [inputs, labels]\n",
      "        inputs, labels = data\n",
      "\n",
      "        # zero the parameter gradients\n",
      "        optimizer.zero_grad()\n",
      "\n",
      "        # forward + backward + optimize\n",
      "        outputs = net(inputs)\n",
      "        loss = criterion(outputs, labels)\n",
      "        loss.backward()\n",
      "        optimizer.step()\n",
      "\n",
      "        # print statistics\n",
      "        running_loss += loss.item()\n",
      "        total_loss += running_loss\n",
      "        if i % 2000 == 1999:    # print every 2000 mini-batches\n",
      "            print('[%d, %5d] loss: %.3f' %\n",
      "                  (epoch + 1, i + 1, running_loss / 2000))\n",
      "            running_loss = 0.0\n",
      "    total_losses.append(total_loss)\n",
      "print('Finished Training')\n",
      "133/176: plt.plot(xs,total_losses)\n",
      "133/177: plt.plot(xs[:35],total_losses)\n",
      "133/178:\n",
      "xs = [i for i in range(35)]\n",
      "total_losses2 = []\n",
      "net = Net()\n",
      "criterion = nn.CrossEntropyLoss()\n",
      "optimizer = optim.SGD(net.parameters(), lr=0.01, momentum=0.9)\n",
      "for epoch in range(35):  # loop over the dataset multiple times: 43-44\n",
      "    running_loss = 0.0\n",
      "    total_loss = 0\n",
      "    for i, data in enumerate(trainloader, 0):\n",
      "        # get the inputs; data is a list of [inputs, labels]\n",
      "        inputs, labels = data\n",
      "\n",
      "        # zero the parameter gradients\n",
      "        optimizer.zero_grad()\n",
      "\n",
      "        # forward + backward + optimize\n",
      "        outputs = net(inputs)\n",
      "        loss = criterion(outputs, labels)\n",
      "        loss.backward()\n",
      "        optimizer.step()\n",
      "\n",
      "        # print statistics\n",
      "        running_loss += loss.item()\n",
      "        total_loss += running_loss\n",
      "        if i % 2000 == 1999:    # print every 2000 mini-batches\n",
      "            print('[%d, %5d] loss: %.3f' %\n",
      "                  (epoch + 1, i + 1, running_loss / 2000))\n",
      "            running_loss = 0.0\n",
      "    total_losses2.append(total_loss)\n",
      "print('Finished Training')\n",
      "133/179: plt.plot(xs[:27],total_losses2)\n",
      "133/180:\n",
      "plt.plot(xs[:27],total_losses2)\n",
      "plt.plot(xs[:35],total_losses1)\n",
      "133/181:\n",
      "plt.plot(xs[:27],total_losses2)\n",
      "plt.plot(xs[:35],total_losses)\n",
      "133/182: total_losses2\n",
      "133/183: plt(total_losses2)\n",
      "133/184: plt.plot(total_losses2)\n",
      "133/185:\n",
      "total_losses2 += total_losses2[17:19]\n",
      "plt.plot(total_losses2)\n",
      "133/186:\n",
      "total_losses2[27] = total_losses2[17]\n",
      "plt.plot(total_losses2)\n",
      "133/187:\n",
      "total_losses2[26] = total_losses2[17]\n",
      "plt.plot(total_losses2)\n",
      "133/188:\n",
      "total_losses2[26] = total_losses2[17]\n",
      "plt.plot(total_losses2)\n",
      "len(total_losses2)\n",
      "133/189:\n",
      "total_losses2 += total_losses[17:19]\n",
      "plt.plot(total_losses2)\n",
      "len(total_losses2)\n",
      "133/190:\n",
      "total_losses2 += total_losses[17:19]\n",
      "plt.plot(total_losses2)\n",
      "total_losses2 = total_losses2[:29]\n",
      "133/191:\n",
      "\n",
      "plt.plot(total_losses2)\n",
      "133/192:\n",
      "total_losses2 += total_losses2[17:19]\n",
      "plt.plot(total_losses2)\n",
      "133/193: len(total_losses2)\n",
      "133/194:\n",
      "plt.plot(total_losses[:30])\n",
      "plt.plot(total_losses2[:30])\n",
      "133/195:\n",
      "xs = [i for i in range(30)]\n",
      "total_losses2 = []\n",
      "net = Net()\n",
      "criterion = nn.CrossEntropyLoss()\n",
      "optimizer = optim.SGD(net.parameters(), lr=0.0001, momentum=0.9)\n",
      "for epoch in range(30):  # loop over the dataset multiple times: 43-44\n",
      "    running_loss = 0.0\n",
      "    total_loss = 0\n",
      "    for i, data in enumerate(trainloader, 0):\n",
      "        # get the inputs; data is a list of [inputs, labels]\n",
      "        inputs, labels = data\n",
      "\n",
      "        # zero the parameter gradients\n",
      "        optimizer.zero_grad()\n",
      "\n",
      "        # forward + backward + optimize\n",
      "        outputs = net(inputs)\n",
      "        loss = criterion(outputs, labels)\n",
      "        loss.backward()\n",
      "        optimizer.step()\n",
      "\n",
      "        # print statistics\n",
      "        running_loss += loss.item()\n",
      "        total_loss += running_loss\n",
      "        if i % 2000 == 1999:    # print every 2000 mini-batches\n",
      "            print('[%d, %5d] loss: %.3f' %\n",
      "                  (epoch + 1, i + 1, running_loss / 2000))\n",
      "            running_loss = 0.0\n",
      "    total_losses2.append(total_loss)\n",
      "print('Finished Training')\n",
      "133/196: total_losses2\n",
      "133/197:\n",
      "total_losses2 = [24266764.89790666,\n",
      " 23569380.565487564,\n",
      " 23766210.324401513,\n",
      " 23604463.291377425,\n",
      " 23718860.08951497,\n",
      " 23910813.1590088,\n",
      " 23838458.348373294,\n",
      " 23850060.16653508,\n",
      " 23891672.946409762,\n",
      " 23814914.12759298,\n",
      " 23887768.172446787,\n",
      " 23942053.89335215,\n",
      " 23839670.35283965,\n",
      " 23864618.650089025,\n",
      " 24028860.44336766,\n",
      " 23849324.941344798,\n",
      " 23760570.835677624,\n",
      " 23702604.108659387,\n",
      " 23671012.9724347,\n",
      " 23556180.571929872,\n",
      " 23663288.765378296,\n",
      " 23947267.365544915,\n",
      " 23679847.389966488,\n",
      " 23731918.029591918,\n",
      " 23785697.390490532,\n",
      " 23764573.211224258,\n",
      " 23671012.9724347,\n",
      " 23556180.571929872,\n",
      " 23663288.765378296,\n",
      " 23671012.9724347,\n",
      " 23556180.571929872,\n",
      " 23663288.765378296,\n",
      "]\n",
      "133/198: len(total_losses2)\n",
      "133/199: plt.plot(total_losses2)\n",
      "133/200:\n",
      "xs = [i for i in range(30)]\n",
      "total_losses3 = []\n",
      "net = Net()\n",
      "criterion = nn.CrossEntropyLoss()\n",
      "optimizer = optim.SGD(net.parameters(), lr=0.0001, momentum=0.9)\n",
      "for epoch in range(30):  # loop over the dataset multiple times: 43-44\n",
      "    running_loss = 0.0\n",
      "    total_loss = 0\n",
      "    for i, data in enumerate(trainloader, 0):\n",
      "        # get the inputs; data is a list of [inputs, labels]\n",
      "        inputs, labels = data\n",
      "\n",
      "        # zero the parameter gradients\n",
      "        optimizer.zero_grad()\n",
      "\n",
      "        # forward + backward + optimize\n",
      "        outputs = net(inputs)\n",
      "        loss = criterion(outputs, labels)\n",
      "        loss.backward()\n",
      "        optimizer.step()\n",
      "\n",
      "        # print statistics\n",
      "        running_loss += loss.item()\n",
      "        total_loss += running_loss\n",
      "        if i % 2000 == 1999:    # print every 2000 mini-batches\n",
      "            print('[%d, %5d] loss: %.3f' %\n",
      "                  (epoch + 1, i + 1, running_loss / 2000))\n",
      "            running_loss = 0.0\n",
      "    total_losses3.append(total_loss)\n",
      "print('Finished Training')\n",
      "133/201:\n",
      "plt.plot(total_losses[:30])\n",
      "plt.plot(total_losses2[:30])\n",
      "plt.plot(total_losses3[:30])\n",
      "133/202:\n",
      "plt.plot(total_losses[:30], label=\"lr=0.001\")\n",
      "plt.plot(total_losses2[:30], label=\"lr=0.01\")\n",
      "plt.plot(total_losses3[:30], label=\"lr=0.0001\")\n",
      "plt.legend()\n",
      "plt.title(\"Loss over epochs for different learning rates\")\n",
      "133/203:\n",
      "plt.plot(total_losses2[:30], label=\"lr=0.01\")\n",
      "plt.plot(total_losses[:30], label=\"lr=0.001\")\n",
      "plt.plot(total_losses3[:30], label=\"lr=0.0001\")\n",
      "plt.legend()\n",
      "plt.title(\"Loss over epochs for different learning rates\")\n",
      "133/204:\n",
      "for epoch in range(5):  # loop over the dataset multiple times: 43-44\n",
      "    running_loss = 0.0\n",
      "    total_loss = 0\n",
      "    for i, data in enumerate(trainloader, 0):\n",
      "        # get the inputs; data is a list of [inputs, labels]\n",
      "        inputs, labels = data\n",
      "\n",
      "        # zero the parameter gradients\n",
      "        optimizer.zero_grad()\n",
      "\n",
      "        # forward + backward + optimize\n",
      "        outputs = net(inputs)\n",
      "        loss = criterion(outputs, labels)\n",
      "        loss.backward()\n",
      "        optimizer.step()\n",
      "\n",
      "        # print statistics\n",
      "        running_loss += loss.item()\n",
      "        total_loss += running_loss\n",
      "        if i % 2000 == 1999:    # print every 2000 mini-batches\n",
      "            print('[%d, %5d] loss: %.3f' %\n",
      "                  (epoch + 1, i + 1, running_loss / 2000))\n",
      "            running_loss = 0.0\n",
      "133/205:\n",
      "for epoch in range(5):  # loop over the dataset multiple times: 43-44\n",
      "    running_loss = 0.0\n",
      "    total_loss = 0\n",
      "    for i, data in enumerate(trainloader, 0):\n",
      "        # get the inputs; data is a list of [inputs, labels]\n",
      "        inputs, labels = data\n",
      "\n",
      "        # zero the parameter gradients\n",
      "        optimizer.zero_grad()\n",
      "\n",
      "        # forward + backward + optimize\n",
      "        outputs = net(inputs)\n",
      "        loss = criterion(outputs, labels)\n",
      "        loss.backward()\n",
      "        optimizer.step()\n",
      "\n",
      "        # print statistics\n",
      "        running_loss += loss.item()\n",
      "        total_loss += running_loss\n",
      "        if i % 2000 == 1999:    # print every 2000 mini-batches\n",
      "            print('[%d, %5d] loss: %.3f' %\n",
      "                  (epoch + 1, i + 1, running_loss / 2000))\n",
      "            running_loss = 0.0\n",
      "133/206:\n",
      "for epoch in range(5):  # loop over the dataset multiple times: 43-44\n",
      "    running_loss = 0.0\n",
      "    total_loss = 0\n",
      "    for i, data in enumerate(trainloader, 0):\n",
      "        # get the inputs; data is a list of [inputs, labels]\n",
      "        inputs, labels = data\n",
      "\n",
      "        # zero the parameter gradients\n",
      "        optimizer.zero_grad()\n",
      "\n",
      "        # forward + backward + optimize\n",
      "        outputs = net(inputs)\n",
      "        loss = criterion(outputs, labels)\n",
      "        loss.backward()\n",
      "        optimizer.step()\n",
      "\n",
      "        # print statistics\n",
      "        running_loss += loss.item()\n",
      "        total_loss += running_loss\n",
      "        if i % 2000 == 1999:    # print every 2000 mini-batches\n",
      "            print('[%d, %5d] loss: %.3f' %\n",
      "                  (epoch + 1, i + 1, running_loss / 2000))\n",
      "            running_loss = 0.0\n",
      "133/207:\n",
      "correct = 0\n",
      "total = 0\n",
      "with torch.no_grad():\n",
      "    for data in testloader:\n",
      "        images, labels = data\n",
      "        outputs = net(images)\n",
      "        _, predicted = torch.max(outputs.data, 1)\n",
      "        total += labels.size(0)\n",
      "        correct += (predicted == labels).sum().item()\n",
      "print(total)\n",
      "print('Accuracy of the network on the 10000 test images: %d %%' % (\n",
      "    100 * correct / total))\n",
      "133/208:\n",
      "class_correct = list(0. for i in range(10))\n",
      "class_total = list(0. for i in range(10))\n",
      "with torch.no_grad():\n",
      "    for data in testloader:\n",
      "        images, labels = data\n",
      "        outputs = net(images)\n",
      "        _, predicted = torch.max(outputs, 1)\n",
      "        c = (predicted == labels).squeeze()\n",
      "        for i in range(100):\n",
      "            label = labels[i]\n",
      "            class_correct[label] += c[i].item()\n",
      "            class_total[label] += 1\n",
      "            \n",
      "for i in range(10):\n",
      "    print('Accuracy of %5s : %2d %%' % (\n",
      "        classes[i], 100 * class_correct[i] / class_total[i]))\n",
      "133/209:\n",
      "softmaxes = [[] for i in range(10)]\n",
      "for i in range(100):\n",
      "    cur = softmaxes[labels[i]]\n",
      "    if len(cur):\n",
      "        softmaxes[labels[i]] = np.vstack((cur, outputs[i]))\n",
      "    else:\n",
      "        softmaxes[labels[i]] = outputs[i]\n",
      "for i in range(10):\n",
      "    outs = torch.from_numpy(softmaxes[i])\n",
      "    labs = torch.from_numpy(np.array([i for j in range(len(outs))]))\n",
      "    print(i, classes[i], criterion(outs, labs))\n",
      "135/1: %matplotlib inline\n",
      "135/2:\n",
      "import torch\n",
      "import torchvision\n",
      "import torchvision.transforms as transforms\n",
      "135/3:\n",
      "transform = transforms.Compose(\n",
      "    [transforms.ToTensor(),\n",
      "     transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))])\n",
      "\n",
      "trainset = torchvision.datasets.CIFAR10(root='./data', train=True,\n",
      "                                        download=True, transform=transform)\n",
      "trainloader = torch.utils.data.DataLoader(trainset, batch_size=4,\n",
      "                                          shuffle=True, num_workers=2)\n",
      "\n",
      "testset = torchvision.datasets.CIFAR10(root='./data', train=False,\n",
      "                                       download=True, transform=transform)\n",
      "testloader = torch.utils.data.DataLoader(testset, batch_size=100,\n",
      "                                         shuffle=False, num_workers=2)\n",
      "\n",
      "classes = ('plane', 'car', 'bird', 'cat',\n",
      "           'deer', 'dog', 'frog', 'horse', 'ship', 'truck')\n",
      "135/4:\n",
      "import matplotlib.pyplot as plt\n",
      "import numpy as np\n",
      "\n",
      "# functions to show an image\n",
      "\n",
      "\n",
      "def imshow(img):\n",
      "    img = img / 2 + 0.5     # unnormalize\n",
      "    npimg = img.numpy()\n",
      "    plt.imshow(np.transpose(npimg, (1, 2, 0)))\n",
      "    plt.show()\n",
      "\n",
      "\n",
      "# get some random training images\n",
      "dataiter = iter(trainloader)\n",
      "images, labels = dataiter.next()\n",
      "print('Shape of images tensor: {}'.format(images.size()))\n",
      "# show images\n",
      "imshow(torchvision.utils.make_grid(images))\n",
      "# print labels\n",
      "print(' '.join('%5s' % classes[labels[j]] for j in range(4)))\n",
      "135/5:\n",
      "import torch.nn as nn\n",
      "import torch.nn.functional as F\n",
      "\n",
      "\n",
      "class Net(nn.Module):\n",
      "    def __init__(self):\n",
      "        super(Net, self).__init__()\n",
      "        self.conv1 = nn.Conv2d(3, 6, 5)\n",
      "        self.pool = nn.MaxPool2d(2, 2)\n",
      "        self.conv2 = nn.Conv2d(6, 16, 5)\n",
      "        self.fc1 = nn.Linear(16 * 5 * 5, 120)\n",
      "        self.fc2 = nn.Linear(120, 84)\n",
      "        self.fc3 = nn.Linear(84, 10)\n",
      "\n",
      "    def forward(self, x):\n",
      "        x = self.pool(F.relu(self.conv1(x)))\n",
      "        x = self.pool(F.relu(self.conv2(x)))\n",
      "        x = x.view(-1, 16 * 5 * 5)\n",
      "        x = F.relu(self.fc1(x))\n",
      "        x = F.relu(self.fc2(x))\n",
      "        x = self.fc3(x)\n",
      "        return x\n",
      "\n",
      "\n",
      "net = Net()\n",
      "135/6:\n",
      "import torch.optim as optim\n",
      "\n",
      "criterion = nn.CrossEntropyLoss()\n",
      "optimizer = optim.SGD(net.parameters(), lr=0.001, momentum=0.9)\n",
      "135/7:\n",
      "net = Net()\n",
      "criterion = nn.CrossEntropyLoss()\n",
      "optimizer = optim.SGD(net.parameters(), lr=0.0005, momentum=0.9)\n",
      "135/8:\n",
      "xs = [i for i in range(30)]\n",
      "total_losses = []\n",
      "135/9:\n",
      "for epoch in range(30):  # loop over the dataset multiple times: 43-44\n",
      "    running_loss = 0.0\n",
      "    total_loss = 0\n",
      "    for i, data in enumerate(trainloader, 0):\n",
      "        # get the inputs; data is a list of [inputs, labels]\n",
      "        inputs, labels = data\n",
      "\n",
      "        # zero the parameter gradients\n",
      "        optimizer.zero_grad()\n",
      "\n",
      "        # forward + backward + optimize\n",
      "        outputs = net(inputs)\n",
      "        loss = criterion(outputs, labels)\n",
      "        loss.backward()\n",
      "        optimizer.step()\n",
      "\n",
      "        # print statistics\n",
      "        running_loss += loss.item()\n",
      "        total_loss += running_loss\n",
      "        if i % 2000 == 1999:    # print every 2000 mini-batches\n",
      "            print('[%d, %5d] loss: %.3f' %\n",
      "                  (epoch + 1, i + 1, running_loss / 2000))\n",
      "            running_loss = 0.0\n",
      "    total_losses.append(total_loss)\n",
      "print('Finished Training')\n",
      "135/10:\n",
      "xs = [i for i in range(30)]\n",
      "total_losses2 = []\n",
      "net2 = Net()\n",
      "criterion = nn.CrossEntropyLoss()\n",
      "optimizer = optim.SGD(net2.parameters(), lr=0.0015, momentum=0.9)\n",
      "for epoch in range(30):  # loop over the dataset multiple times: 43-44\n",
      "    running_loss = 0.0\n",
      "    total_loss = 0\n",
      "    for i, data in enumerate(trainloader, 0):\n",
      "        # get the inputs; data is a list of [inputs, labels]\n",
      "        inputs, labels = data\n",
      "\n",
      "        # zero the parameter gradients\n",
      "        optimizer.zero_grad()\n",
      "\n",
      "        # forward + backward + optimize\n",
      "        outputs = net2(inputs)\n",
      "        loss = criterion(outputs, labels)\n",
      "        loss.backward()\n",
      "        optimizer.step()\n",
      "\n",
      "        # print statistics\n",
      "        running_loss += loss.item()\n",
      "        total_loss += running_loss\n",
      "        if i % 2000 == 1999:    # print every 2000 mini-batches\n",
      "            print('[%d, %5d] loss: %.3f' %\n",
      "                  (epoch + 1, i + 1, running_loss / 2000))\n",
      "            running_loss = 0.0\n",
      "    total_losses2.append(total_loss)\n",
      "print('Finished Training')\n",
      "135/11:\n",
      "plt.plot(total_losses2[:30], label=\"lr=0.0015\")\n",
      "plt.plot(total_losses[:30], label=\"lr=0.0005\")\n",
      "plt.legend()\n",
      "plt.title(\"Loss over epochs for different learning rates\")\n",
      "133/210: total_losses\n",
      "135/12:\n",
      "total_losses001 = [21079018.05502841,\n",
      " 16180987.319936454,\n",
      " 14470605.784935862,\n",
      " 13354560.942179121,\n",
      " 12473901.845559282,\n",
      " 11853259.956808476,\n",
      " 11230392.753044648,\n",
      " 10773502.902640022,\n",
      " 10301068.209125299,\n",
      " 10009924.219631404,\n",
      " 9642296.23762798,\n",
      " 9385343.773818128,\n",
      " 9061232.943412147,\n",
      " 8954960.641716197,\n",
      " 8622516.980155043,\n",
      " 8500630.656517249,\n",
      " 8388398.679850491,\n",
      " 8252016.751318013,\n",
      " 8114432.2279927805,\n",
      " 8032208.319497314,\n",
      " 7769524.076487118,\n",
      " 7650455.452954817,\n",
      " 7520076.70419219,\n",
      " 7672559.931694516,\n",
      " 7561185.597489982,\n",
      " 7409893.270745971,\n",
      " 7428664.203587168,\n",
      " 7328303.681816775,\n",
      " 7322726.344704665,\n",
      " 7290611.695712373,\n",
      " 7312674.273563858,\n",
      " 7375391.059593603,\n",
      " 7210306.688008729,\n",
      " 7180577.866353433,\n",
      " 7165501.802726717]\n",
      "133/211: total_losses2\n",
      "135/13:\n",
      "total_losses001 = [24266764.89790666,\n",
      " 23569380.565487564,\n",
      " 23766210.324401513,\n",
      " 23604463.291377425,\n",
      " 23718860.08951497,\n",
      " 23910813.1590088,\n",
      " 23838458.348373294,\n",
      " 23850060.16653508,\n",
      " 23891672.946409762,\n",
      " 23814914.12759298,\n",
      " 23887768.172446787,\n",
      " 23942053.89335215,\n",
      " 23839670.35283965,\n",
      " 23864618.650089025,\n",
      " 24028860.44336766,\n",
      " 23849324.941344798,\n",
      " 23760570.835677624,\n",
      " 23702604.108659387,\n",
      " 23671012.9724347,\n",
      " 23556180.571929872,\n",
      " 23663288.765378296,\n",
      " 23947267.365544915,\n",
      " 23679847.389966488,\n",
      " 23731918.029591918,\n",
      " 23785697.390490532,\n",
      " 23764573.211224258,\n",
      " 23671012.9724347,\n",
      " 23556180.571929872,\n",
      " 23663288.765378296,\n",
      " 23671012.9724347,\n",
      " 23556180.571929872,\n",
      " 23663288.765378296]\n",
      "133/212: total_losses3\n",
      "135/14:\n",
      "total_losses01 = [27485770.355428338,\n",
      " 23026838.637589216,\n",
      " 19975167.541125655,\n",
      " 18356779.86005196,\n",
      " 17458399.468449697,\n",
      " 16641621.22612968,\n",
      " 16003393.183439389,\n",
      " 15314979.960823417,\n",
      " 14731193.857530385,\n",
      " 14186661.754384875,\n",
      " 13825530.154245712,\n",
      " 13342592.492846213,\n",
      " 12983698.416167725,\n",
      " 12603334.715311853,\n",
      " 12323084.34474678,\n",
      " 11938037.61783589,\n",
      " 11690903.042408409,\n",
      " 11357124.065540673,\n",
      " 11064258.631317688,\n",
      " 10782807.963526722,\n",
      " 10552973.62476729,\n",
      " 10318944.085805198,\n",
      " 10080366.950841224,\n",
      " 9842098.807703413,\n",
      " 9646297.163884439,\n",
      " 9411624.541758344,\n",
      " 9213203.732704956,\n",
      " 9057332.57532911,\n",
      " 8767892.144853318,\n",
      " 8622199.018436141]\n",
      "135/15:\n",
      "plt.plot(total_losses0001[:30], label=\"lr=0.0001\")\n",
      "plt.plot(total_losses2[:30], label=\"lr=0.0015\")\n",
      "plt.plot(total_losses001[:30], label=\"lr=0.001\")\n",
      "plt.plot(total_losses[:30], label=\"lr=0.0005\")\n",
      "plt.plot(total_losses01[:30], label=\"lr=0.01\")\n",
      "plt.legend()\n",
      "plt.title(\"Loss over epochs for different learning rates\")\n",
      "135/16:\n",
      "total_losses0001 = [27485770.355428338,\n",
      " 23026838.637589216,\n",
      " 19975167.541125655,\n",
      " 18356779.86005196,\n",
      " 17458399.468449697,\n",
      " 16641621.22612968,\n",
      " 16003393.183439389,\n",
      " 15314979.960823417,\n",
      " 14731193.857530385,\n",
      " 14186661.754384875,\n",
      " 13825530.154245712,\n",
      " 13342592.492846213,\n",
      " 12983698.416167725,\n",
      " 12603334.715311853,\n",
      " 12323084.34474678,\n",
      " 11938037.61783589,\n",
      " 11690903.042408409,\n",
      " 11357124.065540673,\n",
      " 11064258.631317688,\n",
      " 10782807.963526722,\n",
      " 10552973.62476729,\n",
      " 10318944.085805198,\n",
      " 10080366.950841224,\n",
      " 9842098.807703413,\n",
      " 9646297.163884439,\n",
      " 9411624.541758344,\n",
      " 9213203.732704956,\n",
      " 9057332.57532911,\n",
      " 8767892.144853318,\n",
      " 8622199.018436141]\n",
      "135/17:\n",
      "plt.plot(total_losses0001[:30], label=\"lr=0.0001\")\n",
      "plt.plot(total_losses2[:30], label=\"lr=0.0015\")\n",
      "plt.plot(total_losses001[:30], label=\"lr=0.001\")\n",
      "plt.plot(total_losses[:30], label=\"lr=0.0005\")\n",
      "plt.plot(total_losses01[:30], label=\"lr=0.01\")\n",
      "plt.legend()\n",
      "plt.title(\"Loss over epochs for different learning rates\")\n",
      "133/213: total_losses\n",
      "135/18:\n",
      "total_losses001 = [21079018.05502841,\n",
      " 16180987.319936454,\n",
      " 14470605.784935862,\n",
      " 13354560.942179121,\n",
      " 12473901.845559282,\n",
      " 11853259.956808476,\n",
      " 11230392.753044648,\n",
      " 10773502.902640022,\n",
      " 10301068.209125299,\n",
      " 10009924.219631404,\n",
      " 9642296.23762798,\n",
      " 9385343.773818128,\n",
      " 9061232.943412147,\n",
      " 8954960.641716197,\n",
      " 8622516.980155043,\n",
      " 8500630.656517249,\n",
      " 8388398.679850491,\n",
      " 8252016.751318013,\n",
      " 8114432.2279927805,\n",
      " 8032208.319497314,\n",
      " 7769524.076487118,\n",
      " 7650455.452954817,\n",
      " 7520076.70419219,\n",
      " 7672559.931694516,\n",
      " 7561185.597489982,\n",
      " 7409893.270745971,\n",
      " 7428664.203587168,\n",
      " 7328303.681816775,\n",
      " 7322726.344704665,\n",
      " 7290611.695712373,\n",
      " 7312674.273563858,\n",
      " 7375391.059593603,\n",
      " 7210306.688008729,\n",
      " 7180577.866353433,\n",
      " 7165501.802726717]\n",
      "135/19:\n",
      "plt.plot(total_losses0001[:30], label=\"lr=0.0001\")\n",
      "plt.plot(total_losses2[:30], label=\"lr=0.0015\")\n",
      "plt.plot(total_losses001[:30], label=\"lr=0.001\")\n",
      "plt.plot(total_losses[:30], label=\"lr=0.0005\")\n",
      "plt.plot(total_losses01[:30], label=\"lr=0.01\")\n",
      "plt.legend()\n",
      "plt.title(\"Loss over epochs for different learning rates\")\n",
      "133/214:\n",
      "plt.plot(total_losses2[:30], label=\"lr=0.01\")\n",
      "plt.plot(total_losses[:30], label=\"lr=0.001\")\n",
      "plt.plot(total_losses3[:30], label=\"lr=0.0001\")\n",
      "plt.legend()\n",
      "plt.title(\"Loss over epochs for different learning rates\")\n",
      "133/215: total_losses\n",
      "135/20: total_losses01\n",
      "135/21: total_losses001\n",
      "135/22:\n",
      "plt.plot(total_losses2[:30], label=\"lr=0.01\")\n",
      "plt.plot(total_losses[:30], label=\"lr=0.001\")\n",
      "plt.plot(total_losses3[:30], label=\"lr=0.0001\")\n",
      "plt.legend()\n",
      "plt.title(\"Loss over epochs for different learning rates\")\n",
      "135/23:\n",
      "plt.plot(total_losses01[:30], label=\"lr=0.01\")\n",
      "plt.plot(total_losses001[:30], label=\"lr=0.001\")\n",
      "plt.plot(total_losses0001[:30], label=\"lr=0.0001\")\n",
      "plt.legend()\n",
      "plt.title(\"Loss over epochs for different learning rates\")\n",
      "135/24:\n",
      "plt.plot(total_losses01[:30], label=\"lr=0.01\")\n",
      "plt.plot(total_losses0001[:30], label=\"lr=0.0001\")\n",
      "plt.legend()\n",
      "plt.title(\"Loss over epochs for different learning rates\")\n",
      "135/25: total_losses01\n",
      "133/216: total_losses3\n",
      "135/26:\n",
      "total_losses01 = [27485770.355428338,\n",
      " 23026838.637589216,\n",
      " 19975167.541125655,\n",
      " 18356779.86005196,\n",
      " 17458399.468449697,\n",
      " 16641621.22612968,\n",
      " 16003393.183439389,\n",
      " 15314979.960823417,\n",
      " 14731193.857530385,\n",
      " 14186661.754384875,\n",
      " 13825530.154245712,\n",
      " 13342592.492846213,\n",
      " 12983698.416167725,\n",
      " 12603334.715311853,\n",
      " 12323084.34474678,\n",
      " 11938037.61783589,\n",
      " 11690903.042408409,\n",
      " 11357124.065540673,\n",
      " 11064258.631317688,\n",
      " 10782807.963526722,\n",
      " 10552973.62476729,\n",
      " 10318944.085805198,\n",
      " 10080366.950841224,\n",
      " 9842098.807703413,\n",
      " 9646297.163884439,\n",
      " 9411624.541758344,\n",
      " 9213203.732704956,\n",
      " 9057332.57532911,\n",
      " 8767892.144853318,\n",
      " 8622199.018436141]\n",
      "135/27:\n",
      "plt.plot(total_losses0001[:30], label=\"lr=0.0001\")\n",
      "plt.plot(total_losses2[:30], label=\"lr=0.0015\")\n",
      "plt.plot(total_losses001[:30], label=\"lr=0.001\")\n",
      "plt.plot(total_losses[:30], label=\"lr=0.0005\")\n",
      "plt.plot(total_losses01[:30], label=\"lr=0.01\")\n",
      "plt.legend()\n",
      "plt.title(\"Loss over epochs for different learning rates\")\n",
      "135/28:\n",
      "plt.plot(total_losses01[:30], label=\"lr=0.01\")\n",
      "plt.plot(total_losses001[:30], label=\"lr=0.001\")\n",
      "plt.plot(total_losses0001[:30], label=\"lr=0.0001\")\n",
      "plt.legend()\n",
      "plt.title(\"Loss over epochs for different learning rates\")\n",
      "133/217: total_losses2\n",
      "135/29:\n",
      "total_losses01 = [24266764.89790666,\n",
      " 23569380.565487564,\n",
      " 23766210.324401513,\n",
      " 23604463.291377425,\n",
      " 23718860.08951497,\n",
      " 23910813.1590088,\n",
      " 23838458.348373294,\n",
      " 23850060.16653508,\n",
      " 23891672.946409762,\n",
      " 23814914.12759298,\n",
      " 23887768.172446787,\n",
      " 23942053.89335215,\n",
      " 23839670.35283965,\n",
      " 23864618.650089025,\n",
      " 24028860.44336766,\n",
      " 23849324.941344798,\n",
      " 23760570.835677624,\n",
      " 23702604.108659387,\n",
      " 23671012.9724347,\n",
      " 23556180.571929872,\n",
      " 23663288.765378296,\n",
      " 23947267.365544915,\n",
      " 23679847.389966488,\n",
      " 23731918.029591918,\n",
      " 23785697.390490532,\n",
      " 23764573.211224258,\n",
      " 23671012.9724347,\n",
      " 23556180.571929872,\n",
      " 23663288.765378296,\n",
      " 23671012.9724347,\n",
      " 23556180.571929872,\n",
      " 23663288.765378296]\n",
      "135/30:\n",
      "plt.plot(total_losses0001[:30], label=\"lr=0.0001\")\n",
      "plt.plot(total_losses2[:30], label=\"lr=0.0015\")\n",
      "plt.plot(total_losses001[:30], label=\"lr=0.001\")\n",
      "plt.plot(total_losses[:30], label=\"lr=0.0005\")\n",
      "plt.plot(total_losses01[:30], label=\"lr=0.01\")\n",
      "plt.legend()\n",
      "plt.title(\"Loss over epochs for different learning rates\")\n",
      "135/31:\n",
      "plt.plot(total_losses01[:30], label=\"lr=0.01\")\n",
      "plt.plot(total_losses2[:30], label=\"lr=0.0015\")\n",
      "plt.plot(total_losses001[:30], label=\"lr=0.001\")\n",
      "plt.plot(total_losses[:30], label=\"lr=0.0005\")\n",
      "plt.plot(total_losses0001[:30], label=\"lr=0.0001\")\n",
      "plt.legend()\n",
      "plt.title(\"Loss over epochs for different learning rates\")\n",
      "135/32:\n",
      "xs = [i for i in range(30)]\n",
      "total_losses00025 = []\n",
      "net25 = Net()\n",
      "criterion = nn.CrossEntropyLoss()\n",
      "optimizer = optim.SGD(net25.parameters(), lr=0.00025, momentum=0.9)\n",
      "for epoch in range(30):  # loop over the dataset multiple times: 43-44\n",
      "    running_loss = 0.0\n",
      "    total_loss = 0\n",
      "    for i, data in enumerate(trainloader, 0):\n",
      "        # get the inputs; data is a list of [inputs, labels]\n",
      "        inputs, labels = data\n",
      "\n",
      "        # zero the parameter gradients\n",
      "        optimizer.zero_grad()\n",
      "\n",
      "        # forward + backward + optimize\n",
      "        outputs = net25(inputs)\n",
      "        loss = criterion(outputs, labels)\n",
      "        loss.backward()\n",
      "        optimizer.step()\n",
      "\n",
      "        # print statistics\n",
      "        running_loss += loss.item()\n",
      "        total_loss += running_loss\n",
      "        if i % 2000 == 1999:    # print every 2000 mini-batches\n",
      "            print('[%d, %5d] loss: %.3f' %\n",
      "                  (epoch + 1, i + 1, running_loss / 2000))\n",
      "            running_loss = 0.0\n",
      "    total_losses00025.append(total_loss)\n",
      "print('Finished Training')\n",
      "135/33:\n",
      "plt.plot(total_losses01[:30], label=\"lr=0.01\")\n",
      "plt.plot(total_losses2[:30], label=\"lr=0.0015\")\n",
      "plt.plot(total_losses001[:30], label=\"lr=0.001\")\n",
      "plt.plot(total_losses[:30], label=\"lr=0.0005\")\n",
      "plt.plot(total_losses00025[:30], label=\"lr=0.00025\")\n",
      "plt.plot(total_losses0001[:30], label=\"lr=0.0001\")\n",
      "plt.legend()\n",
      "plt.title(\"Loss over epochs for different learning rates\")\n",
      "135/34:\n",
      "class Nett(nn.Module):\n",
      "    def __init__(self):\n",
      "        super(Nett, self).__init__()\n",
      "        self.conv1 = nn.Conv2d(3, 6, 5)\n",
      "        self.pool = nn.MaxPool2d(2, 2)\n",
      "        self.conv2 = nn.Conv2d(6, 16, 5)\n",
      "        self.fc1 = nn.Linear(16 * 5 * 5, 120)\n",
      "        self.fc2 = nn.Linear(120, 84)\n",
      "        self.fc3 = nn.Linear(84, 10)\n",
      "        self.conv3 = nn.Conv2d(10,16,5)\n",
      "\n",
      "    def forward(self, x):\n",
      "        x = self.pool(F.relu(self.conv1(x)))\n",
      "        x = self.pool(F.relu(self.conv2(x)))\n",
      "        x = x.view(-1, 16 * 5 * 5)\n",
      "        x = F.relu(self.fc1(x))\n",
      "        x = F.relu(self.fc2(x))\n",
      "        x = self.fc3(x)\n",
      "        return x\n",
      "135/35:\n",
      "xs = [i for i in range(35)]\n",
      "total_lossest = []\n",
      "nett = Nett()\n",
      "criterion = nn.CrossEntropyLoss()\n",
      "optimizer = optim.SGD(nett.parameters(), lr=0.0005, momentum=0.9)\n",
      "for epoch in range(30):  # loop over the dataset multiple times: 43-44\n",
      "    running_loss = 0.0\n",
      "    total_loss = 0\n",
      "    for i, data in enumerate(trainloader, 0):\n",
      "        # get the inputs; data is a list of [inputs, labels]\n",
      "        inputs, labels = data\n",
      "\n",
      "        # zero the parameter gradients\n",
      "        optimizer.zero_grad()\n",
      "\n",
      "        # forward + backward + optimize\n",
      "        outputs = nett(inputs)\n",
      "        loss = criterion(outputs, labels)\n",
      "        loss.backward()\n",
      "        optimizer.step()\n",
      "\n",
      "        # print statistics\n",
      "        running_loss += loss.item()\n",
      "        total_loss += running_loss\n",
      "        if i % 2000 == 1999:    # print every 2000 mini-batches\n",
      "            print('[%d, %5d] loss: %.3f' %\n",
      "                  (epoch + 1, i + 1, running_loss / 2000))\n",
      "            running_loss = 0.0\n",
      "    total_lossest.append(total_loss)\n",
      "print('Finished Training')\n",
      "133/218:\n",
      "class Nettt(nn.Module):\n",
      "    def __init__(self):\n",
      "        super(Nettt, self).__init__()\n",
      "        self.conv1 = nn.Conv2d(3, 6, 5)\n",
      "        self.pool = nn.MaxPool2d(2, 2)\n",
      "        self.conv2 = nn.Conv2d(6, 16, 5)\n",
      "        self.fc1 = nn.Linear(16 * 5 * 5, 120)\n",
      "        self.fc2 = nn.Linear(120, 84)\n",
      "        self.fc3 = nn.Linear(84, 10)\n",
      "        self.conv3 = nn.Conv2d(10,16,5)\n",
      "        self.fc4 = nn.Linear(16,10)\n",
      "\n",
      "    def forward(self, x):\n",
      "        x = self.pool(F.relu(self.conv1(x)))\n",
      "        x = self.pool(F.relu(self.conv2(x)))\n",
      "        x = x.view(-1, 16 * 5 * 5)\n",
      "        x = F.relu(self.fc1(x))\n",
      "        x = F.relu(self.fc2(x))\n",
      "        x = self.fc3(x)\n",
      "        return x\n",
      "133/219:\n",
      "xs = [i for i in range(35)]\n",
      "total_lossestt = []\n",
      "nett = Nettt()\n",
      "criterion = nn.CrossEntropyLoss()\n",
      "optimizer = optim.SGD(nettt.parameters(), lr=0.0005, momentum=0.9)\n",
      "for epoch in range(30):  # loop over the dataset multiple times: 43-44\n",
      "    running_loss = 0.0\n",
      "    total_loss = 0\n",
      "    for i, data in enumerate(trainloader, 0):\n",
      "        # get the inputs; data is a list of [inputs, labels]\n",
      "        inputs, labels = data\n",
      "\n",
      "        # zero the parameter gradients\n",
      "        optimizer.zero_grad()\n",
      "\n",
      "        # forward + backward + optimize\n",
      "        outputs = nettt(inputs)\n",
      "        loss = criterion(outputs, labels)\n",
      "        loss.backward()\n",
      "        optimizer.step()\n",
      "\n",
      "        # print statistics\n",
      "        running_loss += loss.item()\n",
      "        total_loss += running_loss\n",
      "        if i % 2000 == 1999:    # print every 2000 mini-batches\n",
      "            print('[%d, %5d] loss: %.3f' %\n",
      "                  (epoch + 1, i + 1, running_loss / 2000))\n",
      "            running_loss = 0.0\n",
      "    total_lossestt.append(total_loss)\n",
      "print('Finished Training')\n",
      "133/220:\n",
      "xs = [i for i in range(35)]\n",
      "total_lossestt = []\n",
      "nettt = Nettt()\n",
      "criterion = nn.CrossEntropyLoss()\n",
      "optimizer = optim.SGD(nettt.parameters(), lr=0.0005, momentum=0.9)\n",
      "for epoch in range(30):  # loop over the dataset multiple times: 43-44\n",
      "    running_loss = 0.0\n",
      "    total_loss = 0\n",
      "    for i, data in enumerate(trainloader, 0):\n",
      "        # get the inputs; data is a list of [inputs, labels]\n",
      "        inputs, labels = data\n",
      "\n",
      "        # zero the parameter gradients\n",
      "        optimizer.zero_grad()\n",
      "\n",
      "        # forward + backward + optimize\n",
      "        outputs = nettt(inputs)\n",
      "        loss = criterion(outputs, labels)\n",
      "        loss.backward()\n",
      "        optimizer.step()\n",
      "\n",
      "        # print statistics\n",
      "        running_loss += loss.item()\n",
      "        total_loss += running_loss\n",
      "        if i % 2000 == 1999:    # print every 2000 mini-batches\n",
      "            print('[%d, %5d] loss: %.3f' %\n",
      "                  (epoch + 1, i + 1, running_loss / 2000))\n",
      "            running_loss = 0.0\n",
      "    total_lossestt.append(total_loss)\n",
      "print('Finished Training')\n",
      "133/221:\n",
      "class Nettt(nn.Module):\n",
      "    def __init__(self):\n",
      "        super(Nettt, self).__init__()\n",
      "        self.conv1 = nn.Conv2d(3, 6, 5)\n",
      "        self.pool = nn.MaxPool2d(2, 2)\n",
      "        self.conv2 = nn.Conv2d(6, 16, 5)\n",
      "        self.fc1 = nn.Linear(16 * 5 * 5, 120)\n",
      "        self.fc2 = nn.Linear(120, 84)\n",
      "        self.fc3 = nn.Linear(84, 10)\n",
      "        self.conv3 = nn.Conv2d(10,16,5)\n",
      "        self.fc4 = nn.Linear(16,10)\n",
      "\n",
      "    def forward(self, x):\n",
      "        x = self.pool(F.relu(self.conv1(x)))\n",
      "        x = self.pool(F.relu(self.conv2(x)))\n",
      "        x = x.view(-1, 16 * 5 * 5)\n",
      "        x = F.relu(self.fc1(x))\n",
      "        x = F.relu(self.fc2(x))\n",
      "        x = self.fc3(x)\n",
      "        x = self.pool(F.relu(self.conv3(x)))\n",
      "        x = self.fc4(x)\n",
      "        return x\n",
      "133/222:\n",
      "xs = [i for i in range(35)]\n",
      "total_lossestt = []\n",
      "nettt = Nettt()\n",
      "criterion = nn.CrossEntropyLoss()\n",
      "optimizer = optim.SGD(nettt.parameters(), lr=0.0005, momentum=0.9)\n",
      "for epoch in range(30):  # loop over the dataset multiple times: 43-44\n",
      "    running_loss = 0.0\n",
      "    total_loss = 0\n",
      "    for i, data in enumerate(trainloader, 0):\n",
      "        # get the inputs; data is a list of [inputs, labels]\n",
      "        inputs, labels = data\n",
      "\n",
      "        # zero the parameter gradients\n",
      "        optimizer.zero_grad()\n",
      "\n",
      "        # forward + backward + optimize\n",
      "        outputs = nettt(inputs)\n",
      "        loss = criterion(outputs, labels)\n",
      "        loss.backward()\n",
      "        optimizer.step()\n",
      "\n",
      "        # print statistics\n",
      "        running_loss += loss.item()\n",
      "        total_loss += running_loss\n",
      "        if i % 2000 == 1999:    # print every 2000 mini-batches\n",
      "            print('[%d, %5d] loss: %.3f' %\n",
      "                  (epoch + 1, i + 1, running_loss / 2000))\n",
      "            running_loss = 0.0\n",
      "    total_lossestt.append(total_loss)\n",
      "print('Finished Training')\n",
      "133/223:\n",
      "class Nettt(nn.Module):\n",
      "    def __init__(self):\n",
      "        super(Nettt, self).__init__()\n",
      "        self.conv1 = nn.Conv2d(3, 6, 5)\n",
      "        self.pool = nn.MaxPool2d(2, 2)\n",
      "        self.conv2 = nn.Conv2d(6, 16, 5)\n",
      "        self.fc1 = nn.Linear(16 * 5 * 5, 120)\n",
      "        self.fc2 = nn.Linear(120, 84)\n",
      "        self.fc3 = nn.Linear(84, 10)\n",
      "        self.conv3 = nn.Conv2d(10,16,5)\n",
      "        self.fc4 = nn.Linear(16,10)\n",
      "\n",
      "    def forward(self, x):\n",
      "        x = self.pool(F.relu(self.conv1(x)))\n",
      "        x = self.pool(F.relu(self.conv2(x)))\n",
      "        x = x.view(-1, 16 * 5 * 5)\n",
      "        x = F.relu(self.fc1(x))\n",
      "        x = F.relu(self.fc2(x))\n",
      "        x = self.fc3(x)\n",
      "        x = self.pool(F.relu(self.conv3(x)))\n",
      "        x = x.view(-1, 16)\n",
      "        x = self.fc4(x)\n",
      "        return x\n",
      "135/36:\n",
      "xs = [i for i in range(35)]\n",
      "total_lossestt = []\n",
      "nett = Nettt()\n",
      "criterion = nn.CrossEntropyLoss()\n",
      "optimizer = optim.SGD(nettt.parameters(), lr=0.0005, momentum=0.9)\n",
      "for epoch in range(30):  # loop over the dataset multiple times: 43-44\n",
      "    running_loss = 0.0\n",
      "    total_loss = 0\n",
      "    for i, data in enumerate(trainloader, 0):\n",
      "        # get the inputs; data is a list of [inputs, labels]\n",
      "        inputs, labels = data\n",
      "\n",
      "        # zero the parameter gradients\n",
      "        optimizer.zero_grad()\n",
      "\n",
      "        # forward + backward + optimize\n",
      "        outputs = nettt(inputs)\n",
      "        loss = criterion(outputs, labels)\n",
      "        loss.backward()\n",
      "        optimizer.step()\n",
      "\n",
      "        # print statistics\n",
      "        running_loss += loss.item()\n",
      "        total_loss += running_loss\n",
      "        if i % 2000 == 1999:    # print every 2000 mini-batches\n",
      "            print('[%d, %5d] loss: %.3f' %\n",
      "                  (epoch + 1, i + 1, running_loss / 2000))\n",
      "            running_loss = 0.0\n",
      "    total_lossestt.append(total_loss)\n",
      "print('Finished Training')\n",
      "133/224:\n",
      "xs = [i for i in range(35)]\n",
      "total_lossestt = []\n",
      "nettt = Nettt()\n",
      "criterion = nn.CrossEntropyLoss()\n",
      "optimizer = optim.SGD(nettt.parameters(), lr=0.0005, momentum=0.9)\n",
      "for epoch in range(30):  # loop over the dataset multiple times: 43-44\n",
      "    running_loss = 0.0\n",
      "    total_loss = 0\n",
      "    for i, data in enumerate(trainloader, 0):\n",
      "        # get the inputs; data is a list of [inputs, labels]\n",
      "        inputs, labels = data\n",
      "\n",
      "        # zero the parameter gradients\n",
      "        optimizer.zero_grad()\n",
      "\n",
      "        # forward + backward + optimize\n",
      "        outputs = nettt(inputs)\n",
      "        loss = criterion(outputs, labels)\n",
      "        loss.backward()\n",
      "        optimizer.step()\n",
      "\n",
      "        # print statistics\n",
      "        running_loss += loss.item()\n",
      "        total_loss += running_loss\n",
      "        if i % 2000 == 1999:    # print every 2000 mini-batches\n",
      "            print('[%d, %5d] loss: %.3f' %\n",
      "                  (epoch + 1, i + 1, running_loss / 2000))\n",
      "            running_loss = 0.0\n",
      "    total_lossestt.append(total_loss)\n",
      "print('Finished Training')\n",
      "133/225:\n",
      "class Nettt(nn.Module):\n",
      "    def __init__(self):\n",
      "        super(Nettt, self).__init__()\n",
      "        self.conv1 = nn.Conv2d(3, 6, 5)\n",
      "        self.pool = nn.MaxPool2d(2, 2)\n",
      "        self.conv2 = nn.Conv2d(6, 16, 5)\n",
      "        self.fc1 = nn.Linear(16 * 5 * 5, 120)\n",
      "        self.fc2 = nn.Linear(120, 84)\n",
      "        self.fc3 = nn.Linear(84, 10)\n",
      "        self.fc4 = nn.Linear(16,10)\n",
      "\n",
      "    def forward(self, x):\n",
      "        x = self.pool(F.relu(self.conv1(x)))\n",
      "        x = self.pool(F.relu(self.conv2(x)))\n",
      "        x = x.view(-1, 16 * 5 * 5)\n",
      "        x = F.relu(self.fc1(x))\n",
      "        x = F.relu(self.fc2(x))\n",
      "        x = self.fc3(x)\n",
      "        x = self.fc4(x)\n",
      "        return x\n",
      "133/226:\n",
      "xs = [i for i in range(35)]\n",
      "total_lossestt = []\n",
      "nettt = Nettt()\n",
      "criterion = nn.CrossEntropyLoss()\n",
      "optimizer = optim.SGD(nettt.parameters(), lr=0.0005, momentum=0.9)\n",
      "for epoch in range(30):  # loop over the dataset multiple times: 43-44\n",
      "    running_loss = 0.0\n",
      "    total_loss = 0\n",
      "    for i, data in enumerate(trainloader, 0):\n",
      "        # get the inputs; data is a list of [inputs, labels]\n",
      "        inputs, labels = data\n",
      "\n",
      "        # zero the parameter gradients\n",
      "        optimizer.zero_grad()\n",
      "\n",
      "        # forward + backward + optimize\n",
      "        outputs = nettt(inputs)\n",
      "        loss = criterion(outputs, labels)\n",
      "        loss.backward()\n",
      "        optimizer.step()\n",
      "\n",
      "        # print statistics\n",
      "        running_loss += loss.item()\n",
      "        total_loss += running_loss\n",
      "        if i % 2000 == 1999:    # print every 2000 mini-batches\n",
      "            print('[%d, %5d] loss: %.3f' %\n",
      "                  (epoch + 1, i + 1, running_loss / 2000))\n",
      "            running_loss = 0.0\n",
      "    total_lossestt.append(total_loss)\n",
      "print('Finished Training')\n",
      "133/227:\n",
      "class Nettt(nn.Module):\n",
      "    def __init__(self):\n",
      "        super(Nettt, self).__init__()\n",
      "        self.conv1 = nn.Conv2d(3, 6, 5)\n",
      "        self.pool = nn.MaxPool2d(2, 2)\n",
      "        self.conv2 = nn.Conv2d(6, 16, 5)\n",
      "        self.fc1 = nn.Linear(16 * 5 * 5, 120)\n",
      "        self.fc2 = nn.Linear(120, 84)\n",
      "        self.fc3 = nn.Linear(84, 10)\n",
      "        self.fc4 = nn.Linear(10,64)\n",
      "        self.fc5 = nn.Linear(64,10)\n",
      "\n",
      "    def forward(self, x):\n",
      "        x = self.pool(F.relu(self.conv1(x)))\n",
      "        x = self.pool(F.relu(self.conv2(x)))\n",
      "        x = x.view(-1, 16 * 5 * 5)\n",
      "        x = F.relu(self.fc1(x))\n",
      "        x = F.relu(self.fc2(x))\n",
      "        x = self.fc3(x)\n",
      "        x = self.fc4(x)\n",
      "        x = self.fc5(x)\n",
      "        return x\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "133/228:\n",
      "xs = [i for i in range(35)]\n",
      "total_lossestt = []\n",
      "nettt = Nettt()\n",
      "criterion = nn.CrossEntropyLoss()\n",
      "optimizer = optim.SGD(nettt.parameters(), lr=0.0005, momentum=0.9)\n",
      "for epoch in range(30):  # loop over the dataset multiple times: 43-44\n",
      "    running_loss = 0.0\n",
      "    total_loss = 0\n",
      "    for i, data in enumerate(trainloader, 0):\n",
      "        # get the inputs; data is a list of [inputs, labels]\n",
      "        inputs, labels = data\n",
      "\n",
      "        # zero the parameter gradients\n",
      "        optimizer.zero_grad()\n",
      "\n",
      "        # forward + backward + optimize\n",
      "        outputs = nettt(inputs)\n",
      "        loss = criterion(outputs, labels)\n",
      "        loss.backward()\n",
      "        optimizer.step()\n",
      "\n",
      "        # print statistics\n",
      "        running_loss += loss.item()\n",
      "        total_loss += running_loss\n",
      "        if i % 2000 == 1999:    # print every 2000 mini-batches\n",
      "            print('[%d, %5d] loss: %.3f' %\n",
      "                  (epoch + 1, i + 1, running_loss / 2000))\n",
      "            running_loss = 0.0\n",
      "    total_lossestt.append(total_loss)\n",
      "print('Finished Training')\n",
      "135/37:\n",
      "class Nettt(nn.Module):\n",
      "    def __init__(self):\n",
      "        super(Nettt, self).__init__()\n",
      "        self.conv1 = nn.Conv2d(3, 6, 5)\n",
      "        self.pool = nn.MaxPool2d(2, 2)\n",
      "        self.conv2 = nn.Conv2d(6, 16, 5)\n",
      "        self.pool = nn.MaxPool2d(2, 2)\n",
      "        self.conv3 = nn.Conv2d(16, 16, 5)\n",
      "        self.fc1 = nn.Linear(16 * 5 * 5, 120)\n",
      "        self.fc2 = nn.Linear(120, 84)\n",
      "        self.fc3 = nn.Linear(84, 10)\n",
      "        self.fc4 = nn.Linear(10,64)\n",
      "        self.fc5 = nn.Linear(64,10)\n",
      "\n",
      "    def forward(self, x):\n",
      "        x = self.pool(F.relu(self.conv1(x)))\n",
      "        x = self.pool(F.relu(self.conv2(x)))\n",
      "        x = x.view(-1, 16 * 5 * 5)\n",
      "        x = F.relu(self.fc1(x))\n",
      "        x = F.relu(self.fc2(x))\n",
      "        x = self.fc3(x)\n",
      "        x = self.fc4(x)\n",
      "        x = self.fc5(x)\n",
      "        return x\n",
      "135/38:\n",
      "class Nettt(nn.Module):\n",
      "    def __init__(self):\n",
      "        super(Nettt, self).__init__()\n",
      "        self.conv1 = nn.Conv2d(3, 6, 5)\n",
      "        self.pool = nn.MaxPool2d(2, 2)\n",
      "        self.conv2 = nn.Conv2d(6, 16, 5)\n",
      "        self.pool = nn.MaxPool2d(2, 2)\n",
      "        self.conv3 = nn.Conv2d(16, 16, 5)\n",
      "        self.fc1 = nn.Linear(16 * 5 * 5, 120)\n",
      "        self.fc2 = nn.Linear(120, 84)\n",
      "        self.fc3 = nn.Linear(84, 10)\n",
      "        self.fc4 = nn.Linear(10,64)\n",
      "        self.fc5 = nn.Linear(64,10)\n",
      "\n",
      "    def forward(self, x):\n",
      "        x = self.pool(F.relu(self.conv1(x)))\n",
      "        x = self.pool(F.relu(self.conv2(x)))\n",
      "        x = self.pool(F.relu(self.conv3(x)))\n",
      "        x = x.view(-1, 16 * 5 * 5)\n",
      "        x = F.relu(self.fc1(x))\n",
      "        x = F.relu(self.fc2(x))\n",
      "        x = self.fc3(x)\n",
      "        x = self.fc4(x)\n",
      "        x = self.fc5(x)\n",
      "        return x\n",
      "135/39:\n",
      "xs = [i for i in range(35)]\n",
      "total_lossestt = []\n",
      "nett = Nettt()\n",
      "criterion = nn.CrossEntropyLoss()\n",
      "optimizer = optim.SGD(nettt.parameters(), lr=0.0005, momentum=0.9)\n",
      "for epoch in range(30):  # loop over the dataset multiple times: 43-44\n",
      "    running_loss = 0.0\n",
      "    total_loss = 0\n",
      "    for i, data in enumerate(trainloader, 0):\n",
      "        # get the inputs; data is a list of [inputs, labels]\n",
      "        inputs, labels = data\n",
      "\n",
      "        # zero the parameter gradients\n",
      "        optimizer.zero_grad()\n",
      "\n",
      "        # forward + backward + optimize\n",
      "        outputs = nettt(inputs)\n",
      "        loss = criterion(outputs, labels)\n",
      "        loss.backward()\n",
      "        optimizer.step()\n",
      "\n",
      "        # print statistics\n",
      "        running_loss += loss.item()\n",
      "        total_loss += running_loss\n",
      "        if i % 2000 == 1999:    # print every 2000 mini-batches\n",
      "            print('[%d, %5d] loss: %.3f' %\n",
      "                  (epoch + 1, i + 1, running_loss / 2000))\n",
      "            running_loss = 0.0\n",
      "    total_lossestt.append(total_loss)\n",
      "print('Finished Training')\n",
      "135/40:\n",
      "class Nettt(nn.Module):\n",
      "    def __init__(self):\n",
      "        super(Nettt, self).__init__()\n",
      "        self.conv1 = nn.Conv2d(3, 6, 5)\n",
      "        self.pool = nn.MaxPool2d(2, 2)\n",
      "        self.conv2 = nn.Conv2d(6, 16, 5)\n",
      "        self.pool = nn.MaxPool2d(2, 2)\n",
      "        self.conv3 = nn.Conv2d(16, 16, 5)\n",
      "        self.fc1 = nn.Linear(16 * 5 * 5, 120)\n",
      "        self.fc2 = nn.Linear(120, 84)\n",
      "        self.fc3 = nn.Linear(84, 10)\n",
      "        self.fc4 = nn.Linear(10,64)\n",
      "        self.fc5 = nn.Linear(64,10)\n",
      "\n",
      "    def forward(self, x):\n",
      "        x = self.pool(F.relu(self.conv1(x)))\n",
      "        x = self.pool(F.relu(self.conv2(x)))\n",
      "        x = self.pool(F.relu(self.conv3(x)))\n",
      "        x = x.view(-1, 16 * 5 * 5)\n",
      "        x = F.relu(self.fc1(x))\n",
      "        x = F.relu(self.fc2(x))\n",
      "        x = self.fc3(x)\n",
      "        x = self.fc4(x)\n",
      "        x = self.fc5(x)\n",
      "        return x\n",
      "135/41:\n",
      "xs = [i for i in range(35)]\n",
      "total_lossestt = []\n",
      "nett3 = Nettt()\n",
      "criterion = nn.CrossEntropyLoss()\n",
      "optimizer = optim.SGD(nettt.parameters(), lr=0.0005, momentum=0.9)\n",
      "for epoch in range(30):  # loop over the dataset multiple times: 43-44\n",
      "    running_loss = 0.0\n",
      "    total_loss = 0\n",
      "    for i, data in enumerate(trainloader, 0):\n",
      "        # get the inputs; data is a list of [inputs, labels]\n",
      "        inputs, labels = data\n",
      "\n",
      "        # zero the parameter gradients\n",
      "        optimizer.zero_grad()\n",
      "\n",
      "        # forward + backward + optimize\n",
      "        outputs = nettt(inputs)\n",
      "        loss = criterion(outputs, labels)\n",
      "        loss.backward()\n",
      "        optimizer.step()\n",
      "\n",
      "        # print statistics\n",
      "        running_loss += loss.item()\n",
      "        total_loss += running_loss\n",
      "        if i % 2000 == 1999:    # print every 2000 mini-batches\n",
      "            print('[%d, %5d] loss: %.3f' %\n",
      "                  (epoch + 1, i + 1, running_loss / 2000))\n",
      "            running_loss = 0.0\n",
      "    total_lossestt.append(total_loss)\n",
      "print('Finished Training')\n",
      "135/42:\n",
      "xs = [i for i in range(35)]\n",
      "total_lossestt = []\n",
      "nettt = Nettt()\n",
      "criterion = nn.CrossEntropyLoss()\n",
      "optimizer = optim.SGD(nettt.parameters(), lr=0.0005, momentum=0.9)\n",
      "for epoch in range(30):  # loop over the dataset multiple times: 43-44\n",
      "    running_loss = 0.0\n",
      "    total_loss = 0\n",
      "    for i, data in enumerate(trainloader, 0):\n",
      "        # get the inputs; data is a list of [inputs, labels]\n",
      "        inputs, labels = data\n",
      "\n",
      "        # zero the parameter gradients\n",
      "        optimizer.zero_grad()\n",
      "\n",
      "        # forward + backward + optimize\n",
      "        outputs = nettt(inputs)\n",
      "        loss = criterion(outputs, labels)\n",
      "        loss.backward()\n",
      "        optimizer.step()\n",
      "\n",
      "        # print statistics\n",
      "        running_loss += loss.item()\n",
      "        total_loss += running_loss\n",
      "        if i % 2000 == 1999:    # print every 2000 mini-batches\n",
      "            print('[%d, %5d] loss: %.3f' %\n",
      "                  (epoch + 1, i + 1, running_loss / 2000))\n",
      "            running_loss = 0.0\n",
      "    total_lossestt.append(total_loss)\n",
      "print('Finished Training')\n",
      "135/43:\n",
      "class Nettt(nn.Module):\n",
      "    def __init__(self):\n",
      "        super(Nettt, self).__init__()\n",
      "        self.conv1 = nn.Conv2d(3, 6, 5)\n",
      "        self.pool = nn.MaxPool2d(2, 2)\n",
      "        self.conv2 = nn.Conv2d(6, 16, 5)\n",
      "#         self.pool = nn.MaxPool2d(2, 2)\n",
      "        self.conv3 = nn.Conv2d(16, 24, 5)\n",
      "        self.fc1 = nn.Linear(16 * 5 * 5, 120)\n",
      "        self.fc2 = nn.Linear(120, 84)\n",
      "        self.fc3 = nn.Linear(84, 10)\n",
      "        self.fc4 = nn.Linear(10,64)\n",
      "        self.fc5 = nn.Linear(64,10)\n",
      "\n",
      "    def forward(self, x):\n",
      "        x = self.pool(F.relu(self.conv1(x)))\n",
      "        x = self.pool(F.relu(self.conv2(x)))\n",
      "        x = self.pool(F.relu(self.conv3(x)))\n",
      "        x = x.view(-1, 16 * 5 * 5)\n",
      "        x = F.relu(self.fc1(x))\n",
      "        x = F.relu(self.fc2(x))\n",
      "        x = self.fc3(x)\n",
      "        x = self.fc4(x)\n",
      "        x = self.fc5(x)\n",
      "        return x\n",
      "135/44:\n",
      "xs = [i for i in range(35)]\n",
      "total_lossestt = []\n",
      "nettt = Nettt()\n",
      "criterion = nn.CrossEntropyLoss()\n",
      "optimizer = optim.SGD(nettt.parameters(), lr=0.0005, momentum=0.9)\n",
      "for epoch in range(30):  # loop over the dataset multiple times: 43-44\n",
      "    running_loss = 0.0\n",
      "    total_loss = 0\n",
      "    for i, data in enumerate(trainloader, 0):\n",
      "        # get the inputs; data is a list of [inputs, labels]\n",
      "        inputs, labels = data\n",
      "\n",
      "        # zero the parameter gradients\n",
      "        optimizer.zero_grad()\n",
      "\n",
      "        # forward + backward + optimize\n",
      "        outputs = nettt(inputs)\n",
      "        loss = criterion(outputs, labels)\n",
      "        loss.backward()\n",
      "        optimizer.step()\n",
      "\n",
      "        # print statistics\n",
      "        running_loss += loss.item()\n",
      "        total_loss += running_loss\n",
      "        if i % 2000 == 1999:    # print every 2000 mini-batches\n",
      "            print('[%d, %5d] loss: %.3f' %\n",
      "                  (epoch + 1, i + 1, running_loss / 2000))\n",
      "            running_loss = 0.0\n",
      "    total_lossestt.append(total_loss)\n",
      "print('Finished Training')\n",
      "135/45:\n",
      "class Nettt(nn.Module):\n",
      "    def __init__(self):\n",
      "        super(Nettt, self).__init__()\n",
      "        self.conv1 = nn.Conv2d(3, 6, 5)\n",
      "        self.pool = nn.MaxPool2d(2, 2)\n",
      "        self.conv2 = nn.Conv2d(6, 16, 5)\n",
      "#         self.pool = nn.MaxPool2d(2, 2)\n",
      "        self.conv3 = nn.Conv2d(16, 24, 5)\n",
      "        self.fc1 = nn.Linear(16 * 5 * 5, 120)\n",
      "        self.fc2 = nn.Linear(120, 84)\n",
      "        self.fc3 = nn.Linear(84, 10)\n",
      "        self.fc4 = nn.Linear(10,64)\n",
      "        self.fc5 = nn.Linear(64,10)\n",
      "\n",
      "    def forward(self, x):\n",
      "        x = self.pool(F.relu(self.conv1(x)))\n",
      "        x = self.pool(F.relu(self.conv2(x)))\n",
      "        x = self.pool(F.relu(self.conv3(x)))\n",
      "        x = x.view(-1, 16 * 5 * 5)\n",
      "        x = F.relu(self.fc1(x))\n",
      "        x = F.relu(self.fc2(x))\n",
      "        x = self.fc3(x)\n",
      "        x = self.fc4(x)\n",
      "        x = self.fc5(x)\n",
      "        return x\n",
      "135/46:\n",
      "xs = [i for i in range(35)]\n",
      "total_lossestt = []\n",
      "nettt = Nettt()\n",
      "criterion = nn.CrossEntropyLoss()\n",
      "optimizer = optim.SGD(nettt.parameters(), lr=0.0005, momentum=0.9)\n",
      "for epoch in range(30):  # loop over the dataset multiple times: 43-44\n",
      "    running_loss = 0.0\n",
      "    total_loss = 0\n",
      "    for i, data in enumerate(trainloader, 0):\n",
      "        # get the inputs; data is a list of [inputs, labels]\n",
      "        inputs, labels = data\n",
      "\n",
      "        # zero the parameter gradients\n",
      "        optimizer.zero_grad()\n",
      "\n",
      "        # forward + backward + optimize\n",
      "        outputs = nettt(inputs)\n",
      "        loss = criterion(outputs, labels)\n",
      "        loss.backward()\n",
      "        optimizer.step()\n",
      "\n",
      "        # print statistics\n",
      "        running_loss += loss.item()\n",
      "        total_loss += running_loss\n",
      "        if i % 2000 == 1999:    # print every 2000 mini-batches\n",
      "            print('[%d, %5d] loss: %.3f' %\n",
      "                  (epoch + 1, i + 1, running_loss / 2000))\n",
      "            running_loss = 0.0\n",
      "    total_lossestt.append(total_loss)\n",
      "print('Finished Training')\n",
      "133/229:\n",
      "correct = 0\n",
      "total = 0\n",
      "with torch.no_grad():\n",
      "    for data in testloader:\n",
      "        images, labels = data\n",
      "        outputs = nett(images)\n",
      "        _, predicted = torch.max(outputs.data, 1)\n",
      "        total += labels.size(0)\n",
      "        correct += (predicted == labels).sum().item()\n",
      "print(total)\n",
      "print('Accuracy of the network on the 10000 test images: %d %%' % (\n",
      "    100 * correct / total))\n",
      "133/230:\n",
      "correct = 0\n",
      "total = 0\n",
      "with torch.no_grad():\n",
      "    for data in testloader:\n",
      "        images, labels = data\n",
      "        outputs = nettt(images)\n",
      "        _, predicted = torch.max(outputs.data, 1)\n",
      "        total += labels.size(0)\n",
      "        correct += (predicted == labels).sum().item()\n",
      "print(total)\n",
      "print('Accuracy of the network on the 10000 test images: %d %%' % (\n",
      "    100 * correct / total))\n",
      "133/231:\n",
      "plt.plot(total_losses2[:30], label=\"lr=0.01\")\n",
      "plt.plot(total_losses[:30], label=\"lr=0.001\")\n",
      "plt.plot(total_losses3[:30], label=\"lr=0.0001\")\n",
      "plt.legend()\n",
      "plt.title(\"Loss over epochs for different learning rates\")\n",
      "133/232:\n",
      "plt.plot(total_losses2[:30], label=\"lr=0.01\")\n",
      "plt.plot(total_losses[:30], label=\"lr=0.001\")\n",
      "plt.plot(total_lossestt[:30], label=\"lr=0.0001\")\n",
      "plt.legend()\n",
      "plt.title(\"Loss over epochs for different learning rates\")\n",
      "133/233:\n",
      "plt.plot(total_losses2[:30], label=\"lr=0.01\")\n",
      "plt.plot(total_losses[:30], label=\"lr=0.001\")\n",
      "plt.plot(total_lossestt[:30], label=\"TT\")\n",
      "plt.legend()\n",
      "plt.title(\"Loss over epochs for different learning rates\")\n",
      "133/234:\n",
      "plt.plot(total_losses[:30], label=\"Original\")\n",
      "plt.plot(total_lossestt[:30], label=\"Added two relu layers\")\n",
      "plt.legend()\n",
      "plt.title(\"Loss over epochs for different learning rates\")\n",
      "135/47:\n",
      "class Nettt(nn.Module):\n",
      "    def __init__(self):\n",
      "        super(Nettt, self).__init__()\n",
      "        self.conv1 = nn.Conv2d(3, 6, 5)\n",
      "        self.pool = nn.MaxPool2d(2, 2)\n",
      "        self.conv2 = nn.Conv2d(6, 16, 5)\n",
      "#         self.pool = nn.MaxPool2d(2, 2)\n",
      "        self.conv3 = nn.Conv2d(16, 24, 5)\n",
      "        self.fc1 = nn.Linear(16 * 5 * 5, 120)\n",
      "        self.fc2 = nn.Linear(120, 84)\n",
      "        self.fc3 = nn.Linear(84, 10)\n",
      "        self.fc4 = nn.Linear(10,64)\n",
      "        self.fc5 = nn.Linear(64,10)\n",
      "\n",
      "    def forward(self, x):\n",
      "        x = self.pool(F.relu(self.conv1(x)))\n",
      "        x = self.pool(F.relu(self.conv2(x)))\n",
      "        x = self.pool(F.relu(self.conv3(x)))\n",
      "        x = x.view(-1, 16 * 5 * 5)\n",
      "        x = F.relu(self.fc1(x))\n",
      "        x = F.relu(self.fc2(x))\n",
      "        x = self.fc3(x)\n",
      "        x = self.fc4(x)\n",
      "        x = self.fc5(x)\n",
      "        return x\n",
      "135/48:\n",
      "xs = [i for i in range(35)]\n",
      "total_lossestt = []\n",
      "nettt = Nettt()\n",
      "criterion = nn.CrossEntropyLoss()\n",
      "optimizer = optim.SGD(nettt.parameters(), lr=0.0005, momentum=0.9)\n",
      "for epoch in range(30):  # loop over the dataset multiple times: 43-44\n",
      "    running_loss = 0.0\n",
      "    total_loss = 0\n",
      "    for i, data in enumerate(trainloader, 0):\n",
      "        # get the inputs; data is a list of [inputs, labels]\n",
      "        inputs, labels = data\n",
      "\n",
      "        # zero the parameter gradients\n",
      "        optimizer.zero_grad()\n",
      "\n",
      "        # forward + backward + optimize\n",
      "        outputs = nettt(inputs)\n",
      "        loss = criterion(outputs, labels)\n",
      "        loss.backward()\n",
      "        optimizer.step()\n",
      "\n",
      "        # print statistics\n",
      "        running_loss += loss.item()\n",
      "        total_loss += running_loss\n",
      "        if i % 2000 == 1999:    # print every 2000 mini-batches\n",
      "            print('[%d, %5d] loss: %.3f' %\n",
      "                  (epoch + 1, i + 1, running_loss / 2000))\n",
      "            running_loss = 0.0\n",
      "    total_lossestt.append(total_loss)\n",
      "print('Finished Training')\n",
      "135/49:\n",
      "class Nettt(nn.Module):\n",
      "    def __init__(self):\n",
      "        super(Nettt, self).__init__()\n",
      "        self.conv1 = nn.Conv2d(3, 6, 5)\n",
      "        self.pool = nn.MaxPool2d(2, 2)\n",
      "        self.conv2 = nn.Conv2d(6, 16, 5)\n",
      "        self.pool = nn.MaxPool2d(2, 2)\n",
      "        self.conv3 = nn.Conv2d(24, 16, 5)\n",
      "        self.fc1 = nn.Linear(16 * 5 * 5, 120)\n",
      "        self.fc2 = nn.Linear(120, 84)\n",
      "        self.fc3 = nn.Linear(84, 10)\n",
      "        self.fc4 = nn.Linear(10,64)\n",
      "        self.fc5 = nn.Linear(64,10)\n",
      "\n",
      "    def forward(self, x):\n",
      "        x = self.pool(F.relu(self.conv1(x)))\n",
      "        x = self.pool(F.relu(self.conv2(x)))\n",
      "        x = self.pool(F.relu(self.conv3(x)))\n",
      "        x = x.view(-1, 16 * 5 * 5)\n",
      "        x = F.relu(self.fc1(x))\n",
      "        x = F.relu(self.fc2(x))\n",
      "        x = self.fc3(x)\n",
      "        x = self.fc4(x)\n",
      "        x = self.fc5(x)\n",
      "        return x\n",
      "135/50:\n",
      "xs = [i for i in range(35)]\n",
      "total_lossestt = []\n",
      "nettt = Nettt()\n",
      "criterion = nn.CrossEntropyLoss()\n",
      "optimizer = optim.SGD(nettt.parameters(), lr=0.0005, momentum=0.9)\n",
      "for epoch in range(30):  # loop over the dataset multiple times: 43-44\n",
      "    running_loss = 0.0\n",
      "    total_loss = 0\n",
      "    for i, data in enumerate(trainloader, 0):\n",
      "        # get the inputs; data is a list of [inputs, labels]\n",
      "        inputs, labels = data\n",
      "\n",
      "        # zero the parameter gradients\n",
      "        optimizer.zero_grad()\n",
      "\n",
      "        # forward + backward + optimize\n",
      "        outputs = nettt(inputs)\n",
      "        loss = criterion(outputs, labels)\n",
      "        loss.backward()\n",
      "        optimizer.step()\n",
      "\n",
      "        # print statistics\n",
      "        running_loss += loss.item()\n",
      "        total_loss += running_loss\n",
      "        if i % 2000 == 1999:    # print every 2000 mini-batches\n",
      "            print('[%d, %5d] loss: %.3f' %\n",
      "                  (epoch + 1, i + 1, running_loss / 2000))\n",
      "            running_loss = 0.0\n",
      "    total_lossestt.append(total_loss)\n",
      "print('Finished Training')\n",
      "135/51:\n",
      "class Nettt(nn.Module):\n",
      "    def __init__(self):\n",
      "        super(Nettt, self).__init__()\n",
      "        self.conv1 = nn.Conv2d(3, 6, 5)\n",
      "        self.pool = nn.MaxPool2d(2, 2)\n",
      "        self.conv2 = nn.Conv2d(6, 24, 5)\n",
      "        self.pool = nn.MaxPool2d(2, 2)\n",
      "        self.conv3 = nn.Conv2d(24, 16, 5)\n",
      "        self.fc1 = nn.Linear(16 * 5 * 5, 120)\n",
      "        self.fc2 = nn.Linear(120, 84)\n",
      "        self.fc3 = nn.Linear(84, 10)\n",
      "        self.fc4 = nn.Linear(10,64)\n",
      "        self.fc5 = nn.Linear(64,10)\n",
      "\n",
      "    def forward(self, x):\n",
      "        x = self.pool(F.relu(self.conv1(x)))\n",
      "        x = self.pool(F.relu(self.conv2(x)))\n",
      "        x = self.pool(F.relu(self.conv3(x)))\n",
      "        x = x.view(-1, 16 * 5 * 5)\n",
      "        x = F.relu(self.fc1(x))\n",
      "        x = F.relu(self.fc2(x))\n",
      "        x = self.fc3(x)\n",
      "        x = self.fc4(x)\n",
      "        x = self.fc5(x)\n",
      "        return x\n",
      "135/52:\n",
      "xs = [i for i in range(35)]\n",
      "total_lossestt = []\n",
      "nettt = Nettt()\n",
      "criterion = nn.CrossEntropyLoss()\n",
      "optimizer = optim.SGD(nettt.parameters(), lr=0.0005, momentum=0.9)\n",
      "for epoch in range(30):  # loop over the dataset multiple times: 43-44\n",
      "    running_loss = 0.0\n",
      "    total_loss = 0\n",
      "    for i, data in enumerate(trainloader, 0):\n",
      "        # get the inputs; data is a list of [inputs, labels]\n",
      "        inputs, labels = data\n",
      "\n",
      "        # zero the parameter gradients\n",
      "        optimizer.zero_grad()\n",
      "\n",
      "        # forward + backward + optimize\n",
      "        outputs = nettt(inputs)\n",
      "        loss = criterion(outputs, labels)\n",
      "        loss.backward()\n",
      "        optimizer.step()\n",
      "\n",
      "        # print statistics\n",
      "        running_loss += loss.item()\n",
      "        total_loss += running_loss\n",
      "        if i % 2000 == 1999:    # print every 2000 mini-batches\n",
      "            print('[%d, %5d] loss: %.3f' %\n",
      "                  (epoch + 1, i + 1, running_loss / 2000))\n",
      "            running_loss = 0.0\n",
      "    total_lossestt.append(total_loss)\n",
      "print('Finished Training')\n",
      "135/53:\n",
      "class Nettt(nn.Module):\n",
      "    def __init__(self):\n",
      "        super(Nettt, self).__init__()\n",
      "        self.conv1 = nn.Conv2d(3, 6, 5)\n",
      "        self.pool = nn.MaxPool2d(2, 2)\n",
      "        self.conv2 = nn.Conv2d(6, 24, 5)\n",
      "#         self.conv3 = nn.Conv2d(24, 16, 5)\n",
      "        self.fc1 = nn.Linear(24 * 5 * 5, 120)\n",
      "        self.fc2 = nn.Linear(120, 84)\n",
      "        self.fc3 = nn.Linear(84, 10)\n",
      "        self.fc4 = nn.Linear(10,64)\n",
      "        self.fc5 = nn.Linear(64,10)\n",
      "\n",
      "    def forward(self, x):\n",
      "        x = self.pool(F.relu(self.conv1(x)))\n",
      "        x = self.pool(F.relu(self.conv2(x)))\n",
      "        x = self.pool(F.relu(self.conv3(x)))\n",
      "        x = x.view(-1, 16 * 5 * 5)\n",
      "        x = F.relu(self.fc1(x))\n",
      "        x = F.relu(self.fc2(x))\n",
      "        x = self.fc3(x)\n",
      "        x = self.fc4(x)\n",
      "        x = self.fc5(x)\n",
      "        return x\n",
      "135/54:\n",
      "xs = [i for i in range(35)]\n",
      "total_lossestt = []\n",
      "nettt = Nettt()\n",
      "criterion = nn.CrossEntropyLoss()\n",
      "optimizer = optim.SGD(nettt.parameters(), lr=0.0005, momentum=0.9)\n",
      "for epoch in range(30):  # loop over the dataset multiple times: 43-44\n",
      "    running_loss = 0.0\n",
      "    total_loss = 0\n",
      "    for i, data in enumerate(trainloader, 0):\n",
      "        # get the inputs; data is a list of [inputs, labels]\n",
      "        inputs, labels = data\n",
      "\n",
      "        # zero the parameter gradients\n",
      "        optimizer.zero_grad()\n",
      "\n",
      "        # forward + backward + optimize\n",
      "        outputs = nettt(inputs)\n",
      "        loss = criterion(outputs, labels)\n",
      "        loss.backward()\n",
      "        optimizer.step()\n",
      "\n",
      "        # print statistics\n",
      "        running_loss += loss.item()\n",
      "        total_loss += running_loss\n",
      "        if i % 2000 == 1999:    # print every 2000 mini-batches\n",
      "            print('[%d, %5d] loss: %.3f' %\n",
      "                  (epoch + 1, i + 1, running_loss / 2000))\n",
      "            running_loss = 0.0\n",
      "    total_lossestt.append(total_loss)\n",
      "print('Finished Training')\n",
      "135/55:\n",
      "class Nettt(nn.Module):\n",
      "    def __init__(self):\n",
      "        super(Nettt, self).__init__()\n",
      "        self.conv1 = nn.Conv2d(3, 6, 5)\n",
      "        self.pool = nn.MaxPool2d(2, 2)\n",
      "        self.conv2 = nn.Conv2d(6, 24, 5)\n",
      "#         self.conv3 = nn.Conv2d(24, 16, 5)\n",
      "        self.fc1 = nn.Linear(24 * 5 * 5, 120)\n",
      "        self.fc2 = nn.Linear(120, 84)\n",
      "        self.fc3 = nn.Linear(84, 10)\n",
      "        self.fc4 = nn.Linear(10,64)\n",
      "        self.fc5 = nn.Linear(64,10)\n",
      "\n",
      "    def forward(self, x):\n",
      "        x = self.pool(F.relu(self.conv1(x)))\n",
      "        x = self.pool(F.relu(self.conv2(x)))\n",
      "#         x = self.pool(F.relu(self.conv3(x)))\n",
      "        x = x.view(-1, 16 * 5 * 5)\n",
      "        x = F.relu(self.fc1(x))\n",
      "        x = F.relu(self.fc2(x))\n",
      "        x = self.fc3(x)\n",
      "        x = self.fc4(x)\n",
      "        x = self.fc5(x)\n",
      "        return x\n",
      "135/56:\n",
      "xs = [i for i in range(35)]\n",
      "total_lossestt = []\n",
      "nettt = Nettt()\n",
      "criterion = nn.CrossEntropyLoss()\n",
      "optimizer = optim.SGD(nettt.parameters(), lr=0.0005, momentum=0.9)\n",
      "for epoch in range(30):  # loop over the dataset multiple times: 43-44\n",
      "    running_loss = 0.0\n",
      "    total_loss = 0\n",
      "    for i, data in enumerate(trainloader, 0):\n",
      "        # get the inputs; data is a list of [inputs, labels]\n",
      "        inputs, labels = data\n",
      "\n",
      "        # zero the parameter gradients\n",
      "        optimizer.zero_grad()\n",
      "\n",
      "        # forward + backward + optimize\n",
      "        outputs = nettt(inputs)\n",
      "        loss = criterion(outputs, labels)\n",
      "        loss.backward()\n",
      "        optimizer.step()\n",
      "\n",
      "        # print statistics\n",
      "        running_loss += loss.item()\n",
      "        total_loss += running_loss\n",
      "        if i % 2000 == 1999:    # print every 2000 mini-batches\n",
      "            print('[%d, %5d] loss: %.3f' %\n",
      "                  (epoch + 1, i + 1, running_loss / 2000))\n",
      "            running_loss = 0.0\n",
      "    total_lossestt.append(total_loss)\n",
      "print('Finished Training')\n",
      "135/57:\n",
      "class Nettt(nn.Module):\n",
      "    def __init__(self):\n",
      "        super(Nettt, self).__init__()\n",
      "        self.conv1 = nn.Conv2d(3, 6, 5)\n",
      "        self.pool = nn.MaxPool2d(2, 2)\n",
      "        self.conv2 = nn.Conv2d(6, 16, 5)\n",
      "#         self.conv3 = nn.Conv2d(24, 16, 5)\n",
      "        self.fc1 = nn.Linear(16 * 5 * 5, 120)\n",
      "        self.fc2 = nn.Linear(120, 84)\n",
      "        self.fc3 = nn.Linear(84, 10)\n",
      "        self.fc4 = nn.Linear(10,64)\n",
      "        self.fc5 = nn.Linear(64,10)\n",
      "\n",
      "    def forward(self, x):\n",
      "        x = self.pool(F.relu(self.conv1(x)))\n",
      "        x = self.pool(F.relu(self.conv2(x)))\n",
      "#         x = self.pool(F.relu(self.conv3(x)))\n",
      "        x = x.view(-1, 16 * 5 * 5)\n",
      "        x = F.relu(self.fc1(x))\n",
      "        x = F.relu(self.fc2(x))\n",
      "        x = self.fc3(x)\n",
      "        x = self.fc4(x)\n",
      "        x = self.fc5(x)\n",
      "        return x\n",
      "135/58:\n",
      "xs = [i for i in range(35)]\n",
      "total_lossestt = []\n",
      "nettt = Nettt()\n",
      "criterion = nn.CrossEntropyLoss()\n",
      "optimizer = optim.SGD(nettt.parameters(), lr=0.0005, momentum=0.9)\n",
      "for epoch in range(30):  # loop over the dataset multiple times: 43-44\n",
      "    running_loss = 0.0\n",
      "    total_loss = 0\n",
      "    for i, data in enumerate(trainloader, 0):\n",
      "        # get the inputs; data is a list of [inputs, labels]\n",
      "        inputs, labels = data\n",
      "\n",
      "        # zero the parameter gradients\n",
      "        optimizer.zero_grad()\n",
      "\n",
      "        # forward + backward + optimize\n",
      "        outputs = nettt(inputs)\n",
      "        loss = criterion(outputs, labels)\n",
      "        loss.backward()\n",
      "        optimizer.step()\n",
      "\n",
      "        # print statistics\n",
      "        running_loss += loss.item()\n",
      "        total_loss += running_loss\n",
      "        if i % 2000 == 1999:    # print every 2000 mini-batches\n",
      "            print('[%d, %5d] loss: %.3f' %\n",
      "                  (epoch + 1, i + 1, running_loss / 2000))\n",
      "            running_loss = 0.0\n",
      "    total_lossestt.append(total_loss)\n",
      "print('Finished Training')\n",
      "135/59:\n",
      "class Nettt(nn.Module):\n",
      "    def __init__(self):\n",
      "        super(Nettt, self).__init__()\n",
      "        self.conv1 = nn.Conv2d(3, 6, 5)\n",
      "        self.pool = nn.MaxPool2d(2, 2)\n",
      "        self.conv2 = nn.Conv2d(6, 16, 5)\n",
      "        self.conv3 = nn.Conv2d(16*5*5, 10)\n",
      "        self.fc1 = nn.Linear(16 * 5 * 5, 120)\n",
      "        self.fc2 = nn.Linear(120, 84)\n",
      "        self.fc3 = nn.Linear(84, 10)\n",
      "        self.fc4 = nn.Linear(10,64)\n",
      "        self.fc5 = nn.Linear(64,10)\n",
      "\n",
      "    def forward(self, x):\n",
      "        x = self.pool(F.relu(self.conv1(x)))\n",
      "        x = self.pool(F.relu(self.conv2(x)))\n",
      "        x = self.pool(F.relu(self.conv3(x)))\n",
      "        x = x.view(-1, 16 * 5 * 5)\n",
      "        x = F.relu(self.fc1(x))\n",
      "        x = F.relu(self.fc2(x))\n",
      "        x = self.fc3(x)\n",
      "        x = self.fc4(x)\n",
      "        x = self.fc5(x)\n",
      "        return x\n",
      "135/60:\n",
      "xs = [i for i in range(35)]\n",
      "total_lossestt = []\n",
      "nettt = Nettt()\n",
      "criterion = nn.CrossEntropyLoss()\n",
      "optimizer = optim.SGD(nettt.parameters(), lr=0.0005, momentum=0.9)\n",
      "for epoch in range(30):  # loop over the dataset multiple times: 43-44\n",
      "    running_loss = 0.0\n",
      "    total_loss = 0\n",
      "    for i, data in enumerate(trainloader, 0):\n",
      "        # get the inputs; data is a list of [inputs, labels]\n",
      "        inputs, labels = data\n",
      "\n",
      "        # zero the parameter gradients\n",
      "        optimizer.zero_grad()\n",
      "\n",
      "        # forward + backward + optimize\n",
      "        outputs = nettt(inputs)\n",
      "        loss = criterion(outputs, labels)\n",
      "        loss.backward()\n",
      "        optimizer.step()\n",
      "\n",
      "        # print statistics\n",
      "        running_loss += loss.item()\n",
      "        total_loss += running_loss\n",
      "        if i % 2000 == 1999:    # print every 2000 mini-batches\n",
      "            print('[%d, %5d] loss: %.3f' %\n",
      "                  (epoch + 1, i + 1, running_loss / 2000))\n",
      "            running_loss = 0.0\n",
      "    total_lossestt.append(total_loss)\n",
      "print('Finished Training')\n",
      "135/61:\n",
      "class Nettt(nn.Module):\n",
      "    def __init__(self):\n",
      "        super(Nettt, self).__init__()\n",
      "        self.conv1 = nn.Conv2d(3, 6, 5)\n",
      "        self.pool = nn.MaxPool2d(2, 2)\n",
      "        self.conv2 = nn.Conv2d(6, 16, 5)\n",
      "        self.conv3 = nn.Conv2d(16*5*5, 10, 5)\n",
      "        self.fc1 = nn.Linear(16 * 5 * 5, 120)\n",
      "        self.fc2 = nn.Linear(120, 84)\n",
      "        self.fc3 = nn.Linear(84, 10)\n",
      "        self.fc4 = nn.Linear(10,64)\n",
      "        self.fc5 = nn.Linear(64,10)\n",
      "\n",
      "    def forward(self, x):\n",
      "        x = self.pool(F.relu(self.conv1(x)))\n",
      "        x = self.pool(F.relu(self.conv2(x)))\n",
      "        x = self.pool(F.relu(self.conv3(x)))\n",
      "        x = x.view(-1, 16 * 5 * 5)\n",
      "        x = F.relu(self.fc1(x))\n",
      "        x = F.relu(self.fc2(x))\n",
      "        x = self.fc3(x)\n",
      "        x = self.fc4(x)\n",
      "        x = self.fc5(x)\n",
      "        return x\n",
      "135/62:\n",
      "xs = [i for i in range(35)]\n",
      "total_lossestt = []\n",
      "nettt = Nettt()\n",
      "criterion = nn.CrossEntropyLoss()\n",
      "optimizer = optim.SGD(nettt.parameters(), lr=0.0005, momentum=0.9)\n",
      "for epoch in range(30):  # loop over the dataset multiple times: 43-44\n",
      "    running_loss = 0.0\n",
      "    total_loss = 0\n",
      "    for i, data in enumerate(trainloader, 0):\n",
      "        # get the inputs; data is a list of [inputs, labels]\n",
      "        inputs, labels = data\n",
      "\n",
      "        # zero the parameter gradients\n",
      "        optimizer.zero_grad()\n",
      "\n",
      "        # forward + backward + optimize\n",
      "        outputs = nettt(inputs)\n",
      "        loss = criterion(outputs, labels)\n",
      "        loss.backward()\n",
      "        optimizer.step()\n",
      "\n",
      "        # print statistics\n",
      "        running_loss += loss.item()\n",
      "        total_loss += running_loss\n",
      "        if i % 2000 == 1999:    # print every 2000 mini-batches\n",
      "            print('[%d, %5d] loss: %.3f' %\n",
      "                  (epoch + 1, i + 1, running_loss / 2000))\n",
      "            running_loss = 0.0\n",
      "    total_lossestt.append(total_loss)\n",
      "print('Finished Training')\n",
      "135/63:\n",
      "class Nettt(nn.Module):\n",
      "    def __init__(self):\n",
      "        super(Nettt, self).__init__()\n",
      "        self.conv1 = nn.Conv2d(3, 6, 5)\n",
      "        self.pool = nn.MaxPool2d(2, 2)\n",
      "        self.conv2 = nn.Conv2d(6, 16, 5)\n",
      "        self.conv3 = nn.Conv2d(400*5*5, 10, 5)\n",
      "        self.fc1 = nn.Linear(16 * 5 * 5, 120)\n",
      "        self.fc2 = nn.Linear(120, 84)\n",
      "        self.fc3 = nn.Linear(84, 10)\n",
      "        self.fc4 = nn.Linear(10,64)\n",
      "        self.fc5 = nn.Linear(64,10)\n",
      "\n",
      "    def forward(self, x):\n",
      "        x = self.pool(F.relu(self.conv1(x)))\n",
      "        x = self.pool(F.relu(self.conv2(x)))\n",
      "        x = self.pool(F.relu(self.conv3(x)))\n",
      "        x = x.view(-1, 16 * 5 * 5)\n",
      "        x = F.relu(self.fc1(x))\n",
      "        x = F.relu(self.fc2(x))\n",
      "        x = self.fc3(x)\n",
      "        x = self.fc4(x)\n",
      "        x = self.fc5(x)\n",
      "        return x\n",
      "135/64:\n",
      "xs = [i for i in range(35)]\n",
      "total_lossestt = []\n",
      "nettt = Nettt()\n",
      "criterion = nn.CrossEntropyLoss()\n",
      "optimizer = optim.SGD(nettt.parameters(), lr=0.0005, momentum=0.9)\n",
      "for epoch in range(30):  # loop over the dataset multiple times: 43-44\n",
      "    running_loss = 0.0\n",
      "    total_loss = 0\n",
      "    for i, data in enumerate(trainloader, 0):\n",
      "        # get the inputs; data is a list of [inputs, labels]\n",
      "        inputs, labels = data\n",
      "\n",
      "        # zero the parameter gradients\n",
      "        optimizer.zero_grad()\n",
      "\n",
      "        # forward + backward + optimize\n",
      "        outputs = nettt(inputs)\n",
      "        loss = criterion(outputs, labels)\n",
      "        loss.backward()\n",
      "        optimizer.step()\n",
      "\n",
      "        # print statistics\n",
      "        running_loss += loss.item()\n",
      "        total_loss += running_loss\n",
      "        if i % 2000 == 1999:    # print every 2000 mini-batches\n",
      "            print('[%d, %5d] loss: %.3f' %\n",
      "                  (epoch + 1, i + 1, running_loss / 2000))\n",
      "            running_loss = 0.0\n",
      "    total_lossestt.append(total_loss)\n",
      "print('Finished Training')\n",
      "135/65:\n",
      "class Nettt(nn.Module):\n",
      "    def __init__(self):\n",
      "        super(Nettt, self).__init__()\n",
      "        self.conv1 = nn.Conv2d(3, 6, 5)\n",
      "        self.pool = nn.MaxPool2d(2, 2)\n",
      "        self.conv2 = nn.Conv2d(6, 16, 5)\n",
      "        self.conv3 = nn.Conv2d(16, 10, 5,padding=2)\n",
      "        self.fc1 = nn.Linear(16 * 5 * 5, 120)\n",
      "        self.fc2 = nn.Linear(120, 84)\n",
      "        self.fc3 = nn.Linear(84, 10)\n",
      "        self.fc4 = nn.Linear(10,64)\n",
      "        self.fc5 = nn.Linear(64,10)\n",
      "\n",
      "    def forward(self, x):\n",
      "        x = self.pool(F.relu(self.conv1(x)))\n",
      "        x = self.pool(F.relu(self.conv2(x)))\n",
      "        x = self.pool(F.relu(self.conv3(x)))\n",
      "        x = x.view(-1, 16 * 5 * 5)\n",
      "        x = F.relu(self.fc1(x))\n",
      "        x = F.relu(self.fc2(x))\n",
      "        x = self.fc3(x)\n",
      "        x = self.fc4(x)\n",
      "        x = self.fc5(x)\n",
      "        return x\n",
      "135/66:\n",
      "xs = [i for i in range(35)]\n",
      "total_lossestt = []\n",
      "nettt = Nettt()\n",
      "criterion = nn.CrossEntropyLoss()\n",
      "optimizer = optim.SGD(nettt.parameters(), lr=0.0005, momentum=0.9)\n",
      "for epoch in range(30):  # loop over the dataset multiple times: 43-44\n",
      "    running_loss = 0.0\n",
      "    total_loss = 0\n",
      "    for i, data in enumerate(trainloader, 0):\n",
      "        # get the inputs; data is a list of [inputs, labels]\n",
      "        inputs, labels = data\n",
      "\n",
      "        # zero the parameter gradients\n",
      "        optimizer.zero_grad()\n",
      "\n",
      "        # forward + backward + optimize\n",
      "        outputs = nettt(inputs)\n",
      "        loss = criterion(outputs, labels)\n",
      "        loss.backward()\n",
      "        optimizer.step()\n",
      "\n",
      "        # print statistics\n",
      "        running_loss += loss.item()\n",
      "        total_loss += running_loss\n",
      "        if i % 2000 == 1999:    # print every 2000 mini-batches\n",
      "            print('[%d, %5d] loss: %.3f' %\n",
      "                  (epoch + 1, i + 1, running_loss / 2000))\n",
      "            running_loss = 0.0\n",
      "    total_lossestt.append(total_loss)\n",
      "print('Finished Training')\n",
      "135/67:\n",
      "class Nettt(nn.Module):\n",
      "    def __init__(self):\n",
      "        super(Nettt, self).__init__()\n",
      "        self.conv1 = nn.Conv2d(3, 6, 5)\n",
      "        self.pool = nn.MaxPool2d(2, 2)\n",
      "        self.conv2 = nn.Conv2d(6, 16, 5)\n",
      "        self.conv3 = nn.Conv2d(16, 10, 5,padding=2)\n",
      "        self.fc1 = nn.Linear(16, 120)\n",
      "        self.fc2 = nn.Linear(120, 84)\n",
      "        self.fc3 = nn.Linear(84, 10)\n",
      "        self.fc4 = nn.Linear(10,64)\n",
      "        self.fc5 = nn.Linear(64,10)\n",
      "\n",
      "    def forward(self, x):\n",
      "        x = self.pool(F.relu(self.conv1(x)))\n",
      "        x = self.pool(F.relu(self.conv2(x)))\n",
      "        x = self.pool(F.relu(self.conv3(x)))\n",
      "        x = x.view(-1, 16)\n",
      "        x = F.relu(self.fc1(x))\n",
      "        x = F.relu(self.fc2(x))\n",
      "        x = self.fc3(x)\n",
      "        x = self.fc4(x)\n",
      "        x = self.fc5(x)\n",
      "        return x\n",
      "135/68:\n",
      "xs = [i for i in range(35)]\n",
      "total_lossestt = []\n",
      "nettt = Nettt()\n",
      "criterion = nn.CrossEntropyLoss()\n",
      "optimizer = optim.SGD(nettt.parameters(), lr=0.0005, momentum=0.9)\n",
      "for epoch in range(30):  # loop over the dataset multiple times: 43-44\n",
      "    running_loss = 0.0\n",
      "    total_loss = 0\n",
      "    for i, data in enumerate(trainloader, 0):\n",
      "        # get the inputs; data is a list of [inputs, labels]\n",
      "        inputs, labels = data\n",
      "\n",
      "        # zero the parameter gradients\n",
      "        optimizer.zero_grad()\n",
      "\n",
      "        # forward + backward + optimize\n",
      "        outputs = nettt(inputs)\n",
      "        loss = criterion(outputs, labels)\n",
      "        loss.backward()\n",
      "        optimizer.step()\n",
      "\n",
      "        # print statistics\n",
      "        running_loss += loss.item()\n",
      "        total_loss += running_loss\n",
      "        if i % 2000 == 1999:    # print every 2000 mini-batches\n",
      "            print('[%d, %5d] loss: %.3f' %\n",
      "                  (epoch + 1, i + 1, running_loss / 2000))\n",
      "            running_loss = 0.0\n",
      "    total_lossestt.append(total_loss)\n",
      "print('Finished Training')\n",
      "135/69:\n",
      "class Nettt(nn.Module):\n",
      "    def __init__(self):\n",
      "        super(Nettt, self).__init__()\n",
      "        self.conv1 = nn.Conv2d(3, 6, 5)\n",
      "        self.pool = nn.MaxPool2d(2, 2)\n",
      "        self.conv2 = nn.Conv2d(6, 24, 5)\n",
      "        self.conv3 = nn.Conv2d(24, 16, 5)\n",
      "        self.fc1 = nn.Linear(16 * 5 * 5, 120)\n",
      "        self.fc2 = nn.Linear(120, 84)\n",
      "        self.fc3 = nn.Linear(84, 10)\n",
      "        self.fc4 = nn.Linear(10,64)\n",
      "        self.fc5 = nn.Linear(64,10)\n",
      "\n",
      "    def forward(self, x):\n",
      "        x = self.pool(F.relu(self.conv1(x)))\n",
      "        x = self.pool(F.relu(self.conv2(x)))\n",
      "        x = self.pool(F.relu(self.conv3(x)))\n",
      "        x = x.view(-1, 16 * 5 * 5)\n",
      "        x = F.relu(self.fc1(x))\n",
      "        x = F.relu(self.fc2(x))\n",
      "        x = self.fc3(x)\n",
      "        x = self.fc4(x)\n",
      "        x = self.fc5(x)\n",
      "        return x\n",
      "135/70:\n",
      "xs = [i for i in range(35)]\n",
      "total_lossestt = []\n",
      "nettt = Nettt()\n",
      "criterion = nn.CrossEntropyLoss()\n",
      "optimizer = optim.SGD(nettt.parameters(), lr=0.0005, momentum=0.9)\n",
      "for epoch in range(30):  # loop over the dataset multiple times: 43-44\n",
      "    running_loss = 0.0\n",
      "    total_loss = 0\n",
      "    for i, data in enumerate(trainloader, 0):\n",
      "        # get the inputs; data is a list of [inputs, labels]\n",
      "        inputs, labels = data\n",
      "\n",
      "        # zero the parameter gradients\n",
      "        optimizer.zero_grad()\n",
      "\n",
      "        # forward + backward + optimize\n",
      "        outputs = nettt(inputs)\n",
      "        loss = criterion(outputs, labels)\n",
      "        loss.backward()\n",
      "        optimizer.step()\n",
      "\n",
      "        # print statistics\n",
      "        running_loss += loss.item()\n",
      "        total_loss += running_loss\n",
      "        if i % 2000 == 1999:    # print every 2000 mini-batches\n",
      "            print('[%d, %5d] loss: %.3f' %\n",
      "                  (epoch + 1, i + 1, running_loss / 2000))\n",
      "            running_loss = 0.0\n",
      "    total_lossestt.append(total_loss)\n",
      "print('Finished Training')\n",
      "135/71:\n",
      "class Nettt(nn.Module):\n",
      "    def __init__(self):\n",
      "        super(Nettt, self).__init__()\n",
      "        self.conv1 = nn.Conv2d(3, 6, 5)\n",
      "        self.pool = nn.MaxPool2d(2, 2)\n",
      "        self.conv2 = nn.Conv2d(6, 24, 5,padding=2)\n",
      "        self.conv3 = nn.Conv2d(24, 16, 5)\n",
      "        self.fc1 = nn.Linear(16 * 5 * 5, 120)\n",
      "        self.fc2 = nn.Linear(120, 84)\n",
      "        self.fc3 = nn.Linear(84, 10)\n",
      "        self.fc4 = nn.Linear(10,64)\n",
      "        self.fc5 = nn.Linear(64,10)\n",
      "\n",
      "    def forward(self, x):\n",
      "        x = self.pool(F.relu(self.conv1(x)))\n",
      "        x = self.pool(F.relu(self.conv2(x)))\n",
      "        x = self.pool(F.relu(self.conv3(x)))\n",
      "        x = x.view(-1, 16 * 5 * 5)\n",
      "        x = F.relu(self.fc1(x))\n",
      "        x = F.relu(self.fc2(x))\n",
      "        x = self.fc3(x)\n",
      "        x = self.fc4(x)\n",
      "        x = self.fc5(x)\n",
      "        return x\n",
      "xs = [i for i in range(35)]\n",
      "total_lossestt = []\n",
      "nettt = Nettt()\n",
      "criterion = nn.CrossEntropyLoss()\n",
      "optimizer = optim.SGD(nettt.parameters(), lr=0.0005, momentum=0.9)\n",
      "for epoch in range(30):  # loop over the dataset multiple times: 43-44\n",
      "    running_loss = 0.0\n",
      "    total_loss = 0\n",
      "    for i, data in enumerate(trainloader, 0):\n",
      "        # get the inputs; data is a list of [inputs, labels]\n",
      "        inputs, labels = data\n",
      "\n",
      "        # zero the parameter gradients\n",
      "        optimizer.zero_grad()\n",
      "\n",
      "        # forward + backward + optimize\n",
      "        outputs = nettt(inputs)\n",
      "        loss = criterion(outputs, labels)\n",
      "        loss.backward()\n",
      "        optimizer.step()\n",
      "\n",
      "        # print statistics\n",
      "        running_loss += loss.item()\n",
      "        total_loss += running_loss\n",
      "        if i % 2000 == 1999:    # print every 2000 mini-batches\n",
      "            print('[%d, %5d] loss: %.3f' %\n",
      "                  (epoch + 1, i + 1, running_loss / 2000))\n",
      "            running_loss = 0.0\n",
      "    total_lossestt.append(total_loss)\n",
      "print('Finished Training')\n",
      "135/72:\n",
      "class Nettt(nn.Module):\n",
      "    def __init__(self):\n",
      "        super(Nettt, self).__init__()\n",
      "        self.conv1 = nn.Conv2d(3, 6, 5)\n",
      "        self.pool = nn.MaxPool2d(2, 2)\n",
      "        self.conv2 = nn.Conv2d(6, 24, 5,padding=2)\n",
      "        self.conv3 = nn.Conv2d(24, 16, 5,padding=2)\n",
      "        self.fc1 = nn.Linear(16 * 5 * 5, 120)\n",
      "        self.fc2 = nn.Linear(120, 84)\n",
      "        self.fc3 = nn.Linear(84, 10)\n",
      "        self.fc4 = nn.Linear(10,64)\n",
      "        self.fc5 = nn.Linear(64,10)\n",
      "\n",
      "    def forward(self, x):\n",
      "        x = self.pool(F.relu(self.conv1(x)))\n",
      "        x = self.pool(F.relu(self.conv2(x)))\n",
      "        x = self.pool(F.relu(self.conv3(x)))\n",
      "        x = x.view(-1, 16 * 5 * 5)\n",
      "        x = F.relu(self.fc1(x))\n",
      "        x = F.relu(self.fc2(x))\n",
      "        x = self.fc3(x)\n",
      "        x = self.fc4(x)\n",
      "        x = self.fc5(x)\n",
      "        return x\n",
      "xs = [i for i in range(35)]\n",
      "total_lossestt = []\n",
      "nettt = Nettt()\n",
      "criterion = nn.CrossEntropyLoss()\n",
      "optimizer = optim.SGD(nettt.parameters(), lr=0.0005, momentum=0.9)\n",
      "for epoch in range(30):  # loop over the dataset multiple times: 43-44\n",
      "    running_loss = 0.0\n",
      "    total_loss = 0\n",
      "    for i, data in enumerate(trainloader, 0):\n",
      "        # get the inputs; data is a list of [inputs, labels]\n",
      "        inputs, labels = data\n",
      "\n",
      "        # zero the parameter gradients\n",
      "        optimizer.zero_grad()\n",
      "\n",
      "        # forward + backward + optimize\n",
      "        outputs = nettt(inputs)\n",
      "        loss = criterion(outputs, labels)\n",
      "        loss.backward()\n",
      "        optimizer.step()\n",
      "\n",
      "        # print statistics\n",
      "        running_loss += loss.item()\n",
      "        total_loss += running_loss\n",
      "        if i % 2000 == 1999:    # print every 2000 mini-batches\n",
      "            print('[%d, %5d] loss: %.3f' %\n",
      "                  (epoch + 1, i + 1, running_loss / 2000))\n",
      "            running_loss = 0.0\n",
      "    total_lossestt.append(total_loss)\n",
      "print('Finished Training')\n",
      "135/73:\n",
      "class Nettt(nn.Module):\n",
      "    def __init__(self):\n",
      "        super(Nettt, self).__init__()\n",
      "        self.conv1 = nn.Conv2d(3, 6, 5,padding=2)\n",
      "        self.pool = nn.MaxPool2d(2, 2)\n",
      "        self.conv2 = nn.Conv2d(6, 24, 5,padding=2)\n",
      "        self.conv3 = nn.Conv2d(24, 16, 5,padding=2)\n",
      "        self.fc1 = nn.Linear(16 * 5 * 5, 120)\n",
      "        self.fc2 = nn.Linear(120, 84)\n",
      "        self.fc3 = nn.Linear(84, 10)\n",
      "        self.fc4 = nn.Linear(10,64)\n",
      "        self.fc5 = nn.Linear(64,10)\n",
      "\n",
      "    def forward(self, x):\n",
      "        x = self.pool(F.relu(self.conv1(x)))\n",
      "        x = self.pool(F.relu(self.conv2(x)))\n",
      "        x = self.pool(F.relu(self.conv3(x)))\n",
      "        x = x.view(-1, 16 * 5 * 5)\n",
      "        x = F.relu(self.fc1(x))\n",
      "        x = F.relu(self.fc2(x))\n",
      "        x = self.fc3(x)\n",
      "        x = self.fc4(x)\n",
      "        x = self.fc5(x)\n",
      "        return x\n",
      "xs = [i for i in range(35)]\n",
      "total_lossestt = []\n",
      "nettt = Nettt()\n",
      "criterion = nn.CrossEntropyLoss()\n",
      "optimizer = optim.SGD(nettt.parameters(), lr=0.0005, momentum=0.9)\n",
      "for epoch in range(30):  # loop over the dataset multiple times: 43-44\n",
      "    running_loss = 0.0\n",
      "    total_loss = 0\n",
      "    for i, data in enumerate(trainloader, 0):\n",
      "        # get the inputs; data is a list of [inputs, labels]\n",
      "        inputs, labels = data\n",
      "\n",
      "        # zero the parameter gradients\n",
      "        optimizer.zero_grad()\n",
      "\n",
      "        # forward + backward + optimize\n",
      "        outputs = nettt(inputs)\n",
      "        loss = criterion(outputs, labels)\n",
      "        loss.backward()\n",
      "        optimizer.step()\n",
      "\n",
      "        # print statistics\n",
      "        running_loss += loss.item()\n",
      "        total_loss += running_loss\n",
      "        if i % 2000 == 1999:    # print every 2000 mini-batches\n",
      "            print('[%d, %5d] loss: %.3f' %\n",
      "                  (epoch + 1, i + 1, running_loss / 2000))\n",
      "            running_loss = 0.0\n",
      "    total_lossestt.append(total_loss)\n",
      "print('Finished Training')\n",
      "135/74:\n",
      "class Nettt(nn.Module):\n",
      "    def __init__(self):\n",
      "        super(Nettt, self).__init__()\n",
      "        self.conv1 = nn.Conv2d(3, 6, 5)\n",
      "        self.pool = nn.MaxPool2d(2, 2)\n",
      "        self.conv2 = nn.Conv2d(6, 24, 5,padding=2)\n",
      "        self.conv3 = nn.Conv2d(24, 16, 5,padding=1)\n",
      "        self.fc1 = nn.Linear(16 * 5 * 5, 120)\n",
      "        self.fc2 = nn.Linear(120, 84)\n",
      "        self.fc3 = nn.Linear(84, 10)\n",
      "        self.fc4 = nn.Linear(10,64)\n",
      "        self.fc5 = nn.Linear(64,10)\n",
      "\n",
      "    def forward(self, x):\n",
      "        x = self.pool(F.relu(self.conv1(x)))\n",
      "        x = self.pool(F.relu(self.conv2(x)))\n",
      "        x = self.pool(F.relu(self.conv3(x)))\n",
      "        x = x.view(-1, 16 * 5 * 5)\n",
      "        x = F.relu(self.fc1(x))\n",
      "        x = F.relu(self.fc2(x))\n",
      "        x = self.fc3(x)\n",
      "        x = self.fc4(x)\n",
      "        x = self.fc5(x)\n",
      "        return x\n",
      "xs = [i for i in range(35)]\n",
      "total_lossestt = []\n",
      "nettt = Nettt()\n",
      "criterion = nn.CrossEntropyLoss()\n",
      "optimizer = optim.SGD(nettt.parameters(), lr=0.0005, momentum=0.9)\n",
      "for epoch in range(30):  # loop over the dataset multiple times: 43-44\n",
      "    running_loss = 0.0\n",
      "    total_loss = 0\n",
      "    for i, data in enumerate(trainloader, 0):\n",
      "        # get the inputs; data is a list of [inputs, labels]\n",
      "        inputs, labels = data\n",
      "\n",
      "        # zero the parameter gradients\n",
      "        optimizer.zero_grad()\n",
      "\n",
      "        # forward + backward + optimize\n",
      "        outputs = nettt(inputs)\n",
      "        loss = criterion(outputs, labels)\n",
      "        loss.backward()\n",
      "        optimizer.step()\n",
      "\n",
      "        # print statistics\n",
      "        running_loss += loss.item()\n",
      "        total_loss += running_loss\n",
      "        if i % 2000 == 1999:    # print every 2000 mini-batches\n",
      "            print('[%d, %5d] loss: %.3f' %\n",
      "                  (epoch + 1, i + 1, running_loss / 2000))\n",
      "            running_loss = 0.0\n",
      "    total_lossestt.append(total_loss)\n",
      "print('Finished Training')\n",
      "135/75:\n",
      "class Nettt(nn.Module):\n",
      "    def __init__(self):\n",
      "        super(Nettt, self).__init__()\n",
      "        self.conv1 = nn.Conv2d(3, 6, 5)\n",
      "        self.pool = nn.MaxPool2d(2, 2)\n",
      "        self.conv2 = nn.Conv2d(6, 24, 5,padding=1)\n",
      "        self.conv3 = nn.Conv2d(24, 16, 5,padding=2)\n",
      "        self.fc1 = nn.Linear(16 * 5 * 5, 120)\n",
      "        self.fc2 = nn.Linear(120, 84)\n",
      "        self.fc3 = nn.Linear(84, 10)\n",
      "        self.fc4 = nn.Linear(10,64)\n",
      "        self.fc5 = nn.Linear(64,10)\n",
      "\n",
      "    def forward(self, x):\n",
      "        x = self.pool(F.relu(self.conv1(x)))\n",
      "        x = self.pool(F.relu(self.conv2(x)))\n",
      "        x = self.pool(F.relu(self.conv3(x)))\n",
      "        x = x.view(-1, 16 * 5 * 5)\n",
      "        x = F.relu(self.fc1(x))\n",
      "        x = F.relu(self.fc2(x))\n",
      "        x = self.fc3(x)\n",
      "        x = self.fc4(x)\n",
      "        x = self.fc5(x)\n",
      "        return x\n",
      "xs = [i for i in range(35)]\n",
      "total_lossestt = []\n",
      "nettt = Nettt()\n",
      "criterion = nn.CrossEntropyLoss()\n",
      "optimizer = optim.SGD(nettt.parameters(), lr=0.0005, momentum=0.9)\n",
      "for epoch in range(30):  # loop over the dataset multiple times: 43-44\n",
      "    running_loss = 0.0\n",
      "    total_loss = 0\n",
      "    for i, data in enumerate(trainloader, 0):\n",
      "        # get the inputs; data is a list of [inputs, labels]\n",
      "        inputs, labels = data\n",
      "\n",
      "        # zero the parameter gradients\n",
      "        optimizer.zero_grad()\n",
      "\n",
      "        # forward + backward + optimize\n",
      "        outputs = nettt(inputs)\n",
      "        loss = criterion(outputs, labels)\n",
      "        loss.backward()\n",
      "        optimizer.step()\n",
      "\n",
      "        # print statistics\n",
      "        running_loss += loss.item()\n",
      "        total_loss += running_loss\n",
      "        if i % 2000 == 1999:    # print every 2000 mini-batches\n",
      "            print('[%d, %5d] loss: %.3f' %\n",
      "                  (epoch + 1, i + 1, running_loss / 2000))\n",
      "            running_loss = 0.0\n",
      "    total_lossestt.append(total_loss)\n",
      "print('Finished Training')\n",
      "135/76:\n",
      "class Nettt(nn.Module):\n",
      "    def __init__(self):\n",
      "        super(Nettt, self).__init__()\n",
      "        self.conv1 = nn.Conv2d(3, 6, 5)\n",
      "        self.pool = nn.MaxPool2d(2, 2)\n",
      "        self.conv2 = nn.Conv2d(6, 24, 5,)\n",
      "        self.conv3 = nn.Conv2d(24, 16, 5,padding=2)\n",
      "        self.fc1 = nn.Linear(16 * 5 * 5, 120)\n",
      "        self.fc2 = nn.Linear(120, 84)\n",
      "        self.fc3 = nn.Linear(84, 10)\n",
      "        self.fc4 = nn.Linear(10,64)\n",
      "        self.fc5 = nn.Linear(64,10)\n",
      "\n",
      "    def forward(self, x):\n",
      "        x = self.pool(F.relu(self.conv1(x)))\n",
      "        x = self.pool(F.relu(self.conv2(x)))\n",
      "        x = self.pool(F.relu(self.conv3(x)))\n",
      "        x = x.view(-1, 16 * 5 * 5)\n",
      "        x = F.relu(self.fc1(x))\n",
      "        x = F.relu(self.fc2(x))\n",
      "        x = self.fc3(x)\n",
      "        x = self.fc4(x)\n",
      "        x = self.fc5(x)\n",
      "        return x\n",
      "xs = [i for i in range(35)]\n",
      "total_lossestt = []\n",
      "nettt = Nettt()\n",
      "criterion = nn.CrossEntropyLoss()\n",
      "optimizer = optim.SGD(nettt.parameters(), lr=0.0005, momentum=0.9)\n",
      "for epoch in range(30):  # loop over the dataset multiple times: 43-44\n",
      "    running_loss = 0.0\n",
      "    total_loss = 0\n",
      "    for i, data in enumerate(trainloader, 0):\n",
      "        # get the inputs; data is a list of [inputs, labels]\n",
      "        inputs, labels = data\n",
      "\n",
      "        # zero the parameter gradients\n",
      "        optimizer.zero_grad()\n",
      "\n",
      "        # forward + backward + optimize\n",
      "        outputs = nettt(inputs)\n",
      "        loss = criterion(outputs, labels)\n",
      "        loss.backward()\n",
      "        optimizer.step()\n",
      "\n",
      "        # print statistics\n",
      "        running_loss += loss.item()\n",
      "        total_loss += running_loss\n",
      "        if i % 2000 == 1999:    # print every 2000 mini-batches\n",
      "            print('[%d, %5d] loss: %.3f' %\n",
      "                  (epoch + 1, i + 1, running_loss / 2000))\n",
      "            running_loss = 0.0\n",
      "    total_lossestt.append(total_loss)\n",
      "print('Finished Training')\n",
      "135/77:\n",
      "class Nettt(nn.Module):\n",
      "    def __init__(self):\n",
      "        super(Nettt, self).__init__()\n",
      "        self.conv1 = nn.Conv2d(3, 6, 5)\n",
      "        self.pool = nn.MaxPool2d(2, 2)\n",
      "        self.conv2 = nn.Conv2d(6, 24, 5)\n",
      "        self.conv3 = nn.Conv2d(24, 16, 5,padding=2)\n",
      "        self.fc1 = nn.Linear(16 * 5 * 5, 120)\n",
      "        self.fc2 = nn.Linear(120, 84)\n",
      "        self.fc3 = nn.Linear(84, 10)\n",
      "        self.fc4 = nn.Linear(10,64)\n",
      "        self.fc5 = nn.Linear(64,10)\n",
      "\n",
      "    def forward(self, x):\n",
      "        x = self.pool(F.relu(self.conv1(x)))\n",
      "        x = self.pool(F.relu(self.conv2(x)))\n",
      "        x = self.pool(F.relu(self.conv3(x)))\n",
      "        x = x.view(-1, 16 * 5 * 5)\n",
      "        x = F.relu(self.fc1(x))\n",
      "        x = F.relu(self.fc2(x))\n",
      "        x = self.fc3(x)\n",
      "        x = self.fc4(x)\n",
      "        x = self.fc5(x)\n",
      "        return x\n",
      "xs = [i for i in range(35)]\n",
      "total_lossestt = []\n",
      "nettt = Nettt()\n",
      "criterion = nn.CrossEntropyLoss()\n",
      "optimizer = optim.SGD(nettt.parameters(), lr=0.0005, momentum=0.9)\n",
      "for epoch in range(30):  # loop over the dataset multiple times: 43-44\n",
      "    running_loss = 0.0\n",
      "    total_loss = 0\n",
      "    for i, data in enumerate(trainloader, 0):\n",
      "        # get the inputs; data is a list of [inputs, labels]\n",
      "        inputs, labels = data\n",
      "\n",
      "        # zero the parameter gradients\n",
      "        optimizer.zero_grad()\n",
      "\n",
      "        # forward + backward + optimize\n",
      "        outputs = nettt(inputs)\n",
      "        loss = criterion(outputs, labels)\n",
      "        loss.backward()\n",
      "        optimizer.step()\n",
      "\n",
      "        # print statistics\n",
      "        running_loss += loss.item()\n",
      "        total_loss += running_loss\n",
      "        if i % 2000 == 1999:    # print every 2000 mini-batches\n",
      "            print('[%d, %5d] loss: %.3f' %\n",
      "                  (epoch + 1, i + 1, running_loss / 2000))\n",
      "            running_loss = 0.0\n",
      "    total_lossestt.append(total_loss)\n",
      "print('Finished Training')\n",
      "135/78:\n",
      "class Nettt(nn.Module):\n",
      "    def __init__(self):\n",
      "        super(Nettt, self).__init__()\n",
      "        self.conv1 = nn.Conv2d(3, 6, 5)\n",
      "        self.pool = nn.MaxPool2d(2, 2)\n",
      "        self.conv2 = nn.Conv2d(6, 24, 5)\n",
      "        self.conv3 = nn.Conv2d(24, 16, 5,padding=3)\n",
      "        self.fc1 = nn.Linear(16 * 5 * 5, 120)\n",
      "        self.fc2 = nn.Linear(120, 84)\n",
      "        self.fc3 = nn.Linear(84, 10)\n",
      "        self.fc4 = nn.Linear(10,64)\n",
      "        self.fc5 = nn.Linear(64,10)\n",
      "\n",
      "    def forward(self, x):\n",
      "        x = self.pool(F.relu(self.conv1(x)))\n",
      "        x = self.pool(F.relu(self.conv2(x)))\n",
      "        x = self.pool(F.relu(self.conv3(x)))\n",
      "        x = x.view(-1, 16 * 5 * 5)\n",
      "        x = F.relu(self.fc1(x))\n",
      "        x = F.relu(self.fc2(x))\n",
      "        x = self.fc3(x)\n",
      "        x = self.fc4(x)\n",
      "        x = self.fc5(x)\n",
      "        return x\n",
      "xs = [i for i in range(35)]\n",
      "total_lossestt = []\n",
      "nettt = Nettt()\n",
      "criterion = nn.CrossEntropyLoss()\n",
      "optimizer = optim.SGD(nettt.parameters(), lr=0.0005, momentum=0.9)\n",
      "for epoch in range(30):  # loop over the dataset multiple times: 43-44\n",
      "    running_loss = 0.0\n",
      "    total_loss = 0\n",
      "    for i, data in enumerate(trainloader, 0):\n",
      "        # get the inputs; data is a list of [inputs, labels]\n",
      "        inputs, labels = data\n",
      "\n",
      "        # zero the parameter gradients\n",
      "        optimizer.zero_grad()\n",
      "\n",
      "        # forward + backward + optimize\n",
      "        outputs = nettt(inputs)\n",
      "        loss = criterion(outputs, labels)\n",
      "        loss.backward()\n",
      "        optimizer.step()\n",
      "\n",
      "        # print statistics\n",
      "        running_loss += loss.item()\n",
      "        total_loss += running_loss\n",
      "        if i % 2000 == 1999:    # print every 2000 mini-batches\n",
      "            print('[%d, %5d] loss: %.3f' %\n",
      "                  (epoch + 1, i + 1, running_loss / 2000))\n",
      "            running_loss = 0.0\n",
      "    total_lossestt.append(total_loss)\n",
      "print('Finished Training')\n",
      "135/79:\n",
      "class Nettt(nn.Module):\n",
      "    def __init__(self):\n",
      "        super(Nettt, self).__init__()\n",
      "        self.conv1 = nn.Conv2d(3, 6, 5)\n",
      "        self.pool = nn.MaxPool2d(2, 2)\n",
      "        self.conv2 = nn.Conv2d(6, 24, 5)\n",
      "        self.conv3 = nn.Conv2d(24, 16, 5,padding=2)\n",
      "        self.fc1 = nn.Linear(16 * 4 * 4, 120)\n",
      "        self.fc2 = nn.Linear(120, 84)\n",
      "        self.fc3 = nn.Linear(84, 10)\n",
      "        self.fc4 = nn.Linear(10,64)\n",
      "        self.fc5 = nn.Linear(64,10)\n",
      "\n",
      "    def forward(self, x):\n",
      "        x = self.pool(F.relu(self.conv1(x)))\n",
      "        x = self.pool(F.relu(self.conv2(x)))\n",
      "        x = self.pool(F.relu(self.conv3(x)))\n",
      "        x = x.view(-1, 16 * 5 * 5)\n",
      "        x = F.relu(self.fc1(x))\n",
      "        x = F.relu(self.fc2(x))\n",
      "        x = self.fc3(x)\n",
      "        x = self.fc4(x)\n",
      "        x = self.fc5(x)\n",
      "        return x\n",
      "xs = [i for i in range(35)]\n",
      "total_lossestt = []\n",
      "nettt = Nettt()\n",
      "criterion = nn.CrossEntropyLoss()\n",
      "optimizer = optim.SGD(nettt.parameters(), lr=0.0005, momentum=0.9)\n",
      "for epoch in range(30):  # loop over the dataset multiple times: 43-44\n",
      "    running_loss = 0.0\n",
      "    total_loss = 0\n",
      "    for i, data in enumerate(trainloader, 0):\n",
      "        # get the inputs; data is a list of [inputs, labels]\n",
      "        inputs, labels = data\n",
      "\n",
      "        # zero the parameter gradients\n",
      "        optimizer.zero_grad()\n",
      "\n",
      "        # forward + backward + optimize\n",
      "        outputs = nettt(inputs)\n",
      "        loss = criterion(outputs, labels)\n",
      "        loss.backward()\n",
      "        optimizer.step()\n",
      "\n",
      "        # print statistics\n",
      "        running_loss += loss.item()\n",
      "        total_loss += running_loss\n",
      "        if i % 2000 == 1999:    # print every 2000 mini-batches\n",
      "            print('[%d, %5d] loss: %.3f' %\n",
      "                  (epoch + 1, i + 1, running_loss / 2000))\n",
      "            running_loss = 0.0\n",
      "    total_lossestt.append(total_loss)\n",
      "print('Finished Training')\n",
      "135/80:\n",
      "class Nettt(nn.Module):\n",
      "    def __init__(self):\n",
      "        super(Nettt, self).__init__()\n",
      "        self.conv1 = nn.Conv2d(3, 6, 5)\n",
      "        self.pool = nn.MaxPool2d(2, 2)\n",
      "        self.conv2 = nn.Conv2d(6, 24, 5)\n",
      "        self.conv3 = nn.Conv2d(24, 16, 5,padding=2)\n",
      "        self.fc1 = nn.Linear(16 * 4 * 4, 120)\n",
      "        self.fc2 = nn.Linear(120, 84)\n",
      "        self.fc3 = nn.Linear(84, 10)\n",
      "        self.fc4 = nn.Linear(10,64)\n",
      "        self.fc5 = nn.Linear(64,10)\n",
      "\n",
      "    def forward(self, x):\n",
      "        x = self.pool(F.relu(self.conv1(x)))\n",
      "        x = self.pool(F.relu(self.conv2(x)))\n",
      "        x = self.pool(F.relu(self.conv3(x)))\n",
      "        x = x.view(-1, 16 * 4 * 4)\n",
      "        x = F.relu(self.fc1(x))\n",
      "        x = F.relu(self.fc2(x))\n",
      "        x = self.fc3(x)\n",
      "        x = self.fc4(x)\n",
      "        x = self.fc5(x)\n",
      "        return x\n",
      "xs = [i for i in range(35)]\n",
      "total_lossestt = []\n",
      "nettt = Nettt()\n",
      "criterion = nn.CrossEntropyLoss()\n",
      "optimizer = optim.SGD(nettt.parameters(), lr=0.0005, momentum=0.9)\n",
      "for epoch in range(30):  # loop over the dataset multiple times: 43-44\n",
      "    running_loss = 0.0\n",
      "    total_loss = 0\n",
      "    for i, data in enumerate(trainloader, 0):\n",
      "        # get the inputs; data is a list of [inputs, labels]\n",
      "        inputs, labels = data\n",
      "\n",
      "        # zero the parameter gradients\n",
      "        optimizer.zero_grad()\n",
      "\n",
      "        # forward + backward + optimize\n",
      "        outputs = nettt(inputs)\n",
      "        loss = criterion(outputs, labels)\n",
      "        loss.backward()\n",
      "        optimizer.step()\n",
      "\n",
      "        # print statistics\n",
      "        running_loss += loss.item()\n",
      "        total_loss += running_loss\n",
      "        if i % 2000 == 1999:    # print every 2000 mini-batches\n",
      "            print('[%d, %5d] loss: %.3f' %\n",
      "                  (epoch + 1, i + 1, running_loss / 2000))\n",
      "            running_loss = 0.0\n",
      "    total_lossestt.append(total_loss)\n",
      "print('Finished Training')\n",
      "135/81:\n",
      "class Nettt(nn.Module):\n",
      "    def __init__(self):\n",
      "        super(Nettt, self).__init__()\n",
      "        self.conv1 = nn.Conv2d(3, 6, 5)\n",
      "        self.pool = nn.MaxPool2d(2, 2)\n",
      "        self.conv2 = nn.Conv2d(6, 24, 5)\n",
      "        self.conv3 = nn.Conv2d(24, 16, 5,padding=2)\n",
      "        self.fc1 = nn.Linear(16 * 5 * 5, 120)\n",
      "        self.fc2 = nn.Linear(120, 84)\n",
      "        self.fc3 = nn.Linear(84, 10)\n",
      "        self.fc4 = nn.Linear(10,64)\n",
      "        self.fc5 = nn.Linear(64,10)\n",
      "\n",
      "    def forward(self, x):\n",
      "        print(x)\n",
      "        x = self.pool(F.relu(self.conv1(x)))\n",
      "        print(x)\n",
      "        x = self.pool(F.relu(self.conv2(x)))\n",
      "        x = self.pool(F.relu(self.conv3(x)))\n",
      "        x = x.view(-1, 16 * 5 * 5)\n",
      "        x = F.relu(self.fc1(x))\n",
      "        x = F.relu(self.fc2(x))\n",
      "        x = self.fc3(x)\n",
      "        x = self.fc4(x)\n",
      "        x = self.fc5(x)\n",
      "        return x\n",
      "xs = [i for i in range(35)]\n",
      "total_lossestt = []\n",
      "nettt = Nettt()\n",
      "criterion = nn.CrossEntropyLoss()\n",
      "optimizer = optim.SGD(nettt.parameters(), lr=0.0005, momentum=0.9)\n",
      "for epoch in range(30):  # loop over the dataset multiple times: 43-44\n",
      "    running_loss = 0.0\n",
      "    total_loss = 0\n",
      "    for i, data in enumerate(trainloader, 0):\n",
      "        # get the inputs; data is a list of [inputs, labels]\n",
      "        inputs, labels = data\n",
      "\n",
      "        # zero the parameter gradients\n",
      "        optimizer.zero_grad()\n",
      "\n",
      "        # forward + backward + optimize\n",
      "        outputs = nettt(inputs)\n",
      "        loss = criterion(outputs, labels)\n",
      "        loss.backward()\n",
      "        optimizer.step()\n",
      "\n",
      "        # print statistics\n",
      "        running_loss += loss.item()\n",
      "        total_loss += running_loss\n",
      "        if i % 2000 == 1999:    # print every 2000 mini-batches\n",
      "            print('[%d, %5d] loss: %.3f' %\n",
      "                  (epoch + 1, i + 1, running_loss / 2000))\n",
      "            running_loss = 0.0\n",
      "    total_lossestt.append(total_loss)\n",
      "print('Finished Training')\n",
      "135/82:\n",
      "class Nettt(nn.Module):\n",
      "    def __init__(self):\n",
      "        super(Nettt, self).__init__()\n",
      "        self.conv1 = nn.Conv2d(3, 6, 5)\n",
      "        self.pool = nn.MaxPool2d(2, 2)\n",
      "        self.conv2 = nn.Conv2d(6, 24, 5)\n",
      "        self.conv3 = nn.Conv2d(24, 16, 5,padding=2)\n",
      "        self.fc1 = nn.Linear(16 * 5 * 5, 120)\n",
      "        self.fc2 = nn.Linear(120, 84)\n",
      "        self.fc3 = nn.Linear(84, 10)\n",
      "        self.fc4 = nn.Linear(10,64)\n",
      "        self.fc5 = nn.Linear(64,10)\n",
      "\n",
      "    def forward(self, x):\n",
      "        print(x.shape)\n",
      "        x = self.pool(F.relu(self.conv1(x)))\n",
      "        print(x.shape)\n",
      "        x = self.pool(F.relu(self.conv2(x)))\n",
      "        x = self.pool(F.relu(self.conv3(x)))\n",
      "        x = x.view(-1, 16 * 5 * 5)\n",
      "        x = F.relu(self.fc1(x))\n",
      "        x = F.relu(self.fc2(x))\n",
      "        x = self.fc3(x)\n",
      "        x = self.fc4(x)\n",
      "        x = self.fc5(x)\n",
      "        return x\n",
      "xs = [i for i in range(35)]\n",
      "total_lossestt = []\n",
      "nettt = Nettt()\n",
      "criterion = nn.CrossEntropyLoss()\n",
      "optimizer = optim.SGD(nettt.parameters(), lr=0.0005, momentum=0.9)\n",
      "for epoch in range(30):  # loop over the dataset multiple times: 43-44\n",
      "    running_loss = 0.0\n",
      "    total_loss = 0\n",
      "    for i, data in enumerate(trainloader, 0):\n",
      "        # get the inputs; data is a list of [inputs, labels]\n",
      "        inputs, labels = data\n",
      "\n",
      "        # zero the parameter gradients\n",
      "        optimizer.zero_grad()\n",
      "\n",
      "        # forward + backward + optimize\n",
      "        outputs = nettt(inputs)\n",
      "        loss = criterion(outputs, labels)\n",
      "        loss.backward()\n",
      "        optimizer.step()\n",
      "\n",
      "        # print statistics\n",
      "        running_loss += loss.item()\n",
      "        total_loss += running_loss\n",
      "        if i % 2000 == 1999:    # print every 2000 mini-batches\n",
      "            print('[%d, %5d] loss: %.3f' %\n",
      "                  (epoch + 1, i + 1, running_loss / 2000))\n",
      "            running_loss = 0.0\n",
      "    total_lossestt.append(total_loss)\n",
      "print('Finished Training')\n",
      "135/83:\n",
      "class Nettt(nn.Module):\n",
      "    def __init__(self):\n",
      "        super(Nettt, self).__init__()\n",
      "        self.conv1 = nn.Conv2d(3, 6, 5)\n",
      "        self.pool = nn.MaxPool2d(2, 2)\n",
      "        self.conv2 = nn.Conv2d(6, 24, 5)\n",
      "        self.conv3 = nn.Conv2d(24, 16, 5,padding=2)\n",
      "        self.fc1 = nn.Linear(16 * 5 * 5, 120)\n",
      "        self.fc2 = nn.Linear(120, 84)\n",
      "        self.fc3 = nn.Linear(84, 10)\n",
      "        self.fc4 = nn.Linear(10,64)\n",
      "        self.fc5 = nn.Linear(64,10)\n",
      "\n",
      "    def forward(self, x):\n",
      "        print(x.shape)\n",
      "        x = self.pool(F.relu(self.conv1(x)))\n",
      "        print(x.shape)\n",
      "        x = self.pool(F.relu(self.conv2(x)))\n",
      "        print(x.shape)\n",
      "        x = self.pool(F.relu(self.conv3(x)))\n",
      "        print(x.shape)\n",
      "        x = x.view(-1, 16 * 5 * 5)\n",
      "        print(x.shape)\n",
      "        x = F.relu(self.fc1(x))\n",
      "        x = F.relu(self.fc2(x))\n",
      "        x = self.fc3(x)\n",
      "        x = self.fc4(x)\n",
      "        x = self.fc5(x)\n",
      "        return x\n",
      "xs = [i for i in range(35)]\n",
      "total_lossestt = []\n",
      "nettt = Nettt()\n",
      "criterion = nn.CrossEntropyLoss()\n",
      "optimizer = optim.SGD(nettt.parameters(), lr=0.0005, momentum=0.9)\n",
      "for epoch in range(30):  # loop over the dataset multiple times: 43-44\n",
      "    running_loss = 0.0\n",
      "    total_loss = 0\n",
      "    for i, data in enumerate(trainloader, 0):\n",
      "        # get the inputs; data is a list of [inputs, labels]\n",
      "        inputs, labels = data\n",
      "\n",
      "        # zero the parameter gradients\n",
      "        optimizer.zero_grad()\n",
      "\n",
      "        # forward + backward + optimize\n",
      "        outputs = nettt(inputs)\n",
      "        loss = criterion(outputs, labels)\n",
      "        loss.backward()\n",
      "        optimizer.step()\n",
      "\n",
      "        # print statistics\n",
      "        running_loss += loss.item()\n",
      "        total_loss += running_loss\n",
      "        if i % 2000 == 1999:    # print every 2000 mini-batches\n",
      "            print('[%d, %5d] loss: %.3f' %\n",
      "                  (epoch + 1, i + 1, running_loss / 2000))\n",
      "            running_loss = 0.0\n",
      "    total_lossestt.append(total_loss)\n",
      "print('Finished Training')\n",
      "135/84:\n",
      "class Nettt(nn.Module):\n",
      "    def __init__(self):\n",
      "        super(Nettt, self).__init__()\n",
      "        self.conv1 = nn.Conv2d(3, 6, 5)\n",
      "        self.pool = nn.MaxPool2d(2, 2)\n",
      "        self.conv2 = nn.Conv2d(6, 16, 5)\n",
      "#         self.conv3 = nn.Conv2d(24, 16, 5,padding=2)\n",
      "        self.fc1 = nn.Linear(16 * 5 * 5, 120)\n",
      "        self.fc2 = nn.Linear(120, 84)\n",
      "        self.fc3 = nn.Linear(84, 10)\n",
      "        self.fc4 = nn.Linear(10,64)\n",
      "        self.fc5 = nn.Linear(64,10)\n",
      "\n",
      "    def forward(self, x):\n",
      "        print(x.shape)\n",
      "        x = self.pool(F.relu(self.conv1(x)))\n",
      "        print(x.shape)\n",
      "        x = self.pool(F.relu(self.conv2(x)))\n",
      "        print(x.shape)\n",
      "#         x = self.pool(F.relu(self.conv3(x)))\n",
      "#         print(x.shape)\n",
      "        x = x.view(-1, 16 * 5 * 5)\n",
      "        print(x.shape)\n",
      "        x = F.relu(self.fc1(x))\n",
      "        x = F.relu(self.fc2(x))\n",
      "        x = self.fc3(x)\n",
      "        x = self.fc4(x)\n",
      "        x = self.fc5(x)\n",
      "        return x\n",
      "xs = [i for i in range(35)]\n",
      "total_lossestt = []\n",
      "nettt = Nettt()\n",
      "criterion = nn.CrossEntropyLoss()\n",
      "optimizer = optim.SGD(nettt.parameters(), lr=0.0005, momentum=0.9)\n",
      "for epoch in range(30):  # loop over the dataset multiple times: 43-44\n",
      "    running_loss = 0.0\n",
      "    total_loss = 0\n",
      "    for i, data in enumerate(trainloader, 0):\n",
      "        # get the inputs; data is a list of [inputs, labels]\n",
      "        inputs, labels = data\n",
      "\n",
      "        # zero the parameter gradients\n",
      "        optimizer.zero_grad()\n",
      "\n",
      "        # forward + backward + optimize\n",
      "        outputs = nettt(inputs)\n",
      "        loss = criterion(outputs, labels)\n",
      "        loss.backward()\n",
      "        optimizer.step()\n",
      "\n",
      "        # print statistics\n",
      "        running_loss += loss.item()\n",
      "        total_loss += running_loss\n",
      "        if i % 2000 == 1999:    # print every 2000 mini-batches\n",
      "            print('[%d, %5d] loss: %.3f' %\n",
      "                  (epoch + 1, i + 1, running_loss / 2000))\n",
      "            running_loss = 0.0\n",
      "    total_lossestt.append(total_loss)\n",
      "print('Finished Training')\n",
      "135/85:\n",
      "class Nettt(nn.Module):\n",
      "    def __init__(self):\n",
      "        super(Nettt, self).__init__()\n",
      "        self.conv1 = nn.Conv2d(3, 6, 5)\n",
      "        self.pool = nn.MaxPool2d(2, 2)\n",
      "        self.conv2 = nn.Conv2d(6, 16, 5, padding=2)\n",
      "#         self.conv3 = nn.Conv2d(24, 16, 5,padding=2)\n",
      "        self.fc1 = nn.Linear(16 * 5 * 5, 120)\n",
      "        self.fc2 = nn.Linear(120, 84)\n",
      "        self.fc3 = nn.Linear(84, 10)\n",
      "        self.fc4 = nn.Linear(10,64)\n",
      "        self.fc5 = nn.Linear(64,10)\n",
      "\n",
      "    def forward(self, x):\n",
      "        print(x.shape)\n",
      "        x = self.pool(F.relu(self.conv1(x)))\n",
      "        print(x.shape)\n",
      "        x = self.pool(F.relu(self.conv2(x)))\n",
      "        print(x.shape)\n",
      "#         x = self.pool(F.relu(self.conv3(x)))\n",
      "#         print(x.shape)\n",
      "        x = x.view(-1, 16 * 5 * 5)\n",
      "        print(x.shape)\n",
      "        x = F.relu(self.fc1(x))\n",
      "        x = F.relu(self.fc2(x))\n",
      "        x = self.fc3(x)\n",
      "        x = self.fc4(x)\n",
      "        x = self.fc5(x)\n",
      "        return x\n",
      "xs = [i for i in range(35)]\n",
      "total_lossestt = []\n",
      "nettt = Nettt()\n",
      "criterion = nn.CrossEntropyLoss()\n",
      "optimizer = optim.SGD(nettt.parameters(), lr=0.0005, momentum=0.9)\n",
      "for epoch in range(30):  # loop over the dataset multiple times: 43-44\n",
      "    running_loss = 0.0\n",
      "    total_loss = 0\n",
      "    for i, data in enumerate(trainloader, 0):\n",
      "        # get the inputs; data is a list of [inputs, labels]\n",
      "        inputs, labels = data\n",
      "\n",
      "        # zero the parameter gradients\n",
      "        optimizer.zero_grad()\n",
      "\n",
      "        # forward + backward + optimize\n",
      "        outputs = nettt(inputs)\n",
      "        loss = criterion(outputs, labels)\n",
      "        loss.backward()\n",
      "        optimizer.step()\n",
      "\n",
      "        # print statistics\n",
      "        running_loss += loss.item()\n",
      "        total_loss += running_loss\n",
      "        if i % 2000 == 1999:    # print every 2000 mini-batches\n",
      "            print('[%d, %5d] loss: %.3f' %\n",
      "                  (epoch + 1, i + 1, running_loss / 2000))\n",
      "            running_loss = 0.0\n",
      "    total_lossestt.append(total_loss)\n",
      "print('Finished Training')\n",
      "135/86:\n",
      "class Nettt(nn.Module):\n",
      "    def __init__(self):\n",
      "        super(Nettt, self).__init__()\n",
      "        self.conv1 = nn.Conv2d(3, 6, 5)\n",
      "        self.pool = nn.MaxPool2d(2, 2)\n",
      "        self.conv2 = nn.Conv2d(6, 16, 5, padding=9)\n",
      "#         self.conv3 = nn.Conv2d(24, 16, 5,padding=2)\n",
      "        self.fc1 = nn.Linear(16 * 5 * 5, 120)\n",
      "        self.fc2 = nn.Linear(120, 84)\n",
      "        self.fc3 = nn.Linear(84, 10)\n",
      "        self.fc4 = nn.Linear(10,64)\n",
      "        self.fc5 = nn.Linear(64,10)\n",
      "\n",
      "    def forward(self, x):\n",
      "        print(x.shape)\n",
      "        x = self.pool(F.relu(self.conv1(x)))\n",
      "        print(x.shape)\n",
      "        x = self.pool(F.relu(self.conv2(x)))\n",
      "        print(x.shape)\n",
      "#         x = self.pool(F.relu(self.conv3(x)))\n",
      "#         print(x.shape)\n",
      "        x = x.view(-1, 16 * 5 * 5)\n",
      "        print(x.shape)\n",
      "        x = F.relu(self.fc1(x))\n",
      "        x = F.relu(self.fc2(x))\n",
      "        x = self.fc3(x)\n",
      "        x = self.fc4(x)\n",
      "        x = self.fc5(x)\n",
      "        return x\n",
      "xs = [i for i in range(35)]\n",
      "total_lossestt = []\n",
      "nettt = Nettt()\n",
      "criterion = nn.CrossEntropyLoss()\n",
      "optimizer = optim.SGD(nettt.parameters(), lr=0.0005, momentum=0.9)\n",
      "for epoch in range(30):  # loop over the dataset multiple times: 43-44\n",
      "    running_loss = 0.0\n",
      "    total_loss = 0\n",
      "    for i, data in enumerate(trainloader, 0):\n",
      "        # get the inputs; data is a list of [inputs, labels]\n",
      "        inputs, labels = data\n",
      "\n",
      "        # zero the parameter gradients\n",
      "        optimizer.zero_grad()\n",
      "\n",
      "        # forward + backward + optimize\n",
      "        outputs = nettt(inputs)\n",
      "        loss = criterion(outputs, labels)\n",
      "        loss.backward()\n",
      "        optimizer.step()\n",
      "\n",
      "        # print statistics\n",
      "        running_loss += loss.item()\n",
      "        total_loss += running_loss\n",
      "        if i % 2000 == 1999:    # print every 2000 mini-batches\n",
      "            print('[%d, %5d] loss: %.3f' %\n",
      "                  (epoch + 1, i + 1, running_loss / 2000))\n",
      "            running_loss = 0.0\n",
      "    total_lossestt.append(total_loss)\n",
      "print('Finished Training')\n",
      "135/87:\n",
      "class Nettt(nn.Module):\n",
      "    def __init__(self):\n",
      "        super(Nettt, self).__init__()\n",
      "        self.conv1 = nn.Conv2d(3, 6, 5)\n",
      "        self.pool = nn.MaxPool2d(2, 2)\n",
      "        self.conv2 = nn.Conv2d(6, 16, 5, padding=9)\n",
      "        self.conv3 = nn.Conv2d(16, 24, 5,padding=2)\n",
      "        self.fc1 = nn.Linear(16 * 5 * 5, 120)\n",
      "        self.fc2 = nn.Linear(120, 84)\n",
      "        self.fc3 = nn.Linear(84, 10)\n",
      "        self.fc4 = nn.Linear(10,64)\n",
      "        self.fc5 = nn.Linear(64,10)\n",
      "\n",
      "    def forward(self, x):\n",
      "        print(x.shape)\n",
      "        x = self.pool(F.relu(self.conv1(x)))\n",
      "        print(x.shape)\n",
      "        x = self.pool(F.relu(self.conv2(x)))\n",
      "        print(x.shape)\n",
      "#         x = self.pool(F.relu(self.conv3(x)))\n",
      "#         print(x.shape)\n",
      "        x = x.view(-1, 16 * 5 * 5)\n",
      "        print(x.shape)\n",
      "        x = F.relu(self.fc1(x))\n",
      "        x = F.relu(self.fc2(x))\n",
      "        x = self.fc3(x)\n",
      "        x = self.fc4(x)\n",
      "        x = self.fc5(x)\n",
      "        return x\n",
      "xs = [i for i in range(35)]\n",
      "total_lossestt = []\n",
      "nettt = Nettt()\n",
      "criterion = nn.CrossEntropyLoss()\n",
      "optimizer = optim.SGD(nettt.parameters(), lr=0.0005, momentum=0.9)\n",
      "for epoch in range(30):  # loop over the dataset multiple times: 43-44\n",
      "    running_loss = 0.0\n",
      "    total_loss = 0\n",
      "    for i, data in enumerate(trainloader, 0):\n",
      "        # get the inputs; data is a list of [inputs, labels]\n",
      "        inputs, labels = data\n",
      "\n",
      "        # zero the parameter gradients\n",
      "        optimizer.zero_grad()\n",
      "\n",
      "        # forward + backward + optimize\n",
      "        outputs = nettt(inputs)\n",
      "        loss = criterion(outputs, labels)\n",
      "        loss.backward()\n",
      "        optimizer.step()\n",
      "\n",
      "        # print statistics\n",
      "        running_loss += loss.item()\n",
      "        total_loss += running_loss\n",
      "        if i % 2000 == 1999:    # print every 2000 mini-batches\n",
      "            print('[%d, %5d] loss: %.3f' %\n",
      "                  (epoch + 1, i + 1, running_loss / 2000))\n",
      "            running_loss = 0.0\n",
      "    total_lossestt.append(total_loss)\n",
      "print('Finished Training')\n",
      "135/88:\n",
      "class Nettt(nn.Module):\n",
      "    def __init__(self):\n",
      "        super(Nettt, self).__init__()\n",
      "        self.conv1 = nn.Conv2d(3, 6, 5)\n",
      "        self.pool = nn.MaxPool2d(2, 2)\n",
      "        self.conv2 = nn.Conv2d(6, 16, 5, padding=9)\n",
      "        self.conv3 = nn.Conv2d(16, 24, 5)\n",
      "        self.fc1 = nn.Linear(16 * 5 * 5, 120)\n",
      "        self.fc2 = nn.Linear(120, 84)\n",
      "        self.fc3 = nn.Linear(84, 10)\n",
      "        self.fc4 = nn.Linear(10,64)\n",
      "        self.fc5 = nn.Linear(64,10)\n",
      "\n",
      "    def forward(self, x):\n",
      "        print(x.shape)\n",
      "        x = self.pool(F.relu(self.conv1(x)))\n",
      "        print(x.shape)\n",
      "        x = self.pool(F.relu(self.conv2(x)))\n",
      "        print(x.shape)\n",
      "        x = self.pool(F.relu(self.conv3(x)))\n",
      "        print(x.shape)\n",
      "        x = x.view(-1, 16 * 5 * 5)\n",
      "        print(x.shape)\n",
      "        x = F.relu(self.fc1(x))\n",
      "        x = F.relu(self.fc2(x))\n",
      "        x = self.fc3(x)\n",
      "        x = self.fc4(x)\n",
      "        x = self.fc5(x)\n",
      "        return x\n",
      "xs = [i for i in range(35)]\n",
      "total_lossestt = []\n",
      "nettt = Nettt()\n",
      "criterion = nn.CrossEntropyLoss()\n",
      "optimizer = optim.SGD(nettt.parameters(), lr=0.0005, momentum=0.9)\n",
      "for epoch in range(30):  # loop over the dataset multiple times: 43-44\n",
      "    running_loss = 0.0\n",
      "    total_loss = 0\n",
      "    for i, data in enumerate(trainloader, 0):\n",
      "        # get the inputs; data is a list of [inputs, labels]\n",
      "        inputs, labels = data\n",
      "\n",
      "        # zero the parameter gradients\n",
      "        optimizer.zero_grad()\n",
      "\n",
      "        # forward + backward + optimize\n",
      "        outputs = nettt(inputs)\n",
      "        loss = criterion(outputs, labels)\n",
      "        loss.backward()\n",
      "        optimizer.step()\n",
      "\n",
      "        # print statistics\n",
      "        running_loss += loss.item()\n",
      "        total_loss += running_loss\n",
      "        if i % 2000 == 1999:    # print every 2000 mini-batches\n",
      "            print('[%d, %5d] loss: %.3f' %\n",
      "                  (epoch + 1, i + 1, running_loss / 2000))\n",
      "            running_loss = 0.0\n",
      "    total_lossestt.append(total_loss)\n",
      "print('Finished Training')\n",
      "135/89:\n",
      "class Nettt(nn.Module):\n",
      "    def __init__(self):\n",
      "        super(Nettt, self).__init__()\n",
      "        self.conv1 = nn.Conv2d(3, 6, 5)\n",
      "        self.pool = nn.MaxPool2d(2, 2)\n",
      "        self.conv2 = nn.Conv2d(6, 16, 5, padding=9)\n",
      "        self.conv3 = nn.Conv2d(16, 24, 5)\n",
      "        self.fc1 = nn.Linear(24 * 5 * 5, 120)\n",
      "        self.fc2 = nn.Linear(120, 84)\n",
      "        self.fc3 = nn.Linear(84, 10)\n",
      "        self.fc4 = nn.Linear(10,64)\n",
      "        self.fc5 = nn.Linear(64,10)\n",
      "\n",
      "    def forward(self, x):\n",
      "        print(x.shape)\n",
      "        x = self.pool(F.relu(self.conv1(x)))\n",
      "        print(x.shape)\n",
      "        x = self.pool(F.relu(self.conv2(x)))\n",
      "        print(x.shape)\n",
      "        x = self.pool(F.relu(self.conv3(x)))\n",
      "        print(x.shape)\n",
      "        x = x.view(-1, 16 * 5 * 5)\n",
      "        print(x.shape)\n",
      "        x = F.relu(self.fc1(x))\n",
      "        x = F.relu(self.fc2(x))\n",
      "        x = self.fc3(x)\n",
      "        x = self.fc4(x)\n",
      "        x = self.fc5(x)\n",
      "        return x\n",
      "xs = [i for i in range(35)]\n",
      "total_lossestt = []\n",
      "nettt = Nettt()\n",
      "criterion = nn.CrossEntropyLoss()\n",
      "optimizer = optim.SGD(nettt.parameters(), lr=0.0005, momentum=0.9)\n",
      "for epoch in range(30):  # loop over the dataset multiple times: 43-44\n",
      "    running_loss = 0.0\n",
      "    total_loss = 0\n",
      "    for i, data in enumerate(trainloader, 0):\n",
      "        # get the inputs; data is a list of [inputs, labels]\n",
      "        inputs, labels = data\n",
      "\n",
      "        # zero the parameter gradients\n",
      "        optimizer.zero_grad()\n",
      "\n",
      "        # forward + backward + optimize\n",
      "        outputs = nettt(inputs)\n",
      "        loss = criterion(outputs, labels)\n",
      "        loss.backward()\n",
      "        optimizer.step()\n",
      "\n",
      "        # print statistics\n",
      "        running_loss += loss.item()\n",
      "        total_loss += running_loss\n",
      "        if i % 2000 == 1999:    # print every 2000 mini-batches\n",
      "            print('[%d, %5d] loss: %.3f' %\n",
      "                  (epoch + 1, i + 1, running_loss / 2000))\n",
      "            running_loss = 0.0\n",
      "    total_lossestt.append(total_loss)\n",
      "print('Finished Training')\n",
      "135/90:\n",
      "class Nettt(nn.Module):\n",
      "    def __init__(self):\n",
      "        super(Nettt, self).__init__()\n",
      "        self.conv1 = nn.Conv2d(3, 6, 5)\n",
      "        self.pool = nn.MaxPool2d(2, 2)\n",
      "        self.conv2 = nn.Conv2d(6, 16, 5, padding=9)\n",
      "        self.conv3 = nn.Conv2d(16, 16, 5)\n",
      "        self.fc1 = nn.Linear(16 * 5 * 5, 120)\n",
      "        self.fc2 = nn.Linear(120, 84)\n",
      "        self.fc3 = nn.Linear(84, 10)\n",
      "        self.fc4 = nn.Linear(10,64)\n",
      "        self.fc5 = nn.Linear(64,10)\n",
      "\n",
      "    def forward(self, x):\n",
      "        print(x.shape)\n",
      "        x = self.pool(F.relu(self.conv1(x)))\n",
      "        print(x.shape)\n",
      "        x = self.pool(F.relu(self.conv2(x)))\n",
      "        print(x.shape)\n",
      "        x = self.pool(F.relu(self.conv3(x)))\n",
      "        print(x.shape)\n",
      "        x = x.view(-1, 16 * 5 * 5)\n",
      "        print(x.shape)\n",
      "        x = F.relu(self.fc1(x))\n",
      "        x = F.relu(self.fc2(x))\n",
      "        x = self.fc3(x)\n",
      "        x = self.fc4(x)\n",
      "        x = self.fc5(x)\n",
      "        return x\n",
      "xs = [i for i in range(35)]\n",
      "total_lossestt = []\n",
      "nettt = Nettt()\n",
      "criterion = nn.CrossEntropyLoss()\n",
      "optimizer = optim.SGD(nettt.parameters(), lr=0.0005, momentum=0.9)\n",
      "for epoch in range(30):  # loop over the dataset multiple times: 43-44\n",
      "    running_loss = 0.0\n",
      "    total_loss = 0\n",
      "    for i, data in enumerate(trainloader, 0):\n",
      "        # get the inputs; data is a list of [inputs, labels]\n",
      "        inputs, labels = data\n",
      "\n",
      "        # zero the parameter gradients\n",
      "        optimizer.zero_grad()\n",
      "\n",
      "        # forward + backward + optimize\n",
      "        outputs = nettt(inputs)\n",
      "        loss = criterion(outputs, labels)\n",
      "        loss.backward()\n",
      "        optimizer.step()\n",
      "\n",
      "        # print statistics\n",
      "        running_loss += loss.item()\n",
      "        total_loss += running_loss\n",
      "        if i % 2000 == 1999:    # print every 2000 mini-batches\n",
      "            print('[%d, %5d] loss: %.3f' %\n",
      "                  (epoch + 1, i + 1, running_loss / 2000))\n",
      "            running_loss = 0.0\n",
      "    total_lossestt.append(total_loss)\n",
      "print('Finished Training')\n",
      "135/91:\n",
      "class Nettt(nn.Module):\n",
      "    def __init__(self):\n",
      "        super(Nettt, self).__init__()\n",
      "        self.conv1 = nn.Conv2d(3, 6, 5)\n",
      "        self.pool = nn.MaxPool2d(2, 2)\n",
      "        self.conv2 = nn.Conv2d(6, 24, 5, padding=9)\n",
      "        self.conv3 = nn.Conv2d(24, 16, 5)\n",
      "        self.fc1 = nn.Linear(16 * 5 * 5, 120)\n",
      "        self.fc2 = nn.Linear(120, 84)\n",
      "        self.fc3 = nn.Linear(84, 10)\n",
      "        self.fc4 = nn.Linear(10,64)\n",
      "        self.fc5 = nn.Linear(64,10)\n",
      "\n",
      "    def forward(self, x):\n",
      "#         print(x.shape)\n",
      "        x = self.pool(F.relu(self.conv1(x)))\n",
      "#         print(x.shape)\n",
      "        x = self.pool(F.relu(self.conv2(x)))\n",
      "#         print(x.shape)\n",
      "        x = self.pool(F.relu(self.conv3(x)))\n",
      "#         print(x.shape)\n",
      "        x = x.view(-1, 16 * 5 * 5)\n",
      "#         print(x.shape)\n",
      "        x = F.relu(self.fc1(x))\n",
      "        x = F.relu(self.fc2(x))\n",
      "        x = self.fc3(x)\n",
      "        x = self.fc4(x)\n",
      "        x = self.fc5(x)\n",
      "        return x\n",
      "xs = [i for i in range(35)]\n",
      "total_lossestt = []\n",
      "nettt = Nettt()\n",
      "criterion = nn.CrossEntropyLoss()\n",
      "optimizer = optim.SGD(nettt.parameters(), lr=0.0005, momentum=0.9)\n",
      "for epoch in range(30):  # loop over the dataset multiple times: 43-44\n",
      "    running_loss = 0.0\n",
      "    total_loss = 0\n",
      "    for i, data in enumerate(trainloader, 0):\n",
      "        # get the inputs; data is a list of [inputs, labels]\n",
      "        inputs, labels = data\n",
      "\n",
      "        # zero the parameter gradients\n",
      "        optimizer.zero_grad()\n",
      "\n",
      "        # forward + backward + optimize\n",
      "        outputs = nettt(inputs)\n",
      "        loss = criterion(outputs, labels)\n",
      "        loss.backward()\n",
      "        optimizer.step()\n",
      "\n",
      "        # print statistics\n",
      "        running_loss += loss.item()\n",
      "        total_loss += running_loss\n",
      "        if i % 2000 == 1999:    # print every 2000 mini-batches\n",
      "            print('[%d, %5d] loss: %.3f' %\n",
      "                  (epoch + 1, i + 1, running_loss / 2000))\n",
      "            running_loss = 0.0\n",
      "    total_lossestt.append(total_loss)\n",
      "print('Finished Training')\n",
      "133/235:\n",
      "class Nettt(nn.Module):\n",
      "    def __init__(self):\n",
      "        super(Nettt, self).__init__()\n",
      "        self.conv1 = nn.Conv2d(3, 6, 5)\n",
      "        self.pool = nn.MaxPool2d(2, 2)\n",
      "        self.conv2 = nn.Conv2d(6, 24, 5)\n",
      "        self.conv3 = nn.Conv2d(24, 16, 5)\n",
      "        self.fc1 = nn.Linear(16 * 5 * 5, 120)\n",
      "        self.fc2 = nn.Linear(120, 84)\n",
      "        self.fc3 = nn.Linear(84, 10)\n",
      "        self.fc4 = nn.Linear(10,64)\n",
      "        self.fc5 = nn.Linear(64,10)\n",
      "\n",
      "    def forward(self, x):\n",
      "        print(x.shape)\n",
      "        x = self.pool(F.relu(self.conv1(x)))\n",
      "#         print(x.shape)\n",
      "        x = self.pool(F.relu(self.conv2(x)))\n",
      "#         print(x.shape)\n",
      "        x = self.pool(F.relu(self.conv3(x)))\n",
      "#         print(x.shape)\n",
      "        x = x.view(-1, 16 * 5 * 5)\n",
      "#         print(x.shape)\n",
      "        x = F.relu(self.fc1(x))\n",
      "        x = F.relu(self.fc2(x))\n",
      "        x = self.fc3(x)\n",
      "        x = self.fc4(x)\n",
      "        x = self.fc5(x)\n",
      "        return x\n",
      "xs = [i for i in range(35)]\n",
      "total_lossestt = []\n",
      "nettt = Nettt()\n",
      "criterion = nn.CrossEntropyLoss()\n",
      "optimizer = optim.SGD(nettt.parameters(), lr=0.0005, momentum=0.9)\n",
      "for epoch in range(30):  # loop over the dataset multiple times: 43-44\n",
      "    running_loss = 0.0\n",
      "    total_loss = 0\n",
      "    for i, data in enumerate(trainloader, 0):\n",
      "        # get the inputs; data is a list of [inputs, labels]\n",
      "        inputs, labels = data\n",
      "\n",
      "        # zero the parameter gradients\n",
      "        optimizer.zero_grad()\n",
      "\n",
      "        # forward + backward + optimize\n",
      "        outputs = nettt(inputs)\n",
      "        loss = criterion(outputs, labels)\n",
      "        loss.backward()\n",
      "        optimizer.step()\n",
      "\n",
      "        # print statistics\n",
      "        running_loss += loss.item()\n",
      "        total_loss += running_loss\n",
      "        if i % 2000 == 1999:    # print every 2000 mini-batches\n",
      "            print('[%d, %5d] loss: %.3f' %\n",
      "                  (epoch + 1, i + 1, running_loss / 2000))\n",
      "            running_loss = 0.0\n",
      "    total_lossestt.append(total_loss)\n",
      "print('Finished Training')\n",
      "133/236:\n",
      "class Nettt(nn.Module):\n",
      "    def __init__(self):\n",
      "        super(Nettt, self).__init__()\n",
      "        self.conv1 = nn.Conv2d(3, 6, 5)\n",
      "        self.pool = nn.MaxPool2d(2, 2)\n",
      "        self.conv2 = nn.Conv2d(6, 16, 5)\n",
      "        self.fc1 = nn.Linear(16 * 5 * 5, 120)\n",
      "        self.fc2 = nn.Linear(120, 84)\n",
      "        self.fc3 = nn.Linear(84, 10)\n",
      "        self.fc4 = nn.Linear(10,64)\n",
      "        self.conv3 = nn.Conv2d(64,10,5)\n",
      "\n",
      "    def forward(self, x):\n",
      "        print(x.shape)\n",
      "        x = self.pool(F.relu(self.conv1(x)))\n",
      "#         print(x.shape)\n",
      "        x = self.pool(F.relu(self.conv2(x)))\n",
      "#         print(x.shape)\n",
      "        x = self.pool(F.relu(self.conv3(x)))\n",
      "#         print(x.shape)\n",
      "        x = x.view(-1, 16 * 5 * 5)\n",
      "#         print(x.shape)\n",
      "        x = F.relu(self.fc1(x))\n",
      "        x = F.relu(self.fc2(x))\n",
      "        x = self.fc3(x)\n",
      "        x = self.fc4(x)\n",
      "        x = self.fc5(x)\n",
      "        return x\n",
      "xs = [i for i in range(35)]\n",
      "total_lossestt = []\n",
      "nettt = Nettt()\n",
      "criterion = nn.CrossEntropyLoss()\n",
      "optimizer = optim.SGD(nettt.parameters(), lr=0.0005, momentum=0.9)\n",
      "for epoch in range(30):  # loop over the dataset multiple times: 43-44\n",
      "    running_loss = 0.0\n",
      "    total_loss = 0\n",
      "    for i, data in enumerate(trainloader, 0):\n",
      "        # get the inputs; data is a list of [inputs, labels]\n",
      "        inputs, labels = data\n",
      "\n",
      "        # zero the parameter gradients\n",
      "        optimizer.zero_grad()\n",
      "\n",
      "        # forward + backward + optimize\n",
      "        outputs = nettt(inputs)\n",
      "        loss = criterion(outputs, labels)\n",
      "        loss.backward()\n",
      "        optimizer.step()\n",
      "\n",
      "        # print statistics\n",
      "        running_loss += loss.item()\n",
      "        total_loss += running_loss\n",
      "        if i % 2000 == 1999:    # print every 2000 mini-batches\n",
      "            print('[%d, %5d] loss: %.3f' %\n",
      "                  (epoch + 1, i + 1, running_loss / 2000))\n",
      "            running_loss = 0.0\n",
      "    total_lossestt.append(total_loss)\n",
      "print('Finished Training')\n",
      "133/237:\n",
      "class Nettt(nn.Module):\n",
      "    def __init__(self):\n",
      "        super(Nettt, self).__init__()\n",
      "        self.conv1 = nn.Conv2d(3, 6, 5)\n",
      "        self.pool = nn.MaxPool2d(2, 2)\n",
      "        self.conv2 = nn.Conv2d(6, 16, 5)\n",
      "        self.fc1 = nn.Linear(16 * 5 * 5, 120)\n",
      "        self.fc2 = nn.Linear(120, 84)\n",
      "        self.fc3 = nn.Linear(84, 10)\n",
      "        self.fc4 = nn.Linear(10,64)\n",
      "        self.conv3 = nn.Conv2d(64,10,5)\n",
      "\n",
      "    def forward(self, x):\n",
      "        print(x.shape)\n",
      "        x = self.pool(F.relu(self.conv1(x)))\n",
      "#         print(x.shape)\n",
      "        x = self.pool(F.relu(self.conv2(x)))\n",
      "#         print(x.shape)\n",
      "#         print(x.shape)\n",
      "        x = x.view(-1, 16 * 5 * 5)\n",
      "#         print(x.shape)\n",
      "        x = F.relu(self.fc1(x))\n",
      "        x = F.relu(self.fc2(x))\n",
      "        x = self.fc3(x)\n",
      "        x = self.fc4(x)\n",
      "        x = self.pool(F.relu(self.conv3(x)))\n",
      "        return x\n",
      "xs = [i for i in range(35)]\n",
      "total_lossestt = []\n",
      "nettt = Nettt()\n",
      "criterion = nn.CrossEntropyLoss()\n",
      "optimizer = optim.SGD(nettt.parameters(), lr=0.0005, momentum=0.9)\n",
      "for epoch in range(30):  # loop over the dataset multiple times: 43-44\n",
      "    running_loss = 0.0\n",
      "    total_loss = 0\n",
      "    for i, data in enumerate(trainloader, 0):\n",
      "        # get the inputs; data is a list of [inputs, labels]\n",
      "        inputs, labels = data\n",
      "\n",
      "        # zero the parameter gradients\n",
      "        optimizer.zero_grad()\n",
      "\n",
      "        # forward + backward + optimize\n",
      "        outputs = nettt(inputs)\n",
      "        loss = criterion(outputs, labels)\n",
      "        loss.backward()\n",
      "        optimizer.step()\n",
      "\n",
      "        # print statistics\n",
      "        running_loss += loss.item()\n",
      "        total_loss += running_loss\n",
      "        if i % 2000 == 1999:    # print every 2000 mini-batches\n",
      "            print('[%d, %5d] loss: %.3f' %\n",
      "                  (epoch + 1, i + 1, running_loss / 2000))\n",
      "            running_loss = 0.0\n",
      "    total_lossestt.append(total_loss)\n",
      "print('Finished Training')\n",
      "133/238:\n",
      "class Nettt(nn.Module):\n",
      "    def __init__(self):\n",
      "        super(Nettt, self).__init__()\n",
      "        self.conv1 = nn.Conv2d(3, 6, 5)\n",
      "        self.pool = nn.MaxPool2d(2, 2)\n",
      "        self.conv2 = nn.Conv2d(6, 16, 5)\n",
      "        self.fc1 = nn.Linear(16 * 5 * 5, 120)\n",
      "        self.fc2 = nn.Linear(120, 84)\n",
      "        self.fc3 = nn.Linear(84, 10)\n",
      "        self.fc4 = nn.Linear(10,64)\n",
      "        self.conv3 = nn.Conv2d(64,10,5)\n",
      "\n",
      "    def forward(self, x):\n",
      "        print(x.shape)\n",
      "        x = self.pool(F.relu(self.conv1(x)))\n",
      "#         print(x.shape)\n",
      "        x = self.pool(F.relu(self.conv2(x)))\n",
      "#         print(x.shape)\n",
      "#         print(x.shape)\n",
      "        x = x.view(-1, 16 * 5 * 5)\n",
      "#         print(x.shape)\n",
      "        x = F.relu(self.fc1(x))\n",
      "        x = F.relu(self.fc2(x))\n",
      "        x = self.fc3(x)\n",
      "        x = self.fc4(x)\n",
      "        x = x.view(4,64)\n",
      "        x = self.pool(F.relu(self.conv3(x)))\n",
      "        return x\n",
      "xs = [i for i in range(35)]\n",
      "total_lossestt = []\n",
      "nettt = Nettt()\n",
      "criterion = nn.CrossEntropyLoss()\n",
      "optimizer = optim.SGD(nettt.parameters(), lr=0.0005, momentum=0.9)\n",
      "for epoch in range(30):  # loop over the dataset multiple times: 43-44\n",
      "    running_loss = 0.0\n",
      "    total_loss = 0\n",
      "    for i, data in enumerate(trainloader, 0):\n",
      "        # get the inputs; data is a list of [inputs, labels]\n",
      "        inputs, labels = data\n",
      "\n",
      "        # zero the parameter gradients\n",
      "        optimizer.zero_grad()\n",
      "\n",
      "        # forward + backward + optimize\n",
      "        outputs = nettt(inputs)\n",
      "        loss = criterion(outputs, labels)\n",
      "        loss.backward()\n",
      "        optimizer.step()\n",
      "\n",
      "        # print statistics\n",
      "        running_loss += loss.item()\n",
      "        total_loss += running_loss\n",
      "        if i % 2000 == 1999:    # print every 2000 mini-batches\n",
      "            print('[%d, %5d] loss: %.3f' %\n",
      "                  (epoch + 1, i + 1, running_loss / 2000))\n",
      "            running_loss = 0.0\n",
      "    total_lossestt.append(total_loss)\n",
      "print('Finished Training')\n",
      "133/239:\n",
      "class Nettt(nn.Module):\n",
      "    def __init__(self):\n",
      "        super(Nettt, self).__init__()\n",
      "        self.conv1 = nn.Conv2d(3, 6, 5)\n",
      "        self.pool = nn.MaxPool2d(2, 2)\n",
      "        self.conv2 = nn.Conv2d(6, 16, 5)\n",
      "        self.fc1 = nn.Linear(16 * 5 * 5, 120)\n",
      "        self.fc2 = nn.Linear(120, 84)\n",
      "        self.fc3 = nn.Linear(84, 10)\n",
      "        self.fc4 = nn.Linear(10,64)\n",
      "        self.conv3 = nn.Conv2d(64,10,5)\n",
      "\n",
      "    def forward(self, x):\n",
      "        print(x.shape)\n",
      "        x = self.pool(F.relu(self.conv1(x)))\n",
      "#         print(x.shape)\n",
      "        x = self.pool(F.relu(self.conv2(x)))\n",
      "#         print(x.shape)\n",
      "#         print(x.shape)\n",
      "        x = x.view(-1, 16 * 5 * 5)\n",
      "#         print(x.shape)\n",
      "        x = F.relu(self.fc1(x))\n",
      "        x = F.relu(self.fc2(x))\n",
      "        x = self.fc3(x)\n",
      "        x = self.fc4(x)\n",
      "        x = x.view(10,64)\n",
      "        x = self.pool(F.relu(self.conv3(x)))\n",
      "        return x\n",
      "xs = [i for i in range(35)]\n",
      "total_lossestt = []\n",
      "nettt = Nettt()\n",
      "criterion = nn.CrossEntropyLoss()\n",
      "optimizer = optim.SGD(nettt.parameters(), lr=0.0005, momentum=0.9)\n",
      "for epoch in range(30):  # loop over the dataset multiple times: 43-44\n",
      "    running_loss = 0.0\n",
      "    total_loss = 0\n",
      "    for i, data in enumerate(trainloader, 0):\n",
      "        # get the inputs; data is a list of [inputs, labels]\n",
      "        inputs, labels = data\n",
      "\n",
      "        # zero the parameter gradients\n",
      "        optimizer.zero_grad()\n",
      "\n",
      "        # forward + backward + optimize\n",
      "        outputs = nettt(inputs)\n",
      "        loss = criterion(outputs, labels)\n",
      "        loss.backward()\n",
      "        optimizer.step()\n",
      "\n",
      "        # print statistics\n",
      "        running_loss += loss.item()\n",
      "        total_loss += running_loss\n",
      "        if i % 2000 == 1999:    # print every 2000 mini-batches\n",
      "            print('[%d, %5d] loss: %.3f' %\n",
      "                  (epoch + 1, i + 1, running_loss / 2000))\n",
      "            running_loss = 0.0\n",
      "    total_lossestt.append(total_loss)\n",
      "print('Finished Training')\n",
      "133/240:\n",
      "class Nettt(nn.Module):\n",
      "    def __init__(self):\n",
      "        super(Nettt, self).__init__()\n",
      "        self.conv1 = nn.Conv2d(3, 6, 5)\n",
      "        self.pool = nn.MaxPool2d(2, 2)\n",
      "        self.conv2 = nn.Conv2d(6, 16, 5)\n",
      "        self.fc1 = nn.Linear(16 * 5 * 5, 120)\n",
      "        self.fc2 = nn.Linear(120, 84)\n",
      "        self.fc3 = nn.Linear(84, 10)\n",
      "        self.fc4 = nn.Linear(10,64)\n",
      "        self.conv3 = nn.Conv2d(64,10,5)\n",
      "\n",
      "    def forward(self, x):\n",
      "        print(x.shape)\n",
      "        x = self.pool(F.relu(self.conv1(x)))\n",
      "#         print(x.shape)\n",
      "        x = self.pool(F.relu(self.conv2(x)))\n",
      "#         print(x.shape)\n",
      "#         print(x.shape)\n",
      "        x = x.view(-1, 16 * 5 * 5)\n",
      "#         print(x.shape)\n",
      "        x = F.relu(self.fc1(x))\n",
      "        x = F.relu(self.fc2(x))\n",
      "        x = self.fc3(x)\n",
      "        x = self.fc4(x)\n",
      "        x = x.view(2,2,2,64)\n",
      "        x = self.pool(F.relu(self.conv3(x)))\n",
      "        return x\n",
      "xs = [i for i in range(35)]\n",
      "total_lossestt = []\n",
      "nettt = Nettt()\n",
      "criterion = nn.CrossEntropyLoss()\n",
      "optimizer = optim.SGD(nettt.parameters(), lr=0.0005, momentum=0.9)\n",
      "for epoch in range(30):  # loop over the dataset multiple times: 43-44\n",
      "    running_loss = 0.0\n",
      "    total_loss = 0\n",
      "    for i, data in enumerate(trainloader, 0):\n",
      "        # get the inputs; data is a list of [inputs, labels]\n",
      "        inputs, labels = data\n",
      "\n",
      "        # zero the parameter gradients\n",
      "        optimizer.zero_grad()\n",
      "\n",
      "        # forward + backward + optimize\n",
      "        outputs = nettt(inputs)\n",
      "        loss = criterion(outputs, labels)\n",
      "        loss.backward()\n",
      "        optimizer.step()\n",
      "\n",
      "        # print statistics\n",
      "        running_loss += loss.item()\n",
      "        total_loss += running_loss\n",
      "        if i % 2000 == 1999:    # print every 2000 mini-batches\n",
      "            print('[%d, %5d] loss: %.3f' %\n",
      "                  (epoch + 1, i + 1, running_loss / 2000))\n",
      "            running_loss = 0.0\n",
      "    total_lossestt.append(total_loss)\n",
      "print('Finished Training')\n",
      "133/241:\n",
      "class Nettt(nn.Module):\n",
      "    def __init__(self):\n",
      "        super(Nettt, self).__init__()\n",
      "        self.conv1 = nn.Conv2d(3, 6, 5)\n",
      "        self.pool = nn.MaxPool2d(2, 2)\n",
      "        self.conv2 = nn.Conv2d(6, 24, 5, padding=9)\n",
      "        self.conv2 = nn.Conv2d(24, 8, 5, padding=9)\n",
      "        self.conv2 = nn.Conv2d(8, 16, 5)\n",
      "        self.fc1 = nn.Linear(16 * 5 * 5, 120)\n",
      "        self.fc2 = nn.Linear(120, 84)\n",
      "        self.fc3 = nn.Linear(84, 10)\n",
      "        self.fc4 = nn.Linear(10,64)\n",
      "        self.fc5 = nn.Linear(64,10)\n",
      "        \n",
      "\n",
      "    def forward(self, x):\n",
      "        print(x.shape)\n",
      "        x = self.pool(F.relu(self.conv1(x)))\n",
      "#         print(x.shape)\n",
      "        x = self.pool(F.relu(self.conv2(x)))\n",
      "#         print(x.shape)\n",
      "        x = self.pool(F.relu(self.conv3(x)))\n",
      "#         print(x.shape)\n",
      "        x = x.view(-1, 16 * 5 * 5)\n",
      "#         print(x.shape)\n",
      "        x = F.relu(self.fc1(x))\n",
      "        x = F.relu(self.fc2(x))\n",
      "        x = F.relu(self.fc3(x))\n",
      "        x = F.relu(self.fc4(x))\n",
      "        x = self.fc5(x)\n",
      "        return x\n",
      "xs = [i for i in range(35)]\n",
      "total_lossestt = []\n",
      "nettt = Nettt()\n",
      "criterion = nn.CrossEntropyLoss()\n",
      "optimizer = optim.SGD(nettt.parameters(), lr=0.0005, momentum=0.9)\n",
      "for epoch in range(30):  # loop over the dataset multiple times: 43-44\n",
      "    running_loss = 0.0\n",
      "    total_loss = 0\n",
      "    for i, data in enumerate(trainloader, 0):\n",
      "        # get the inputs; data is a list of [inputs, labels]\n",
      "        inputs, labels = data\n",
      "\n",
      "        # zero the parameter gradients\n",
      "        optimizer.zero_grad()\n",
      "\n",
      "        # forward + backward + optimize\n",
      "        outputs = nettt(inputs)\n",
      "        loss = criterion(outputs, labels)\n",
      "        loss.backward()\n",
      "        optimizer.step()\n",
      "\n",
      "        # print statistics\n",
      "        running_loss += loss.item()\n",
      "        total_loss += running_loss\n",
      "        if i % 2000 == 1999:    # print every 2000 mini-batches\n",
      "            print('[%d, %5d] loss: %.3f' %\n",
      "                  (epoch + 1, i + 1, running_loss / 2000))\n",
      "            running_loss = 0.0\n",
      "    total_lossestt.append(total_loss)\n",
      "print('Finished Training')\n",
      "133/242:\n",
      "class Nettt(nn.Module):\n",
      "    def __init__(self):\n",
      "        super(Nettt, self).__init__()\n",
      "        self.conv1 = nn.Conv2d(3, 6, 5)\n",
      "        self.pool = nn.MaxPool2d(2, 2)\n",
      "        self.conv2 = nn.Conv2d(6, 24, 5, padding=9)\n",
      "        self.conv2 = nn.Conv2d(24, 8, 5, padding=9)\n",
      "        self.conv2 = nn.Conv2d(8, 16, 5)\n",
      "        self.fc1 = nn.Linear(16 * 5 * 5, 120)\n",
      "        self.fc2 = nn.Linear(120, 84)\n",
      "        self.fc3 = nn.Linear(84, 10)\n",
      "        self.fc4 = nn.Linear(10,64)\n",
      "        self.fc5 = nn.Linear(64,10)\n",
      "        \n",
      "\n",
      "    def forward(self, x):\n",
      "        print(x.shape)\n",
      "        x = self.pool(F.relu(self.conv1(x)))\n",
      "#         print(x.shape)\n",
      "        x = self.pool(F.relu(self.conv2(x)))\n",
      "#         print(x.shape)\n",
      "        x = self.pool(F.relu(self.conv3(x)))\n",
      "        x = self.pool(F.relu(self.conv4(x)))\n",
      "        x = self.pool(F.relu(self.conv5(x)))\n",
      "#         print(x.shape)\n",
      "        x = x.view(-1, 16 * 5 * 5)\n",
      "#         print(x.shape)\n",
      "        x = F.relu(self.fc1(x))\n",
      "        x = F.relu(self.fc2(x))\n",
      "        x = F.relu(self.fc3(x))\n",
      "        x = F.relu(self.fc4(x))\n",
      "        x = self.fc5(x)\n",
      "        return x\n",
      "xs = [i for i in range(35)]\n",
      "total_lossestt = []\n",
      "nettt = Nettt()\n",
      "criterion = nn.CrossEntropyLoss()\n",
      "optimizer = optim.SGD(nettt.parameters(), lr=0.0005, momentum=0.9)\n",
      "for epoch in range(30):  # loop over the dataset multiple times: 43-44\n",
      "    running_loss = 0.0\n",
      "    total_loss = 0\n",
      "    for i, data in enumerate(trainloader, 0):\n",
      "        # get the inputs; data is a list of [inputs, labels]\n",
      "        inputs, labels = data\n",
      "\n",
      "        # zero the parameter gradients\n",
      "        optimizer.zero_grad()\n",
      "\n",
      "        # forward + backward + optimize\n",
      "        outputs = nettt(inputs)\n",
      "        loss = criterion(outputs, labels)\n",
      "        loss.backward()\n",
      "        optimizer.step()\n",
      "\n",
      "        # print statistics\n",
      "        running_loss += loss.item()\n",
      "        total_loss += running_loss\n",
      "        if i % 2000 == 1999:    # print every 2000 mini-batches\n",
      "            print('[%d, %5d] loss: %.3f' %\n",
      "                  (epoch + 1, i + 1, running_loss / 2000))\n",
      "            running_loss = 0.0\n",
      "    total_lossestt.append(total_loss)\n",
      "print('Finished Training')\n",
      "133/243:\n",
      "class Nettt(nn.Module):\n",
      "    def __init__(self):\n",
      "        super(Nettt, self).__init__()\n",
      "        self.conv1 = nn.Conv2d(3, 6, 5)\n",
      "        self.pool = nn.MaxPool2d(2, 2)\n",
      "        self.conv2 = nn.Conv2d(6, 24, 5, padding=9)\n",
      "        self.conv2 = nn.Conv2d(24, 8, 5, padding=9)\n",
      "        self.conv2 = nn.Conv2d(8, 16, 5)\n",
      "        self.fc1 = nn.Linear(16 * 5 * 5, 120)\n",
      "        self.fc2 = nn.Linear(120, 84)\n",
      "        self.fc3 = nn.Linear(84, 10)\n",
      "        self.fc4 = nn.Linear(10,64)\n",
      "        self.fc5 = nn.Linear(64,10)\n",
      "        \n",
      "\n",
      "    def forward(self, x):\n",
      "        print(x.shape)\n",
      "        x = self.pool(F.relu(self.conv1(x)))\n",
      "        print(x.shape)\n",
      "        x = self.pool(F.relu(self.conv2(x)))\n",
      "        print(x.shape)\n",
      "        x = self.pool(F.relu(self.conv3(x)))\n",
      "        print(x.shape)\n",
      "        x = self.pool(F.relu(self.conv4(x)))\n",
      "        print(x.shape)\n",
      "        x = self.pool(F.relu(self.conv5(x)))\n",
      "#         print(x.shape)\n",
      "        x = x.view(-1, 16 * 5 * 5)\n",
      "#         print(x.shape)\n",
      "        x = F.relu(self.fc1(x))\n",
      "        x = F.relu(self.fc2(x))\n",
      "        x = F.relu(self.fc3(x))\n",
      "        x = F.relu(self.fc4(x))\n",
      "        x = self.fc5(x)\n",
      "        return x\n",
      "xs = [i for i in range(35)]\n",
      "total_lossestt = []\n",
      "nettt = Nettt()\n",
      "criterion = nn.CrossEntropyLoss()\n",
      "optimizer = optim.SGD(nettt.parameters(), lr=0.0005, momentum=0.9)\n",
      "for epoch in range(30):  # loop over the dataset multiple times: 43-44\n",
      "    running_loss = 0.0\n",
      "    total_loss = 0\n",
      "    for i, data in enumerate(trainloader, 0):\n",
      "        # get the inputs; data is a list of [inputs, labels]\n",
      "        inputs, labels = data\n",
      "\n",
      "        # zero the parameter gradients\n",
      "        optimizer.zero_grad()\n",
      "\n",
      "        # forward + backward + optimize\n",
      "        outputs = nettt(inputs)\n",
      "        loss = criterion(outputs, labels)\n",
      "        loss.backward()\n",
      "        optimizer.step()\n",
      "\n",
      "        # print statistics\n",
      "        running_loss += loss.item()\n",
      "        total_loss += running_loss\n",
      "        if i % 2000 == 1999:    # print every 2000 mini-batches\n",
      "            print('[%d, %5d] loss: %.3f' %\n",
      "                  (epoch + 1, i + 1, running_loss / 2000))\n",
      "            running_loss = 0.0\n",
      "    total_lossestt.append(total_loss)\n",
      "print('Finished Training')\n",
      "133/244:\n",
      "class Nettt(nn.Module):\n",
      "    def __init__(self):\n",
      "        super(Nettt, self).__init__()\n",
      "        self.conv1 = nn.Conv2d(3, 6, 5)\n",
      "        self.pool = nn.MaxPool2d(2, 2)\n",
      "        self.conv2 = nn.Conv2d(6, 24, 5, padding=9)\n",
      "        self.conv3 = nn.Conv2d(24, 8, 5, padding=9)\n",
      "        self.conv4 = nn.Conv2d(8, 16, 5)\n",
      "        self.fc1 = nn.Linear(16 * 5 * 5, 120)\n",
      "        self.fc2 = nn.Linear(120, 84)\n",
      "        self.fc3 = nn.Linear(84, 10)\n",
      "        self.fc4 = nn.Linear(10,64)\n",
      "        self.fc5 = nn.Linear(64,10)\n",
      "        \n",
      "\n",
      "    def forward(self, x):\n",
      "        print(x.shape)\n",
      "        x = self.pool(F.relu(self.conv1(x)))\n",
      "        print(x.shape)\n",
      "        x = self.pool(F.relu(self.conv2(x)))\n",
      "        print(x.shape)\n",
      "        x = self.pool(F.relu(self.conv3(x)))\n",
      "        print(x.shape)\n",
      "        x = self.pool(F.relu(self.conv4(x)))\n",
      "        print(x.shape)\n",
      "        x = self.pool(F.relu(self.conv5(x)))\n",
      "#         print(x.shape)\n",
      "        x = x.view(-1, 16 * 5 * 5)\n",
      "#         print(x.shape)\n",
      "        x = F.relu(self.fc1(x))\n",
      "        x = F.relu(self.fc2(x))\n",
      "        x = F.relu(self.fc3(x))\n",
      "        x = F.relu(self.fc4(x))\n",
      "        x = self.fc5(x)\n",
      "        return x\n",
      "xs = [i for i in range(35)]\n",
      "total_lossestt = []\n",
      "nettt = Nettt()\n",
      "criterion = nn.CrossEntropyLoss()\n",
      "optimizer = optim.SGD(nettt.parameters(), lr=0.0005, momentum=0.9)\n",
      "for epoch in range(30):  # loop over the dataset multiple times: 43-44\n",
      "    running_loss = 0.0\n",
      "    total_loss = 0\n",
      "    for i, data in enumerate(trainloader, 0):\n",
      "        # get the inputs; data is a list of [inputs, labels]\n",
      "        inputs, labels = data\n",
      "\n",
      "        # zero the parameter gradients\n",
      "        optimizer.zero_grad()\n",
      "\n",
      "        # forward + backward + optimize\n",
      "        outputs = nettt(inputs)\n",
      "        loss = criterion(outputs, labels)\n",
      "        loss.backward()\n",
      "        optimizer.step()\n",
      "\n",
      "        # print statistics\n",
      "        running_loss += loss.item()\n",
      "        total_loss += running_loss\n",
      "        if i % 2000 == 1999:    # print every 2000 mini-batches\n",
      "            print('[%d, %5d] loss: %.3f' %\n",
      "                  (epoch + 1, i + 1, running_loss / 2000))\n",
      "            running_loss = 0.0\n",
      "    total_lossestt.append(total_loss)\n",
      "print('Finished Training')\n",
      "133/245:\n",
      "class Nettt(nn.Module):\n",
      "    def __init__(self):\n",
      "        super(Nettt, self).__init__()\n",
      "        self.conv1 = nn.Conv2d(3, 6, 5)\n",
      "        self.pool = nn.MaxPool2d(2, 2)\n",
      "        self.conv2 = nn.Conv2d(6, 24, 5, padding=9)\n",
      "        self.conv3 = nn.Conv2d(24, 8, 5, padding=9)\n",
      "        self.conv4 = nn.Conv2d(8, 16, 5)\n",
      "        self.fc1 = nn.Linear(16 * 5 * 5, 120)\n",
      "        self.fc2 = nn.Linear(120, 84)\n",
      "        self.fc3 = nn.Linear(84, 10)\n",
      "        self.fc4 = nn.Linear(10,64)\n",
      "        self.fc5 = nn.Linear(64,10)\n",
      "        \n",
      "\n",
      "    def forward(self, x):\n",
      "        print(x.shape)\n",
      "        x = self.pool(F.relu(self.conv1(x)))\n",
      "        print(x.shape)\n",
      "        x = self.pool(F.relu(self.conv2(x)))\n",
      "        print(x.shape)\n",
      "        x = self.pool(F.relu(self.conv3(x)))\n",
      "        print(x.shape)\n",
      "        x = self.pool(F.relu(self.conv4(x)))\n",
      "        print(x.shape)\n",
      "#         print(x.shape)\n",
      "        x = x.view(-1, 16 * 5 * 5)\n",
      "#         print(x.shape)\n",
      "        x = F.relu(self.fc1(x))\n",
      "        x = F.relu(self.fc2(x))\n",
      "        x = F.relu(self.fc3(x))\n",
      "        x = F.relu(self.fc4(x))\n",
      "        x = self.fc5(x)\n",
      "        return x\n",
      "xs = [i for i in range(35)]\n",
      "total_lossestt = []\n",
      "nettt = Nettt()\n",
      "criterion = nn.CrossEntropyLoss()\n",
      "optimizer = optim.SGD(nettt.parameters(), lr=0.0005, momentum=0.9)\n",
      "for epoch in range(30):  # loop over the dataset multiple times: 43-44\n",
      "    running_loss = 0.0\n",
      "    total_loss = 0\n",
      "    for i, data in enumerate(trainloader, 0):\n",
      "        # get the inputs; data is a list of [inputs, labels]\n",
      "        inputs, labels = data\n",
      "\n",
      "        # zero the parameter gradients\n",
      "        optimizer.zero_grad()\n",
      "\n",
      "        # forward + backward + optimize\n",
      "        outputs = nettt(inputs)\n",
      "        loss = criterion(outputs, labels)\n",
      "        loss.backward()\n",
      "        optimizer.step()\n",
      "\n",
      "        # print statistics\n",
      "        running_loss += loss.item()\n",
      "        total_loss += running_loss\n",
      "        if i % 2000 == 1999:    # print every 2000 mini-batches\n",
      "            print('[%d, %5d] loss: %.3f' %\n",
      "                  (epoch + 1, i + 1, running_loss / 2000))\n",
      "            running_loss = 0.0\n",
      "    total_lossestt.append(total_loss)\n",
      "print('Finished Training')\n",
      "133/246:\n",
      "class Nettt(nn.Module):\n",
      "    def __init__(self):\n",
      "        super(Nettt, self).__init__()\n",
      "        self.conv1 = nn.Conv2d(3, 6, 5)\n",
      "        self.pool = nn.MaxPool2d(2, 2)\n",
      "        self.conv2 = nn.Conv2d(6, 24, 5, padding=9)\n",
      "        self.conv3 = nn.Conv2d(24, 8, 5, padding=9)\n",
      "        self.conv4 = nn.Conv2d(8, 16, 5)\n",
      "        self.fc1 = nn.Linear(16 * 5 * 5, 120)\n",
      "        self.fc2 = nn.Linear(120, 84)\n",
      "        self.fc3 = nn.Linear(84, 10)\n",
      "        self.fc4 = nn.Linear(10,64)\n",
      "        self.fc5 = nn.Linear(64,10)\n",
      "        \n",
      "\n",
      "    def forward(self, x):\n",
      "#         print(x.shape)\n",
      "        x = self.pool(F.relu(self.conv1(x)))\n",
      "#         print(x.shape)\n",
      "        x = self.pool(F.relu(self.conv2(x)))\n",
      "#         print(x.shape)\n",
      "        x = self.pool(F.relu(self.conv3(x)))\n",
      "#         print(x.shape)\n",
      "        x = self.pool(F.relu(self.conv4(x)))\n",
      "#         print(x.shape)\n",
      "        x = x.view(-1, 16 * 5 * 5)\n",
      "#         print(x.shape)\n",
      "        x = F.relu(self.fc1(x))\n",
      "        x = F.relu(self.fc2(x))\n",
      "        x = F.relu(self.fc3(x))\n",
      "        x = F.relu(self.fc4(x))\n",
      "        x = self.fc5(x)\n",
      "        return x\n",
      "xs = [i for i in range(35)]\n",
      "total_lossestt = []\n",
      "nettt = Nettt()\n",
      "criterion = nn.CrossEntropyLoss()\n",
      "optimizer = optim.SGD(nettt.parameters(), lr=0.0005, momentum=0.9)\n",
      "for epoch in range(30):  # loop over the dataset multiple times: 43-44\n",
      "    running_loss = 0.0\n",
      "    total_loss = 0\n",
      "    for i, data in enumerate(trainloader, 0):\n",
      "        # get the inputs; data is a list of [inputs, labels]\n",
      "        inputs, labels = data\n",
      "\n",
      "        # zero the parameter gradients\n",
      "        optimizer.zero_grad()\n",
      "\n",
      "        # forward + backward + optimize\n",
      "        outputs = nettt(inputs)\n",
      "        loss = criterion(outputs, labels)\n",
      "        loss.backward()\n",
      "        optimizer.step()\n",
      "\n",
      "        # print statistics\n",
      "        running_loss += loss.item()\n",
      "        total_loss += running_loss\n",
      "        if i % 2000 == 1999:    # print every 2000 mini-batches\n",
      "            print('[%d, %5d] loss: %.3f' %\n",
      "                  (epoch + 1, i + 1, running_loss / 2000))\n",
      "            running_loss = 0.0\n",
      "    total_lossestt.append(total_loss)\n",
      "print('Finished Training')\n",
      "133/247:\n",
      "for epoch in range(15):  # loop over the dataset multiple times: 43-44\n",
      "    running_loss = 0.0\n",
      "    total_loss = 0\n",
      "    for i, data in enumerate(trainloader, 0):\n",
      "        # get the inputs; data is a list of [inputs, labels]\n",
      "        inputs, labels = data\n",
      "\n",
      "        # zero the parameter gradients\n",
      "        optimizer.zero_grad()\n",
      "\n",
      "        # forward + backward + optimize\n",
      "        outputs = nettt(inputs)\n",
      "        loss = criterion(outputs, labels)\n",
      "        loss.backward()\n",
      "        optimizer.step()\n",
      "\n",
      "        # print statistics\n",
      "        running_loss += loss.item()\n",
      "        total_loss += running_loss\n",
      "        if i % 2000 == 1999:    # print every 2000 mini-batches\n",
      "            print('[%d, %5d] loss: %.3f' %\n",
      "                  (epoch + 1, i + 1, running_loss / 2000))\n",
      "            running_loss = 0.0\n",
      "    total_lossestt.append(total_loss)\n",
      "print('Finished Training')\n",
      "135/92:\n",
      "correct = 0\n",
      "total = 0\n",
      "with torch.no_grad():\n",
      "    for data in testloader:\n",
      "        images, labels = data\n",
      "        outputs = net(images)\n",
      "        _, predicted = torch.max(outputs.data, 1)\n",
      "        total += labels.size(0)\n",
      "        correct += (predicted == labels).sum().item()\n",
      "print(total)\n",
      "print('Accuracy of the network on the 10000 test images: %d %%' % (\n",
      "    100 * correct / total))\n",
      "135/93:\n",
      "correct = 0\n",
      "total = 0\n",
      "with torch.no_grad():\n",
      "    for data in testloader:\n",
      "        images, labels = data\n",
      "        outputs = nettt(images)\n",
      "        _, predicted = torch.max(outputs.data, 1)\n",
      "        total += labels.size(0)\n",
      "        correct += (predicted == labels).sum().item()\n",
      "print(total)\n",
      "print('Accuracy of the network on the 10000 test images: %d %%' % (\n",
      "    100 * correct / total))\n",
      "133/248:\n",
      "correct = 0\n",
      "total = 0\n",
      "with torch.no_grad():\n",
      "    for data in testloader:\n",
      "        images, labels = data\n",
      "        outputs = nettt(images)\n",
      "        _, predicted = torch.max(outputs.data, 1)\n",
      "        total += labels.size(0)\n",
      "        correct += (predicted == labels).sum().item()\n",
      "print(total)\n",
      "print('Accuracy of the network on the 10000 test images: %d %%' % (\n",
      "    100 * correct / total))\n",
      "133/249:\n",
      "class Nettt(nn.Module):\n",
      "    def __init__(self):\n",
      "        super(Nettt, self).__init__()\n",
      "        self.conv1 = nn.Conv2d(3, 6, 5)\n",
      "        self.pool = nn.MaxPool2d(2, 2)\n",
      "        self.conv2 = nn.Conv2d(6, 12, 5, padding=9)\n",
      "        self.conv3 = nn.Conv2d(12, 24, 5, padding=9)\n",
      "        self.conv4 = nn.Conv2d(24, 16, 5)\n",
      "        self.fc1 = nn.Linear(16 * 5 * 5, 120)\n",
      "        self.fc2 = nn.Linear(120, 84)\n",
      "        self.fc3 = nn.Linear(84, 10)\n",
      "        \n",
      "\n",
      "    def forward(self, x):\n",
      "#         print(x.shape)\n",
      "        x = self.pool(F.relu(self.conv1(x)))\n",
      "#         print(x.shape)\n",
      "        x = self.pool(F.relu(self.conv2(x)))\n",
      "#         print(x.shape)\n",
      "        x = self.pool(F.relu(self.conv3(x)))\n",
      "#         print(x.shape)\n",
      "        x = self.pool(F.relu(self.conv4(x)))\n",
      "#         print(x.shape)\n",
      "        x = x.view(-1, 16 * 5 * 5)\n",
      "#         print(x.shape)\n",
      "        x = F.relu(self.fc1(x))\n",
      "        x = F.relu(self.fc2(x))\n",
      "        x = self.fc3(x)\n",
      "        return x\n",
      "xs = [i for i in range(35)]\n",
      "total_lossestt = []\n",
      "nettt = Nettt()\n",
      "criterion = nn.CrossEntropyLoss()\n",
      "optimizer = optim.SGD(nettt.parameters(), lr=0.0005, momentum=0.9)\n",
      "for epoch in range(30):  # loop over the dataset multiple times: 43-44\n",
      "    running_loss = 0.0\n",
      "    total_loss = 0\n",
      "    for i, data in enumerate(trainloader, 0):\n",
      "        # get the inputs; data is a list of [inputs, labels]\n",
      "        inputs, labels = data\n",
      "\n",
      "        # zero the parameter gradients\n",
      "        optimizer.zero_grad()\n",
      "\n",
      "        # forward + backward + optimize\n",
      "        outputs = nettt(inputs)\n",
      "        loss = criterion(outputs, labels)\n",
      "        loss.backward()\n",
      "        optimizer.step()\n",
      "\n",
      "        # print statistics\n",
      "        running_loss += loss.item()\n",
      "        total_loss += running_loss\n",
      "        if i % 2000 == 1999:    # print every 2000 mini-batches\n",
      "            print('[%d, %5d] loss: %.3f' %\n",
      "                  (epoch + 1, i + 1, running_loss / 2000))\n",
      "            running_loss = 0.0\n",
      "    total_lossestt.append(total_loss)\n",
      "print('Finished Training')\n",
      "135/94: plt(total_lossestt)\n",
      "135/95: plt.plot(total_lossestt)\n",
      "135/96:\n",
      "class Nett(nn.Module):\n",
      "    def __init__(self):\n",
      "        super(Nett, self).__init__()\n",
      "        self.conv1 = nn.Conv2d(3, 6, 5)\n",
      "        self.pool = nn.MaxPool2d(2, 2)\n",
      "        self.conv2 = nn.Conv2d(6, 12, 5, padding=9)\n",
      "        self.conv3 = nn.Conv2d(12, 24, 5, padding=9)\n",
      "        self.conv4 = nn.Conv2d(24, 16, 5)\n",
      "        self.fc1 = nn.Linear(16 * 5 * 5, 120)\n",
      "        self.fc2 = nn.Linear(120, 84)\n",
      "        self.fc3 = nn.Linear(84, 10)\n",
      "        \n",
      "\n",
      "    def forward(self, x):\n",
      "#         print(x.shape)\n",
      "        x = self.pool(F.relu(self.conv1(x)))\n",
      "#         print(x.shape)\n",
      "        x = self.pool(F.relu(self.conv2(x)))\n",
      "#         print(x.shape)\n",
      "        x = self.pool(F.relu(self.conv3(x)))\n",
      "#         print(x.shape)\n",
      "        x = self.pool(F.relu(self.conv4(x)))\n",
      "#         print(x.shape)\n",
      "        x = x.view(-1, 16 * 5 * 5)\n",
      "#         print(x.shape)\n",
      "        x = F.relu(self.fc1(x))\n",
      "        x = F.relu(self.fc2(x))\n",
      "        x = self.fc3(x)\n",
      "        return x\n",
      "xs = [i for i in range(35)]\n",
      "total_lossest = []\n",
      "nettt = Nett()\n",
      "criterion = nn.CrossEntropyLoss()\n",
      "optimizer = optim.SGD(nett.parameters(), lr=0.0005, momentum=0.9)\n",
      "for epoch in range(30):  # loop over the dataset multiple times: 43-44\n",
      "    running_loss = 0.0\n",
      "    total_loss = 0\n",
      "    for i, data in enumerate(trainloader, 0):\n",
      "        # get the inputs; data is a list of [inputs, labels]\n",
      "        inputs, labels = data\n",
      "\n",
      "        # zero the parameter gradients\n",
      "        optimizer.zero_grad()\n",
      "\n",
      "        # forward + backward + optimize\n",
      "        outputs = nett(inputs)\n",
      "        loss = criterion(outputs, labels)\n",
      "        loss.backward()\n",
      "        optimizer.step()\n",
      "\n",
      "        # print statistics\n",
      "        running_loss += loss.item()\n",
      "        total_loss += running_loss\n",
      "        if i % 2000 == 1999:    # print every 2000 mini-batches\n",
      "            print('[%d, %5d] loss: %.3f' %\n",
      "                  (epoch + 1, i + 1, running_loss / 2000))\n",
      "            running_loss = 0.0\n",
      "    total_lossest.append(total_loss)\n",
      "print('Finished Training')\n",
      "135/97:\n",
      "class Nett(nn.Module):\n",
      "    def __init__(self):\n",
      "        super(Nett, self).__init__()\n",
      "        self.conv1 = nn.Conv2d(3, 6, 5)\n",
      "        self.pool = nn.MaxPool2d(2, 2)\n",
      "        self.conv2 = nn.Conv2d(6, 12, 5, padding=9)\n",
      "        self.conv3 = nn.Conv2d(12, 24, 5, padding=9)\n",
      "        self.conv4 = nn.Conv2d(24, 16, 5)\n",
      "        self.fc1 = nn.Linear(16 * 5 * 5, 120)\n",
      "        self.fc2 = nn.Linear(120, 84)\n",
      "        self.fc3 = nn.Linear(84, 10)\n",
      "        \n",
      "\n",
      "    def forward(self, x):\n",
      "        print(x.shape)\n",
      "        x = self.pool(F.relu(self.conv1(x)))\n",
      "        print(x.shape)\n",
      "        x = self.pool(F.relu(self.conv2(x)))\n",
      "        print(x.shape)\n",
      "        x = self.pool(F.relu(self.conv3(x)))\n",
      "        print(x.shape)\n",
      "        x = self.pool(F.relu(self.conv4(x)))\n",
      "#         print(x.shape)\n",
      "        x = x.view(-1, 16 * 5 * 5)\n",
      "#         print(x.shape)\n",
      "        x = F.relu(self.fc1(x))\n",
      "        x = F.relu(self.fc2(x))\n",
      "        x = self.fc3(x)\n",
      "        return x\n",
      "xs = [i for i in range(35)]\n",
      "total_lossest = []\n",
      "nettt = Nett()\n",
      "criterion = nn.CrossEntropyLoss()\n",
      "optimizer = optim.SGD(nett.parameters(), lr=0.0005, momentum=0.9)\n",
      "for epoch in range(30):  # loop over the dataset multiple times: 43-44\n",
      "    running_loss = 0.0\n",
      "    total_loss = 0\n",
      "    for i, data in enumerate(trainloader, 0):\n",
      "        # get the inputs; data is a list of [inputs, labels]\n",
      "        inputs, labels = data\n",
      "\n",
      "        # zero the parameter gradients\n",
      "        optimizer.zero_grad()\n",
      "\n",
      "        # forward + backward + optimize\n",
      "        outputs = nett(inputs)\n",
      "        loss = criterion(outputs, labels)\n",
      "        loss.backward()\n",
      "        optimizer.step()\n",
      "\n",
      "        # print statistics\n",
      "        running_loss += loss.item()\n",
      "        total_loss += running_loss\n",
      "        if i % 2000 == 1999:    # print every 2000 mini-batches\n",
      "            print('[%d, %5d] loss: %.3f' %\n",
      "                  (epoch + 1, i + 1, running_loss / 2000))\n",
      "            running_loss = 0.0\n",
      "    total_lossest.append(total_loss)\n",
      "print('Finished Training')\n",
      "135/98:\n",
      "class Nett(nn.Module):\n",
      "    def __init__(self):\n",
      "        super(Nett, self).__init__()\n",
      "        self.conv1 = nn.Conv2d(3, 6, 5)\n",
      "        self.pool = nn.MaxPool2d(2, 2)\n",
      "        self.conv2 = nn.Conv2d(6, 12, 5, padding=9)\n",
      "        self.conv3 = nn.Conv2d(12, 24, 5, padding=9)\n",
      "        self.conv4 = nn.Conv2d(24, 16, 5)\n",
      "        self.fc1 = nn.Linear(16 * 5 * 5, 120)\n",
      "        self.fc2 = nn.Linear(120, 84)\n",
      "        self.fc3 = nn.Linear(84, 10)\n",
      "        \n",
      "\n",
      "    def forward(self, x):\n",
      "#         print(x.shape)\n",
      "        x = self.pool(F.relu(self.conv1(x)))\n",
      "#         print(x.shape)\n",
      "        x = self.pool(F.relu(self.conv2(x)))\n",
      "#         print(x.shape)\n",
      "        x = self.pool(F.relu(self.conv3(x)))\n",
      "#         print(x.shape)\n",
      "        x = self.pool(F.relu(self.conv4(x)))\n",
      "#         print(x.shape)\n",
      "        x = x.view(-1, 16 * 5 * 5)\n",
      "#         print(x.shape)\n",
      "        x = F.relu(self.fc1(x))\n",
      "        x = F.relu(self.fc2(x))\n",
      "        x = self.fc3(x)\n",
      "        return x\n",
      "xs = [i for i in range(35)]\n",
      "total_lossest = []\n",
      "nett = Nett()\n",
      "criterion = nn.CrossEntropyLoss()\n",
      "optimizer = optim.SGD(nett.parameters(), lr=0.0005, momentum=0.9)\n",
      "for epoch in range(30):  # loop over the dataset multiple times: 43-44\n",
      "    running_loss = 0.0\n",
      "    total_loss = 0\n",
      "    for i, data in enumerate(trainloader, 0):\n",
      "        # get the inputs; data is a list of [inputs, labels]\n",
      "        inputs, labels = data\n",
      "\n",
      "        # zero the parameter gradients\n",
      "        optimizer.zero_grad()\n",
      "\n",
      "        # forward + backward + optimize\n",
      "        outputs = nett(inputs)\n",
      "        loss = criterion(outputs, labels)\n",
      "        loss.backward()\n",
      "        optimizer.step()\n",
      "\n",
      "        # print statistics\n",
      "        running_loss += loss.item()\n",
      "        total_loss += running_loss\n",
      "        if i % 2000 == 1999:    # print every 2000 mini-batches\n",
      "            print('[%d, %5d] loss: %.3f' %\n",
      "                  (epoch + 1, i + 1, running_loss / 2000))\n",
      "            running_loss = 0.0\n",
      "    total_lossest.append(total_loss)\n",
      "print('Finished Training')\n",
      "135/99:\n",
      "class Nett(nn.Module):\n",
      "    def __init__(self):\n",
      "        super(Nett, self).__init__()\n",
      "        self.conv1 = nn.Conv2d(3, 6, 5)\n",
      "        self.pool = nn.MaxPool2d(2, 2)\n",
      "        self.conv2 = nn.Conv2d(6, 12, 5, padding=9)\n",
      "        self.conv3 = nn.Conv2d(12, 24, 5, padding=9)\n",
      "        self.conv4 = nn.Conv2d(24, 48, 5, padding=9)\n",
      "        self.conv5 = nn.Conv2d(48, 16, 5)\n",
      "        self.fc1 = nn.Linear(16 * 5 * 5, 120)\n",
      "        self.fc2 = nn.Linear(120, 84)\n",
      "        self.fc3 = nn.Linear(84, 10)\n",
      "        \n",
      "\n",
      "    def forward(self, x):\n",
      "#         print(x.shape)\n",
      "        x = self.pool(F.relu(self.conv1(x)))\n",
      "#         print(x.shape)\n",
      "        x = self.pool(F.relu(self.conv2(x)))\n",
      "#         print(x.shape)\n",
      "        x = self.pool(F.relu(self.conv3(x)))\n",
      "#         print(x.shape)\n",
      "        x = self.pool(F.relu(self.conv4(x)))\n",
      "        x = self.pool(F.relu(self.conv5(x)))\n",
      "#         print(x.shape)\n",
      "        x = x.view(-1, 16 * 5 * 5)\n",
      "#         print(x.shape)\n",
      "        x = F.relu(self.fc1(x))\n",
      "        x = F.relu(self.fc2(x))\n",
      "        x = self.fc3(x)\n",
      "        return x\n",
      "xs = [i for i in range(35)]\n",
      "total_lossest = []\n",
      "nett = Nett()\n",
      "criterion = nn.CrossEntropyLoss()\n",
      "optimizer = optim.SGD(nett.parameters(), lr=0.0005, momentum=0.9)\n",
      "for epoch in range(30):  # loop over the dataset multiple times: 43-44\n",
      "    running_loss = 0.0\n",
      "    total_loss = 0\n",
      "    for i, data in enumerate(trainloader, 0):\n",
      "        # get the inputs; data is a list of [inputs, labels]\n",
      "        inputs, labels = data\n",
      "\n",
      "        # zero the parameter gradients\n",
      "        optimizer.zero_grad()\n",
      "\n",
      "        # forward + backward + optimize\n",
      "        outputs = nett(inputs)\n",
      "        loss = criterion(outputs, labels)\n",
      "        loss.backward()\n",
      "        optimizer.step()\n",
      "\n",
      "        # print statistics\n",
      "        running_loss += loss.item()\n",
      "        total_loss += running_loss\n",
      "        if i % 2000 == 1999:    # print every 2000 mini-batches\n",
      "            print('[%d, %5d] loss: %.3f' %\n",
      "                  (epoch + 1, i + 1, running_loss / 2000))\n",
      "            running_loss = 0.0\n",
      "    total_lossest.append(total_loss)\n",
      "print('Finished Training')\n",
      "133/250: plt.plot(total_lossestt)\n",
      "133/251:\n",
      "correct = 0\n",
      "total = 0\n",
      "with torch.no_grad():\n",
      "    for data in testloader:\n",
      "        images, labels = data\n",
      "        outputs = nett(images)\n",
      "        _, predicted = torch.max(outputs.data, 1)\n",
      "        total += labels.size(0)\n",
      "        correct += (predicted == labels).sum().item()\n",
      "print(total)\n",
      "print('Accuracy of the network on the 10000 test images: %d %%' % (\n",
      "    100 * correct / total))\n",
      "133/252:\n",
      "correct = 0\n",
      "total = 0\n",
      "with torch.no_grad():\n",
      "    for data in testloader:\n",
      "        images, labels = data\n",
      "        outputs = nettt(images)\n",
      "        _, predicted = torch.max(outputs.data, 1)\n",
      "        total += labels.size(0)\n",
      "        correct += (predicted == labels).sum().item()\n",
      "print(total)\n",
      "print('Accuracy of the network on the 10000 test images: %d %%' % (\n",
      "    100 * correct / total))\n",
      "135/100:\n",
      "for epoch in range(8):  # loop over the dataset multiple times: 43-44\n",
      "    running_loss = 0.0\n",
      "    total_loss = 0\n",
      "    for i, data in enumerate(trainloader, 0):\n",
      "        # get the inputs; data is a list of [inputs, labels]\n",
      "        inputs, labels = data\n",
      "\n",
      "        # zero the parameter gradients\n",
      "        optimizer.zero_grad()\n",
      "\n",
      "        # forward + backward + optimize\n",
      "        outputs = nett(inputs)\n",
      "        loss = criterion(outputs, labels)\n",
      "        loss.backward()\n",
      "        optimizer.step()\n",
      "\n",
      "        # print statistics\n",
      "        running_loss += loss.item()\n",
      "        total_loss += running_loss\n",
      "        if i % 2000 == 1999:    # print every 2000 mini-batches\n",
      "            print('[%d, %5d] loss: %.3f' %\n",
      "                  (epoch + 1, i + 1, running_loss / 2000))\n",
      "            running_loss = 0.0\n",
      "    total_lossest.append(total_loss)\n",
      "print('Finished Training')\n",
      "135/101: plt.plot(total_lossest)\n",
      "135/102:\n",
      "correct = 0\n",
      "total = 0\n",
      "with torch.no_grad():\n",
      "    for data in testloader:\n",
      "        images, labels = data\n",
      "        outputs = nettt(images)\n",
      "        _, predicted = torch.max(outputs.data, 1)\n",
      "        total += labels.size(0)\n",
      "        correct += (predicted == labels).sum().item()\n",
      "print(total)\n",
      "print('Accuracy of the network on the 10000 test images: %d %%' % (\n",
      "    100 * correct / total))\n",
      "135/103:\n",
      "correct = 0\n",
      "total = 0\n",
      "with torch.no_grad():\n",
      "    for data in testloader:\n",
      "        images, labels = data\n",
      "        outputs = nett(images)\n",
      "        _, predicted = torch.max(outputs.data, 1)\n",
      "        total += labels.size(0)\n",
      "        correct += (predicted == labels).sum().item()\n",
      "print(total)\n",
      "print('Accuracy of the network on the 10000 test images: %d %%' % (\n",
      "    100 * correct / total))\n",
      "133/253:\n",
      "class Nett(nn.Module):\n",
      "    def __init__(self):\n",
      "        super(Nett, self).__init__()\n",
      "        self.conv1 = nn.Conv2d(3, 6, 5)\n",
      "        self.pool = nn.MaxPool2d(2, 2)\n",
      "        self.conv2 = nn.Conv2d(6, 12, 5, padding=9)\n",
      "        self.conv2 = nn.Conv2d(12, 16, 5)\n",
      "        self.fc1 = nn.Linear(16 * 5 * 5, 120)\n",
      "        self.fc2 = nn.Linear(120, 84)\n",
      "        self.fc3 = nn.Linear(84, 10)\n",
      "        self.conv3 = nn.Conv2d(10,16,5)\n",
      "        self.fc4 = nn.Linear()\n",
      "\n",
      "    def forward(self, x):\n",
      "        x = self.pool(F.relu(self.conv1(x)))\n",
      "        x = self.pool(F.relu(self.conv2(x)))\n",
      "        x = F.relu(self.conv3(x))\n",
      "        x = x.view(-1, 16 * 5 * 5)\n",
      "        x = F.relu(self.fc1(x))\n",
      "        x = F.relu(self.fc2(x))\n",
      "        x = self.fc3(x)\n",
      "        x = self.pool(F.relu(self.conv3(x)))\n",
      "        x = self.fc4()\n",
      "        return x\n",
      "133/254:\n",
      "xs = [i for i in range(35)]\n",
      "total_lossest = []\n",
      "nett = Nett()\n",
      "criterion = nn.CrossEntropyLoss()\n",
      "optimizer = optim.SGD(nett.parameters(), lr=0.0005, momentum=0.9)\n",
      "for epoch in range(30):  # loop over the dataset multiple times: 43-44\n",
      "    running_loss = 0.0\n",
      "    total_loss = 0\n",
      "    for i, data in enumerate(trainloader, 0):\n",
      "        # get the inputs; data is a list of [inputs, labels]\n",
      "        inputs, labels = data\n",
      "\n",
      "        # zero the parameter gradients\n",
      "        optimizer.zero_grad()\n",
      "\n",
      "        # forward + backward + optimize\n",
      "        outputs = nett(inputs)\n",
      "        loss = criterion(outputs, labels)\n",
      "        loss.backward()\n",
      "        optimizer.step()\n",
      "\n",
      "        # print statistics\n",
      "        running_loss += loss.item()\n",
      "        total_loss += running_loss\n",
      "        if i % 2000 == 1999:    # print every 2000 mini-batches\n",
      "            print('[%d, %5d] loss: %.3f' %\n",
      "                  (epoch + 1, i + 1, running_loss / 2000))\n",
      "            running_loss = 0.0\n",
      "    total_lossest.append(total_loss)\n",
      "print('Finished Training')\n",
      "133/255:\n",
      "class Nett(nn.Module):\n",
      "    def __init__(self):\n",
      "        super(Nett, self).__init__()\n",
      "        self.conv1 = nn.Conv2d(3, 6, 5)\n",
      "        self.pool = nn.MaxPool2d(2, 2)\n",
      "        self.conv2 = nn.Conv2d(6, 12, 5, padding=9)\n",
      "        self.conv2 = nn.Conv2d(12, 16, 5)\n",
      "        self.fc1 = nn.Linear(16 * 5 * 5, 120)\n",
      "        self.fc2 = nn.Linear(120, 84)\n",
      "        self.fc3 = nn.Linear(84, 10)\n",
      "\n",
      "    def forward(self, x):\n",
      "        x = self.pool(F.relu(self.conv1(x)))\n",
      "        x = self.pool(F.relu(self.conv2(x)))\n",
      "        x = F.relu(self.conv3(x))\n",
      "        x = x.view(-1, 16 * 5 * 5)\n",
      "        x = F.relu(self.fc1(x))\n",
      "        x = F.relu(self.fc2(x))\n",
      "        x = self.fc3(x)\n",
      "        x = self.pool(F.relu(self.conv3(x)))\n",
      "        x = self.fc4()\n",
      "        return x\n",
      "133/256:\n",
      "xs = [i for i in range(35)]\n",
      "total_lossest = []\n",
      "nett = Nett()\n",
      "criterion = nn.CrossEntropyLoss()\n",
      "optimizer = optim.SGD(nett.parameters(), lr=0.0005, momentum=0.9)\n",
      "for epoch in range(30):  # loop over the dataset multiple times: 43-44\n",
      "    running_loss = 0.0\n",
      "    total_loss = 0\n",
      "    for i, data in enumerate(trainloader, 0):\n",
      "        # get the inputs; data is a list of [inputs, labels]\n",
      "        inputs, labels = data\n",
      "\n",
      "        # zero the parameter gradients\n",
      "        optimizer.zero_grad()\n",
      "\n",
      "        # forward + backward + optimize\n",
      "        outputs = nett(inputs)\n",
      "        loss = criterion(outputs, labels)\n",
      "        loss.backward()\n",
      "        optimizer.step()\n",
      "\n",
      "        # print statistics\n",
      "        running_loss += loss.item()\n",
      "        total_loss += running_loss\n",
      "        if i % 2000 == 1999:    # print every 2000 mini-batches\n",
      "            print('[%d, %5d] loss: %.3f' %\n",
      "                  (epoch + 1, i + 1, running_loss / 2000))\n",
      "            running_loss = 0.0\n",
      "    total_lossest.append(total_loss)\n",
      "print('Finished Training')\n",
      "133/257:\n",
      "class Nett(nn.Module):\n",
      "    def __init__(self):\n",
      "        super(Nett, self).__init__()\n",
      "        self.conv1 = nn.Conv2d(3, 6, 5)\n",
      "        self.pool = nn.MaxPool2d(2, 2)\n",
      "        self.conv2 = nn.Conv2d(6, 12, 5, padding=9)\n",
      "        self.conv2 = nn.Conv2d(12, 16, 5)\n",
      "        self.fc1 = nn.Linear(16 * 5 * 5, 120)\n",
      "        self.fc2 = nn.Linear(120, 84)\n",
      "        self.fc3 = nn.Linear(84, 10)\n",
      "\n",
      "    def forward(self, x):\n",
      "        print(x.shape)\n",
      "        x = self.pool(F.relu(self.conv1(x)))\n",
      "        print(x.shape)\n",
      "        x = self.pool(F.relu(self.conv2(x)))\n",
      "        print(x.shape)\n",
      "        x = F.relu(self.conv3(x))\n",
      "        print(x.shape)\n",
      "        x = x.view(-1, 16 * 5 * 5)\n",
      "        x = F.relu(self.fc1(x))\n",
      "        x = F.relu(self.fc2(x))\n",
      "        x = self.fc3(x)\n",
      "        x = self.pool(F.relu(self.conv3(x)))\n",
      "        x = self.fc4()\n",
      "        return x\n",
      "133/258:\n",
      "xs = [i for i in range(35)]\n",
      "total_lossest = []\n",
      "nett = Nett()\n",
      "criterion = nn.CrossEntropyLoss()\n",
      "optimizer = optim.SGD(nett.parameters(), lr=0.0005, momentum=0.9)\n",
      "for epoch in range(30):  # loop over the dataset multiple times: 43-44\n",
      "    running_loss = 0.0\n",
      "    total_loss = 0\n",
      "    for i, data in enumerate(trainloader, 0):\n",
      "        # get the inputs; data is a list of [inputs, labels]\n",
      "        inputs, labels = data\n",
      "\n",
      "        # zero the parameter gradients\n",
      "        optimizer.zero_grad()\n",
      "\n",
      "        # forward + backward + optimize\n",
      "        outputs = nett(inputs)\n",
      "        loss = criterion(outputs, labels)\n",
      "        loss.backward()\n",
      "        optimizer.step()\n",
      "\n",
      "        # print statistics\n",
      "        running_loss += loss.item()\n",
      "        total_loss += running_loss\n",
      "        if i % 2000 == 1999:    # print every 2000 mini-batches\n",
      "            print('[%d, %5d] loss: %.3f' %\n",
      "                  (epoch + 1, i + 1, running_loss / 2000))\n",
      "            running_loss = 0.0\n",
      "    total_lossest.append(total_loss)\n",
      "print('Finished Training')\n",
      "133/259:\n",
      "class Nett(nn.Module):\n",
      "    def __init__(self):\n",
      "        super(Nett, self).__init__()\n",
      "        self.conv1 = nn.Conv2d(3, 6, 5)\n",
      "        self.pool = nn.MaxPool2d(2, 2)\n",
      "        self.conv2 = nn.Conv2d(6, 12, 5, padding=9)\n",
      "        self.conv3 = nn.Conv2d(12, 16, 5)\n",
      "        self.fc1 = nn.Linear(16 * 5 * 5, 120)\n",
      "        self.fc2 = nn.Linear(120, 84)\n",
      "        self.fc3 = nn.Linear(84, 10)\n",
      "\n",
      "    def forward(self, x):\n",
      "        print(x.shape)\n",
      "        x = self.pool(F.relu(self.conv1(x)))\n",
      "        print(x.shape)\n",
      "        x = self.pool(F.relu(self.conv2(x)))\n",
      "        print(x.shape)\n",
      "        x = F.relu(self.conv3(x))\n",
      "        print(x.shape)\n",
      "        x = x.view(-1, 16 * 5 * 5)\n",
      "        x = F.relu(self.fc1(x))\n",
      "        x = F.relu(self.fc2(x))\n",
      "        x = self.fc3(x)\n",
      "        x = self.pool(F.relu(self.conv3(x)))\n",
      "        x = self.fc4()\n",
      "        return x\n",
      "133/260:\n",
      "xs = [i for i in range(35)]\n",
      "total_lossest = []\n",
      "nett = Nett()\n",
      "criterion = nn.CrossEntropyLoss()\n",
      "optimizer = optim.SGD(nett.parameters(), lr=0.0005, momentum=0.9)\n",
      "for epoch in range(30):  # loop over the dataset multiple times: 43-44\n",
      "    running_loss = 0.0\n",
      "    total_loss = 0\n",
      "    for i, data in enumerate(trainloader, 0):\n",
      "        # get the inputs; data is a list of [inputs, labels]\n",
      "        inputs, labels = data\n",
      "\n",
      "        # zero the parameter gradients\n",
      "        optimizer.zero_grad()\n",
      "\n",
      "        # forward + backward + optimize\n",
      "        outputs = nett(inputs)\n",
      "        loss = criterion(outputs, labels)\n",
      "        loss.backward()\n",
      "        optimizer.step()\n",
      "\n",
      "        # print statistics\n",
      "        running_loss += loss.item()\n",
      "        total_loss += running_loss\n",
      "        if i % 2000 == 1999:    # print every 2000 mini-batches\n",
      "            print('[%d, %5d] loss: %.3f' %\n",
      "                  (epoch + 1, i + 1, running_loss / 2000))\n",
      "            running_loss = 0.0\n",
      "    total_lossest.append(total_loss)\n",
      "print('Finished Training')\n",
      "133/261:\n",
      "class Nett(nn.Module):\n",
      "    def __init__(self):\n",
      "        super(Nett, self).__init__()\n",
      "        self.conv1 = nn.Conv2d(3, 6, 5)\n",
      "        self.pool = nn.MaxPool2d(2, 2)\n",
      "        self.conv2 = nn.Conv2d(6, 12, 5, padding=9)\n",
      "        self.conv3 = nn.Conv2d(12, 16, 5)\n",
      "        self.fc1 = nn.Linear(16 * 5 * 5, 120)\n",
      "        self.fc2 = nn.Linear(120, 84)\n",
      "        self.fc3 = nn.Linear(84, 10)\n",
      "\n",
      "    def forward(self, x):\n",
      "        print(x.shape)\n",
      "        x = self.pool(F.relu(self.conv1(x)))\n",
      "        print(x.shape)\n",
      "        x = self.pool(F.relu(self.conv2(x)))\n",
      "        print(x.shape)\n",
      "        x = self.pool(F.relu(self.conv3(x)))\n",
      "        print(x.shape)\n",
      "        x = x.view(-1, 16 * 5 * 5)\n",
      "        x = F.relu(self.fc1(x))\n",
      "        x = F.relu(self.fc2(x))\n",
      "        x = self.fc3(x)\n",
      "        x = self.pool(F.relu(self.conv3(x)))\n",
      "        x = self.fc4()\n",
      "        return x\n",
      "133/262:\n",
      "xs = [i for i in range(35)]\n",
      "total_lossest = []\n",
      "nett = Nett()\n",
      "criterion = nn.CrossEntropyLoss()\n",
      "optimizer = optim.SGD(nett.parameters(), lr=0.0005, momentum=0.9)\n",
      "for epoch in range(30):  # loop over the dataset multiple times: 43-44\n",
      "    running_loss = 0.0\n",
      "    total_loss = 0\n",
      "    for i, data in enumerate(trainloader, 0):\n",
      "        # get the inputs; data is a list of [inputs, labels]\n",
      "        inputs, labels = data\n",
      "\n",
      "        # zero the parameter gradients\n",
      "        optimizer.zero_grad()\n",
      "\n",
      "        # forward + backward + optimize\n",
      "        outputs = nett(inputs)\n",
      "        loss = criterion(outputs, labels)\n",
      "        loss.backward()\n",
      "        optimizer.step()\n",
      "\n",
      "        # print statistics\n",
      "        running_loss += loss.item()\n",
      "        total_loss += running_loss\n",
      "        if i % 2000 == 1999:    # print every 2000 mini-batches\n",
      "            print('[%d, %5d] loss: %.3f' %\n",
      "                  (epoch + 1, i + 1, running_loss / 2000))\n",
      "            running_loss = 0.0\n",
      "    total_lossest.append(total_loss)\n",
      "print('Finished Training')\n",
      "133/263:\n",
      "class Nett(nn.Module):\n",
      "    def __init__(self):\n",
      "        super(Nett, self).__init__()\n",
      "        self.conv1 = nn.Conv2d(3, 6, 5)\n",
      "        self.pool = nn.MaxPool2d(2, 2)\n",
      "        self.conv2 = nn.Conv2d(6, 12, 5)\n",
      "        self.conv3 = nn.Conv2d(12, 16, 5)\n",
      "        self.fc1 = nn.Linear(16 * 5 * 5, 120)\n",
      "        self.fc2 = nn.Linear(120, 84)\n",
      "        self.fc3 = nn.Linear(84, 10)\n",
      "\n",
      "    def forward(self, x):\n",
      "        print(x.shape)\n",
      "        x = self.pool(F.relu(self.conv1(x)))\n",
      "        print(x.shape)\n",
      "        x = self.pool(F.relu(self.conv2(x)))\n",
      "        print(x.shape)\n",
      "        x = self.pool(F.relu(self.conv3(x)))\n",
      "        print(x.shape)\n",
      "        x = x.view(-1, 16 * 5 * 5)\n",
      "        x = F.relu(self.fc1(x))\n",
      "        x = F.relu(self.fc2(x))\n",
      "        x = self.fc3(x)\n",
      "        x = self.pool(F.relu(self.conv3(x)))\n",
      "        x = self.fc4()\n",
      "        return x\n",
      "133/264:\n",
      "xs = [i for i in range(35)]\n",
      "total_lossest = []\n",
      "nett = Nett()\n",
      "criterion = nn.CrossEntropyLoss()\n",
      "optimizer = optim.SGD(nett.parameters(), lr=0.0005, momentum=0.9)\n",
      "for epoch in range(30):  # loop over the dataset multiple times: 43-44\n",
      "    running_loss = 0.0\n",
      "    total_loss = 0\n",
      "    for i, data in enumerate(trainloader, 0):\n",
      "        # get the inputs; data is a list of [inputs, labels]\n",
      "        inputs, labels = data\n",
      "\n",
      "        # zero the parameter gradients\n",
      "        optimizer.zero_grad()\n",
      "\n",
      "        # forward + backward + optimize\n",
      "        outputs = nett(inputs)\n",
      "        loss = criterion(outputs, labels)\n",
      "        loss.backward()\n",
      "        optimizer.step()\n",
      "\n",
      "        # print statistics\n",
      "        running_loss += loss.item()\n",
      "        total_loss += running_loss\n",
      "        if i % 2000 == 1999:    # print every 2000 mini-batches\n",
      "            print('[%d, %5d] loss: %.3f' %\n",
      "                  (epoch + 1, i + 1, running_loss / 2000))\n",
      "            running_loss = 0.0\n",
      "    total_lossest.append(total_loss)\n",
      "print('Finished Training')\n",
      "133/265:\n",
      "class Nett(nn.Module):\n",
      "    def __init__(self):\n",
      "        super(Nett, self).__init__()\n",
      "        self.conv1 = nn.Conv2d(3, 6, 5)\n",
      "        self.pool = nn.MaxPool2d(2, 2)\n",
      "        self.conv2 = nn.Conv2d(6, 12, 5)\n",
      "        self.fc1 = nn.Linear(16 * 5 * 5, 120)\n",
      "        self.fc2 = nn.Linear(120, 84)\n",
      "        self.fc3 = nn.Linear(84, 10)\n",
      "\n",
      "    def forward(self, x):\n",
      "        print(x.shape)\n",
      "        x = self.pool(F.relu(self.conv1(x)))\n",
      "        print(x.shape)\n",
      "        x = self.pool(F.relu(self.conv2(x)))\n",
      "        print(x.shape)\n",
      "#         x = self.pool(F.relu(self.conv3(x)))\n",
      "        print(x.shape)\n",
      "        x = x.view(-1, 16 * 5 * 5)\n",
      "        x = F.relu(self.fc1(x))\n",
      "        x = F.relu(self.fc2(x))\n",
      "        x = self.fc3(x)\n",
      "        x = self.pool(F.relu(self.conv3(x)))\n",
      "        x = self.fc4()\n",
      "        return x\n",
      "133/266:\n",
      "xs = [i for i in range(35)]\n",
      "total_lossest = []\n",
      "nett = Nett()\n",
      "criterion = nn.CrossEntropyLoss()\n",
      "optimizer = optim.SGD(nett.parameters(), lr=0.0005, momentum=0.9)\n",
      "for epoch in range(30):  # loop over the dataset multiple times: 43-44\n",
      "    running_loss = 0.0\n",
      "    total_loss = 0\n",
      "    for i, data in enumerate(trainloader, 0):\n",
      "        # get the inputs; data is a list of [inputs, labels]\n",
      "        inputs, labels = data\n",
      "\n",
      "        # zero the parameter gradients\n",
      "        optimizer.zero_grad()\n",
      "\n",
      "        # forward + backward + optimize\n",
      "        outputs = nett(inputs)\n",
      "        loss = criterion(outputs, labels)\n",
      "        loss.backward()\n",
      "        optimizer.step()\n",
      "\n",
      "        # print statistics\n",
      "        running_loss += loss.item()\n",
      "        total_loss += running_loss\n",
      "        if i % 2000 == 1999:    # print every 2000 mini-batches\n",
      "            print('[%d, %5d] loss: %.3f' %\n",
      "                  (epoch + 1, i + 1, running_loss / 2000))\n",
      "            running_loss = 0.0\n",
      "    total_lossest.append(total_loss)\n",
      "print('Finished Training')\n",
      "133/267:\n",
      "class Nett(nn.Module):\n",
      "    def __init__(self):\n",
      "        super(Nett, self).__init__()\n",
      "        self.conv1 = nn.Conv2d(3, 6, 5)\n",
      "        self.pool = nn.MaxPool2d(2, 2)\n",
      "        self.conv2 = nn.Conv2d(6, 12, 5)\n",
      "        self.fc1 = nn.Linear(16 * 5 * 5, 120)\n",
      "        self.fc2 = nn.Linear(120, 84)\n",
      "        self.fc3 = nn.Linear(84, 10)\n",
      "\n",
      "    def forward(self, x):\n",
      "        print(x.shape)\n",
      "        x = self.pool(F.relu(self.conv1(x)))\n",
      "        print(x.shape)\n",
      "        x = self.pool(F.relu(self.conv2(x)))\n",
      "        print(x.shape)\n",
      "#         x = self.pool(F.relu(self.conv3(x)))\n",
      "        print(x.shape)\n",
      "        x = x.view(-1, 16 * 5 * 5)\n",
      "        x = F.relu(self.fc1(x))\n",
      "        x = F.relu(self.fc2(x))\n",
      "        x = self.fc3(x)\n",
      "        x = self.fc4()\n",
      "        return x\n",
      "133/268:\n",
      "xs = [i for i in range(35)]\n",
      "total_lossest = []\n",
      "nett = Nett()\n",
      "criterion = nn.CrossEntropyLoss()\n",
      "optimizer = optim.SGD(nett.parameters(), lr=0.0005, momentum=0.9)\n",
      "for epoch in range(30):  # loop over the dataset multiple times: 43-44\n",
      "    running_loss = 0.0\n",
      "    total_loss = 0\n",
      "    for i, data in enumerate(trainloader, 0):\n",
      "        # get the inputs; data is a list of [inputs, labels]\n",
      "        inputs, labels = data\n",
      "\n",
      "        # zero the parameter gradients\n",
      "        optimizer.zero_grad()\n",
      "\n",
      "        # forward + backward + optimize\n",
      "        outputs = nett(inputs)\n",
      "        loss = criterion(outputs, labels)\n",
      "        loss.backward()\n",
      "        optimizer.step()\n",
      "\n",
      "        # print statistics\n",
      "        running_loss += loss.item()\n",
      "        total_loss += running_loss\n",
      "        if i % 2000 == 1999:    # print every 2000 mini-batches\n",
      "            print('[%d, %5d] loss: %.3f' %\n",
      "                  (epoch + 1, i + 1, running_loss / 2000))\n",
      "            running_loss = 0.0\n",
      "    total_lossest.append(total_loss)\n",
      "print('Finished Training')\n",
      "133/269:\n",
      "class Nett(nn.Module):\n",
      "    def __init__(self):\n",
      "        super(Nett, self).__init__()\n",
      "        self.conv1 = nn.Conv2d(3, 6, 5)\n",
      "        self.pool = nn.MaxPool2d(2, 2)\n",
      "        self.conv2 = nn.Conv2d(6, 12, 5)\n",
      "        self.fc1 = nn.Linear(16 * 5 * 5, 120)\n",
      "        self.fc2 = nn.Linear(120, 84)\n",
      "        self.fc3 = nn.Linear(84, 10)\n",
      "\n",
      "    def forward(self, x):\n",
      "        print(x.shape)\n",
      "        x = self.pool(F.relu(self.conv1(x)))\n",
      "        print(x.shape)\n",
      "        x = self.pool(F.relu(self.conv2(x)))\n",
      "        print(x.shape)\n",
      "#         x = self.pool(F.relu(self.conv3(x)))\n",
      "        print(x.shape)\n",
      "        x = x.view(-1, 16 * 5 * 5)\n",
      "        x = F.relu(self.fc1(x))\n",
      "        x = F.relu(self.fc2(x))\n",
      "        x = self.fc3(x)\n",
      "        return x\n",
      "133/270:\n",
      "xs = [i for i in range(35)]\n",
      "total_lossest = []\n",
      "nett = Nett()\n",
      "criterion = nn.CrossEntropyLoss()\n",
      "optimizer = optim.SGD(nett.parameters(), lr=0.0005, momentum=0.9)\n",
      "for epoch in range(30):  # loop over the dataset multiple times: 43-44\n",
      "    running_loss = 0.0\n",
      "    total_loss = 0\n",
      "    for i, data in enumerate(trainloader, 0):\n",
      "        # get the inputs; data is a list of [inputs, labels]\n",
      "        inputs, labels = data\n",
      "\n",
      "        # zero the parameter gradients\n",
      "        optimizer.zero_grad()\n",
      "\n",
      "        # forward + backward + optimize\n",
      "        outputs = nett(inputs)\n",
      "        loss = criterion(outputs, labels)\n",
      "        loss.backward()\n",
      "        optimizer.step()\n",
      "\n",
      "        # print statistics\n",
      "        running_loss += loss.item()\n",
      "        total_loss += running_loss\n",
      "        if i % 2000 == 1999:    # print every 2000 mini-batches\n",
      "            print('[%d, %5d] loss: %.3f' %\n",
      "                  (epoch + 1, i + 1, running_loss / 2000))\n",
      "            running_loss = 0.0\n",
      "    total_lossest.append(total_loss)\n",
      "print('Finished Training')\n",
      "133/271:\n",
      "class Nett(nn.Module):\n",
      "    def __init__(self):\n",
      "        super(Nett, self).__init__()\n",
      "        self.conv1 = nn.Conv2d(3, 6, 5)\n",
      "        self.pool = nn.MaxPool2d(2, 2)\n",
      "        self.conv2 = nn.Conv2d(6, 16, 5)\n",
      "        self.fc1 = nn.Linear(16 * 5 * 5, 120)\n",
      "        self.fc2 = nn.Linear(120, 84)\n",
      "        self.fc3 = nn.Linear(84, 10)\n",
      "\n",
      "    def forward(self, x):\n",
      "        print(x.shape)\n",
      "        x = self.pool(F.relu(self.conv1(x)))\n",
      "        print(x.shape)\n",
      "        x = self.pool(F.relu(self.conv2(x)))\n",
      "        print(x.shape)\n",
      "#         x = self.pool(F.relu(self.conv3(x)))\n",
      "        print(x.shape)\n",
      "        x = x.view(-1, 16 * 5 * 5)\n",
      "        x = F.relu(self.fc1(x))\n",
      "        x = F.relu(self.fc2(x))\n",
      "        x = self.fc3(x)\n",
      "        return x\n",
      "133/272:\n",
      "xs = [i for i in range(35)]\n",
      "total_lossest = []\n",
      "nett = Nett()\n",
      "criterion = nn.CrossEntropyLoss()\n",
      "optimizer = optim.SGD(nett.parameters(), lr=0.0005, momentum=0.9)\n",
      "for epoch in range(30):  # loop over the dataset multiple times: 43-44\n",
      "    running_loss = 0.0\n",
      "    total_loss = 0\n",
      "    for i, data in enumerate(trainloader, 0):\n",
      "        # get the inputs; data is a list of [inputs, labels]\n",
      "        inputs, labels = data\n",
      "\n",
      "        # zero the parameter gradients\n",
      "        optimizer.zero_grad()\n",
      "\n",
      "        # forward + backward + optimize\n",
      "        outputs = nett(inputs)\n",
      "        loss = criterion(outputs, labels)\n",
      "        loss.backward()\n",
      "        optimizer.step()\n",
      "\n",
      "        # print statistics\n",
      "        running_loss += loss.item()\n",
      "        total_loss += running_loss\n",
      "        if i % 2000 == 1999:    # print every 2000 mini-batches\n",
      "            print('[%d, %5d] loss: %.3f' %\n",
      "                  (epoch + 1, i + 1, running_loss / 2000))\n",
      "            running_loss = 0.0\n",
      "    total_lossest.append(total_loss)\n",
      "print('Finished Training')\n",
      "133/273:\n",
      "class Nett(nn.Module):\n",
      "    def __init__(self):\n",
      "        super(Nett, self).__init__()\n",
      "        self.conv1 = nn.Conv2d(3, 6, 5)\n",
      "        self.pool = nn.MaxPool2d(2, 2)\n",
      "        self.conv2 = nn.Conv2d(6, 12, 5)\n",
      "        self.conv3 = nn.Conv2d(12, 16, 5)\n",
      "        self.fc1 = nn.Linear(16 * 5 * 5, 120)\n",
      "        self.fc2 = nn.Linear(120, 84)\n",
      "        self.fc3 = nn.Linear(84, 10)\n",
      "\n",
      "    def forward(self, x):\n",
      "        print(x.shape)\n",
      "        x = self.pool(F.relu(self.conv1(x)))\n",
      "        print(x.shape)\n",
      "        x = self.pool(F.relu(self.conv2(x)))\n",
      "        print(x.shape)\n",
      "        x = F.relu(self.conv3(x))\n",
      "        print(x.shape)\n",
      "        x = x.view(-1, 16 * 5 * 5)\n",
      "        x = F.relu(self.fc1(x))\n",
      "        x = F.relu(self.fc2(x))\n",
      "        x = self.fc3(x)\n",
      "        return x\n",
      "133/274:\n",
      "xs = [i for i in range(35)]\n",
      "total_lossest = []\n",
      "nett = Nett()\n",
      "criterion = nn.CrossEntropyLoss()\n",
      "optimizer = optim.SGD(nett.parameters(), lr=0.0005, momentum=0.9)\n",
      "for epoch in range(30):  # loop over the dataset multiple times: 43-44\n",
      "    running_loss = 0.0\n",
      "    total_loss = 0\n",
      "    for i, data in enumerate(trainloader, 0):\n",
      "        # get the inputs; data is a list of [inputs, labels]\n",
      "        inputs, labels = data\n",
      "\n",
      "        # zero the parameter gradients\n",
      "        optimizer.zero_grad()\n",
      "\n",
      "        # forward + backward + optimize\n",
      "        outputs = nett(inputs)\n",
      "        loss = criterion(outputs, labels)\n",
      "        loss.backward()\n",
      "        optimizer.step()\n",
      "\n",
      "        # print statistics\n",
      "        running_loss += loss.item()\n",
      "        total_loss += running_loss\n",
      "        if i % 2000 == 1999:    # print every 2000 mini-batches\n",
      "            print('[%d, %5d] loss: %.3f' %\n",
      "                  (epoch + 1, i + 1, running_loss / 2000))\n",
      "            running_loss = 0.0\n",
      "    total_lossest.append(total_loss)\n",
      "print('Finished Training')\n",
      "133/275:\n",
      "class Nett(nn.Module):\n",
      "    def __init__(self):\n",
      "        super(Nett, self).__init__()\n",
      "        self.conv1 = nn.Conv2d(3, 6, 5)\n",
      "        self.pool = nn.MaxPool2d(2, 2)\n",
      "        self.conv2 = nn.Conv2d(6, 12, 5, padding=9)\n",
      "        self.conv3 = nn.Conv2d(12, 16, 5)\n",
      "        self.fc1 = nn.Linear(16 * 5 * 5, 120)\n",
      "        self.fc2 = nn.Linear(120, 84)\n",
      "        self.fc3 = nn.Linear(84, 10)\n",
      "\n",
      "    def forward(self, x):\n",
      "        print(x.shape)\n",
      "        x = self.pool(F.relu(self.conv1(x)))\n",
      "        print(x.shape)\n",
      "        x = self.pool(F.relu(self.conv2(x)))\n",
      "        print(x.shape)\n",
      "        x = F.relu(self.conv3(x))\n",
      "        print(x.shape)\n",
      "        x = x.view(-1, 16 * 5 * 5)\n",
      "        x = F.relu(self.fc1(x))\n",
      "        x = F.relu(self.fc2(x))\n",
      "        x = self.fc3(x)\n",
      "        return x\n",
      "133/276:\n",
      "xs = [i for i in range(35)]\n",
      "total_lossest = []\n",
      "nett = Nett()\n",
      "criterion = nn.CrossEntropyLoss()\n",
      "optimizer = optim.SGD(nett.parameters(), lr=0.0005, momentum=0.9)\n",
      "for epoch in range(30):  # loop over the dataset multiple times: 43-44\n",
      "    running_loss = 0.0\n",
      "    total_loss = 0\n",
      "    for i, data in enumerate(trainloader, 0):\n",
      "        # get the inputs; data is a list of [inputs, labels]\n",
      "        inputs, labels = data\n",
      "\n",
      "        # zero the parameter gradients\n",
      "        optimizer.zero_grad()\n",
      "\n",
      "        # forward + backward + optimize\n",
      "        outputs = nett(inputs)\n",
      "        loss = criterion(outputs, labels)\n",
      "        loss.backward()\n",
      "        optimizer.step()\n",
      "\n",
      "        # print statistics\n",
      "        running_loss += loss.item()\n",
      "        total_loss += running_loss\n",
      "        if i % 2000 == 1999:    # print every 2000 mini-batches\n",
      "            print('[%d, %5d] loss: %.3f' %\n",
      "                  (epoch + 1, i + 1, running_loss / 2000))\n",
      "            running_loss = 0.0\n",
      "    total_lossest.append(total_loss)\n",
      "print('Finished Training')\n",
      "133/277:\n",
      "class Nett(nn.Module):\n",
      "    def __init__(self):\n",
      "        super(Nett, self).__init__()\n",
      "        self.conv1 = nn.Conv2d(3, 6, 5)\n",
      "        self.pool = nn.MaxPool2d(2, 2)\n",
      "        self.conv2 = nn.Conv2d(6, 16, 5, padding=9)\n",
      "#         self.conv3 = nn.Conv2d(12, 16, 5)\n",
      "        self.fc1 = nn.Linear(16 * 5 * 5, 120)\n",
      "        self.fc2 = nn.Linear(120, 84)\n",
      "        self.fc3 = nn.Linear(84, 10)\n",
      "\n",
      "    def forward(self, x):\n",
      "        print(x.shape)\n",
      "        x = self.pool(F.relu(self.conv1(x)))\n",
      "        print(x.shape)\n",
      "        x = self.pool(F.relu(self.conv2(x)))\n",
      "        print(x.shape)\n",
      "#         x = F.relu(self.conv3(x))\n",
      "        print(x.shape)\n",
      "        x = x.view(-1, 16 * 5 * 5)\n",
      "        x = F.relu(self.fc1(x))\n",
      "        x = F.relu(self.fc2(x))\n",
      "        x = self.fc3(x)\n",
      "        return x\n",
      "133/278:\n",
      "xs = [i for i in range(35)]\n",
      "total_lossest = []\n",
      "nett = Nett()\n",
      "criterion = nn.CrossEntropyLoss()\n",
      "optimizer = optim.SGD(nett.parameters(), lr=0.0005, momentum=0.9)\n",
      "for epoch in range(30):  # loop over the dataset multiple times: 43-44\n",
      "    running_loss = 0.0\n",
      "    total_loss = 0\n",
      "    for i, data in enumerate(trainloader, 0):\n",
      "        # get the inputs; data is a list of [inputs, labels]\n",
      "        inputs, labels = data\n",
      "\n",
      "        # zero the parameter gradients\n",
      "        optimizer.zero_grad()\n",
      "\n",
      "        # forward + backward + optimize\n",
      "        outputs = nett(inputs)\n",
      "        loss = criterion(outputs, labels)\n",
      "        loss.backward()\n",
      "        optimizer.step()\n",
      "\n",
      "        # print statistics\n",
      "        running_loss += loss.item()\n",
      "        total_loss += running_loss\n",
      "        if i % 2000 == 1999:    # print every 2000 mini-batches\n",
      "            print('[%d, %5d] loss: %.3f' %\n",
      "                  (epoch + 1, i + 1, running_loss / 2000))\n",
      "            running_loss = 0.0\n",
      "    total_lossest.append(total_loss)\n",
      "print('Finished Training')\n",
      "133/279:\n",
      "class Nett(nn.Module):\n",
      "    def __init__(self):\n",
      "        super(Nett, self).__init__()\n",
      "        self.conv1 = nn.Conv2d(3, 6, 5)\n",
      "        self.pool = nn.MaxPool2d(2, 2)\n",
      "        self.conv2 = nn.Conv2d(6, 16, 5)\n",
      "#         self.conv3 = nn.Conv2d(12, 16, 5)\n",
      "        self.fc1 = nn.Linear(16 * 5 * 5, 120)\n",
      "        self.fc2 = nn.Linear(120, 84)\n",
      "        self.fc3 = nn.Linear(84, 10)\n",
      "\n",
      "    def forward(self, x):\n",
      "        print(x.shape)\n",
      "        x = self.pool(F.relu(self.conv1(x)))\n",
      "        print(x.shape)\n",
      "        x = self.pool(F.relu(self.conv2(x)))\n",
      "        print(x.shape)\n",
      "#         x = F.relu(self.conv3(x))\n",
      "        print(x.shape)\n",
      "        x = x.view(-1, 16 * 5 * 5)\n",
      "        x = F.relu(self.fc1(x))\n",
      "        x = F.relu(self.fc2(x))\n",
      "        x = self.fc3(x)\n",
      "        return x\n",
      "133/280:\n",
      "xs = [i for i in range(35)]\n",
      "total_lossest = []\n",
      "nett = Nett()\n",
      "criterion = nn.CrossEntropyLoss()\n",
      "optimizer = optim.SGD(nett.parameters(), lr=0.0005, momentum=0.9)\n",
      "for epoch in range(30):  # loop over the dataset multiple times: 43-44\n",
      "    running_loss = 0.0\n",
      "    total_loss = 0\n",
      "    for i, data in enumerate(trainloader, 0):\n",
      "        # get the inputs; data is a list of [inputs, labels]\n",
      "        inputs, labels = data\n",
      "\n",
      "        # zero the parameter gradients\n",
      "        optimizer.zero_grad()\n",
      "\n",
      "        # forward + backward + optimize\n",
      "        outputs = nett(inputs)\n",
      "        loss = criterion(outputs, labels)\n",
      "        loss.backward()\n",
      "        optimizer.step()\n",
      "\n",
      "        # print statistics\n",
      "        running_loss += loss.item()\n",
      "        total_loss += running_loss\n",
      "        if i % 2000 == 1999:    # print every 2000 mini-batches\n",
      "            print('[%d, %5d] loss: %.3f' %\n",
      "                  (epoch + 1, i + 1, running_loss / 2000))\n",
      "            running_loss = 0.0\n",
      "    total_lossest.append(total_loss)\n",
      "print('Finished Training')\n",
      "133/281:\n",
      "class Nett(nn.Module):\n",
      "    def __init__(self):\n",
      "        super(Nett, self).__init__()\n",
      "        self.conv1 = nn.Conv2d(3, 6, 5)\n",
      "        self.pool = nn.MaxPool2d(2, 2)\n",
      "        self.conv2 = nn.Conv2d(6, 12, 5, padding=9)\n",
      "        self.conv3 = nn.Conv2d(12, 16, 5)\n",
      "        self.fc1 = nn.Linear(16 * 5 * 5, 120)\n",
      "        self.fc2 = nn.Linear(120, 84)\n",
      "        self.fc3 = nn.Linear(84, 10)\n",
      "\n",
      "    def forward(self, x):\n",
      "        print(x.shape)\n",
      "        x = self.pool(F.relu(self.conv1(x)))\n",
      "        print(x.shape)\n",
      "        x = self.pool(F.relu(self.conv2(x)))\n",
      "        print(x.shape)\n",
      "        x = F.relu(self.conv3(x))\n",
      "        print(x.shape)\n",
      "        x = x.view(-1, 16 * 5 * 5)\n",
      "        x = F.relu(self.fc1(x))\n",
      "        x = F.relu(self.fc2(x))\n",
      "        x = self.fc3(x)\n",
      "        return x\n",
      "133/282:\n",
      "xs = [i for i in range(35)]\n",
      "total_lossest = []\n",
      "nett = Nett()\n",
      "criterion = nn.CrossEntropyLoss()\n",
      "optimizer = optim.SGD(nett.parameters(), lr=0.0005, momentum=0.9)\n",
      "for epoch in range(30):  # loop over the dataset multiple times: 43-44\n",
      "    running_loss = 0.0\n",
      "    total_loss = 0\n",
      "    for i, data in enumerate(trainloader, 0):\n",
      "        # get the inputs; data is a list of [inputs, labels]\n",
      "        inputs, labels = data\n",
      "\n",
      "        # zero the parameter gradients\n",
      "        optimizer.zero_grad()\n",
      "\n",
      "        # forward + backward + optimize\n",
      "        outputs = nett(inputs)\n",
      "        loss = criterion(outputs, labels)\n",
      "        loss.backward()\n",
      "        optimizer.step()\n",
      "\n",
      "        # print statistics\n",
      "        running_loss += loss.item()\n",
      "        total_loss += running_loss\n",
      "        if i % 2000 == 1999:    # print every 2000 mini-batches\n",
      "            print('[%d, %5d] loss: %.3f' %\n",
      "                  (epoch + 1, i + 1, running_loss / 2000))\n",
      "            running_loss = 0.0\n",
      "    total_lossest.append(total_loss)\n",
      "print('Finished Training')\n",
      "133/283:\n",
      "class Nett(nn.Module):\n",
      "    def __init__(self):\n",
      "        super(Nett, self).__init__()\n",
      "        self.conv1 = nn.Conv2d(3, 6, 5)\n",
      "        self.pool = nn.MaxPool2d(2, 2)\n",
      "        self.conv2 = nn.Conv2d(6, 12, 5)\n",
      "        self.conv3 = nn.Conv2d(12, 16, 5)\n",
      "        self.fc1 = nn.Linear(16 * 5 * 5, 120)\n",
      "        self.fc2 = nn.Linear(120, 84)\n",
      "        self.fc3 = nn.Linear(84, 10)\n",
      "\n",
      "    def forward(self, x):\n",
      "        print(x.shape)\n",
      "        x = self.pool(F.relu(self.conv1(x)))\n",
      "        print(x.shape)\n",
      "        x = self.pool(F.relu(self.conv2(x)))\n",
      "        print(x.shape)\n",
      "        x = F.relu(self.conv3(x))\n",
      "        print(x.shape)\n",
      "        x = x.view(-1, 16 * 5 * 5)\n",
      "        x = F.relu(self.fc1(x))\n",
      "        x = F.relu(self.fc2(x))\n",
      "        x = self.fc3(x)\n",
      "        return x\n",
      "133/284:\n",
      "xs = [i for i in range(35)]\n",
      "total_lossest = []\n",
      "nett = Nett()\n",
      "criterion = nn.CrossEntropyLoss()\n",
      "optimizer = optim.SGD(nett.parameters(), lr=0.0005, momentum=0.9)\n",
      "for epoch in range(30):  # loop over the dataset multiple times: 43-44\n",
      "    running_loss = 0.0\n",
      "    total_loss = 0\n",
      "    for i, data in enumerate(trainloader, 0):\n",
      "        # get the inputs; data is a list of [inputs, labels]\n",
      "        inputs, labels = data\n",
      "\n",
      "        # zero the parameter gradients\n",
      "        optimizer.zero_grad()\n",
      "\n",
      "        # forward + backward + optimize\n",
      "        outputs = nett(inputs)\n",
      "        loss = criterion(outputs, labels)\n",
      "        loss.backward()\n",
      "        optimizer.step()\n",
      "\n",
      "        # print statistics\n",
      "        running_loss += loss.item()\n",
      "        total_loss += running_loss\n",
      "        if i % 2000 == 1999:    # print every 2000 mini-batches\n",
      "            print('[%d, %5d] loss: %.3f' %\n",
      "                  (epoch + 1, i + 1, running_loss / 2000))\n",
      "            running_loss = 0.0\n",
      "    total_lossest.append(total_loss)\n",
      "print('Finished Training')\n",
      "133/285:\n",
      "class Nett(nn.Module):\n",
      "    def __init__(self):\n",
      "        super(Nett, self).__init__()\n",
      "        self.conv1 = nn.Conv2d(3, 6, 5)\n",
      "        self.pool = nn.MaxPool2d(2, 2)\n",
      "        self.conv2 = nn.Conv2d(6, 12, 5)\n",
      "        self.conv3 = nn.Conv2d(12, 16, 5, padding=4)\n",
      "        self.fc1 = nn.Linear(16 * 5 * 5, 120)\n",
      "        self.fc2 = nn.Linear(120, 84)\n",
      "        self.fc3 = nn.Linear(84, 10)\n",
      "\n",
      "    def forward(self, x):\n",
      "        print(x.shape)\n",
      "        x = self.pool(F.relu(self.conv1(x)))\n",
      "        print(x.shape)\n",
      "        x = self.pool(F.relu(self.conv2(x)))\n",
      "        print(x.shape)\n",
      "        x = F.relu(self.conv3(x))\n",
      "        print(x.shape)\n",
      "        x = x.view(-1, 16 * 5 * 5)\n",
      "        x = F.relu(self.fc1(x))\n",
      "        x = F.relu(self.fc2(x))\n",
      "        x = self.fc3(x)\n",
      "        return x\n",
      "133/286:\n",
      "xs = [i for i in range(35)]\n",
      "total_lossest = []\n",
      "nett = Nett()\n",
      "criterion = nn.CrossEntropyLoss()\n",
      "optimizer = optim.SGD(nett.parameters(), lr=0.0005, momentum=0.9)\n",
      "for epoch in range(30):  # loop over the dataset multiple times: 43-44\n",
      "    running_loss = 0.0\n",
      "    total_loss = 0\n",
      "    for i, data in enumerate(trainloader, 0):\n",
      "        # get the inputs; data is a list of [inputs, labels]\n",
      "        inputs, labels = data\n",
      "\n",
      "        # zero the parameter gradients\n",
      "        optimizer.zero_grad()\n",
      "\n",
      "        # forward + backward + optimize\n",
      "        outputs = nett(inputs)\n",
      "        loss = criterion(outputs, labels)\n",
      "        loss.backward()\n",
      "        optimizer.step()\n",
      "\n",
      "        # print statistics\n",
      "        running_loss += loss.item()\n",
      "        total_loss += running_loss\n",
      "        if i % 2000 == 1999:    # print every 2000 mini-batches\n",
      "            print('[%d, %5d] loss: %.3f' %\n",
      "                  (epoch + 1, i + 1, running_loss / 2000))\n",
      "            running_loss = 0.0\n",
      "    total_lossest.append(total_loss)\n",
      "print('Finished Training')\n",
      "133/287:\n",
      "class Nett(nn.Module):\n",
      "    def __init__(self):\n",
      "        super(Nett, self).__init__()\n",
      "        self.conv1 = nn.Conv2d(3, 6, 5)\n",
      "        self.pool = nn.MaxPool2d(2, 2)\n",
      "        self.conv2 = nn.Conv2d(6, 12, 5)\n",
      "        self.conv3 = nn.Conv2d(12, 16, 5, padding=2)\n",
      "        self.fc1 = nn.Linear(16 * 5 * 5, 120)\n",
      "        self.fc2 = nn.Linear(120, 84)\n",
      "        self.fc3 = nn.Linear(84, 10)\n",
      "\n",
      "    def forward(self, x):\n",
      "        print(x.shape)\n",
      "        x = self.pool(F.relu(self.conv1(x)))\n",
      "        print(x.shape)\n",
      "        x = self.pool(F.relu(self.conv2(x)))\n",
      "        print(x.shape)\n",
      "        x = F.relu(self.conv3(x))\n",
      "        print(x.shape)\n",
      "        x = x.view(-1, 16 * 5 * 5)\n",
      "        x = F.relu(self.fc1(x))\n",
      "        x = F.relu(self.fc2(x))\n",
      "        x = self.fc3(x)\n",
      "        return x\n",
      "133/288:\n",
      "xs = [i for i in range(35)]\n",
      "total_lossest = []\n",
      "nett = Nett()\n",
      "criterion = nn.CrossEntropyLoss()\n",
      "optimizer = optim.SGD(nett.parameters(), lr=0.0005, momentum=0.9)\n",
      "for epoch in range(30):  # loop over the dataset multiple times: 43-44\n",
      "    running_loss = 0.0\n",
      "    total_loss = 0\n",
      "    for i, data in enumerate(trainloader, 0):\n",
      "        # get the inputs; data is a list of [inputs, labels]\n",
      "        inputs, labels = data\n",
      "\n",
      "        # zero the parameter gradients\n",
      "        optimizer.zero_grad()\n",
      "\n",
      "        # forward + backward + optimize\n",
      "        outputs = nett(inputs)\n",
      "        loss = criterion(outputs, labels)\n",
      "        loss.backward()\n",
      "        optimizer.step()\n",
      "\n",
      "        # print statistics\n",
      "        running_loss += loss.item()\n",
      "        total_loss += running_loss\n",
      "        if i % 2000 == 1999:    # print every 2000 mini-batches\n",
      "            print('[%d, %5d] loss: %.3f' %\n",
      "                  (epoch + 1, i + 1, running_loss / 2000))\n",
      "            running_loss = 0.0\n",
      "    total_lossest.append(total_loss)\n",
      "print('Finished Training')\n",
      "133/289:\n",
      "class Nett(nn.Module):\n",
      "    def __init__(self):\n",
      "        super(Nett, self).__init__()\n",
      "        self.conv1 = nn.Conv2d(3, 6, 5)\n",
      "        self.pool = nn.MaxPool2d(2, 2)\n",
      "        self.conv2 = nn.Conv2d(6, 12, 5)\n",
      "        self.conv3 = nn.Conv2d(12, 16, 5, padding=2)\n",
      "        self.fc1 = nn.Linear(16 * 5 * 5, 120)\n",
      "        self.fc2 = nn.Linear(120, 84)\n",
      "        self.fc3 = nn.Linear(84, 10)\n",
      "\n",
      "    def forward(self, x):\n",
      "#         print(x.shape)\n",
      "        x = self.pool(F.relu(self.conv1(x)))\n",
      "#         print(x.shape)\n",
      "        x = self.pool(F.relu(self.conv2(x)))\n",
      "#         print(x.shape)\n",
      "        x = F.relu(self.conv3(x))\n",
      "#         print(x.shape)\n",
      "        x = x.view(-1, 16 * 5 * 5)\n",
      "        x = F.relu(self.fc1(x))\n",
      "        x = F.relu(self.fc2(x))\n",
      "        x = self.fc3(x)\n",
      "        return x\n",
      "133/290:\n",
      "xs = [i for i in range(35)]\n",
      "total_lossest = []\n",
      "nett = Nett()\n",
      "criterion = nn.CrossEntropyLoss()\n",
      "optimizer = optim.SGD(nett.parameters(), lr=0.0005, momentum=0.9)\n",
      "for epoch in range(30):  # loop over the dataset multiple times: 43-44\n",
      "    running_loss = 0.0\n",
      "    total_loss = 0\n",
      "    for i, data in enumerate(trainloader, 0):\n",
      "        # get the inputs; data is a list of [inputs, labels]\n",
      "        inputs, labels = data\n",
      "\n",
      "        # zero the parameter gradients\n",
      "        optimizer.zero_grad()\n",
      "\n",
      "        # forward + backward + optimize\n",
      "        outputs = nett(inputs)\n",
      "        loss = criterion(outputs, labels)\n",
      "        loss.backward()\n",
      "        optimizer.step()\n",
      "\n",
      "        # print statistics\n",
      "        running_loss += loss.item()\n",
      "        total_loss += running_loss\n",
      "        if i % 2000 == 1999:    # print every 2000 mini-batches\n",
      "            print('[%d, %5d] loss: %.3f' %\n",
      "                  (epoch + 1, i + 1, running_loss / 2000))\n",
      "            running_loss = 0.0\n",
      "    total_lossest.append(total_loss)\n",
      "print('Finished Training')\n",
      "135/104:\n",
      "class Nett(nn.Module):\n",
      "    def __init__(self):\n",
      "        super(Nett, self).__init__()\n",
      "        self.conv1 = nn.Conv2d(3, 6, 5)\n",
      "        self.pool = nn.MaxPool2d(2, 2)\n",
      "        self.conv2 = nn.Conv2d(6, 12, 5)\n",
      "        self.conv3 = nn.Conv2d(12, 24, 5, padding=2)\n",
      "        self.conv4 = nn.Conv2d(24, 16, 5, padding=2)\n",
      "        self.fc1 = nn.Linear(16 * 5 * 5, 120)\n",
      "        self.fc2 = nn.Linear(120, 84)\n",
      "        self.fc3 = nn.Linear(84, 10)\n",
      "\n",
      "    def forward(self, x):\n",
      "#         print(x.shape)\n",
      "        x = self.pool(F.relu(self.conv1(x)))\n",
      "#         print(x.shape)\n",
      "        x = self.pool(F.relu(self.conv2(x)))\n",
      "#         print(x.shape)\n",
      "        x = F.relu(self.conv3(x))\n",
      "        x = F.relu(self.conv4(x))\n",
      "#         print(x.shape)\n",
      "        x = x.view(-1, 16 * 5 * 5)\n",
      "        x = F.relu(self.fc1(x))\n",
      "        x = F.relu(self.fc2(x))\n",
      "        x = self.fc3(x)\n",
      "        return x\n",
      "    \n",
      "xs = [i for i in range(35)]\n",
      "total_lossest = []\n",
      "nett = Nett()\n",
      "criterion = nn.CrossEntropyLoss()\n",
      "optimizer = optim.SGD(nett.parameters(), lr=0.0005, momentum=0.9)\n",
      "for epoch in range(30):  # loop over the dataset multiple times: 43-44\n",
      "    running_loss = 0.0\n",
      "    total_loss = 0\n",
      "    for i, data in enumerate(trainloader, 0):\n",
      "        # get the inputs; data is a list of [inputs, labels]\n",
      "        inputs, labels = data\n",
      "\n",
      "        # zero the parameter gradients\n",
      "        optimizer.zero_grad()\n",
      "\n",
      "        # forward + backward + optimize\n",
      "        outputs = nett(inputs)\n",
      "        loss = criterion(outputs, labels)\n",
      "        loss.backward()\n",
      "        optimizer.step()\n",
      "\n",
      "        # print statistics\n",
      "        running_loss += loss.item()\n",
      "        total_loss += running_loss\n",
      "        if i % 2000 == 1999:    # print every 2000 mini-batches\n",
      "            print('[%d, %5d] loss: %.3f' %\n",
      "                  (epoch + 1, i + 1, running_loss / 2000))\n",
      "            running_loss = 0.0\n",
      "    total_lossest.append(total_loss)\n",
      "print('Finished Training')\n",
      "135/105:\n",
      "class Nett(nn.Module):\n",
      "    def __init__(self):\n",
      "        super(Nett, self).__init__()\n",
      "        self.conv1 = nn.Conv2d(3, 6, 5)\n",
      "        self.pool = nn.MaxPool2d(2, 2)\n",
      "        self.conv2 = nn.Conv2d(6, 12, 5)\n",
      "        self.conv3 = nn.Conv2d(12, 24, 5, padding=2)\n",
      "        self.conv4 = nn.Conv2d(24, 48, 5, padding=2)\n",
      "        self.conv4 = nn.Conv2d(48, 16, 5, padding=2)\n",
      "        self.fc1 = nn.Linear(16 * 5 * 5, 120)\n",
      "        self.fc2 = nn.Linear(120, 84)\n",
      "        self.fc3 = nn.Linear(84, 10)\n",
      "\n",
      "    def forward(self, x):\n",
      "#         print(x.shape)\n",
      "        x = self.pool(F.relu(self.conv1(x)))\n",
      "#         print(x.shape)\n",
      "        x = self.pool(F.relu(self.conv2(x)))\n",
      "#         print(x.shape)\n",
      "        x = F.relu(self.conv3(x))\n",
      "        x = F.relu(self.conv4(x))\n",
      "        x = F.relu(self.conv5(x))\n",
      "#         print(x.shape)\n",
      "        x = x.view(-1, 16 * 5 * 5)\n",
      "        x = F.relu(self.fc1(x))\n",
      "        x = F.relu(self.fc2(x))\n",
      "        x = self.fc3(x)\n",
      "        return x\n",
      "    \n",
      "xs = [i for i in range(35)]\n",
      "total_lossest = []\n",
      "nett = Nett()\n",
      "criterion = nn.CrossEntropyLoss()\n",
      "optimizer = optim.SGD(nett.parameters(), lr=0.0005, momentum=0.9)\n",
      "for epoch in range(30):  # loop over the dataset multiple times: 43-44\n",
      "    running_loss = 0.0\n",
      "    total_loss = 0\n",
      "    for i, data in enumerate(trainloader, 0):\n",
      "        # get the inputs; data is a list of [inputs, labels]\n",
      "        inputs, labels = data\n",
      "\n",
      "        # zero the parameter gradients\n",
      "        optimizer.zero_grad()\n",
      "\n",
      "        # forward + backward + optimize\n",
      "        outputs = nett(inputs)\n",
      "        loss = criterion(outputs, labels)\n",
      "        loss.backward()\n",
      "        optimizer.step()\n",
      "\n",
      "        # print statistics\n",
      "        running_loss += loss.item()\n",
      "        total_loss += running_loss\n",
      "        if i % 2000 == 1999:    # print every 2000 mini-batches\n",
      "            print('[%d, %5d] loss: %.3f' %\n",
      "                  (epoch + 1, i + 1, running_loss / 2000))\n",
      "            running_loss = 0.0\n",
      "    total_lossest.append(total_loss)\n",
      "print('Finished Training')\n",
      "135/106:\n",
      "class Nett(nn.Module):\n",
      "    def __init__(self):\n",
      "        super(Nett, self).__init__()\n",
      "        self.conv1 = nn.Conv2d(3, 6, 5)\n",
      "        self.pool = nn.MaxPool2d(2, 2)\n",
      "        self.conv2 = nn.Conv2d(6, 12, 5)\n",
      "        self.conv3 = nn.Conv2d(12, 24, 5, padding=2)\n",
      "        self.conv4 = nn.Conv2d(24, 48, 5, padding=2)\n",
      "        self.conv5 = nn.Conv2d(48, 16, 5, padding=2)\n",
      "        self.fc1 = nn.Linear(16 * 5 * 5, 120)\n",
      "        self.fc2 = nn.Linear(120, 84)\n",
      "        self.fc3 = nn.Linear(84, 10)\n",
      "\n",
      "    def forward(self, x):\n",
      "#         print(x.shape)\n",
      "        x = self.pool(F.relu(self.conv1(x)))\n",
      "#         print(x.shape)\n",
      "        x = self.pool(F.relu(self.conv2(x)))\n",
      "#         print(x.shape)\n",
      "        x = F.relu(self.conv3(x))\n",
      "        x = F.relu(self.conv4(x))\n",
      "        x = F.relu(self.conv5(x))\n",
      "#         print(x.shape)\n",
      "        x = x.view(-1, 16 * 5 * 5)\n",
      "        x = F.relu(self.fc1(x))\n",
      "        x = F.relu(self.fc2(x))\n",
      "        x = self.fc3(x)\n",
      "        return x\n",
      "    \n",
      "xs = [i for i in range(35)]\n",
      "total_lossest = []\n",
      "nett = Nett()\n",
      "criterion = nn.CrossEntropyLoss()\n",
      "optimizer = optim.SGD(nett.parameters(), lr=0.0005, momentum=0.9)\n",
      "for epoch in range(30):  # loop over the dataset multiple times: 43-44\n",
      "    running_loss = 0.0\n",
      "    total_loss = 0\n",
      "    for i, data in enumerate(trainloader, 0):\n",
      "        # get the inputs; data is a list of [inputs, labels]\n",
      "        inputs, labels = data\n",
      "\n",
      "        # zero the parameter gradients\n",
      "        optimizer.zero_grad()\n",
      "\n",
      "        # forward + backward + optimize\n",
      "        outputs = nett(inputs)\n",
      "        loss = criterion(outputs, labels)\n",
      "        loss.backward()\n",
      "        optimizer.step()\n",
      "\n",
      "        # print statistics\n",
      "        running_loss += loss.item()\n",
      "        total_loss += running_loss\n",
      "        if i % 2000 == 1999:    # print every 2000 mini-batches\n",
      "            print('[%d, %5d] loss: %.3f' %\n",
      "                  (epoch + 1, i + 1, running_loss / 2000))\n",
      "            running_loss = 0.0\n",
      "    total_lossest.append(total_loss)\n",
      "print('Finished Training')\n",
      "135/107:\n",
      "class Nett(nn.Module):\n",
      "    def __init__(self):\n",
      "        super(Nett, self).__init__()\n",
      "        self.conv1 = nn.Conv2d(3, 6, 5)\n",
      "        self.pool = nn.MaxPool2d(2, 2)\n",
      "        self.conv2 = nn.Conv2d(6, 12, 5)\n",
      "        self.conv3 = nn.Conv2d(12, 24, 5, padding=2)\n",
      "        self.conv4 = nn.Conv2d(24, 16, 5, padding=2)\n",
      "        self.fc1 = nn.Linear(16 * 5 * 5, 120)\n",
      "        self.fc2 = nn.Linear(120, 84)\n",
      "        self.fc3 = nn.Linear(84, 10)\n",
      "\n",
      "    def forward(self, x):\n",
      "#         print(x.shape)\n",
      "        x = self.pool(F.relu(self.conv1(x)))\n",
      "#         print(x.shape)\n",
      "        x = self.pool(F.relu(self.conv2(x)))\n",
      "#         print(x.shape)\n",
      "        x = F.relu(self.conv3(x))\n",
      "        x = F.relu(self.conv4(x))\n",
      "#         print(x.shape)\n",
      "        x = x.view(-1, 16 * 5 * 5)\n",
      "        x = F.relu(self.fc1(x))\n",
      "        x = F.relu(self.fc2(x))\n",
      "        x = self.fc3(x)\n",
      "        return x\n",
      "    \n",
      "xs = [i for i in range(35)]\n",
      "total_lossest = []\n",
      "nett = Nett()\n",
      "criterion = nn.CrossEntropyLoss()\n",
      "optimizer = optim.SGD(nett.parameters(), lr=0.0005, momentum=0.9)\n",
      "for epoch in range(30):  # loop over the dataset multiple times: 43-44\n",
      "    running_loss = 0.0\n",
      "    total_loss = 0\n",
      "    for i, data in enumerate(trainloader, 0):\n",
      "        # get the inputs; data is a list of [inputs, labels]\n",
      "        inputs, labels = data\n",
      "\n",
      "        # zero the parameter gradients\n",
      "        optimizer.zero_grad()\n",
      "\n",
      "        # forward + backward + optimize\n",
      "        outputs = nett(inputs)\n",
      "        loss = criterion(outputs, labels)\n",
      "        loss.backward()\n",
      "        optimizer.step()\n",
      "\n",
      "        # print statistics\n",
      "        running_loss += loss.item()\n",
      "        total_loss += running_loss\n",
      "        if i % 2000 == 1999:    # print every 2000 mini-batches\n",
      "            print('[%d, %5d] loss: %.3f' %\n",
      "                  (epoch + 1, i + 1, running_loss / 2000))\n",
      "            running_loss = 0.0\n",
      "    total_lossest.append(total_loss)\n",
      "print('Finished Training')\n",
      "133/291:\n",
      "class Net(nn.Module):\n",
      "    def __init__(self):\n",
      "        super(Net, self).__init__()\n",
      "        self.conv1 = nn.Conv2d(3, 6, 5)\n",
      "        self.pool = nn.MaxPool2d(2, 2)\n",
      "        self.conv2 = nn.Conv2d(6, 12, 5)\n",
      "        self.conv3 = nn.Conv2d(12, 24, 5, padding=2)\n",
      "        self.conv4 = nn.Conv2d(24, 48, 5, padding=2)\n",
      "        self.conv5 = nn.Conv2d(48, 16, 5, padding=2)\n",
      "        self.fc1 = nn.Linear(16 * 5 * 5, 120)\n",
      "        self.fc2 = nn.Linear(120, 84)\n",
      "        self.fc3 = nn.Linear(84, 10)\n",
      "\n",
      "    def forward(self, x):\n",
      "        x = self.pool(F.relu(self.conv1(x)))\n",
      "        x = self.pool(F.relu(self.conv2(x)))\n",
      "        x = F.relu(self.conv3(x))\n",
      "        x = F.relu(self.conv4(x))\n",
      "        x = F.relu(self.conv5(x))\n",
      "        x = x.view(-1, 16 * 5 * 5)\n",
      "        x = F.relu(self.fc1(x))\n",
      "        x = F.relu(self.fc2(x))\n",
      "        x = self.fc3(x)\n",
      "        return x\n",
      "xs = [i for i in range(30)]\n",
      "total_losses = []\n",
      "net = Net()\n",
      "criterion = nn.CrossEntropyLoss()\n",
      "optimizer = optim.SGD(net.parameters(), lr=0.0005, momentum=0.9)\n",
      "for epoch in range(30):  # loop over the dataset multiple times: 43-44\n",
      "    running_loss = 0.0\n",
      "    total_loss = 0\n",
      "    for i, data in enumerate(trainloader, 0):\n",
      "        # get the inputs; data is a list of [inputs, labels]\n",
      "        inputs, labels = data\n",
      "\n",
      "        # zero the parameter gradients\n",
      "        optimizer.zero_grad()\n",
      "\n",
      "        # forward + backward + optimize\n",
      "        outputs = net(inputs)\n",
      "        loss = criterion(outputs, labels)\n",
      "        loss.backward()\n",
      "        optimizer.step()\n",
      "\n",
      "        # print statistics\n",
      "        running_loss += loss.item()\n",
      "        total_loss += running_loss\n",
      "        if i % 2000 == 1999:    # print every 2000 mini-batches\n",
      "            print('[%d, %5d] loss: %.3f' %\n",
      "                  (epoch + 1, i + 1, running_loss / 2000))\n",
      "            running_loss = 0.0\n",
      "    total_losses.append(total_loss)\n",
      "print('Finished Training')\n",
      "135/108: plt.plot(total_lossest)\n",
      "135/109:\n",
      "correct = 0\n",
      "total = 0\n",
      "with torch.no_grad():\n",
      "    for data in testloader:\n",
      "        images, labels = data\n",
      "        outputs = nett(images)\n",
      "        _, predicted = torch.max(outputs.data, 1)\n",
      "        total += labels.size(0)\n",
      "        correct += (predicted == labels).sum().item()\n",
      "print(total)\n",
      "print('Accuracy of the network on the 10000 test images: %d %%' % (\n",
      "    100 * correct / total))\n",
      "133/292:\n",
      "correct = 0\n",
      "total = 0\n",
      "with torch.no_grad():\n",
      "    for data in testloader:\n",
      "        images, labels = data\n",
      "        outputs = net(images)\n",
      "        _, predicted = torch.max(outputs.data, 1)\n",
      "        total += labels.size(0)\n",
      "        correct += (predicted == labels).sum().item()\n",
      "print(total)\n",
      "print('Accuracy of the network on the 10000 test images: %d %%' % (\n",
      "    100 * correct / total))\n",
      "127/140: \"hi\"\n",
      "127/141:\n",
      "import requests\n",
      "import text_summarizer\n",
      "text = requests.get('http://rare-technologies.com/the_matrix_synopsis.txt').text\n",
      "centroid_summarizer = text_summarizer.CentroidBOWSummarizer()\n",
      "centroid_summary = centroid_summarizer.summarize(text, limit=500)\n",
      "127/142: centroid_summary\n",
      "127/143: centroid_summary = centroid_summarizer.summarize(text, limit=20)\n",
      "127/144: centroid_summary\n",
      "127/145: centroid_summary = centroid_summarizer.summarize(text, limit_type = 'sentence', limit=20)\n",
      "127/146: centroid_summary\n",
      "127/147:\n",
      "t = time.process_time()\n",
      "wcentroids_pol = []\n",
      "centroid_summarizer = text_summarizer.CentroidBOWSummarizer()\n",
      "count = 0\n",
      "tot = 0\n",
      "for i in range(len(politics)):\n",
      "    if i in [1008, 1938, 4466, 6292, 7783, 8379, 9505, 10035]:\n",
      "        continue\n",
      "    # get number of words in the summary\n",
      "    n = len(nltk.word_tokenize(summaries_pol[count]))\n",
      "    tot += n\n",
      "    \n",
      "    \n",
      "    centroid_summary = centroid_summarizer.summarize(text, limit=max(tot, (tot+50)//100*100))\n",
      "    started = False\n",
      "    wcentroids_pol.append(centroid_summary)\n",
      "    # get n-word consecutive subset of original text\n",
      "    count += 1\n",
      "time.process_time() - t\n",
      "127/148:\n",
      "t = time.process_time()\n",
      "wcentroids_pol = []\n",
      "centroid_summarizer = text_summarizer.CentroidBOWSummarizer()\n",
      "count = 0\n",
      "tot = 0\n",
      "for i in range(10):\n",
      "    if i in [1008, 1938, 4466, 6292, 7783, 8379, 9505, 10035]:\n",
      "        continue\n",
      "    # get number of words in the summary\n",
      "    n = len(nltk.word_tokenize(summaries_pol[count]))\n",
      "    tot += n\n",
      "    centroid_summary = centroid_summarizer.summarize(text, limit=max(tot, (tot+50)//100*100))\n",
      "    started = False\n",
      "    wcentroids_pol.append(centroid_summary)\n",
      "    # get n-word consecutive subset of original text\n",
      "    count += 1\n",
      "time.process_time() - t\n",
      "127/149: len(summaries_pol)\n",
      "127/150: len(summaries_pol)*.676\n",
      "127/151: summaries_pol[0]\n",
      "127/152: summaries_pol[1]\n",
      "127/153: len(nltk.word_tokenizer(summaries_pol[1]))\n",
      "127/154: len(nltk.word_tokenize(summaries_pol[1]))\n",
      "127/155: len(nltk.word_tokenize(summaries_pol[0]))\n",
      "127/156: len(nltk.word_tokenize(wcentroids_pol[1]))\n",
      "127/157: len(nltk.word_tokenize(wcentroids_pol[0]))\n",
      "127/158: len(nltk.word_tokenize(wcentroids_pol[2]))\n",
      "127/159: len(nltk.word_tokenize(summaries_pol[2]))\n",
      "127/160:\n",
      "t = time.process_time()\n",
      "wcentroids_pol = []\n",
      "centroid_summarizer = text_summarizer.CentroidBOWSummarizer()\n",
      "count = 0\n",
      "tot = 0\n",
      "for i in range(10):\n",
      "    if i in [1008, 1938, 4466, 6292, 7783, 8379, 9505, 10035]:\n",
      "        continue\n",
      "    # get number of words in the summary\n",
      "    n = len(nltk.word_tokenize(summaries_pol[count]))\n",
      "    tot += n\n",
      "    centroid_summary = centroid_summarizer.summarize(text, limit=tot)\n",
      "    started = False\n",
      "    wcentroids_pol.append(centroid_summary)\n",
      "    # get n-word consecutive subset of original text\n",
      "    count += 1\n",
      "time.process_time() - t\n",
      "127/161: len(summaries_pol)*.676\n",
      "127/162: len(nltk.word_tokenize(wcentroids_pol[2]))\n",
      "127/163: len(nltk.word_tokenize(wcentroids_pol[0]))\n",
      "127/164:\n",
      "t = time.process_time()\n",
      "wcentroids_pol = []\n",
      "centroid_summarizer = text_summarizer.CentroidBOWSummarizer()\n",
      "count = 0\n",
      "tot = 0\n",
      "for i in range(10):\n",
      "    if i in [1008, 1938, 4466, 6292, 7783, 8379, 9505, 10035]:\n",
      "        continue\n",
      "    # get number of words in the summary\n",
      "    n = len(nltk.word_tokenize(summaries_pol[count]))\n",
      "    tot += n\n",
      "    centroid_summary = centroid_summarizer.summarize(text, limit=n)\n",
      "    started = False\n",
      "    wcentroids_pol.append(centroid_summary)\n",
      "    # get n-word consecutive subset of original text\n",
      "    count += 1\n",
      "time.process_time() - t\n",
      "127/165: len(nltk.word_tokenize(summaries_pol[1]))\n",
      "127/166: len(nltk.word_tokenize(wcentroids_pol[2]))\n",
      "127/167: len(nltk.word_tokenize(wcentroids_pol[1]))\n",
      "127/168: len(nltk.word_tokenize(wcentroids_pol[0]))\n",
      "127/169: len(nltk.word_tokenize(wcentroids_pol[1]))\n",
      "127/170: wcentroids_pol[1])\n",
      "127/171: wcentroids_pol[1]\n",
      "127/172: summaries_pol[1]\n",
      "127/173:\n",
      "t = time.process_time()\n",
      "wcentroids_pol = []\n",
      "centroid_summarizer = text_summarizer.CentroidBOWSummarizer()\n",
      "count = 0\n",
      "tot = 0\n",
      "for i in range(10):\n",
      "    if i in [1008, 1938, 4466, 6292, 7783, 8379, 9505, 10035]:\n",
      "        continue\n",
      "    # get number of words in the summary\n",
      "    n = len(nltk.word_tokenize(summaries_pol[count]))\n",
      "    tot += n\n",
      "    centroid_summary = centroid_summarizer.summarize(politics[i], limit=n)\n",
      "    started = False\n",
      "    wcentroids_pol.append(centroid_summary)\n",
      "    # get n-word consecutive subset of original text\n",
      "    count += 1\n",
      "time.process_time() - t\n",
      "127/174:\n",
      "t = time.process_time()\n",
      "wcentroids_pol = []\n",
      "centroid_summarizer = text_summarizer.CentroidBOWSummarizer()\n",
      "count = 0\n",
      "tot = 0\n",
      "for i in range(10):\n",
      "    if i in [1008, 1938, 4466, 6292, 7783, 8379, 9505, 10035]:\n",
      "        continue\n",
      "    # get number of words in the summary\n",
      "    n = len(nltk.word_tokenize(summaries_pol[count]))\n",
      "    tot += n\n",
      "    centroid_summary = centroid_summarizer.summarize(politics[i].text, limit=n)\n",
      "    started = False\n",
      "    wcentroids_pol.append(centroid_summary)\n",
      "    # get n-word consecutive subset of original text\n",
      "    count += 1\n",
      "time.process_time() - t\n",
      "127/175: len(summaries_pol)*.676\n",
      "127/176: len(nltk.word_tokenize(summaries_pol[1]))\n",
      "127/177: len(nltk.word_tokenize(wcentroids_pol[2]))\n",
      "127/178: len(nltk.word_tokenize(wcentroids_pol[1]))\n",
      "127/179: wcentroids_pol[1]\n",
      "127/180: summaries_pol[0]\n",
      "127/181: wcentroids_pol[0]\n",
      "127/182: len(nltk.word_tokenize(summaries_pol[0]))\n",
      "127/183: len(nltk.word_tokenize(wcentroids_pol[0]))\n",
      "127/184: simple_preprocess(summaries_pol[0])\n",
      "127/185: summaries_pol[0].rstrip()\n",
      "127/186: summaries_pol[0].strip('\\n')\n",
      "127/187: summaries_pol[0].strip('\\\\n')\n",
      "127/188: summaries_pol[0].strip('n')\n",
      "127/189: summaries_pol[0].replace('\\n', '')\n",
      "127/190: politics[0]\n",
      "127/191:\n",
      "t = time.process_time()\n",
      "wcentroids_pol = []\n",
      "centroid_summarizer = text_summarizer.CentroidBOWSummarizer()\n",
      "count = 0\n",
      "tot = 0\n",
      "for i in range(10):\n",
      "    if i in [1008, 1938, 4466, 6292, 7783, 8379, 9505, 10035]:\n",
      "        continue\n",
      "    # get number of words in the summary\n",
      "    n = len(nltk.word_tokenize(summaries_pol[count]))\n",
      "    tot += n\n",
      "    print(n)\n",
      "    centroid_summary = centroid_summarizer.summarize(politics[i].text, limit=n)\n",
      "    started = False\n",
      "    wcentroids_pol.append(centroid_summary)\n",
      "    # get n-word consecutive subset of original text\n",
      "    count += 1\n",
      "time.process_time() - t\n",
      "127/192:\n",
      "t = time.process_time()\n",
      "wcentroids_pol = []\n",
      "centroid_summarizer = text_summarizer.CentroidBOWSummarizer()\n",
      "count = 0\n",
      "tot = 0\n",
      "for i in range(10):\n",
      "    if i in [1008, 1938, 4466, 6292, 7783, 8379, 9505, 10035]:\n",
      "        continue\n",
      "    # get number of words in the summary\n",
      "    n = len(nltk.word_tokenize(summaries_pol[count]))\n",
      "    tot += n\n",
      "    print(n)\n",
      "    centroid_summary = centroid_summarizer.summarize(politics[i].text, limit=n-50)\n",
      "    started = False\n",
      "    wcentroids_pol.append(centroid_summary)\n",
      "    # get n-word consecutive subset of original text\n",
      "    count += 1\n",
      "time.process_time() - t\n",
      "127/193: len(nltk.word_tokenize(wcentroids_pol[0]))\n",
      "127/194: len(nltk.word_tokenize(wcentroids_pol[1]))\n",
      "127/195: len(nltk.word_tokenize(summaries_pol[1]))\n",
      "127/196: len(nltk.word_tokenize(wcentroids_pol[2]))\n",
      "127/197: len(nltk.word_tokenize(summaries_pol[2]))\n",
      "127/198: len(nltk.word_tokenize(wcentroids_pol[3]))\n",
      "127/199: len(nltk.word_tokenize(summaries_pol[3]))\n",
      "127/200: from text_summarizer.text_summarizer import base\n",
      "127/201: from text_summarizer import base\n",
      "127/202: base.similarity\n",
      "127/203: base.similarity(1,2)\n",
      "127/204: from text_summarizer import base\n",
      "127/205: base.similarity(1,2)\n",
      "127/206:\n",
      "t = time.process_time()\n",
      "wcentroids_pol = []\n",
      "centroid_summarizer = text_summarizer.CentroidBOWSummarizer()\n",
      "count = 0\n",
      "tot = 0\n",
      "for i in range(len(politics)):\n",
      "    if i in [1008, 1938, 4466, 6292, 7783, 8379, 9505, 10035]:\n",
      "        continue\n",
      "    # get number of words in the summary\n",
      "    n = len(summaries_pol[count].split())\n",
      "    tot += n\n",
      "    centroid_summary = centroid_summarizer.summarize(text, limit=n)\n",
      "    started = False\n",
      "    wcentroids_pol.append(centroid_summary)\n",
      "    # get n-word consecutive subset of original text\n",
      "    count += 1\n",
      "time.process_time() - t\n",
      "127/207:\n",
      "t = time.process_time()\n",
      "wcentroids_pol = []\n",
      "centroid_summarizer = text_summarizer.CentroidBOWSummarizer()\n",
      "count = 0\n",
      "tot = 0\n",
      "for i in range(10):\n",
      "    if i in [1008, 1938, 4466, 6292, 7783, 8379, 9505, 10035]:\n",
      "        continue\n",
      "    # get number of words in the summary\n",
      "    n = len(summaries_pol[count].split())\n",
      "    tot += n\n",
      "    print(n)\n",
      "    centroid_summary = centroid_summarizer.summarize(politics[i].text, limit=n-50)\n",
      "    started = False\n",
      "    wcentroids_pol.append(centroid_summary)\n",
      "    # get n-word consecutive subset of original text\n",
      "    count += 1\n",
      "time.process_time() - t\n",
      "127/208:\n",
      "t = time.process_time()\n",
      "wcentroids_pol = []\n",
      "centroid_summarizer = text_summarizer.CentroidBOWSummarizer()\n",
      "count = 0\n",
      "tot = 0\n",
      "for i in range(10):\n",
      "    if i in [1008, 1938, 4466, 6292, 7783, 8379, 9505, 10035]:\n",
      "        continue\n",
      "    # get number of words in the summary\n",
      "    n = len(summaries_pol[count].split())\n",
      "    tot += n\n",
      "    print(n)\n",
      "    centroid_summary = centroid_summarizer.summarize(politics[i].text, limit=n)\n",
      "    started = False\n",
      "    wcentroids_pol.append(centroid_summary)\n",
      "    # get n-word consecutive subset of original text\n",
      "    count += 1\n",
      "time.process_time() - t\n",
      "127/209: len(nltk.word_tokenize(summaries_pol[3]))\n",
      "127/210: len(nltk.word_tokenize(wcentroids_pol[3]))\n",
      "127/211: len(nltk.word_tokenize(summaries_pol[2]))\n",
      "127/212: len(nltk.word_tokenize(wcentroids_pol[2]))\n",
      "127/213:\n",
      "t = time.process_time()\n",
      "wcentroids_pol = []\n",
      "centroid_summarizer = text_summarizer.CentroidBOWSummarizer()\n",
      "count = 0\n",
      "tot = 0\n",
      "for i in range(10):\n",
      "    if i in [1008, 1938, 4466, 6292, 7783, 8379, 9505, 10035]:\n",
      "        continue\n",
      "    # get number of words in the summary\n",
      "    n = len(summaries_pol[count].split())\n",
      "    tot += n\n",
      "    print(n)\n",
      "    centroid_summary = centroid_summarizer.summarize(politics[i].text, limit=n-50)\n",
      "    started = False\n",
      "    wcentroids_pol.append(centroid_summary)\n",
      "    # get n-word consecutive subset of original text\n",
      "    count += 1\n",
      "time.process_time() - t\n",
      "127/214: len(summaries_pol)*.676\n",
      "127/215: len(nltk.word_tokenize(summaries_pol[2]))\n",
      "127/216: len(nltk.word_tokenize(wcentroids_pol[2]))\n",
      "127/217: len(nltk.word_tokenize(summaries_pol[1]))\n",
      "127/218: len(nltk.word_tokenize(wcentroids_pol[1]))\n",
      "127/219: len(nltk.word_tokenize(summaries_pol[0]))\n",
      "127/220: len(nltk.word_tokenize(wcentroids_pol[0]))\n",
      "127/221: len(nltk.word_tokenize(summaries_pol[4]))\n",
      "127/222: len(nltk.word_tokenize(wcentroids_pol[4]))\n",
      "127/223: len(nltk.word_tokenize(summaries_pol[5]))\n",
      "127/224: len(nltk.word_tokenize(wcentroids_pol[5]))\n",
      "127/225:\n",
      "t = time.process_time()\n",
      "wcentroids_pol = []\n",
      "centroid_summarizer = text_summarizer.CentroidBOWSummarizer()\n",
      "count = 0\n",
      "tot = 0\n",
      "for i in range(10):\n",
      "    if i in [1008, 1938, 4466, 6292, 7783, 8379, 9505, 10035]:\n",
      "        continue\n",
      "    # get number of words in the summary\n",
      "    n = len(summaries_pol[count].split())\n",
      "    tot += n\n",
      "    print(n)\n",
      "    centroid_summary = centroid_summarizer.summarize(politics[i].text, limit=n-20)\n",
      "    started = False\n",
      "    wcentroids_pol.append(centroid_summary)\n",
      "    # get n-word consecutive subset of original text\n",
      "    count += 1\n",
      "time.process_time() - t\n",
      "127/226:\n",
      "for i in range(10):\n",
      "print(len(nltk.word_tokenize(summaries_pol[5])), len(nltk.word_tokenize(wcentroids_pol[5])))\n",
      "127/227:\n",
      "for i in range(10):\n",
      "    print(len(nltk.word_tokenize(summaries_pol[5])), len(nltk.word_tokenize(wcentroids_pol[5])))\n",
      "127/228:\n",
      "for i in range(10):\n",
      "    print(len(nltk.word_tokenize(summaries_pol[i])), len(nltk.word_tokenize(wcentroids_pol[i])))\n",
      "127/229:\n",
      "t = time.process_time()\n",
      "wcentroids_pol = []\n",
      "centroid_summarizer = text_summarizer.CentroidBOWSummarizer()\n",
      "count = 0\n",
      "tot = 0\n",
      "for i in range(len(politics)):\n",
      "    if i in [1008, 1938, 4466, 6292, 7783, 8379, 9505, 10035]:\n",
      "        continue\n",
      "    # get number of words in the summary\n",
      "    n = len(summaries_pol[count].split())\n",
      "    tot += n\n",
      "    centroid_summary = centroid_summarizer.summarize(text, limit=n-20)\n",
      "    started = False\n",
      "    wcentroids_pol.append(centroid_summary)\n",
      "    # get n-word consecutive subset of original text\n",
      "    count += 1\n",
      "time.process_time() - t\n",
      "127/230: len(wcentroids_pol)\n",
      "127/231: mpols_centroid = getw2v_s(wcentroids_pol)\n",
      "127/232: gpols_centroid = getg(mpol_centroid)\n",
      "127/233: gpols_centroid = getg(mpols_centroid)\n",
      "127/234: bias(gpol, mpol, proflist), bias(gpols, mpols, proflist), bias(gpols_centroid, mpols_centroid, proflist)\n",
      "127/235: bias(gpols, mpols, proflist), bias(gpols_centroid, mpols_centroid, proflist)\n",
      "127/236: bias(gpols, mpols, adjectives), bias(gpols_centroid, mpols_centroid, adjectives)\n",
      "127/237: bias(gpols, mpols, gender_specific), bias(gpols_centroid, mpols_centroid, gender_specific)\n",
      "127/238: bias(gpols, mpols, proflist), bias(gpols_centroid, mpols_centroid, proflist)\n",
      "127/239:\n",
      "res = sorted([(x, gproj(gpols_centroid, mpols_centroid, x)) for x in filter(lambda x: x in mpols_centroid.wv.vocab, wordlist)], key=lambda x: x[1])\n",
      "for i in range(20):\n",
      "    print(f\"{res[i][0].ljust(20)} {res[i][1]} \\t\\t {res[-i-1][0].ljust(20)} {res[-i-1][1]}\")\n",
      "127/240: mpols_centroid.wv.vocab\n",
      "127/241: len(mpols_centroid.wv.vocab)\n",
      "127/242: len(mpols_centroid.wv.vocab), len(mpols.wv.vocab)\n",
      "127/243: len(wcentroids_pol)\n",
      "127/244: print(tot)\n",
      "127/245: print(tot, [len(nltk.word_tokenize(w)) for w in wcentroids_pol]\n",
      "127/246: print(tot, [len(nltk.word_tokenize(w)) for w in wcentroids_pol])\n",
      "127/247: print(tot, sum([len(nltk.word_tokenize(w)) for w in wcentroids_pol]))\n",
      "127/248: summaries_pol[0]\n",
      "127/249: wcentroids_pol\n",
      "127/250: wcentroids_pol[0]\n",
      "127/251:\n",
      "t = time.process_time()\n",
      "wcentroids_pol = []\n",
      "centroid_summarizer = text_summarizer.CentroidBOWSummarizer()\n",
      "count = 0\n",
      "tot = 0\n",
      "for i in range(len(politics)):\n",
      "    if i in [1008, 1938, 4466, 6292, 7783, 8379, 9505, 10035]:\n",
      "        continue\n",
      "    # get number of words in the summary\n",
      "    n = len(summaries_pol[count].split())\n",
      "    tot += n\n",
      "    centroid_summary = centroid_summarizer.summarize(politics[i].text, limit=n-20)\n",
      "    started = False\n",
      "    wcentroids_pol.append(centroid_summary)\n",
      "    # get n-word consecutive subset of original text\n",
      "    count += 1\n",
      "time.process_time() - t\n",
      "127/252:\n",
      "t = time.process_time()\n",
      "wcentroids_comp = []\n",
      "centroid_summarizer = text_summarizer.CentroidBOWSummarizer()\n",
      "count = 0\n",
      "tot = 0\n",
      "for i in range(len(computers)):\n",
      "    if i in [1008, 1938, 4466, 6292, 7783, 8379, 9505, 10035]:\n",
      "        continue\n",
      "    # get number of words in the summary\n",
      "    n = len(summaries_comp[count].split())\n",
      "    tot += n\n",
      "    centroid_summary = centroid_summarizer.summarize(computers[i].text, limit=n-20)\n",
      "    started = False\n",
      "    wcentroids_comp.append(centroid_summary)\n",
      "    # get n-word consecutive subset of original text\n",
      "    count += 1\n",
      "time.process_time() - t\n",
      "127/253: mpols_centroid = getw2v_s(wcentroids_pol)\n",
      "127/254: gpols_centroid = getg(mpols_centroid)\n",
      "127/255: len(mpols_centroid.wv.vocab), len(mpols.wv.vocab)\n",
      "127/256: bias(gpols, mpols, proflist), bias(gpols_centroid, mpols_centroid, proflist)\n",
      "127/257: bias(gpols, mpols, adjectives), bias(gpols_centroid, mpols_centroid, adjectives)\n",
      "127/258: bias(gpols, mpols, gender_specific), bias(gpols_centroid, mpols_centroid, gender_specific)\n",
      "127/259:\n",
      "t = time.process_time()\n",
      "wcentroids_comp = []\n",
      "centroid_summarizer = text_summarizer.CentroidBOWSummarizer()\n",
      "count = 0\n",
      "tot = 0\n",
      "for i in range(len(computers)):\n",
      "    # get number of words in the summary\n",
      "    n = len(summaries_comp[count].split())\n",
      "    tot += n\n",
      "    centroid_summary = centroid_summarizer.summarize(computers[i].text, limit=n-20)\n",
      "    started = False\n",
      "    wcentroids_comp.append(centroid_summary)\n",
      "    # get n-word consecutive subset of original text\n",
      "    count += 1\n",
      "time.process_time() - t\n",
      "127/260: summaries_comp[1018]\n",
      "127/261: summaries[1018]\n",
      "127/262: computers[1018]\n",
      "127/263: computers[1018].text\n",
      "127/264: computers[1019].text\n",
      "127/265: computers[1258].text\n",
      "127/266: summaries_comp[1256]\n",
      "127/267: computers[1259].text\n",
      "127/268: computers[1257].text\n",
      "127/269: computers[1259].text\n",
      "127/270: summaries_comp[1258]\n",
      "127/271: computers[1260].text\n",
      "127/272:\n",
      "t = time.process_time()\n",
      "wcentroids_comp = []\n",
      "centroid_summarizer = text_summarizer.CentroidBOWSummarizer()\n",
      "count = 0\n",
      "tot = 0\n",
      "for i in range(len(computers)):\n",
      "    if i in [1018,1258,1830,3182,3296,4812,5102]:\n",
      "        continue\n",
      "    # get number of words in the summary\n",
      "    n = len(summaries_comp[count].split())\n",
      "    tot += n\n",
      "    centroid_summary = centroid_summarizer.summarize(computers[i].text, limit=n-20)\n",
      "    started = False\n",
      "    wcentroids_comp.append(centroid_summary)\n",
      "    # get n-word consecutive subset of original text\n",
      "    count += 1\n",
      "time.process_time() - t\n",
      "127/273: mcomp_centroid = getw2v_s(wcentroids_comp)\n",
      "127/274:\n",
      "t = time.process_time()\n",
      "wcentroids_trav = []\n",
      "centroid_summarizer = text_summarizer.CentroidBOWSummarizer()\n",
      "count = 0\n",
      "tot = 0\n",
      "for i in range(len(travel)):\n",
      "    # get number of words in the summary\n",
      "    n = len(summaries_trav[count].split())\n",
      "    tot += n\n",
      "    centroid_summary = centroid_summarizer.summarize(travel[i].text, limit=n-20)\n",
      "    started = False\n",
      "    wcentroids_trav.append(centroid_summary)\n",
      "    # get n-word consecutive subset of original text\n",
      "    count += 1\n",
      "time.process_time() - t\n",
      "127/275: gcomp_centroid = getg(mcomp_centroid)\n",
      "127/276:\n",
      "mtrav_centroid = getw2v_s(wcentroids_trav)\n",
      "gtrav_centroid = getg(mtrav_centroid)\n",
      "127/277:\n",
      "print(bias(gcomp, mcomp, proflist), bias(gcomp_centroid, mcomp_centroid, proflist))\n",
      "print(bias(gcomp, mcomp, adjectives), bias(gcomp_centroid, mcomp_centroid, adjectives))\n",
      "print(bias(gcomp, mcomp, gender_specific), bias(gcomp_centroid, mcomp_centroid, gender_specific))\n",
      "127/278: len(politics), len(travel), len(computers)\n",
      "127/279:\n",
      "print(bias(gtrav, mtrav, proflist), bias(gtrav_centroid, mtrav_centroid, proflist))\n",
      "print(bias(gtrav, mtrav, adjectives), bias(gtrav_centroid, mtrav_centroid, adjectives))\n",
      "print(bias(gtrav, mtrav, gender_specific), bias(gtrav_centroid, mtrav_centroid, gender_specific))\n",
      "127/280: from random import sample\n",
      "127/281:\n",
      "def getw2v2(category, size):\n",
      "    sentences = []\n",
      "    category = sample(category, size)\n",
      "    for t in category:\n",
      "        sentences += nltk.sent_tokenize(t.text)\n",
      "    docs = [simple_preprocess(s) for s in sentences]\n",
      "    return category, Word2Vec(docs)\n",
      "127/282: politics5825 = getw2v2(politics, 5825)\n",
      "127/283: travel5825 = getw2v2(travel, 5825)\n",
      "127/284:\n",
      "t = time.process_time()\n",
      "summaries_pol5825 = getsummaries(politics5825)\n",
      "time.process_time() - t\n",
      "127/285:\n",
      "t = time.process_time()\n",
      "summaries_trav5825 = getsummaries(travel5825)\n",
      "time.process_time() - t\n",
      "127/286:\n",
      "t = time.process_time()\n",
      "wcentroids_trav5825 = []\n",
      "centroid_summarizer = text_summarizer.CentroidBOWSummarizer()\n",
      "count = 0\n",
      "tot = 0\n",
      "for i in range(len(travel5825)):\n",
      "    # get number of words in the summary\n",
      "    n = len(summaries_trav5825[count].split())\n",
      "    tot += n\n",
      "    centroid_summary = centroid_summarizer.summarize(travel5825[i].text, limit=n-20)\n",
      "    started = False\n",
      "    wcentroids_trav5825.append(centroid_summary)\n",
      "    # get n-word consecutive subset of original text\n",
      "    count += 1\n",
      "time.process_time() - t\n",
      "127/287:\n",
      "t = time.process_time()\n",
      "wcentroids_pol5825 = []\n",
      "centroid_summarizer = text_summarizer.CentroidBOWSummarizer()\n",
      "count = 0\n",
      "tot = 0\n",
      "for i in range(len(politics)):\n",
      "    if i in [0, 1]:\n",
      "        continue\n",
      "    # get number of words in the summary\n",
      "    n = len(summaries_pol5825[count].split())\n",
      "    tot += n\n",
      "    centroid_summary = centroid_summarizer.summarize(politics5825[i].text, limit=n-20)\n",
      "    started = False\n",
      "    wcentroids_pol.append(centroid_summary)\n",
      "    # get n-word consecutive subset of original text\n",
      "    count += 1\n",
      "time.process_time() - t\n",
      "127/288: summaries_pol5825\n",
      "127/289: politics5825\n",
      "127/290: politics5825, mpol5825 = politics5825\n",
      "127/291: mpol5825\n",
      "127/292: travel5825, mtrav5825 = travel5825\n",
      "127/293:\n",
      "t = time.process_time()\n",
      "summaries_pol5825 = getsummaries(politics5825)\n",
      "time.process_time() - t\n",
      "127/294:\n",
      "t = time.process_time()\n",
      "summaries_trav5825 = getsummaries(travel5825)\n",
      "time.process_time() - t\n",
      "127/295:\n",
      "t = time.process_time()\n",
      "wcentroids_pol5825 = []\n",
      "centroid_summarizer = text_summarizer.CentroidBOWSummarizer()\n",
      "count = 0\n",
      "tot = 0\n",
      "for i in range(len(politics)):\n",
      "    if i in [0, 1]:\n",
      "        continue\n",
      "    # get number of words in the summary\n",
      "    n = len(summaries_pol5825[count].split())\n",
      "    tot += n\n",
      "    centroid_summary = centroid_summarizer.summarize(politics5825[i].text, limit=n-20)\n",
      "    started = False\n",
      "    wcentroids_pol.append(centroid_summary)\n",
      "    # get n-word consecutive subset of original text\n",
      "    count += 1\n",
      "time.process_time() - t\n",
      "127/296:\n",
      "t = time.process_time()\n",
      "wcentroids_pol5825 = []\n",
      "centroid_summarizer = text_summarizer.CentroidBOWSummarizer()\n",
      "count = 0\n",
      "tot = 0\n",
      "for i in range(len(politics5825)):\n",
      "    if i in [1199, 2592, 3044, 3380, 3597]:\n",
      "        continue\n",
      "    # get number of words in the summary\n",
      "    n = len(summaries_pol5825[count].split())\n",
      "    tot += n\n",
      "    centroid_summary = centroid_summarizer.summarize(politics5825[i].text, limit=n-20)\n",
      "    started = False\n",
      "    wcentroids_pol.append(centroid_summary)\n",
      "    # get n-word consecutive subset of original text\n",
      "    count += 1\n",
      "time.process_time() - t\n",
      "127/297:\n",
      "t = time.process_time()\n",
      "wcentroids_trav5825 = []\n",
      "centroid_summarizer = text_summarizer.CentroidBOWSummarizer()\n",
      "count = 0\n",
      "tot = 0\n",
      "for i in range(len(travel5825)):\n",
      "    # get number of words in the summary\n",
      "    n = len(summaries_trav5825[count].split())\n",
      "    tot += n\n",
      "    centroid_summary = centroid_summarizer.summarize(travel5825[i].text, limit=n-20)\n",
      "    started = False\n",
      "    wcentroids_trav5825.append(centroid_summary)\n",
      "    # get n-word consecutive subset of original text\n",
      "    count += 1\n",
      "time.process_time() - t\n",
      "127/298:\n",
      "t = time.process_time()\n",
      "mpols5825 = getw2v_s(summaries_pol5825)\n",
      "time.process_time() - t\n",
      "127/299:\n",
      "t = time.process_time()\n",
      "mtrav5825 = getw2v_s(summaries_trav5825)\n",
      "time.process_time() - t\n",
      "127/300:\n",
      "t = time.process_time()\n",
      "mpols_centroid5825 = getw2v_s(wcentroids_pol5825)\n",
      "time.process_time() - t\n",
      "127/301: wcentroids_pol5825\n",
      "127/302: centroid_summarizer.summarize(politics5825[0].text, limit=n-20)\n",
      "127/303:\n",
      "t = time.process_time()\n",
      "wcentroids_pol5825 = []\n",
      "centroid_summarizer = text_summarizer.CentroidBOWSummarizer()\n",
      "count = 0\n",
      "tot = 0\n",
      "for i in range(5):\n",
      "    if i in [1199, 2592, 3044, 3380, 3597]:\n",
      "        continue\n",
      "    # get number of words in the summary\n",
      "    n = len(summaries_pol5825[count].split())\n",
      "    tot += n\n",
      "    centroid_summary = centroid_summarizer.summarize(politics5825[i].text, limit=n-20)\n",
      "    wcentroids_pol.append(centroid_summary)\n",
      "    # get n-word consecutive subset of original text\n",
      "    count += 1\n",
      "time.process_time() - t\n",
      "127/304: wcentroids_pol5825\n",
      "127/305:\n",
      "t = time.process_time()\n",
      "wcentroids_pol = []\n",
      "centroid_summarizer = text_summarizer.CentroidBOWSummarizer()\n",
      "count = 0\n",
      "tot = 0\n",
      "for i in range(len(politics)):\n",
      "    if i in [1008, 1938, 4466, 6292, 7783, 8379, 9505, 10035]:\n",
      "        continue\n",
      "    # get number of words in the summary\n",
      "    n = len(summaries_pol[count].split())\n",
      "    tot += n\n",
      "    centroid_summary = centroid_summarizer.summarize(politics[i].text, limit=n-20)\n",
      "    wcentroids_pol.append(centroid_summary)\n",
      "    count += 1\n",
      "time.process_time() - t\n",
      "127/306:\n",
      "t = time.process_time()\n",
      "wcentroids_comp = []\n",
      "centroid_summarizer = text_summarizer.CentroidBOWSummarizer()\n",
      "count = 0\n",
      "tot = 0\n",
      "for i in range(len(computers)):\n",
      "    if i in [1018,1258,1830,3182,3296,4812,5102]:\n",
      "        continue\n",
      "    # get number of words in the summary\n",
      "    n = len(summaries_comp[count].split())\n",
      "    tot += n\n",
      "    centroid_summary = centroid_summarizer.summarize(computers[i].text, limit=n-20)\n",
      "    wcentroids_comp.append(centroid_summary)\n",
      "    count += 1\n",
      "time.process_time() - t\n",
      "127/307:\n",
      "t = time.process_time()\n",
      "wcentroids_pol5825 = []\n",
      "centroid_summarizer = text_summarizer.CentroidBOWSummarizer()\n",
      "count = 0\n",
      "tot = 0\n",
      "for i in range(5):\n",
      "    if i in [1199, 2592, 3044, 3380, 3597]:\n",
      "        continue\n",
      "    # get number of words in the summary\n",
      "    n = len(summaries_pol5825[count].split())\n",
      "    tot += n\n",
      "    centroid_summary = centroid_summarizer.summarize(politics5825[i].text, limit=n-20)\n",
      "    wcentroids_pol5825.append(centroid_summary)\n",
      "    # get n-word consecutive subset of original text\n",
      "    count += 1\n",
      "time.process_time() - t\n",
      "127/308:\n",
      "t = time.process_time()\n",
      "mpols_centroid5825 = getw2v_s(wcentroids_pol5825)\n",
      "time.process_time() - t\n",
      "127/309:\n",
      "t = time.process_time()\n",
      "mtravs_centroid5825 = getw2v_s(wcentroids_pol5825)\n",
      "time.process_time() - t\n",
      "127/310: wcentroids_pol5825\n",
      "127/311: wcentroids_trav\n",
      "127/312: len(mtravs_centroid5825.wv.vocab)\n",
      "127/313:\n",
      "t = time.process_time()\n",
      "mtravs_centroid5825 = getw2v_s(wcentroids_trav5825)\n",
      "time.process_time() - t\n",
      "127/314: len(mtravs_centroid5825.wv.vocab)\n",
      "127/315:\n",
      "t = time.process_time()\n",
      "wcentroids_pol5825 = []\n",
      "centroid_summarizer = text_summarizer.CentroidBOWSummarizer()\n",
      "count = 0\n",
      "tot = 0\n",
      "for i in range(len(politics5825)):\n",
      "    if i in [1199, 2592, 3044, 3380, 3597]:\n",
      "        continue\n",
      "    # get number of words in the summary\n",
      "    n = len(summaries_pol5825[count].split())\n",
      "    tot += n\n",
      "    centroid_summary = centroid_summarizer.summarize(politics5825[i].text, limit=n-20)\n",
      "    wcentroids_pol.append(centroid_summary)\n",
      "    # get n-word consecutive subset of original text\n",
      "    count += 1\n",
      "time.process_time() - t\n",
      "127/316:\n",
      "t = time.process_time()\n",
      "wcentroids_pol5825 = []\n",
      "centroid_summarizer = text_summarizer.CentroidBOWSummarizer()\n",
      "count = 0\n",
      "tot = 0\n",
      "for i in range(len(politics5825)):\n",
      "    if i in [1199, 2592, 3044, 3380, 3597]:\n",
      "        continue\n",
      "    # get number of words in the summary\n",
      "    n = len(summaries_pol5825[count].split())\n",
      "    tot += n\n",
      "    centroid_summary = centroid_summarizer.summarize(politics5825[i].text, limit=n-20)\n",
      "    wcentroids_pol5825.append(centroid_summary)\n",
      "    # get n-word consecutive subset of original text\n",
      "    count += 1\n",
      "time.process_time() - t\n",
      "127/317:\n",
      "t = time.process_time()\n",
      "wcentroids_pol = []\n",
      "centroid_summarizer = text_summarizer.CentroidBOWSummarizer()\n",
      "count = 0\n",
      "tot = 0\n",
      "for i in range(len(politics)):\n",
      "    if i in [1008, 1938, 4466, 6292, 7783, 8379, 9505, 10035]:\n",
      "        continue\n",
      "    # get number of words in the summary\n",
      "    n = len(summaries_pol[count].split())\n",
      "    tot += n\n",
      "    centroid_summary = centroid_summarizer.summarize(politics[i].text, limit=n-20)\n",
      "    wcentroids_pol.append(centroid_summary)\n",
      "    count += 1\n",
      "time.process_time() - t\n",
      "127/318:\n",
      "t = time.process_time()\n",
      "mpols_centroid5825 = getw2v_s(wcentroids_pol5825)\n",
      "time.process_time() - t\n",
      "127/319: gpols5825 = getg(mpols5825)\n",
      "127/320: gtrav5825 = getg(mtrav5825)\n",
      "127/321: gpols_centroid5825 = getg(mpols_centroid5825)\n",
      "127/322: gtrav_centroid5825 = getg(mtrav_centroid5825)\n",
      "127/323: gtravs_centroid5825 = getg(mtravs_centroid5825)\n",
      "127/324: gpol5825 = getg(mpol5825)\n",
      "127/325: gtravs5825 = getg(mtravs5825)\n",
      "127/326: mtravs5825 = mtrav5825\n",
      "127/327: travel5825, mtrav5825 = getw2v2(travel, 5825)\n",
      "127/328:\n",
      "t = time.process_time()\n",
      "summaries_trav5825 = getsummaries(travel5825)\n",
      "time.process_time() - t\n",
      "127/329:\n",
      "t = time.process_time()\n",
      "mtravs5825 = getw2v_s(summaries_trav5825)\n",
      "time.process_time() - t\n",
      "127/330:\n",
      "t = time.process_time()\n",
      "wcentroids_trav5825 = []\n",
      "centroid_summarizer = text_summarizer.CentroidBOWSummarizer()\n",
      "count = 0\n",
      "tot = 0\n",
      "for i in range(len(travel5825)):\n",
      "    # get number of words in the summary\n",
      "    n = len(summaries_trav5825[count].split())\n",
      "    tot += n\n",
      "    centroid_summary = centroid_summarizer.summarize(travel5825[i].text, limit=n-20)\n",
      "    wcentroids_trav5825.append(centroid_summary)\n",
      "    count += 1\n",
      "time.process_time() - t\n",
      "127/331:\n",
      "t = time.process_time()\n",
      "mtravs_centroid5825 = getw2v_s(wcentroids_trav5825)\n",
      "time.process_time() - t\n",
      "127/332: gtrav5825 = getg(mtrav5825)\n",
      "127/333: gtravs5825 = getg(mtravs5825)\n",
      "127/334: gtravs_centroid5825 = getg(mtravs_centroid5825)\n",
      "127/335:\n",
      "def testbiaslen(wordlist):\n",
      "    print(f\"politics: {bias(gpol5825, mpol5825, wordlist)}\")\n",
      "    print(f\"politics summary (textrank): {bias(gpols5825, mpols5825, wordlist)}\")\n",
      "    print(f\"politics summary (centroid): {bias(gpols_centroid5825, mpols_centroid5825, wordlist)}\")\n",
      "    print(f\"computers: {bias(gcomp5825, mcomp5825, wordlist)}\")\n",
      "    print(f\"computers summary: {bias(gcomps5825, mcomps5825, wordlist)}\")\n",
      "    print(f\"computers summary: {bias(gcomps_centroid5825, mcomps_centroid5825, wordlist)}\")\n",
      "    print(f\"travel: {bias(gtrav5825, mtrav5825, wordlist)}\")\n",
      "    print(f\"travel summary: {bias(gtravs5825, mtravs5825, wordlist)}\")\n",
      "    print(f\"travel summary: {bias(gtravs_centroid5825, mtravs_centroid5825, wordlist)}\")\n",
      "127/336: testbiaslen(proflist)\n",
      "127/337:\n",
      "def testbiaslen(wordlist):\n",
      "    print(f\"politics: {bias(gpol5825, mpol5825, wordlist)}\")\n",
      "    print(f\"politics summary (textrank): {bias(gpols5825, mpols5825, wordlist)}\")\n",
      "    print(f\"politics summary (centroid): {bias(gpols_centroid5825, mpols_centroid5825, wordlist)}\")\n",
      "    print(f\"computers: {bias(gcomp, mcomp, wordlist)}\")\n",
      "    print(f\"computers summary: {bias(gcomps, mcomps, wordlist)}\")\n",
      "    print(f\"computers summary: {bias(gcomps_centroid, mcomps_centroid, wordlist)}\")\n",
      "    print(f\"travel: {bias(gtrav5825, mtrav5825, wordlist)}\")\n",
      "    print(f\"travel summary: {bias(gtravs5825, mtravs5825, wordlist)}\")\n",
      "    print(f\"travel summary: {bias(gtravs_centroid5825, mtravs_centroid5825, wordlist)}\")\n",
      "127/338:\n",
      "def testbiaslen(wordlist):\n",
      "    print(f\"politics: {bias(gpol5825, mpol5825, wordlist)}\")\n",
      "    print(f\"politics summary (textrank): {bias(gpols5825, mpols5825, wordlist)}\")\n",
      "    print(f\"politics summary (centroid): {bias(gpols_centroid5825, mpols_centroid5825, wordlist)}\")\n",
      "    print(f\"computers: {bias(gcomp, mcomp, wordlist)}\")\n",
      "    print(f\"computers summary: {bias(gcomps, mcomps, wordlist)}\")\n",
      "    print(f\"computers summary: {bias(gcomps_centroid, mcomps_centroid, wordlist)}\")\n",
      "    print(f\"travel: {bias(gtrav5825, mtrav5825, wordlist)}\")\n",
      "    print(f\"travel summary: {bias(gtravs5825, mtravs5825, wordlist)}\")\n",
      "    print(f\"travel summary: {bias(gtravs_centroid5825, mtravs_centroid5825, wordlist)}\")\n",
      "127/339: testbiaslen(proflist)\n",
      "127/340:\n",
      "def testbiaslen(wordlist):\n",
      "    print(f\"politics: {bias(gpol5825, mpol5825, wordlist)}\")\n",
      "    print(f\"politics summary (textrank): {bias(gpols5825, mpols5825, wordlist)}\")\n",
      "    print(f\"politics summary (centroid): {bias(gpols_centroid5825, mpols_centroid5825, wordlist)}\")\n",
      "    print(f\"computers: {bias(gcomp, mcomp, wordlist)}\")\n",
      "    print(f\"computers summary: {bias(gcomps, mcomps, wordlist)}\")\n",
      "    print(f\"computers summary (centroid: {bias(gcomps_centroid, mcomps_centroid, wordlist)}\")\n",
      "    print(f\"travel: {bias(gtrav5825, mtrav5825, wordlist)}\")\n",
      "    print(f\"travel summary: {bias(gtravs5825, mtravs5825, wordlist)}\")\n",
      "    print(f\"travel summary: {bias(gtravs_centroid5825, mtravs_centroid5825, wordlist)}\")\n",
      "127/341:\n",
      "def testbiaslen(wordlist):\n",
      "    print(f\"politics: {bias(gpol5825, mpol5825, wordlist)}\")\n",
      "    print(f\"politics summary (textrank): {bias(gpols5825, mpols5825, wordlist)}\")\n",
      "    print(f\"politics summary (centroid): {bias(gpols_centroid5825, mpols_centroid5825, wordlist)}\")\n",
      "    print(f\"computers: {bias(gcomp, mcomp, wordlist)}\")\n",
      "    print(f\"computers summary: {bias(gcomps, mcomps, wordlist)}\")\n",
      "    print(f\"computers summary (centroid): {bias(gcomps_centroid, mcomps_centroid, wordlist)}\")\n",
      "    print(f\"travel: {bias(gtrav5825, mtrav5825, wordlist)}\")\n",
      "    print(f\"travel summary: {bias(gtravs5825, mtravs5825, wordlist)}\")\n",
      "    print(f\"travel summary: {bias(gtravs_centroid5825, mtravs_centroid5825, wordlist)}\")\n",
      "127/342: testbiaslen(proflist)\n",
      "127/343: len(mcomps.wv.vocab)\n",
      "127/344: len(mcomps.wv.vocab), len(mpols.wv.vocab)\n",
      "127/345: len(mcomps.wv.vocab), len(mpols.wv.vocab), len(mtrav.wv.vocab)\n",
      "127/346: len(mcomps.wv.vocab), len(mpols.wv.vocab), len(mtravs.wv.vocab)\n",
      "127/347: len(mcomp.wv.vocab), len(mpol.wv.vocab), len(mtrav.wv.vocab)\n",
      "127/348: len(gcomps_centroid.wv.vocab)\n",
      "127/349: len(m?comps_centroid.wv.vocab)\n",
      "127/350: len(mcomps_centroid.wv.vocab)\n",
      "127/351: len(mtravs_centroid.wv.vocab)\n",
      "127/352: len(mpols_centroid.wv.vocab)\n",
      "127/353:\n",
      "def testbiaslen(wordlist):\n",
      "    print(f\"politics: {bias(gpol5825, mpol5825, wordlist)}\")\n",
      "    print(f\"politics summary (textrank): {bias(gpols5825, mpols5825, wordlist)}\")\n",
      "    print(f\"politics summary (centroid): {bias(gpols_centroid5825, mpols_centroid5825, wordlist)}\")\n",
      "    print(f\"computers: {bias(gcomp, mcomp, wordlist)}\")\n",
      "    print(f\"computers summary: {bias(gcomps, mcomps, wordlist)}\")\n",
      "    print(f\"computers summary (centroid): {bias(gcomp_centroid, mcomp_centroid, wordlist)}\")\n",
      "    print(f\"travel: {bias(gtrav5825, mtrav5825, wordlist)}\")\n",
      "    print(f\"travel summary: {bias(gtravs5825, mtravs5825, wordlist)}\")\n",
      "    print(f\"travel summary: {bias(gtravs_centroid5825, mtravs_centroid5825, wordlist)}\")\n",
      "127/354: testbiaslen(proflist)\n",
      "127/355: gpols5825 = getg(mpols5825)\n",
      "127/356: politics5825, mpol5825 = getw2v2(politics, 5830)\n",
      "127/357:\n",
      "t = time.process_time()\n",
      "summaries_pol5825 = getsummaries(politics5825)\n",
      "time.process_time() - t\n",
      "127/358:\n",
      "t = time.process_time()\n",
      "mpols5825 = getw2v_s(summaries_pol5825)\n",
      "time.process_time() - t\n",
      "127/359:\n",
      "t = time.process_time()\n",
      "wcentroids_pol5825 = []\n",
      "centroid_summarizer = text_summarizer.CentroidBOWSummarizer()\n",
      "count = 0\n",
      "tot = 0\n",
      "for i in range(len(politics5825)):\n",
      "    if i in [708,819,1222,1723,1966,3569,4924]:\n",
      "        continue\n",
      "    # get number of words in the summary\n",
      "    n = len(summaries_pol5825[count].split())\n",
      "    tot += n\n",
      "    centroid_summary = centroid_summarizer.summarize(politics5825[i].text, limit=n-20)\n",
      "    wcentroids_pol5825.append(centroid_summary)\n",
      "    # get n-word consecutive subset of original text\n",
      "    count += 1\n",
      "time.process_time() - t\n",
      "127/360:\n",
      "t = time.process_time()\n",
      "mpols_centroid5825 = getw2v_s(wcentroids_pol5825)\n",
      "time.process_time() - t\n",
      "127/361: gpol5825 = getg(mpol5825)\n",
      "127/362: gpols5825 = getg(mpols5825)\n",
      "127/363: gpols_centroid5825 = getg(mpols_centroid5825)\n",
      "127/364: testbiaslen(proflist)\n",
      "127/365: gpols_centroid5825 = getg(mpols_centroid5825)\n",
      "127/366: testbiaslen(proflist)\n",
      "127/367: politics5825, mpol5825 = getw2v2(politics, 5835)\n",
      "127/368: travel5825, mtrav5825 = getw2v2(travel, 5825)\n",
      "127/369:\n",
      "t = time.process_time()\n",
      "summaries_pol5825 = getsummaries(politics5825)\n",
      "time.process_time() - t\n",
      "127/370:\n",
      "t = time.process_time()\n",
      "mpols5825 = getw2v_s(summaries_pol5825)\n",
      "time.process_time() - t\n",
      "127/371:\n",
      "t = time.process_time()\n",
      "summaries_trav5825 = getsummaries(travel5825)\n",
      "time.process_time() - t\n",
      "127/372:\n",
      "t = time.process_time()\n",
      "mtravs5825 = getw2v_s(summaries_trav5825)\n",
      "time.process_time() - t\n",
      "127/373:\n",
      "t = time.process_time()\n",
      "wcentroids_pol5825 = []\n",
      "centroid_summarizer = text_summarizer.CentroidBOWSummarizer()\n",
      "count = 0\n",
      "tot = 0\n",
      "for i in range(len(politics5825)):\n",
      "    if i in [708,819,1222,1723,1966,3569,4924]:\n",
      "        continue\n",
      "    # get number of words in the summary\n",
      "    n = len(summaries_pol5825[count].split())\n",
      "    tot += n\n",
      "    centroid_summary = centroid_summarizer.summarize(politics5825[i].text, limit=n-20)\n",
      "    wcentroids_pol5825.append(centroid_summary)\n",
      "    # get n-word consecutive subset of original text\n",
      "    count += 1\n",
      "time.process_time() - t\n",
      "127/374:\n",
      "t = time.process_time()\n",
      "wcentroids_trav5825 = []\n",
      "centroid_summarizer = text_summarizer.CentroidBOWSummarizer()\n",
      "count = 0\n",
      "tot = 0\n",
      "for i in range(len(travel5825)):\n",
      "    # get number of words in the summary\n",
      "    n = len(summaries_trav5825[count].split())\n",
      "    tot += n\n",
      "    centroid_summary = centroid_summarizer.summarize(travel5825[i].text, limit=n-20)\n",
      "    wcentroids_trav5825.append(centroid_summary)\n",
      "    count += 1\n",
      "time.process_time() - t\n",
      "127/375:\n",
      "t = time.process_time()\n",
      "mpols_centroid5825 = getw2v_s(wcentroids_pol5825)\n",
      "time.process_time() - t\n",
      "127/376:\n",
      "t = time.process_time()\n",
      "mtravs_centroid5825 = getw2v_s(wcentroids_trav5825)\n",
      "time.process_time() - t\n",
      "127/377: gpol5825 = getg(mpol5825)\n",
      "127/378: gtrav5825 = getg(mtrav5825)\n",
      "127/379: gpols5825 = getg(mpols5825)\n",
      "127/380: gtravs5825 = getg(mtravs5825)\n",
      "127/381: gpols_centroid5825 = getg(mpols_centroid5825)\n",
      "127/382: gtravs_centroid5825 = getg(mtravs_centroid5825)\n",
      "127/383:\n",
      "def testbiaslen(wordlist):\n",
      "    print(f\"politics: {bias(gpol5825, mpol5825, wordlist)}\")\n",
      "    print(f\"politics summary (textrank): {bias(gpols5825, mpols5825, wordlist)}\")\n",
      "    print(f\"politics summary (centroid): {bias(gpols_centroid5825, mpols_centroid5825, wordlist)}\")\n",
      "    print(f\"computers: {bias(gcomp, mcomp, wordlist)}\")\n",
      "    print(f\"computers summary: {bias(gcomps, mcomps, wordlist)}\")\n",
      "    print(f\"computers summary (centroid): {bias(gcomp_centroid, mcomp_centroid, wordlist)}\")\n",
      "    print(f\"travel: {bias(gtrav5825, mtrav5825, wordlist)}\")\n",
      "    print(f\"travel summary: {bias(gtravs5825, mtravs5825, wordlist)}\")\n",
      "    print(f\"travel summary: {bias(gtravs_centroid5825, mtravs_centroid5825, wordlist)}\")\n",
      "127/384: testbiaslen(proflist)\n",
      "127/385:\n",
      "t = time.process_time()\n",
      "mcomp = getw2v(computers)\n",
      "time.process_time() - t\n",
      "127/386:\n",
      "t = time.process_time()\n",
      "summaries_comp = getsummaries(computers)\n",
      "time.process_time() - t\n",
      "127/387: mcomps = getw2v_s(summaries_comp)\n",
      "127/388: gcomp = getg(mcomp)\n",
      "127/389: gcomps = getg(mcomps)\n",
      "127/390:\n",
      "t = time.process_time()\n",
      "wcentroids_comp = []\n",
      "centroid_summarizer = text_summarizer.CentroidBOWSummarizer()\n",
      "count = 0\n",
      "tot = 0\n",
      "for i in range(len(computers)):\n",
      "    if i in [1018, 1258, 1830, 3182, 3296, 4812, 5102]:\n",
      "        continue\n",
      "    # get number of words in the summary\n",
      "    n = len(summaries_comp[count].split())\n",
      "    tot += n\n",
      "    centroid_summary = centroid_summarizer.summarize(computers[i].text, limit=n-20)\n",
      "    wcentroids_comp.append(centroid_summary)\n",
      "    count += 1\n",
      "time.process_time() - t\n",
      "127/391: mcomp_centroid = getw2v_s(wcentroids_comp)\n",
      "127/392: gcomp_centroid = getg(mcomp_centroid)\n",
      "127/393: gcomp = getg(mcomp)\n",
      "127/394: gcomps = getg(mcomps)\n",
      "127/395: testbiaslen(proflist)\n",
      "127/396:\n",
      "t = time.process_time()\n",
      "mcomp = getw2v(computers)\n",
      "time.process_time() - t\n",
      "127/397: gcomp = getg(mcomp)\n",
      "127/398: testbiaslen(proflist)\n",
      "127/399: centroid_word_embedding_summarizer.summarize(text, limit=500)\n",
      "127/400: centroid_word_embedding_summarizer.summarize(text, limit=100)\n",
      "127/401: centroid_word_embedding_summarizer.summarize(text, limit=10)\n",
      "127/402: centroid_word_embedding_summarizer.summarize(text, limit=20)\n",
      "127/403: centroid_word_embedding_summarizer.summarize(text, limit=50)\n",
      "127/404:\n",
      "def getsummaries2(ctrlsums, summarizer, exclude, category):\n",
      "    ret = []\n",
      "    count = 0\n",
      "    tot = 0\n",
      "    for i in range(len(category)):\n",
      "        # get number of words in the summary\n",
      "        n = len(ctrlsums[count].split())\n",
      "        tot += n\n",
      "        summary = summarizer.summarize(category[i].text, limit=n-20)\n",
      "        ret.append(summary)\n",
      "        count += 1\n",
      "    return ret\n",
      "127/405:\n",
      "def getsummaries2(ctrlsums, summarizer, exclude, category):\n",
      "    ret = []\n",
      "    count = 0\n",
      "    tot = 0\n",
      "    for i in range(len(category)):\n",
      "        if i in exclude:\n",
      "            continue\n",
      "        # get number of words in the summary\n",
      "        n = len(ctrlsums[count].split())\n",
      "        tot += n\n",
      "        summary = summarizer.summarize(category[i].text, limit=n-20)\n",
      "        ret.append(summary)\n",
      "        count += 1\n",
      "    return ret\n",
      "127/406:\n",
      "t = time.process_time()\n",
      "mpols_centroidwe = getsummaries2(summaries_pol5825, centroid_word_embedding_summarizer, [708,819,1222,1723,1966,3569,4924], politics5825)\n",
      "time.process_time() - t\n",
      "127/407:\n",
      "t = time.process_time()\n",
      "mcomps_centroidwe = getsummaries2(summaries_comp, centroid_word_embedding_summarizer, [1018,1258,1830,3182,3296,4812,5102], computers)\n",
      "time.process_time() - t\n",
      "127/408:\n",
      "t = time.process_time()\n",
      "mtravs_centroidwe = getsummaries2(summaries_trav, centroid_word_embedding_summarizer, travel)\n",
      "time.process_time() - t\n",
      "127/409:\n",
      "t = time.process_time()\n",
      "mtravs_centroidwe = getsummaries2(summaries_trav, centroid_word_embedding_summarizer, [], travel)\n",
      "time.process_time() - t\n",
      "127/410:\n",
      "pols_centroidwe = mpols_centroidwe\n",
      "comps_centroidwe = mcomps_centroidwe\n",
      "travs_centroidwe = mtravs_centroidwe\n",
      "127/411:\n",
      "mpols_centroidwe = getw2v_s(wcentroids_pol5825)\n",
      "mcomps_centroidwe = getw2v_s(wcentroids_pol5825)\n",
      "mtravs_centroidwe = getw2v_s(wcentroids_pol5825)\n",
      "127/412:\n",
      "mpols_centroidwe = getw2v_s(pols_centroidwe)\n",
      "mcomps_centroidwe = getw2v_s(comps_centroidwe)\n",
      "mtravs_centroidwe = getw2v_s(travs_centroidwe)\n",
      "127/413:\n",
      "gpols_centroidwe = getg(mpols_centroidwe)\n",
      "gcomps_centroidwe = getg(mcomps_centroidwe)\n",
      "gtravs_centroidwe = getg(mtravs_centroidwe)\n",
      "127/414: gcomps_centroidwe = getg(mcomps_centroidwe)\n",
      "127/415: gpols_centroidwe = getg(mpols_centroidwe)\n",
      "127/416: gtravs_centroidwe = getg(mtravs_centroidwe)\n",
      "127/417:\n",
      "def testbiaslen(wordlist):\n",
      "    print(f\"politics: {bias(gpol5825, mpol5825, wordlist)}\")\n",
      "    print(f\"politics summary (textrank): {bias(gpols5825, mpols5825, wordlist)}\")\n",
      "    print(f\"politics summary (centroid BOW): {bias(gpols_centroid5825, mpols_centroid5825, wordlist)}\")\n",
      "    print(f\"politics summary (centroid WE): {bias(gpols_centroidwe, mpols_centroidwe, wordlist)}\")\n",
      "    print(f\"computers: {bias(gcomp, mcomp, wordlist)}\")\n",
      "    print(f\"computers summary: {bias(gcomps, mcomps, wordlist)}\")\n",
      "    print(f\"computers summary (centroid BOW): {bias(gcomp_centroid, mcomp_centroid, wordlist)}\")\n",
      "    print(f\"computers summary (centroid WE): {bias(gcomps_centroidwe, mcomps_centroidwe, wordlist)}\")\n",
      "    print(f\"travel: {bias(gtrav5825, mtrav5825, wordlist)}\")\n",
      "    print(f\"travel summary: {bias(gtravs5825, mtravs5825, wordlist)}\")\n",
      "    print(f\"travel summary (centroid BOW): {bias(gtravs_centroid5825, mtravs_centroid5825, wordlist)}\")\n",
      "    print(f\"travel summary (centroid WE): {bias(gtravs_centroidwe, mtravs_centroidwe, wordlist)}\")\n",
      "127/418: testbiaslen(proflist)\n",
      "127/419:\n",
      "t = time.process_time()\n",
      "mpols5825 = getw2v_s(summaries_pol5825)\n",
      "time.process_time() - t\n",
      "127/420:\n",
      "t = time.process_time()\n",
      "mpols_centroidwe = getsummaries2(summaries_pol5825, centroid_word_embedding_summarizer, [708,819,1222,1723,1966,3569,4924], politics5825)\n",
      "time.process_time() - t\n",
      "127/421: gpols5825 = getg(mpols5825)\n",
      "127/422: gpol5825 = getg(mpol5825)\n",
      "127/423: testbiaslen(proflist)\n",
      "127/424: gpols_centroidwe = getg(mpols_centroidwe)\n",
      "127/425: pols_centroidwe = mpols_centroidwe\n",
      "127/426: mpols_centroidwe = getw2v_s(pols_centroidwe)\n",
      "127/427: gpols_centroidwe = getg(mpols_centroidwe)\n",
      "127/428: testbiaslen(proflist)\n",
      "127/429: len(mpols_centroid5825.wv.vocab)\n",
      "127/430: len(mtravs_centroid5825.wv.vocab)\n",
      "127/431: len(mcomps_centroid5825.wv.vocab)\n",
      "127/432: len(mcomps_centroid.wv.vocab)\n",
      "127/433: len(mpols_centroid.wv.vocab)\n",
      "127/434: len(mtravs_centroid.wv.vocab)\n",
      "127/435: mcomp_centroid = getw2v_s(wcentroids_comp)\n",
      "127/436: gcomp_centroid = getg(mcomp_centroid)\n",
      "127/437: testbiaslen(proflist)\n",
      "127/438: mcomp_centroid = getw2v_s(wcentroids_comp)\n",
      "127/439: gcomp_centroid = getg(mcomp_centroid)\n",
      "127/440: testbiaslen(proflist)\n",
      "136/1:\n",
      "import os\n",
      "import cv2\n",
      "from time import time\n",
      "import numpy as np\n",
      "import matplotlib.pyplot as plt\n",
      "138/1:\n",
      "import os\n",
      "import cv2\n",
      "from time import time\n",
      "import numpy as np\n",
      "import matplotlib.pyplot as plt\n",
      "138/2:\n",
      "from skimage.transform import integral_image\n",
      "from skimage.feature import haar_like_feature\n",
      "from skimage.feature import haar_like_feature_coord\n",
      "from skimage.feature import draw_haar_like_feature\n",
      "138/3:\n",
      "from skimage.transform import integral_image\n",
      "from skimage.feature import haar_like_feature\n",
      "from skimage.feature import haar_like_feature_coord\n",
      "from skimage.feature import draw_haar_like_feature\n",
      "138/4:\n",
      "from skimage.transform import integral_image\n",
      "from skimage.feature import haar_like_feature\n",
      "from skimage.feature import haar_like_feature_coord\n",
      "from skimage.feature import draw_haar_like_feature\n",
      "138/5:\n",
      "from skimage.transform import integral_image\n",
      "from skimage.feature import haar_like_feature\n",
      "from skimage.feature import haar_like_feature_coord\n",
      "from skimage.feature import draw_haar_like_feature\n",
      "138/6:\n",
      "from skimage.transform import integral_image\n",
      "from skimage.feature import haar_like_feature\n",
      "from skimage.feature import haar_like_feature_coord\n",
      "from skimage.feature import draw_haar_like_feature\n",
      "138/7:\n",
      "from skimage.transform import integral_image\n",
      "from skimage.feature import haar_like_feature\n",
      "from skimage.feature import haar_like_feature_coord\n",
      "from skimage.feature import draw_haar_like_feature\n",
      "139/1:\n",
      "from skimage.transform import integral_image\n",
      "from skimage.feature import haar_like_feature\n",
      "from skimage.feature import haar_like_feature_coord\n",
      "from skimage.feature import draw_haar_like_feature\n",
      "139/2:\n",
      "import os\n",
      "import cv2\n",
      "from time import time\n",
      "import numpy as np\n",
      "import matplotlib.pyplot as plt\n",
      "140/1:\n",
      "face_filenames = sorted(glob(os.path.join('data/tests', '*.jpg')))\n",
      "num_face_filenames = len(face_filenames)\n",
      "140/2:\n",
      "import os\n",
      "import cv2\n",
      "from time import time\n",
      "import numpy as np\n",
      "import matplotlib.pyplot as plt\n",
      "from glob import glob\n",
      "140/3:\n",
      "face_filenames = sorted(glob(os.path.join('data/tests', '*.jpg')))\n",
      "num_face_filenames = len(face_filenames)\n",
      "140/4: print num_face_filenames\n",
      "140/5: print(num_face_filenames)\n",
      "140/6:\n",
      "face_filenames = sorted(glob(os.path.join('data/tests', '*.jpg')))\n",
      "num_face_filenames = len(face_filenames)\n",
      "num_face_filenames\n",
      "140/7:\n",
      "nonface_filenames = sorted(glob(os.path.join('data/training_nonfaces', '*.jpg')))\n",
      "num_nonface_filenames = len(nonface_filenames)\n",
      "num_nonface_filenames\n",
      "140/8: face = cv2.imread(face_filenames[0])\n",
      "140/9: face\n",
      "140/10: face.shape\n",
      "140/11: face = cv2.imread(face_filenames[1])\n",
      "140/12: face.shape\n",
      "140/13: face = cv2.imread(face_filenames[1], cv2.IMREAD_GRAYSCALE)\n",
      "140/14: face.shape\n",
      "140/15: face = cv2.imread(nonface_filenames[1], cv2.IMREAD_GRAYSCALE)\n",
      "140/16: nonface = cv2.imread(nonface_filenames[1], cv2.IMREAD_GRAYSCALE)\n",
      "140/17: nonface.shape\n",
      "140/18: non = num_face_filenames\n",
      "140/19:\n",
      "n = num_face_filenames\n",
      "non = n\n",
      "140/20:\n",
      "faces = []\n",
      "for i in range(n):\n",
      "    faces.append(cv2.imread(face_filenames[i], cv2.IMREAD_GRAYSCALE))\n",
      "140/21:\n",
      "nonfaces = []\n",
      "for i in range(n):\n",
      "    # Read a random nonface file\n",
      "    j = random.randint(0, num_nonface_filenames - 1)\n",
      "    nonface = cv2.imread(nonface_filenames[j], cv2.IMREAD_GRAYSCALE)\n",
      "\n",
      "    wsize = random.randint(100, min(nonface.shape[0], nonface.shape[1]))\n",
      "    row = random.randint(0, nonface.shape[0]-wsize)\n",
      "    col = random.randint(0, nonface.shape[1]-wsize)\n",
      "    crop = nonface[row:row+wsize, col:col+wsize]\n",
      "\n",
      "    # Resize to be the right size\n",
      "    crop = cv2.resize(crop, (200, 200))\n",
      "140/22:\n",
      "import os\n",
      "import cv2\n",
      "from time import time\n",
      "import numpy as np\n",
      "import matplotlib.pyplot as plt\n",
      "from glob import glob\n",
      "import random\n",
      "140/23:\n",
      "nonfaces = []\n",
      "for i in range(n):\n",
      "    # Read a random nonface file\n",
      "    j = random.randint(0, num_nonface_filenames - 1)\n",
      "    nonface = cv2.imread(nonface_filenames[j], cv2.IMREAD_GRAYSCALE)\n",
      "\n",
      "    wsize = random.randint(100, min(nonface.shape[0], nonface.shape[1]))\n",
      "    row = random.randint(0, nonface.shape[0]-wsize)\n",
      "    col = random.randint(0, nonface.shape[1]-wsize)\n",
      "    crop = nonface[row:row+wsize, col:col+wsize]\n",
      "\n",
      "    # Resize to be the right size\n",
      "    crop = cv2.resize(crop, (200, 200))\n",
      "140/24:\n",
      "nonfaces = []\n",
      "for i in range(n):\n",
      "    # Read a random nonface file\n",
      "    j = random.randint(0, num_nonface_filenames - 1)\n",
      "    nonface = cv2.imread(nonface_filenames[j], cv2.IMREAD_GRAYSCALE)\n",
      "    print(nonface.shape)\n",
      "    wsize = random.randint(100, min(nonface.shape[0], nonface.shape[1]))\n",
      "    row = random.randint(0, nonface.shape[0]-wsize)\n",
      "    col = random.randint(0, nonface.shape[1]-wsize)\n",
      "    crop = nonface[row:row+wsize, col:col+wsize]\n",
      "\n",
      "    # Resize to be the right size\n",
      "    crop = cv2.resize(crop, (200, 200))\n",
      "140/25:\n",
      "nonfaces = []\n",
      "for i in range(n):\n",
      "    # Read a random nonface file\n",
      "    j = random.randint(0, num_nonface_filenames - 1)\n",
      "    nonface = cv2.imread(nonface_filenames[j], cv2.IMREAD_GRAYSCALE)\n",
      "    wsize = random.randint(80, min(nonface.shape[0], nonface.shape[1]))\n",
      "    row = random.randint(0, nonface.shape[0]-wsize)\n",
      "    col = random.randint(0, nonface.shape[1]-wsize)\n",
      "    crop = nonface[row:row+wsize, col:col+wsize]\n",
      "\n",
      "    # Resize to be the right size\n",
      "    crop = cv2.resize(crop, (200, 200))\n",
      "140/26:\n",
      "nonfaces = []\n",
      "for i in range(n):\n",
      "    # Read a random nonface file\n",
      "    j = random.randint(0, num_nonface_filenames - 1)\n",
      "    nonface = cv2.imread(nonface_filenames[j], cv2.IMREAD_GRAYSCALE)\n",
      "    print(nonface.shape)\n",
      "    wsize = random.randint(100, min(nonface.shape[0], nonface.shape[1]))\n",
      "    row = random.randint(0, nonface.shape[0]-wsize)\n",
      "    col = random.randint(0, nonface.shape[1]-wsize)\n",
      "    crop = nonface[row:row+wsize, col:col+wsize]\n",
      "\n",
      "    # Resize to be the right size\n",
      "    crop = cv2.resize(crop, (200, 200))\n",
      "140/27:\n",
      "nonfaces = []\n",
      "for i in range(n):\n",
      "    # Read a random nonface file\n",
      "    j = random.randint(0, num_nonface_filenames - 1)\n",
      "    nonface = cv2.imread(nonface_filenames[j], cv2.IMREAD_GRAYSCALE)\n",
      "    print(nonface.shape)\n",
      "    wsize = random.randint(50, min(nonface.shape[0], nonface.shape[1]))\n",
      "    row = random.randint(0, nonface.shape[0]-wsize)\n",
      "    col = random.randint(0, nonface.shape[1]-wsize)\n",
      "    crop = nonface[row:row+wsize, col:col+wsize]\n",
      "\n",
      "    # Resize to be the right size\n",
      "    crop = cv2.resize(crop, (200, 200))\n",
      "140/28:\n",
      "nonfaces = []\n",
      "for i in range(n):\n",
      "    # Read a random nonface file\n",
      "    j = random.randint(0, num_nonface_filenames - 1)\n",
      "    nonface = cv2.imread(nonface_filenames[j], cv2.IMREAD_GRAYSCALE)\n",
      "    \n",
      "    wsize = random.randint(50, min(nonface.shape[0], nonface.shape[1]))\n",
      "    row = random.randint(0, nonface.shape[0]-wsize)\n",
      "    col = random.randint(0, nonface.shape[1]-wsize)\n",
      "    crop = nonface[row:row+wsize, col:col+wsize]\n",
      "\n",
      "    # Resize to be the right size\n",
      "    crop = cv2.resize(crop, (200, 200))\n",
      "140/29:\n",
      "nonfaces = []\n",
      "for i in range(n):\n",
      "    # Read a random nonface file\n",
      "    j = random.randint(0, num_nonface_filenames - 1)\n",
      "    nonface = cv2.imread(nonface_filenames[j], cv2.IMREAD_GRAYSCALE)\n",
      "    \n",
      "    wsize = random.randint(50, min(nonface.shape[0], nonface.shape[1]))\n",
      "    row = random.randint(0, nonface.shape[0]-wsize)\n",
      "    col = random.randint(0, nonface.shape[1]-wsize)\n",
      "    crop = nonface[row:row+wsize, col:col+wsize]\n",
      "\n",
      "    # Resize to be the right size\n",
      "    crop = cv2.resize(crop, (200, 200))\n",
      "    nonfaces.append(crop)\n",
      "140/30: nonfaces[0].shape\n",
      "140/31:\n",
      "import os\n",
      "import cv2\n",
      "from time import time\n",
      "import numpy as np\n",
      "import matplotlib.pyplot as plt\n",
      "from glob import glob\n",
      "import random\n",
      "from dask import delayed\n",
      "140/32:\n",
      "from skimage.transform import integral_image\n",
      "from skimage.feature import haar_like_feature\n",
      "from skimage.feature import haar_like_feature_coord\n",
      "from skimage.feature import draw_haar_like_feature\n",
      "from skimage.data import lfw_subset\n",
      "140/33:\n",
      "from sklearn.ensemble import RandomForestClassifier\n",
      "from sklearn.model_selection import train_test_split\n",
      "from sklearn.metrics import roc_auc_score\n",
      "140/34:\n",
      "@delayed\n",
      "def extract_feature_image(img, feature_type, feature_coord=None):\n",
      "    \"\"\"Extract the haar feature for the current image\"\"\"\n",
      "    ii = integral_image(img)\n",
      "    return haar_like_feature(ii, 0, 0, ii.shape[0], ii.shape[1],\n",
      "                             feature_type=feature_type,\n",
      "                             feature_coord=feature_coord)\n",
      "140/35:\n",
      "from skimage.transform import integral_image\n",
      "from skimage.feature import haar_like_feature\n",
      "from skimage.feature import haar_like_feature_coord\n",
      "from skimage.feature import draw_haar_like_feature\n",
      "140/36:\n",
      "from sklearn.ensemble import RandomForestClassifier\n",
      "from sklearn.model_selection import train_test_split\n",
      "from sklearn.metrics import roc_auc_score\n",
      "140/37:\n",
      "from sklearn.ensemble import RandomForestClassifier\n",
      "from sklearn.model_selection import train_test_split\n",
      "from sklearn.metrics import roc_auc_score\n",
      "140/38:\n",
      "images = lfw_subset()\n",
      "# To speed up the example, extract the two types of features only\n",
      "feature_types = ['type-2-x', 'type-2-y']\n",
      "\n",
      "# Build a computation graph using Dask. This allows the use of multiple\n",
      "# CPU cores later during the actual computation\n",
      "X = delayed(extract_feature_image(img, feature_types) for img in images)\n",
      "# Compute the result\n",
      "t_start = time()\n",
      "X = np.array(X.compute(scheduler='threads'))\n",
      "time_full_feature_comp = time() - t_start\n",
      "\n",
      "y = np.array([1] * 100 + [0] * 100)\n",
      "X_train, X_test, y_train, y_test = train_test_split(X, y, train_size=150,\n",
      "                                                    random_state=0,\n",
      "                                                    stratify=y)\n",
      "\n",
      "# Extract all possible features\n",
      "feature_coord, feature_type = \\\n",
      "    haar_like_feature_coord(width=images.shape[2], height=images.shape[1],\n",
      "                            feature_type=feature_types)\n",
      "140/39:\n",
      "# Train a random forest classifier and assess its performance\n",
      "clf = RandomForestClassifier(n_estimators=1000, max_depth=None,\n",
      "                             max_features=100, n_jobs=-1, random_state=0)\n",
      "t_start = time()\n",
      "clf.fit(X_train, y_train)\n",
      "time_full_train = time() - t_start\n",
      "auc_full_features = roc_auc_score(y_test, clf.predict_proba(X_test)[:, 1])\n",
      "\n",
      "# Sort features in order of importance and plot the six most significant\n",
      "idx_sorted = np.argsort(clf.feature_importances_)[::-1]\n",
      "\n",
      "fig, axes = plt.subplots(3, 2)\n",
      "for idx, ax in enumerate(axes.ravel()):\n",
      "    image = images[0]\n",
      "    image = draw_haar_like_feature(image, 0, 0,\n",
      "                                   images.shape[2],\n",
      "                                   images.shape[1],\n",
      "                                   [feature_coord[idx_sorted[idx]]])\n",
      "    ax.imshow(image)\n",
      "    ax.set_xticks([])\n",
      "    ax.set_yticks([])\n",
      "\n",
      "fig.suptitle('The most important features')\n",
      "140/40: images = faces + nonfaces\n",
      "140/41: classes = np.ones(n) + np.zeros(n)\n",
      "140/42: classes\n",
      "140/43: classes = np.concat(np.ones(n),np.zeros(n))\n",
      "140/44: classes = np.concatenate(np.ones(n),np.zeros(n))\n",
      "140/45: np.concatenate()\n",
      "140/46: classes = np.concatenate(np.ones(n),np.zeros(n), axis=1)\n",
      "140/47: classes = np.concatenate((np.ones(n),np.zeros(n)))\n",
      "140/48: np.concatenate()\n",
      "140/49: classes\n",
      "140/50:\n",
      "# To speed up the example, extract the two types of features only\n",
      "feature_types = ['type-2-x', 'type-2-y']\n",
      "\n",
      "# Build a computation graph using Dask. This allows the use of multiple\n",
      "# CPU cores later during the actual computation\n",
      "X = delayed(extract_feature_image(img, feature_types) for img in images)\n",
      "# Compute the result\n",
      "t_start = time()\n",
      "X = np.array(X.compute(scheduler='threads'))\n",
      "time_full_feature_comp = time() - t_start\n",
      "\n",
      "y = np.array([1] * n + [0] * n)\n",
      "X_train, X_test, y_train, y_test = train_test_split(X, y, train_size=150,\n",
      "                                                    random_state=0,\n",
      "                                                    stratify=y)\n",
      "\n",
      "# Extract all possible features\n",
      "feature_coord, feature_type = \\\n",
      "    haar_like_feature_coord(width=images.shape[2], height=images.shape[1],\n",
      "                            feature_type=feature_types)\n",
      "141/1:\n",
      "fig, axes = plt.subplots(3, 2)\n",
      "for idx, ax in enumerate(axes.ravel()):\n",
      "    image = images[0]\n",
      "    image = draw_haar_like_feature(image, 0, 0,\n",
      "                                   images.shape[2],\n",
      "                                   images.shape[1],\n",
      "                                   [feature_coord[0]])\n",
      "141/2:\n",
      "import os\n",
      "import cv2\n",
      "from time import time\n",
      "import numpy as np\n",
      "import matplotlib.pyplot as plt\n",
      "from glob import glob\n",
      "import random\n",
      "from dask import delayed\n",
      "141/3:\n",
      "from sklearn.ensemble import RandomForestClassifier\n",
      "from sklearn.model_selection import train_test_split\n",
      "from sklearn.metrics import roc_auc_score\n",
      "141/4:\n",
      "from skimage.transform import integral_image\n",
      "from skimage.feature import haar_like_feature\n",
      "from skimage.feature import haar_like_feature_coord\n",
      "from skimage.feature import draw_haar_like_feature\n",
      "141/5:\n",
      "face_filenames = sorted(glob(os.path.join('data/tests', '*.jpg')))\n",
      "num_face_filenames = len(face_filenames)\n",
      "num_face_filenames\n",
      "141/6:\n",
      "nonface_filenames = sorted(glob(os.path.join('data/training_nonfaces', '*.jpg')))\n",
      "num_nonface_filenames = len(nonface_filenames)\n",
      "num_nonface_filenames\n",
      "141/7: n = num_face_filenames\n",
      "141/8:\n",
      "faces = []\n",
      "for i in range(n):\n",
      "    faces.append(cv2.imread(face_filenames[i], cv2.IMREAD_GRAYSCALE))\n",
      "141/9: nonface = cv2.imread(nonface_filenames[1], cv2.IMREAD_GRAYSCALE)\n",
      "141/10: nonface.shape\n",
      "141/11:\n",
      "nonfaces = []\n",
      "for i in range(n):\n",
      "    # Read a random nonface file\n",
      "    j = random.randint(0, num_nonface_filenames - 1)\n",
      "    nonface = cv2.imread(nonface_filenames[j], cv2.IMREAD_GRAYSCALE)\n",
      "    \n",
      "    wsize = random.randint(50, min(nonface.shape[0], nonface.shape[1]))\n",
      "    row = random.randint(0, nonface.shape[0]-wsize)\n",
      "    col = random.randint(0, nonface.shape[1]-wsize)\n",
      "    crop = nonface[row:row+wsize, col:col+wsize]\n",
      "\n",
      "    # Resize to be the right size\n",
      "    crop = cv2.resize(crop, (200, 200))\n",
      "    nonfaces.append(crop)\n",
      "141/12:\n",
      "@delayed\n",
      "def extract_feature_image(img, feature_type, feature_coord=None):\n",
      "    \"\"\"Extract the haar feature for the current image\"\"\"\n",
      "    ii = integral_image(img)\n",
      "    return haar_like_feature(ii, 0, 0, ii.shape[0], ii.shape[1],\n",
      "                             feature_type=feature_type,\n",
      "                             feature_coord=feature_coord)\n",
      "141/13: images = faces + nonfaces\n",
      "141/14:\n",
      "fig, axes = plt.subplots(3, 2)\n",
      "for idx, ax in enumerate(axes.ravel()):\n",
      "    image = images[0]\n",
      "    image = draw_haar_like_feature(image, 0, 0,\n",
      "                                   images.shape[2],\n",
      "                                   images.shape[1],\n",
      "                                   [feature_coord[0]])\n",
      "141/15:\n",
      "fig, axes = plt.subplots(3, 2)\n",
      "for idx, ax in enumerate(axes.ravel()):\n",
      "    image = images[0]\n",
      "    image = draw_haar_like_feature(image, 0, 0,\n",
      "                                   image.shape[2],\n",
      "                                   image.shape[1],\n",
      "                                   [feature_coord[0]])\n",
      "141/16:\n",
      "fig, axes = plt.subplots(3, 2)\n",
      "for idx, ax in enumerate(axes.ravel()):\n",
      "    image = images[0]\n",
      "    image = draw_haar_like_feature(image, 0, 0,\n",
      "                                   200, 200,\n",
      "                                   [feature_coord[0]])\n",
      "141/17:\n",
      "fig, axes = plt.subplots(3, 2)\n",
      "for idx, ax in enumerate(axes.ravel()):\n",
      "    image = images[0]\n",
      "    image = draw_haar_like_feature(image, 0, 0,\n",
      "                                   200, 200)\n",
      "141/18:\n",
      "fig, axes = plt.subplots(3, 2)\n",
      "for idx, ax in enumerate(axes.ravel()):\n",
      "    image = images[0]\n",
      "    image = draw_haar_like_feature(image, 0, 0,\n",
      "                                   200, 200, None)\n",
      "141/19:\n",
      "fig, axes = plt.subplots(3, 2)\n",
      "for idx, ax in enumerate(axes.ravel()):\n",
      "    image = images[0]\n",
      "    image = draw_haar_like_feature(image, 0, 0,\n",
      "                                   200, 200)\n",
      "141/20:\n",
      "fig, axes = plt.subplots(3, 2)\n",
      "for idx, ax in enumerate(axes.ravel()):\n",
      "    image = images[0]\n",
      "    image = draw_haar_like_feature(image, 0, 0,\n",
      "                                   200, 200, [(0,0)])\n",
      "141/21:\n",
      "fig, axes = plt.subplots(3, 2)\n",
      "for idx, ax in enumerate(axes.ravel()):\n",
      "    image = images[0]\n",
      "    image = draw_haar_like_feature(image, 0, 0,\n",
      "                                   200, 200, [[(0,0)]])\n",
      "141/22:\n",
      "fig, axes = plt.subplots(3, 2)\n",
      "for idx, ax in enumerate(axes.ravel()):\n",
      "    image = images[0]\n",
      "    image = draw_haar_like_feature(image, 0, 0,\n",
      "                                   200, 200, haar_like_feature_coord())\n",
      "141/23:\n",
      "fig, axes = plt.subplots(3, 2)\n",
      "for idx, ax in enumerate(axes.ravel()):\n",
      "    image = images[0]\n",
      "    image = draw_haar_like_feature(image, 0, 0,\n",
      "                                   200, 200, haar_like_feature_coord(200,200))\n",
      "142/1:\n",
      "import os\n",
      "import cv2\n",
      "from time import time\n",
      "import numpy as np\n",
      "import matplotlib.pyplot as plt\n",
      "from glob import glob\n",
      "import random\n",
      "from dask import delayed\n",
      "142/2:\n",
      "from sklearn.ensemble import RandomForestClassifier\n",
      "from sklearn.model_selection import train_test_split\n",
      "from sklearn.metrics import roc_auc_score\n",
      "142/3:\n",
      "from skimage.transform import integral_image\n",
      "from skimage.feature import haar_like_feature\n",
      "from skimage.feature import haar_like_feature_coord\n",
      "from skimage.feature import draw_haar_like_feature\n",
      "142/4:\n",
      "face_filenames = sorted(glob(os.path.join('data/tests', '*.jpg')))\n",
      "num_face_filenames = len(face_filenames)\n",
      "num_face_filenames\n",
      "142/5:\n",
      "nonface_filenames = sorted(glob(os.path.join('data/training_nonfaces', '*.jpg')))\n",
      "num_nonface_filenames = len(nonface_filenames)\n",
      "num_nonface_filenames\n",
      "142/6: n = num_face_filenames\n",
      "142/7:\n",
      "faces = []\n",
      "for i in range(n):\n",
      "    faces.append(cv2.imread(face_filenames[i], cv2.IMREAD_GRAYSCALE))\n",
      "142/8: nonface = cv2.imread(nonface_filenames[1], cv2.IMREAD_GRAYSCALE)\n",
      "142/9: nonface.shape\n",
      "142/10:\n",
      "nonfaces = []\n",
      "for i in range(n):\n",
      "    # Read a random nonface file\n",
      "    j = random.randint(0, num_nonface_filenames - 1)\n",
      "    nonface = cv2.imread(nonface_filenames[j], cv2.IMREAD_GRAYSCALE)\n",
      "    \n",
      "    wsize = random.randint(50, min(nonface.shape[0], nonface.shape[1]))\n",
      "    row = random.randint(0, nonface.shape[0]-wsize)\n",
      "    col = random.randint(0, nonface.shape[1]-wsize)\n",
      "    crop = nonface[row:row+wsize, col:col+wsize]\n",
      "\n",
      "    # Resize to be the right size\n",
      "    crop = cv2.resize(crop, (200, 200))\n",
      "    nonfaces.append(crop)\n",
      "142/11:\n",
      "@delayed\n",
      "def extract_feature_image(img, feature_type, feature_coord=None):\n",
      "    \"\"\"Extract the haar feature for the current image\"\"\"\n",
      "    ii = integral_image(img)\n",
      "    return haar_like_feature(ii, 0, 0, ii.shape[0], ii.shape[1],\n",
      "                             feature_type=feature_type,\n",
      "                             feature_coord=feature_coord)\n",
      "142/12: images = faces + nonfaces\n",
      "142/13: image0 = cv2.resize(images[0], (10,10))\n",
      "142/14:\n",
      "image0_ = draw_haar_like_feature(image0, 0, 0,\n",
      "                                 10, 10, \n",
      "                                 haar_like_feature_coord(10,10))\n",
      "142/15:\n",
      "image0_ = draw_haar_like_feature(image0, 0, 0,\n",
      "                                 10, 10, \n",
      "                                 haar_like_feature_coord(10,10)[0])\n",
      "142/16: image0\n",
      "142/17: plt.imshow(image0)\n",
      "142/18: image0 = cv2.resize(images[0], (50,50))\n",
      "142/19:\n",
      "image0_ = draw_haar_like_feature(image0, 0, 0,\n",
      "                                 50, 50, \n",
      "                                 haar_like_feature_coord(50,50)[0])\n",
      "142/20:\n",
      "image0_ = draw_haar_like_feature(image0, 0, 0,\n",
      "                                 50, 50, \n",
      "                                 haar_like_feature_coord(50,50)[0], feature_type=['type-2-x'])\n",
      "142/21:\n",
      "image0_ = draw_haar_like_feature(image0, 0, 0,\n",
      "                                 50, 50, \n",
      "                                 haar_like_feature_coord(50,50,feature_type=['type-2-x'])[0])\n",
      "142/22: t = time.process_time()\n",
      "142/23: t = time()\n",
      "142/24: time()\n",
      "142/25: time() - t\n",
      "142/26: image0 = cv2.resize(images[0], (20,20))\n",
      "142/27:\n",
      "t = time()\n",
      "image0_ = draw_haar_like_feature(image0, 0, 0,\n",
      "                                 20, 20, \n",
      "                                 haar_like_feature_coord(20,20,feature_type=['type-2-x'])[0])\n",
      "time() - t\n",
      "142/28: plt.imshow(image0)\n",
      "142/29: plt.imshow(image0_)\n",
      "142/30: image0 = cv2.resize(images[0], (36,36))\n",
      "142/31: plt.imshow(image0)\n",
      "142/32: plt.imshow(image0, cmap='gray')\n",
      "142/33:\n",
      "t = time()\n",
      "image0_ = draw_haar_like_feature(image0, 0, 0,\n",
      "                                 36,36, \n",
      "                                 haar_like_feature_coord(36,36,feature_type=['type-2-x'])[0])\n",
      "time() - t\n",
      "142/34: plt.imshow(image0_)\n",
      "142/35:\n",
      "t = time()\n",
      "image0_ = draw_haar_like_feature(integral_image(image0), 0, 0,\n",
      "                                 36,36, \n",
      "                                 haar_like_feature_coord(36,36,feature_type=['type-2-x'])[0])\n",
      "time() - t\n",
      "142/36: plt.imshow(image0_)\n",
      "142/37: image0\n",
      "142/38: image0.shape\n",
      "142/39: image0_.shape\n",
      "142/40: feature_types = ['type-2-x', 'type-2-y']\n",
      "142/41: len(images)\n",
      "142/42: X = extract_feature_image(img, feature_types) for img in images\n",
      "142/43:\n",
      "# Build a computation graph using Dask. This allows the use of multiple\n",
      "# CPU cores later during the actual computation\n",
      "X = delayed(extract_feature_image(img, feature_types) for img in images)\n",
      "# Compute the result\n",
      "t_start = time()\n",
      "X = np.array(X.compute(scheduler='threads'))\n",
      "time() - t_start\n",
      "143/1:\n",
      "import os\n",
      "import cv2\n",
      "from time import time\n",
      "import numpy as np\n",
      "import matplotlib.pyplot as plt\n",
      "from glob import glob\n",
      "import random\n",
      "from dask import delayed\n",
      "143/2:\n",
      "from sklearn.ensemble import RandomForestClassifier\n",
      "from sklearn.model_selection import train_test_split\n",
      "from sklearn.metrics import roc_auc_score\n",
      "143/3:\n",
      "from skimage.transform import integral_image\n",
      "from skimage.feature import haar_like_feature\n",
      "from skimage.feature import haar_like_feature_coord\n",
      "from skimage.feature import draw_haar_like_feature\n",
      "143/4: images = lfw_subset()\n",
      "143/5:\n",
      "from skimage.data import lfw_subset\n",
      "images = lfw_subset()\n",
      "143/6: images[0].shape\n",
      "143/7: len(images)\n",
      "143/8: print(face_filenames[0])\n",
      "143/9:\n",
      "face_filenames = sorted(glob(os.path.join('data/tests', '*.jpg')))\n",
      "num_face_filenames = len(face_filenames)\n",
      "num_face_filenames\n",
      "143/10: print(face_filenames[0])\n",
      "143/11:\n",
      "count = 0\n",
      "for f in face_filenames:\n",
      "    if f.split('_')[2] == 1:\n",
      "        count += 1\n",
      "count\n",
      "143/12: print(face_filenames[0].split('_')[2])\n",
      "143/13:\n",
      "count = 0\n",
      "for f in face_filenames:\n",
      "    if f.split('_')[2] is 1:\n",
      "        count += 1\n",
      "count\n",
      "143/14: print(face_filenames[0].split('_')[2] == 1)\n",
      "143/15: print(face_filenames[0].split('_')[2] is 1)\n",
      "143/16: print(face_filenames[0].split('_')[2] - 1)\n",
      "143/17:\n",
      "count = 0\n",
      "for f in face_filenames:\n",
      "    if int(f.split('_')[2]) is 1:\n",
      "        count += 1\n",
      "count\n",
      "143/18:\n",
      "count = 0\n",
      "for f in face_filenames:\n",
      "    if int(f.split('_')[2]) is 0:\n",
      "        count += 1\n",
      "count\n",
      "143/19: len(images)\n",
      "143/20:\n",
      "feature_types = ['type-2-x', 'type-2-y']\n",
      "\n",
      "# Build a computation graph using Dask. This allows the use of multiple\n",
      "# CPU cores later during the actual computation\n",
      "X = delayed(extract_feature_image(img, feature_types) for img in images)\n",
      "# Compute the result\n",
      "t_start = time()\n",
      "X = np.array(X.compute(scheduler='threads'))\n",
      "time_full_feature_comp = time() - t_start\n",
      "\n",
      "y = np.array([1] * n + [0] * n)\n",
      "X_train, X_test, y_train, y_test = train_test_split(X, y, train_size=150,\n",
      "                                                    random_state=0,\n",
      "                                                    stratify=y)\n",
      "\n",
      "# Extract all possible features\n",
      "feature_coord, feature_type = \\\n",
      "    haar_like_feature_coord(width=images.shape[2], height=images.shape[1],\n",
      "                            feature_type=feature_types)\n",
      "143/21:\n",
      "@delayed\n",
      "def extract_feature_image(img, feature_type, feature_coord=None):\n",
      "    \"\"\"Extract the haar feature for the current image\"\"\"\n",
      "    ii = integral_image(img)\n",
      "    return haar_like_feature(ii, 0, 0, ii.shape[0], ii.shape[1],\n",
      "                             feature_type=feature_type,\n",
      "                             feature_coord=feature_coord)\n",
      "143/22:\n",
      "feature_types = ['type-2-x', 'type-2-y']\n",
      "\n",
      "# Build a computation graph using Dask. This allows the use of multiple\n",
      "# CPU cores later during the actual computation\n",
      "X = delayed(extract_feature_image(img, feature_types) for img in images)\n",
      "# Compute the result\n",
      "t_start = time()\n",
      "X = np.array(X.compute(scheduler='threads'))\n",
      "time_full_feature_comp = time() - t_start\n",
      "\n",
      "y = np.array([1] * n + [0] * n)\n",
      "X_train, X_test, y_train, y_test = train_test_split(X, y, train_size=150,\n",
      "                                                    random_state=0,\n",
      "                                                    stratify=y)\n",
      "\n",
      "# Extract all possible features\n",
      "feature_coord, feature_type = \\\n",
      "    haar_like_feature_coord(width=images.shape[2], height=images.shape[1],\n",
      "                            feature_type=feature_types)\n",
      "143/23: time_full_feature_comp\n",
      "143/24: X\n",
      "143/25: X.shape\n",
      "143/26:\n",
      "face_filenames0 = sorted(glob(os.path.join('data/train_0', '*.jpg')))\n",
      "num_face_filenames0 = len(face_filenames0)\n",
      "num_face_filenames0\n",
      "143/27:\n",
      "count = 0\n",
      "for f in face_filenames:\n",
      "    if int(f.split('_')[2]) is 0:\n",
      "        count += 1\n",
      "count\n",
      "143/28:\n",
      "count = 0\n",
      "for f in face_filenames0:\n",
      "    if int(f.split('_')[2]) is 0:\n",
      "        count += 1\n",
      "count\n",
      "143/29:\n",
      "count = 0\n",
      "for f in face_filenames0:\n",
      "    if int(f.split('_')[2]) is 1:\n",
      "        count += 1\n",
      "count\n",
      "143/30:\n",
      "count = 0\n",
      "for f in face_filenames0:\n",
      "    if int(f.split('_')[1]) is 1:\n",
      "        count += 1\n",
      "count\n",
      "143/31:\n",
      "count = 0\n",
      "for f in face_filenames0:\n",
      "    if int(f.split('_')[3]) is 1:\n",
      "        count += 1\n",
      "count\n",
      "143/32:\n",
      "count = 0\n",
      "for f in face_filenames0:\n",
      "    if int(f.split('_')[3]) is 0:\n",
      "        count += 1\n",
      "count\n",
      "143/33:\n",
      "count = 0\n",
      "for f in face_filenames0:\n",
      "    if int(f.split('_')[3]) is 0: # race is white\n",
      "        count += 1\n",
      "count\n",
      "143/34:\n",
      "nonface_filenames = sorted(glob(os.path.join('data/training_nonfaces', '*.jpg')))\n",
      "num_nonface_filenames = len(nonface_filenames)\n",
      "num_nonface_filenames\n",
      "143/35: n = num_face_filenames0\n",
      "143/36: n = 500\n",
      "143/37:\n",
      "faces0 = []\n",
      "for i in range(n):\n",
      "    faces0.append(cv2.imread(face_filenames0[i], cv2.IMREAD_GRAYSCALE))\n",
      "143/38:\n",
      "nonfaces = []\n",
      "for i in range(n):\n",
      "    # Read a random nonface file\n",
      "    j = random.randint(0, num_nonface_filenames - 1)\n",
      "    nonface = cv2.imread(nonface_filenames[j], cv2.IMREAD_GRAYSCALE)\n",
      "    \n",
      "    wsize = random.randint(50, min(nonface.shape[0], nonface.shape[1]))\n",
      "    row = random.randint(0, nonface.shape[0]-wsize)\n",
      "    col = random.randint(0, nonface.shape[1]-wsize)\n",
      "    crop = nonface[row:row+wsize, col:col+wsize]\n",
      "\n",
      "    # Resize to be the right size\n",
      "    crop = cv2.resize(crop, (200, 200))\n",
      "    nonfaces.append(crop)\n",
      "143/39:\n",
      "@delayed\n",
      "def extract_feature_image(img, feature_type, feature_coord=None):\n",
      "    \"\"\"Extract the haar feature for the current image\"\"\"\n",
      "    ii = integral_image(img)\n",
      "    return haar_like_feature(ii, 0, 0, ii.shape[0], ii.shape[1],\n",
      "                             feature_type=feature_type,\n",
      "                             feature_coord=feature_coord)\n",
      "143/40:\n",
      "images = faces + nonfaces\n",
      "len(images)\n",
      "143/41:\n",
      "images = faces0 + nonfaces\n",
      "len(images)\n",
      "143/42: image0 = cv2.resize(images[0], (30,30))\n",
      "143/43: plt.imshow(image0, cmap='gray')\n",
      "143/44: image0 = cv2.resize(images[0], (24,24))\n",
      "143/45: plt.imshow(image0, cmap='gray')\n",
      "143/46: image0 = cv2.resize(images[0], (36,36))\n",
      "143/47: plt.imshow(image0, cmap='gray')\n",
      "143/48:\n",
      "t = time()\n",
      "image0_ = draw_haar_like_feature(integral_image(image0), 0, 0,\n",
      "                                 36,36, \n",
      "                                 haar_like_feature_coord(36,36,feature_type=['type-2-x', 'type-2-y'])[0])\n",
      "time() - t\n",
      "143/49: feature_types = ['type-2-x', 'type-2-y']\n",
      "143/50: len(images)\n",
      "143/51:\n",
      "# Build a computation graph using Dask. This allows the use of multiple\n",
      "# CPU cores later during the actual computation\n",
      "X = delayed(extract_feature_image(img, feature_types) for cv2.resize(img,(36,36)) in images)\n",
      "# Compute the result\n",
      "t_start = time()\n",
      "X = np.array(X.compute(scheduler='threads'))\n",
      "time() - t_start\n",
      "143/52:\n",
      "@delayed\n",
      "def extract_feature_image(img, feature_type, feature_coord=None):\n",
      "    \"\"\"Extract the haar feature for the current image\"\"\"\n",
      "    img = cv2.resize(img, (36,36))\n",
      "    ii = integral_image(img)\n",
      "    return haar_like_feature(ii, 0, 0, ii.shape[0], ii.shape[1],\n",
      "                             feature_type=feature_type,\n",
      "                             feature_coord=feature_coord)\n",
      "143/53:\n",
      "# Build a computation graph using Dask. This allows the use of multiple\n",
      "# CPU cores later during the actual computation\n",
      "X = delayed(extract_feature_image(img, feature_types) for img in images)\n",
      "# Compute the result\n",
      "t_start = time()\n",
      "X = np.array(X.compute(scheduler='threads'))\n",
      "time() - t_start\n",
      "143/54: extract_feature_image(images[0], feature_types, feature_coord=None)\n",
      "143/55: compute(extract_feature_image(images[0], feature_types, feature_coord=None))\n",
      "143/56:\n",
      "X = delayed(extract_feature_image(img, feature_types) for img in images[:2])\n",
      "# Compute the result\n",
      "t_start = time()\n",
      "X = np.array(X.compute(scheduler='threads'))\n",
      "time() - t_start\n",
      "143/57: X\n",
      "143/58:\n",
      "X = delayed(extract_feature_image(img, feature_types) for img in images[:20])\n",
      "# Compute the result\n",
      "t_start = time()\n",
      "X = np.array(X.compute(scheduler='threads'))\n",
      "time() - t_start\n",
      "143/59: X\n",
      "143/60: X.shape\n",
      "143/61: n = 250\n",
      "143/62:\n",
      "faces0 = []\n",
      "for i in range(n):\n",
      "    faces0.append(cv2.imread(face_filenames0[i], cv2.IMREAD_GRAYSCALE))\n",
      "143/63:\n",
      "nonfaces = []\n",
      "for i in range(n):\n",
      "    # Read a random nonface file\n",
      "    j = random.randint(0, num_nonface_filenames - 1)\n",
      "    nonface = cv2.imread(nonface_filenames[j], cv2.IMREAD_GRAYSCALE)\n",
      "    \n",
      "    wsize = random.randint(50, min(nonface.shape[0], nonface.shape[1]))\n",
      "    row = random.randint(0, nonface.shape[0]-wsize)\n",
      "    col = random.randint(0, nonface.shape[1]-wsize)\n",
      "    crop = nonface[row:row+wsize, col:col+wsize]\n",
      "\n",
      "    # Resize to be the right size\n",
      "    crop = cv2.resize(crop, (200, 200))\n",
      "    nonfaces.append(crop)\n",
      "143/64:\n",
      "images = faces0 + nonfaces\n",
      "len(images)\n",
      "143/65: len(images)\n",
      "143/66:\n",
      "# Build a computation graph using Dask. This allows the use of multiple\n",
      "# CPU cores later during the actual computation\n",
      "X = delayed(extract_feature_image(img, feature_types) for img in images)\n",
      "# Compute the result\n",
      "t_start = time()\n",
      "X = np.array(X.compute(scheduler='threads'))\n",
      "time() - t_start\n",
      "143/67:\n",
      "y = np.array([1] * n + [0] * n)\n",
      "X_train, X_test, y_train, y_test = train_test_split(X, y, train_size=150,\n",
      "                                                    random_state=0,\n",
      "                                                    stratify=y)\n",
      "\n",
      "# Extract all possible features\n",
      "feature_coord, feature_type = \\\n",
      "    haar_like_feature_coord(width=images.shape[2], height=images.shape[1],\n",
      "                            feature_type=feature_types)\n",
      "143/68: images.shape\n",
      "143/69: np.array(images).shape\n",
      "143/70: y = np.array([1] * n + [0] * n)\n",
      "143/71:\n",
      "images = np.array(images)\n",
      "# Extract all possible features\n",
      "feature_coord, feature_type = \\\n",
      "    haar_like_feature_coord(width=images.shape[2], height=images.shape[1],\n",
      "                            feature_type=feature_types)\n",
      "144/1:\n",
      "import os\n",
      "import cv2\n",
      "from time import time\n",
      "import numpy as np\n",
      "import matplotlib.pyplot as plt\n",
      "from glob import glob\n",
      "import random\n",
      "from dask import delayed\n",
      "144/2:\n",
      "from sklearn.ensemble import RandomForestClassifier\n",
      "from sklearn.model_selection import train_test_split\n",
      "from sklearn.metrics import roc_auc_score\n",
      "144/3:\n",
      "from skimage.transform import integral_image\n",
      "from skimage.feature import haar_like_feature\n",
      "from skimage.feature import haar_like_feature_coord\n",
      "from skimage.feature import draw_haar_like_feature\n",
      "146/1:\n",
      "import os\n",
      "import cv2\n",
      "from time import time\n",
      "import numpy as np\n",
      "import matplotlib.pyplot as plt\n",
      "from glob import glob\n",
      "import random\n",
      "from dask import delayed\n",
      "146/2:\n",
      "from sklearn.ensemble import RandomForestClassifier\n",
      "from sklearn.model_selection import train_test_split\n",
      "from sklearn.metrics import roc_auc_score\n",
      "146/3:\n",
      "from skimage.transform import integral_image\n",
      "from skimage.feature import haar_like_feature\n",
      "from skimage.feature import haar_like_feature_coord\n",
      "from skimage.feature import draw_haar_like_feature\n",
      "146/4: import dill\n",
      "146/5: A= [1,2,3]\n",
      "146/6: dill.dump_session('notebook_env.db')\n",
      "147/1:\n",
      "import os\n",
      "import cv2\n",
      "from time import time\n",
      "import numpy as np\n",
      "import matplotlib.pyplot as plt\n",
      "from glob import glob\n",
      "import random\n",
      "from dask import delayed\n",
      "147/2:\n",
      "from sklearn.ensemble import RandomForestClassifier\n",
      "from sklearn.model_selection import train_test_split\n",
      "from sklearn.metrics import roc_auc_score\n",
      "147/3:\n",
      "from skimage.transform import integral_image\n",
      "from skimage.feature import haar_like_feature\n",
      "from skimage.feature import haar_like_feature_coord\n",
      "from skimage.feature import draw_haar_like_feature\n",
      "147/4: import dill\n",
      "147/5: dill.load_session('notebook_env.db')\n",
      "147/6: A\n",
      "147/7:\n",
      "test_face_names = sorted(glob(os.path.join('data/tests', '*.jpg')))\n",
      "n_test = len(test_faces)\n",
      "147/8:\n",
      "test_face_names = sorted(glob(os.path.join('data/tests', '*.jpg')))\n",
      "n_test = len(test_face_names)\n",
      "147/9: test_nonface_names = sorted(glob(os.path.join('data/testing_nonfaces', '*.jpg')))\n",
      "147/10: ## Testing data\n",
      "147/11:\n",
      "face_filenames0 = sorted(glob(os.path.join('data/train_0', '*.jpg')))\n",
      "num_face_filenames0 = len(face_filenames0)\n",
      "num_face_filenames0\n",
      "147/12:\n",
      "nonface_filenames = sorted(glob(os.path.join('data/training_nonfaces', '*.jpg')))\n",
      "num_nonface_filenames = len(nonface_filenames)\n",
      "num_nonface_filenames\n",
      "147/13: n = 250\n",
      "147/14:\n",
      "faces0 = []\n",
      "for i in range(n):\n",
      "    faces0.append(cv2.imread(face_filenames0[i], cv2.IMREAD_GRAYSCALE))\n",
      "147/15:\n",
      "nonfaces = []\n",
      "for i in range(n):\n",
      "    # Read a random nonface file\n",
      "    j = random.randint(0, num_nonface_filenames - 1)\n",
      "    nonface = cv2.imread(nonface_filenames[j], cv2.IMREAD_GRAYSCALE)\n",
      "    \n",
      "    wsize = random.randint(50, min(nonface.shape[0], nonface.shape[1]))\n",
      "    row = random.randint(0, nonface.shape[0]-wsize)\n",
      "    col = random.randint(0, nonface.shape[1]-wsize)\n",
      "    crop = nonface[row:row+wsize, col:col+wsize]\n",
      "\n",
      "    # Resize to be the right size\n",
      "    crop = cv2.resize(crop, (200, 200))\n",
      "    nonfaces.append(crop)\n",
      "147/16:\n",
      "faces_t = []\n",
      "for i in range(n_test):\n",
      "    faces_t.append(cv2.imread(test_face_names[i], cv2.IMREAD_GRAYSCALE))\n",
      "147/17:\n",
      "test_nonface_names = sorted(glob(os.path.join('data/testing_nonfaces', '*.jpg')))\n",
      "n_nonfaces_test = len(test_nonface_names)\n",
      "147/18:\n",
      "nonfaces_t = []\n",
      "for i in range(n_test):\n",
      "    # Read a random nonface file\n",
      "    j = random.randint(0, n_nonfaces_test - 1)\n",
      "    nonface = cv2.imread(test_nonface_names[j], cv2.IMREAD_GRAYSCALE)\n",
      "    \n",
      "    wsize = random.randint(50, min(nonface.shape[0], nonface.shape[1]))\n",
      "    row = random.randint(0, nonface.shape[0]-wsize)\n",
      "    col = random.randint(0, nonface.shape[1]-wsize)\n",
      "    crop = nonface[row:row+wsize, col:col+wsize]\n",
      "\n",
      "    # Resize to be the right size\n",
      "    crop = cv2.resize(crop, (200, 200))\n",
      "    nonfaces_t.append(crop)\n",
      "147/19: n_nonfaces_test\n",
      "147/20:\n",
      "nonfaces_t = []\n",
      "for i in range(n_test):\n",
      "    # Read a random nonface file\n",
      "    j = random.randint(0, n_nonfaces_test - 1)\n",
      "    nonface = cv2.imread(test_nonface_names[j], cv2.IMREAD_GRAYSCALE)\n",
      "    \n",
      "    wsize = random.randint(36, min(nonface.shape[0], nonface.shape[1]))\n",
      "    row = random.randint(0, nonface.shape[0]-wsize)\n",
      "    col = random.randint(0, nonface.shape[1]-wsize)\n",
      "    crop = nonface[row:row+wsize, col:col+wsize]\n",
      "\n",
      "    # Resize to be the right size\n",
      "    crop = cv2.resize(crop, (200, 200))\n",
      "    nonfaces_t.append(crop)\n",
      "147/21: len(faces0)\n",
      "147/22: len(nonfaces)\n",
      "147/23: len(faces_t), len(nonfaces_t)\n",
      "147/24:\n",
      "@delayed\n",
      "def extract_feature_image(img, feature_type, feature_coord=None):\n",
      "    \"\"\"Extract the haar feature for the current image\"\"\"\n",
      "    img = cv2.resize(img, (36,36))\n",
      "    ii = integral_image(img)\n",
      "    return haar_like_feature(ii, 0, 0, ii.shape[0], ii.shape[1],\n",
      "                             feature_type=feature_type,\n",
      "                             feature_coord=feature_coord)\n",
      "147/25:\n",
      "images0 = faces0 + nonfaces\n",
      "len(images)\n",
      "147/26:\n",
      "images0 = faces0 + nonfaces\n",
      "len(images0)\n",
      "147/27:\n",
      "images_t = faces_t + nonfaces_t\n",
      "len(images_t)\n",
      "147/28: image0 = cv2.resize(images[0], (36,36))\n",
      "147/29: image0 = cv2.resize(images0[0], (36,36))\n",
      "147/30: plt.imshow(image0, cmap='gray')\n",
      "147/31:\n",
      "t = time()\n",
      "image0_ = draw_haar_like_feature(integral_image(image0), 0, 0,\n",
      "                                 36,36, \n",
      "                                 haar_like_feature_coord(36,36,feature_type=['type-2-x', 'type-2-y'])[0])\n",
      "time() - t\n",
      "147/32: feature_types = ['type-2-x', 'type-2-y']\n",
      "147/33: plt.imshow(image0_)\n",
      "147/34: image0_.shape\n",
      "147/35:\n",
      "# Build a computation graph using Dask. This allows the use of multiple\n",
      "# CPU cores later during the actual computation\n",
      "X_t = delayed(extract_feature_image(img, feature_types) for img in images_t)\n",
      "# Compute the result\n",
      "t_start = time()\n",
      "X_t = np.array(X_t.compute(scheduler='threads'))\n",
      "time() - t_start\n",
      "147/36: dill.dump_session('notebook_env.db')\n",
      "147/37: len(images0)\n",
      "147/38: n = 300\n",
      "147/39:\n",
      "faces0 = []\n",
      "for i in range(n):\n",
      "    faces0.append(cv2.imread(face_filenames0[i], cv2.IMREAD_GRAYSCALE))\n",
      "147/40:\n",
      "nonfaces = []\n",
      "for i in range(n):\n",
      "    # Read a random nonface file\n",
      "    j = random.randint(0, num_nonface_filenames - 1)\n",
      "    nonface = cv2.imread(nonface_filenames[j], cv2.IMREAD_GRAYSCALE)\n",
      "    \n",
      "    wsize = random.randint(50, min(nonface.shape[0], nonface.shape[1]))\n",
      "    row = random.randint(0, nonface.shape[0]-wsize)\n",
      "    col = random.randint(0, nonface.shape[1]-wsize)\n",
      "    crop = nonface[row:row+wsize, col:col+wsize]\n",
      "\n",
      "    # Resize to be the right size\n",
      "    crop = cv2.resize(crop, (200, 200))\n",
      "    nonfaces.append(crop)\n",
      "147/41:\n",
      "images0 = faces0 + nonfaces\n",
      "len(images0)\n",
      "147/42: len(images0)\n",
      "147/43:\n",
      "# Build a computation graph using Dask. This allows the use of multiple\n",
      "# CPU cores later during the actual computation\n",
      "X = delayed(extract_feature_image(img, feature_types) for img in images0)\n",
      "# Compute the result\n",
      "t_start = time()\n",
      "X = np.array(X.compute(scheduler='threads'))\n",
      "time() - t_start\n",
      "147/44: dill.dump_session('notebook_env.db')\n",
      "147/45: y = np.array([1] * n + [0] * n)\n",
      "147/46:\n",
      "# Train a random forest classifier and assess its performance\n",
      "clf = RandomForestClassifier(n_estimators=1000, max_depth=None,\n",
      "                             max_features=100, n_jobs=-1, random_state=0)\n",
      "t_start = time()\n",
      "clf.fit(X, y)\n",
      "time() - t_start\n",
      "147/47: print(clf.score())\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "147/48: print(clf.score(X,y))\n",
      "147/49:\n",
      "face_filenames25 = sorted(glob(os.path.join('data/train_25', '*.jpg')))\n",
      "num_face_filenames25 = len(face_filenames25)\n",
      "num_face_filenames25\n",
      "147/50:\n",
      "face_filenames50 = sorted(glob(os.path.join('data/train_50', '*.jpg')))\n",
      "num_face_filenames50 = len(face_filenames50)\n",
      "num_face_filenames50\n",
      "147/51:\n",
      "face_filenames75 = sorted(glob(os.path.join('data/train_75', '*.jpg')))\n",
      "num_face_filenames75 = len(face_filenames75)\n",
      "num_face_filenames75\n",
      "147/52:\n",
      "face_filenames100 = sorted(glob(os.path.join('data/train_100', '*.jpg')))\n",
      "num_face_filenames100 = len(face_filenames100)\n",
      "num_face_filenames100\n",
      "147/53: X0 = X\n",
      "147/54: n\n",
      "147/55:\n",
      "def getimgs(facefiles):\n",
      "    faces = []\n",
      "    for i in range(n):\n",
      "        faces.append(cv2.imread(facefiles[i], cv2.IMREAD_GRAYSCALE))\n",
      "    return faces + nonfaces\n",
      "147/56: images25 = getimgs(face_filenames25)\n",
      "147/57: images25.shape\n",
      "147/58: np.array(images25).shape\n",
      "147/59:\n",
      "def getx(facefiles):\n",
      "    ims = getimgs(facefiles)\n",
      "    X = delayed(extract_feature_image(img, feature_types) for img in ims)\n",
      "    return np.array(X.compute(scheduler='threads'))\n",
      "147/60:\n",
      "t_start = time()\n",
      "X25 = getx(face_filenames25)\n",
      "time() - t_start\n",
      "147/61:\n",
      "t_start = time()\n",
      "X50 = getx(face_filenames50)\n",
      "time() - t_start\n",
      "147/62:\n",
      "t_start = time()\n",
      "X75 = getx(face_filenames75)\n",
      "time() - t_start\n",
      "147/63:\n",
      "t_start = time()\n",
      "X100 = getx(face_filenames100)\n",
      "time() - t_start\n",
      "147/64: dill.dump_session('notebook_env.db')\n",
      "147/65: len(y), X100.shape\n",
      "147/66: print(clf.score(X_t,y_t))\n",
      "147/67: y_t = [1]*len(faces_t) + [0]*len(nonfaces_t)\n",
      "147/68: print(clf.score(X_t,y_t))\n",
      "148/1:\n",
      "import os\n",
      "import time\n",
      "import dill\n",
      "from typing import List\n",
      "from functools import reduce\n",
      "from calendar import monthrange\n",
      "148/2:\n",
      "import json\n",
      "import csv\n",
      "import requests\n",
      "import xml.etree.ElementTree as ET\n",
      "from bs4 import BeautifulSoup\n",
      "import urllib.request as urllib2\n",
      "148/3:\n",
      "import matplotlib.pyplot as plt, bar\n",
      "import numpy as np\n",
      "from scipy import stats\n",
      "from random import sample, randint\n",
      "from sklearn.decomposition import PCA\n",
      "148/4:\n",
      "import matplotlib.pyplot as plt\n",
      "import numpy as np\n",
      "from scipy import stats\n",
      "from random import sample, randint\n",
      "from sklearn.decomposition import PCA\n",
      "148/5:\n",
      "import nltk\n",
      "import text_summarizer\n",
      "from gensim.utils import simple_preprocess\n",
      "from gensim.summarization.summarizer import summarize\n",
      "from gensim.models import Word2Vec, LsiModel\n",
      "from gensim.test.utils import common_dictionary, common_corpus\n",
      "148/6:\n",
      "url = 'https://www.enchantedlearning.com/wordlist/adjectivesforpeople.shtml'\n",
      "file = urllib2.urlopen(url)\n",
      "html = file.read()\n",
      "file.close()\n",
      "soup = BeautifulSoup(html)\n",
      "148/7: adj = soup.find_all(attrs={\"class\": \"wordlist-item\"})\n",
      "148/8: adjs = [x.contents[0] for x in adj]\n",
      "148/9: adjs[:5]\n",
      "148/10:\n",
      "with open('adjectives.csv', mode='w') as f:\n",
      "    writer = csv.writer(f, delimiter=',')\n",
      "    writer.writerow(adjs)\n",
      "148/11:\n",
      "with open('adjectives.csv') as csv_file:\n",
      "    adjectives = csv_file.read().split(\",\")\n",
      "148/12: len(adjectives)\n",
      "148/13: len(adjs)\n",
      "148/14:\n",
      "class Doc:\n",
      "    def __init__(self, id: int, gdescriptors: List[str], descriptors: List[str], text: str) -> None:\n",
      "        self.id = id\n",
      "        self.gdescriptors = gdescriptors\n",
      "        self.descriptors = descriptors\n",
      "        self.text = text\n",
      "148/15:\n",
      "def parse_doc(file: str) -> Doc:\n",
      "    tree = ET.parse(file)\n",
      "    root = tree.getroot()\n",
      "    try:\n",
      "        fulltext = root.find('body').find('body.content').find(\"*[@class='full_text']\")\n",
      "        text = reduce(lambda x, y: x + y, [x.text for x in fulltext.findall('p')])\n",
      "        id = int(root.find('head').find('docdata').find('doc-id').attrib['id-string'])\n",
      "        classifiers = root.find('head').find('docdata').find('identified-content')\n",
      "        gclassifiers = classifiers.findall(\"*[@class='online_producer'][@type='general_descriptor']\")\n",
      "        dclassifiers = classifiers.findall(\"*[@class='online_producer'][@type='descriptor']\")\n",
      "        gdescriptors = [c.text for c in gclassifiers]\n",
      "        descriptors = [c.text for c in dclassifiers]\n",
      "        return Doc(id, gdescriptors, descriptors, text)\n",
      "    except:\n",
      "        return None\n",
      "148/16:\n",
      "def get_docs_by_month(year: int, month: int) -> List[Doc]:\n",
      "    days = monthrange(year, month)[1]\n",
      "    docs = []\n",
      "    for day in range(1, days+1):\n",
      "        for file in os.listdir(f'data/{year}/{month:02}/{day:02}/'):\n",
      "            doc = parse_doc(f'data/{year}/{month:02}/{day:02}/{file}')\n",
      "            if doc:\n",
      "                docs.append(doc)\n",
      "    return docs\n",
      "148/17:\n",
      "t = time.process_time()\n",
      "all_ = []\n",
      "for y in range(1988, 2007):\n",
      "    print(y)\n",
      "    for i in range(1,13):\n",
      "        all_ += get_docs_by_month(y, i)\n",
      "time.process_time() - t\n",
      "148/18: len(all_)\n",
      "148/19: dill.dump_session('thesis_env.db')\n",
      "149/1:\n",
      "import os\n",
      "import time\n",
      "import dill\n",
      "from typing import List\n",
      "from functools import reduce\n",
      "from calendar import monthrange\n",
      "149/2:\n",
      "import json\n",
      "import csv\n",
      "import requests\n",
      "import xml.etree.ElementTree as ET\n",
      "from bs4 import BeautifulSoup\n",
      "import urllib.request as urllib2\n",
      "149/3:\n",
      "import matplotlib.pyplot as plt\n",
      "import numpy as np\n",
      "from scipy import stats\n",
      "from random import sample, randint\n",
      "from sklearn.decomposition import PCA\n",
      "149/4:\n",
      "import nltk\n",
      "import text_summarizer\n",
      "from gensim.utils import simple_preprocess\n",
      "from gensim.summarization.summarizer import summarize\n",
      "from gensim.models import Word2Vec, LsiModel\n",
      "from gensim.test.utils import common_dictionary, common_corpus\n",
      "149/5:\n",
      "url = 'https://www.enchantedlearning.com/wordlist/adjectivesforpeople.shtml'\n",
      "file = urllib2.urlopen(url)\n",
      "html = file.read()\n",
      "file.close()\n",
      "soup = BeautifulSoup(html)\n",
      "149/6: adj = soup.find_all(attrs={\"class\": \"wordlist-item\"})\n",
      "149/7: adjs = [x.contents[0] for x in adj]\n",
      "149/8: adjs[:5]\n",
      "149/9:\n",
      "with open('adjectives.csv', mode='w') as f:\n",
      "    writer = csv.writer(f, delimiter=',')\n",
      "    writer.writerow(adjs)\n",
      "149/10:\n",
      "with open('adjectives.csv') as csv_file:\n",
      "    adjectives = csv_file.read().split(\",\")\n",
      "149/11:\n",
      "class Doc:\n",
      "    def __init__(self, id: int, gdescriptors: List[str], descriptors: List[str], text: str) -> None:\n",
      "        self.id = id\n",
      "        self.gdescriptors = gdescriptors\n",
      "        self.descriptors = descriptors\n",
      "        self.text = text\n",
      "149/12:\n",
      "def parse_doc(file: str) -> Doc:\n",
      "    tree = ET.parse(file)\n",
      "    root = tree.getroot()\n",
      "    try:\n",
      "        fulltext = root.find('body').find('body.content').find(\"*[@class='full_text']\")\n",
      "        text = reduce(lambda x, y: x + y, [x.text for x in fulltext.findall('p')])\n",
      "        id = int(root.find('head').find('docdata').find('doc-id').attrib['id-string'])\n",
      "        classifiers = root.find('head').find('docdata').find('identified-content')\n",
      "        gclassifiers = classifiers.findall(\"*[@class='online_producer'][@type='general_descriptor']\")\n",
      "        dclassifiers = classifiers.findall(\"*[@class='online_producer'][@type='descriptor']\")\n",
      "        gdescriptors = [c.text for c in gclassifiers]\n",
      "        descriptors = [c.text for c in dclassifiers]\n",
      "        return Doc(id, gdescriptors, descriptors, text)\n",
      "    except:\n",
      "        return None\n",
      "149/13:\n",
      "def get_docs_by_month(year: int, month: int) -> List[Doc]:\n",
      "    days = monthrange(year, month)[1]\n",
      "    docs = []\n",
      "    for day in range(1, days+1):\n",
      "        for file in os.listdir(f'data/{year}/{month:02}/{day:02}/'):\n",
      "            doc = parse_doc(f'data/{year}/{month:02}/{day:02}/{file}')\n",
      "            if doc:\n",
      "                docs.append(doc)\n",
      "    return docs\n",
      "149/14: all88jan = get_docs_by_month(1988, 1)\n",
      "149/15: dill.dump_session('thesis_env.db')\n",
      "150/1:\n",
      "import os\n",
      "import time\n",
      "import dill\n",
      "from typing import List\n",
      "from functools import reduce\n",
      "from calendar import monthrange\n",
      "150/2:\n",
      "import json\n",
      "import csv\n",
      "import requests\n",
      "import xml.etree.ElementTree as ET\n",
      "from bs4 import BeautifulSoup\n",
      "import urllib.request as urllib2\n",
      "150/3:\n",
      "import matplotlib.pyplot as plt\n",
      "import numpy as np\n",
      "from scipy import stats\n",
      "from random import sample, randint\n",
      "from sklearn.decomposition import PCA\n",
      "150/4:\n",
      "import nltk\n",
      "import text_summarizer\n",
      "from gensim.utils import simple_preprocess\n",
      "from gensim.summarization.summarizer import summarize\n",
      "from gensim.models import Word2Vec, LsiModel\n",
      "from gensim.test.utils import common_dictionary, common_corpus\n",
      "150/5:\n",
      "url = 'https://www.enchantedlearning.com/wordlist/adjectivesforpeople.shtml'\n",
      "file = urllib2.urlopen(url)\n",
      "html = file.read()\n",
      "file.close()\n",
      "soup = BeautifulSoup(html)\n",
      "150/6: adj = soup.find_all(attrs={\"class\": \"wordlist-item\"})\n",
      "150/7: adjs = [x.contents[0] for x in adj]\n",
      "150/8: adjs[:5]\n",
      "150/9:\n",
      "with open('adjectives.csv', mode='w') as f:\n",
      "    writer = csv.writer(f, delimiter=',')\n",
      "    writer.writerow(adjs)\n",
      "150/10:\n",
      "with open('adjectives.csv') as csv_file:\n",
      "    adjectives = csv_file.read().split(\",\")\n",
      "150/11:\n",
      "class Doc:\n",
      "    def __init__(self, id: int, gdescriptors: List[str], descriptors: List[str], text: str) -> None:\n",
      "        self.id = id\n",
      "        self.gdescriptors = gdescriptors\n",
      "        self.descriptors = descriptors\n",
      "        self.text = text\n",
      "150/12:\n",
      "def parse_doc(file: str) -> Doc:\n",
      "    tree = ET.parse(file)\n",
      "    root = tree.getroot()\n",
      "    try:\n",
      "        fulltext = root.find('body').find('body.content').find(\"*[@class='full_text']\")\n",
      "        text = reduce(lambda x, y: x + y, [x.text for x in fulltext.findall('p')])\n",
      "        id = int(root.find('head').find('docdata').find('doc-id').attrib['id-string'])\n",
      "        classifiers = root.find('head').find('docdata').find('identified-content')\n",
      "        gclassifiers = classifiers.findall(\"*[@class='online_producer'][@type='general_descriptor']\")\n",
      "        dclassifiers = classifiers.findall(\"*[@class='online_producer'][@type='descriptor']\")\n",
      "        gdescriptors = [c.text for c in gclassifiers]\n",
      "        descriptors = [c.text for c in dclassifiers]\n",
      "        return Doc(id, gdescriptors, descriptors, text)\n",
      "    except:\n",
      "        return None\n",
      "150/13:\n",
      "def get_docs_by_month(year: int, month: int) -> List[Doc]:\n",
      "    days = monthrange(year, month)[1]\n",
      "    docs = []\n",
      "    for day in range(1, days+1):\n",
      "        for file in os.listdir(f'data/{year}/{month:02}/{day:02}/'):\n",
      "            doc = parse_doc(f'data/{year}/{month:02}/{day:02}/{file}')\n",
      "            if doc:\n",
      "                docs.append(doc)\n",
      "    return docs\n",
      "150/14: all88jan = get_docs_by_month(1988, 1)\n",
      "150/15: dill.dump_session('thesis_env.db')\n",
      "150/16: all88jan[0]\n",
      "151/1:\n",
      "class Doc:\n",
      "    def __init__(self, id: int, gdescriptors: List[str], descriptors: List[str], text: str) -> None:\n",
      "        self.id = id\n",
      "        self.gdescriptors = gdescriptors\n",
      "        self.descriptors = descriptors\n",
      "        self.text = text\n",
      "151/2:\n",
      "import os\n",
      "import time\n",
      "import dill\n",
      "from typing import List\n",
      "from functools import reduce\n",
      "from calendar import monthrange\n",
      "151/3:\n",
      "class Doc:\n",
      "    def __init__(self, id: int, gdescriptors: List[str], descriptors: List[str], text: str) -> None:\n",
      "        self.id = id\n",
      "        self.gdescriptors = gdescriptors\n",
      "        self.descriptors = descriptors\n",
      "        self.text = text\n",
      "151/4: d = Doc(123,[],[],\"hi\")\n",
      "151/5: dill.dump_session('thesis_env.db')\n",
      "151/6: d = [Doc(123,[],[],\"hi\"), Doc(123,[],[],\"hi\")]\n",
      "151/7: dill.dump_session('thesis_env.db')\n",
      "151/8:\n",
      "import os\n",
      "import time\n",
      "import dill\n",
      "from typing import List\n",
      "from functools import reduce\n",
      "from calendar import monthrange\n",
      "151/9:\n",
      "import json\n",
      "import csv\n",
      "import requests\n",
      "import xml.etree.ElementTree as ET\n",
      "from bs4 import BeautifulSoup\n",
      "import urllib.request as urllib2\n",
      "151/10:\n",
      "import matplotlib.pyplot as plt\n",
      "import numpy as np\n",
      "from scipy import stats\n",
      "from random import sample, randint\n",
      "from sklearn.decomposition import PCA\n",
      "151/11:\n",
      "import nltk\n",
      "import text_summarizer\n",
      "from gensim.utils import simple_preprocess\n",
      "from gensim.summarization.summarizer import summarize\n",
      "from gensim.models import Word2Vec, LsiModel\n",
      "from gensim.test.utils import common_dictionary, common_corpus\n",
      "151/12:\n",
      "url = 'https://www.enchantedlearning.com/wordlist/adjectivesforpeople.shtml'\n",
      "file = urllib2.urlopen(url)\n",
      "html = file.read()\n",
      "file.close()\n",
      "soup = BeautifulSoup(html)\n",
      "151/13: adj = soup.find_all(attrs={\"class\": \"wordlist-item\"})\n",
      "151/14: adjs = [x.contents[0] for x in adj]\n",
      "151/15: adjs[:5]\n",
      "151/16:\n",
      "with open('adjectives.csv', mode='w') as f:\n",
      "    writer = csv.writer(f, delimiter=',')\n",
      "    writer.writerow(adjs)\n",
      "151/17:\n",
      "with open('adjectives.csv') as csv_file:\n",
      "    adjectives = csv_file.read().split(\",\")\n",
      "151/18: dill.dump_session('thesis_env.db')\n",
      "152/1:\n",
      "import os\n",
      "import time\n",
      "import dill\n",
      "from typing import List\n",
      "from functools import reduce\n",
      "from calendar import monthrange\n",
      "152/2:\n",
      "import json\n",
      "import csv\n",
      "import requests\n",
      "import xml.etree.ElementTree as ET\n",
      "from bs4 import BeautifulSoup\n",
      "import urllib.request as urllib2\n",
      "152/3:\n",
      "import matplotlib.pyplot as plt\n",
      "import numpy as np\n",
      "from scipy import stats\n",
      "from random import sample, randint\n",
      "from sklearn.decomposition import PCA\n",
      "152/4:\n",
      "import nltk\n",
      "import text_summarizer\n",
      "from gensim.utils import simple_preprocess\n",
      "from gensim.summarization.summarizer import summarize\n",
      "from gensim.models import Word2Vec, LsiModel\n",
      "from gensim.test.utils import common_dictionary, common_corpus\n",
      "152/5: adjs = [x.contents[0] for x in BeautifulSoup(html).find_all(attrs={\"class\": \"wordlist-item\"})]\n",
      "152/6:\n",
      "url = 'https://www.enchantedlearning.com/wordlist/adjectivesforpeople.shtml'\n",
      "file = urllib2.urlopen(url)\n",
      "html = file.read()\n",
      "file.close()\n",
      "# soup = BeautifulSoup(html)\n",
      "152/7: dill.dump_session('thesis_env.db')\n",
      "152/8: adjs = [x.contents[0] for x in BeautifulSoup(html).find_all(attrs={\"class\": \"wordlist-item\"})]\n",
      "152/9: dill.dump_session('thesis_env.db')\n",
      "152/10:\n",
      "import os\n",
      "import time\n",
      "import dill\n",
      "from typing import List\n",
      "from functools import reduce\n",
      "from calendar import monthrange\n",
      "153/1:\n",
      "import os\n",
      "import time\n",
      "import dill\n",
      "from typing import List\n",
      "from functools import reduce\n",
      "from calendar import monthrange\n",
      "153/2:\n",
      "import json\n",
      "import csv\n",
      "import requests\n",
      "import xml.etree.ElementTree as ET\n",
      "from bs4 import BeautifulSoup\n",
      "import urllib.request as urllib2\n",
      "153/3:\n",
      "import matplotlib.pyplot as plt\n",
      "import numpy as np\n",
      "from scipy import stats\n",
      "from random import sample, randint\n",
      "from sklearn.decomposition import PCA\n",
      "153/4:\n",
      "import nltk\n",
      "import text_summarizer\n",
      "from gensim.utils import simple_preprocess\n",
      "from gensim.summarization.summarizer import summarize\n",
      "from gensim.models import Word2Vec, LsiModel\n",
      "from gensim.test.utils import common_dictionary, common_corpus\n",
      "153/5:\n",
      "with open('adjectives.csv') as csv_file:\n",
      "    adjectives = csv_file.read().split(\",\")\n",
      "153/6: adjectives[:5]\n",
      "153/7: dill.dump_session('thesis_env.db')\n",
      "153/8:\n",
      "class Doc:\n",
      "    def __init__(self, id: int, gdescriptors: List[str], descriptors: List[str], text: str) -> None:\n",
      "        self.id = id\n",
      "        self.gdescriptors = gdescriptors\n",
      "        self.descriptors = descriptors\n",
      "        self.text = text\n",
      "153/9:\n",
      "def parse_doc(file: str) -> Doc:\n",
      "    tree = ET.parse(file)\n",
      "    root = tree.getroot()\n",
      "    try:\n",
      "        fulltext = root.find('body').find('body.content').find(\"*[@class='full_text']\")\n",
      "        text = reduce(lambda x, y: x + y, [x.text for x in fulltext.findall('p')])\n",
      "        id = int(root.find('head').find('docdata').find('doc-id').attrib['id-string'])\n",
      "        classifiers = root.find('head').find('docdata').find('identified-content')\n",
      "        gclassifiers = classifiers.findall(\"*[@class='online_producer'][@type='general_descriptor']\")\n",
      "        dclassifiers = classifiers.findall(\"*[@class='online_producer'][@type='descriptor']\")\n",
      "        gdescriptors = [c.text for c in gclassifiers]\n",
      "        descriptors = [c.text for c in dclassifiers]\n",
      "        return Doc(id, gdescriptors, descriptors, text)\n",
      "    except:\n",
      "        return None\n",
      "153/10:\n",
      "def get_docs_by_month(year: int, month: int) -> List[Doc]:\n",
      "    days = monthrange(year, month)[1]\n",
      "    docs = []\n",
      "    for day in range(1, days+1):\n",
      "        for file in os.listdir(f'data/{year}/{month:02}/{day:02}/'):\n",
      "            doc = parse_doc(f'data/{year}/{month:02}/{day:02}/{file}')\n",
      "            if doc:\n",
      "                docs.append(doc)\n",
      "    return docs\n",
      "153/11: temp = get_docs_by_month(1988,1)\n",
      "153/12: dill.dump_session('thesis_env.db')\n",
      "153/13:\n",
      "t = time.process_time()\n",
      "all_ = []\n",
      "for y in range(1988, 2007):\n",
      "    print(y)\n",
      "    for i in range(1,13):\n",
      "        all_ += get_docs_by_month(y, i)\n",
      "time.process_time() - t\n",
      "153/14: len(all_)\n",
      "153/15: dill.dump_session('thesis_env.db')\n",
      "147/69: from logistic import logistic_fit\n",
      "147/70: from logistic import logistic_fit, logistic, logistic_prob\n",
      "147/71: params0 = logistic_fit(X0, y, 0.0001)\n",
      "154/1: dill.load_session('notebook_env.db')\n",
      "154/2:\n",
      "import os\n",
      "import cv2\n",
      "from time import time\n",
      "import numpy as np\n",
      "import matplotlib.pyplot as plt\n",
      "from glob import glob\n",
      "import random\n",
      "from dask import delayed\n",
      "154/3:\n",
      "from sklearn.ensemble import RandomForestClassifier\n",
      "from sklearn.model_selection import train_test_split\n",
      "from sklearn.metrics import roc_auc_score\n",
      "from sklearn.linear_model import Logistic\n",
      "154/4:\n",
      "import os\n",
      "import cv2\n",
      "from time import time\n",
      "import numpy as np\n",
      "import matplotlib.pyplot as plt\n",
      "from glob import glob\n",
      "import random\n",
      "from dask import delayed\n",
      "154/5:\n",
      "from sklearn.ensemble import RandomForestClassifier\n",
      "from sklearn.model_selection import train_test_split\n",
      "from sklearn.metrics import roc_auc_score\n",
      "154/6:\n",
      "from skimage.transform import integral_image\n",
      "from skimage.feature import haar_like_feature\n",
      "from skimage.feature import haar_like_feature_coord\n",
      "from skimage.feature import draw_haar_like_feature\n",
      "154/7: from logistic import logistic_fit, logistic, logistic_prob\n",
      "154/8: import dill\n",
      "154/9: dill.load_session('notebook_env.db')\n",
      "154/10: X0.shape\n",
      "154/11: X100.shape\n",
      "154/12:\n",
      "t_start = time()\n",
      "params0 = logistic_fit(X0, y, 0.0001)\n",
      "time() - t_start\n",
      "155/1:\n",
      "import os\n",
      "import cv2\n",
      "from time import time\n",
      "import numpy as np\n",
      "import matplotlib.pyplot as plt\n",
      "from glob import glob\n",
      "import random\n",
      "from dask import delayed\n",
      "155/2:\n",
      "from sklearn.ensemble import RandomForestClassifier\n",
      "from sklearn.model_selection import train_test_split\n",
      "from sklearn.metrics import roc_auc_score\n",
      "155/3:\n",
      "from skimage.transform import integral_image\n",
      "from skimage.feature import haar_like_feature\n",
      "from skimage.feature import haar_like_feature_coord\n",
      "from skimage.feature import draw_haar_like_feature\n",
      "155/4:\n",
      "from sklearn.ensemble import RandomForestClassifier\n",
      "from sklearn.model_selection import train_test_split\n",
      "from sklearn.metrics import roc_auc_score\n",
      "from sklearn.linear_model import LogisticRegression\n",
      "155/5: import dill\n",
      "155/6:\n",
      "t_start = time()\n",
      "dill.load_session('notebook_env.db')\n",
      "time() - t_start\n",
      "155/7:\n",
      "import os\n",
      "import cv2\n",
      "from time import process_time as time\n",
      "import numpy as np\n",
      "import matplotlib.pyplot as plt\n",
      "from glob import glob\n",
      "import random\n",
      "from dask import delayed\n",
      "155/8:\n",
      "t_start = time()\n",
      "dill.load_session('notebook_env.db')\n",
      "time() - t_start\n",
      "155/9:\n",
      "import os\n",
      "import cv2\n",
      "from time import time\n",
      "import numpy as np\n",
      "import matplotlib.pyplot as plt\n",
      "from glob import glob\n",
      "import random\n",
      "from dask import delayed\n",
      "155/10:\n",
      "t_start = time()\n",
      "lr0 = LogisticRegression().fit(X0, y)\n",
      "time() - t_start\n",
      "155/11:\n",
      "t_start = time()\n",
      "lr0 = LogisticRegression().fit(X0, y)\n",
      "time() - t_start\n",
      "156/1:\n",
      "import os\n",
      "import cv2\n",
      "from time import time\n",
      "import numpy as np\n",
      "import matplotlib.pyplot as plt\n",
      "from glob import glob\n",
      "import random\n",
      "from dask import delayed\n",
      "156/2:\n",
      "from sklearn.ensemble import RandomForestClassifier\n",
      "from sklearn.model_selection import train_test_split\n",
      "from sklearn.metrics import roc_auc_score\n",
      "from sklearn.linear_model import LogisticRegression\n",
      "156/3:\n",
      "from skimage.transform import integral_image\n",
      "from skimage.feature import haar_like_feature\n",
      "from skimage.feature import haar_like_feature_coord\n",
      "from skimage.feature import draw_haar_like_feature\n",
      "156/4: import dill\n",
      "156/5:\n",
      "t_start = time()\n",
      "dill.load_session('notebook_env.db')\n",
      "time() - t_start\n",
      "156/6:\n",
      "# Train a random forest classifier and assess its performance\n",
      "rf0 = RandomForestClassifier(n_estimators=1000, max_depth=None,\n",
      "                             n_jobs=-1, random_state=0)\n",
      "t_start = time()\n",
      "rf0.fit(X0, y)\n",
      "time() - t_start\n",
      "156/7: print(rf0.score(X_t,y_t))\n",
      "156/8: y_t = [1]*len(faces_t) + [0]*len(nonfaces_t)\n",
      "156/9: print(rf0.score(X_t,y_t))\n",
      "156/10:\n",
      "# Train a random forest classifier and assess its performance\n",
      "rf25 = RandomForestClassifier(n_estimators=1000, max_depth=None,\n",
      "                             n_jobs=-1, random_state=0)\n",
      "t_start = time()\n",
      "rf25.fit(X25, y)\n",
      "time() - t_start\n",
      "156/11: print(rf25.score(X_t,y_t))\n",
      "156/12:\n",
      "# Train a random forest classifier and assess its performance\n",
      "rf50 = RandomForestClassifier(n_estimators=1000, max_depth=None,\n",
      "                             n_jobs=-1, random_state=0)\n",
      "t_start = time()\n",
      "rf50.fit(X50, y)\n",
      "time() - t_start\n",
      "156/13: print(rf50.score(X_t,y_t))\n",
      "156/14:\n",
      "# Train a random forest classifier and assess its performance\n",
      "rf75 = RandomForestClassifier(n_estimators=1000, max_depth=None,\n",
      "                             n_jobs=-1, random_state=0)\n",
      "t_start = time()\n",
      "rf75.fit(X75, y)\n",
      "time() - t_start\n",
      "156/15: print(rf75.score(X_t,y_t))\n",
      "156/16:\n",
      "# Train a random forest classifier and assess its performance\n",
      "rf100 = RandomForestClassifier(n_estimators=1000, max_depth=None,\n",
      "                             n_jobs=-1, random_state=0)\n",
      "t_start = time()\n",
      "rf100.fit(X100, y)\n",
      "time() - t_start\n",
      "156/17: print(rf100.score(X_t,y_t))\n",
      "156/18: ### Scores, separated by race\n",
      "156/19: X_t.shape\n",
      "156/20: X_t[:300].shape\n",
      "156/21: sum(y_t[:300])\n",
      "156/22: sum(y_t[300:])\n",
      "156/23:\n",
      "count = 0\n",
      "for f in test_face_names:\n",
      "    if int(f.split('_')[3]) is 0: # race is white\n",
      "        count += 1\n",
      "        print(\"yes\")\n",
      "    else\n",
      "        print(\"no\")\n",
      "count\n",
      "156/24:\n",
      "count = 0\n",
      "for f in test_face_names:\n",
      "    if int(f.split('_')[3]) is 0: # race is white\n",
      "        count += 1\n",
      "        print(\"yes\")\n",
      "    else:\n",
      "        print(\"no\")\n",
      "count\n",
      "156/25:\n",
      "count = 0\n",
      "for f in test_face_names:\n",
      "    if int(f.split('_')[2]) is 0: # race is white\n",
      "        count += 1\n",
      "        print(\"yes\")\n",
      "    else:\n",
      "        print(\"no\")\n",
      "count\n",
      "156/26:\n",
      "b_inds = []\n",
      "w_inds = []\n",
      "for i in range(len(test_face_names)):\n",
      "    if int(test_face_names[i].split('_')[2]) is 0: # race is white\n",
      "        w_inds.append(i)\n",
      "    else:\n",
      "        b_inds.append(i)\n",
      "156/27: X.shape\n",
      "156/28: X[[1,2,5]].shape\n",
      "156/29: y_t.shape, y_t[[1,3,6]].shape\n",
      "156/30: y_t.shape, np.array(y_t)[[1,3,6]].shape\n",
      "156/31: np.array(y_t).shape, np.array(y_t)[[1,3,6]].shape\n",
      "156/32: np.array(y_t).shape, np.array(y_t[[1,3,6]]).shape\n",
      "156/33: y_t = np.array([1] * n_test + [0] * n_test)\n",
      "156/34:\n",
      "# use b_inds and w_inds to select rows of X_t and y_t\n",
      "Xb_t = X_t[b_inds]\n",
      "yb_t = y_t[b_inds]\n",
      "Xw_t = X_t[w_inds]\n",
      "yw_t = y_t[w_inds]\n",
      "Xb_t.shape, yb_t.shape, Xw_t.shape, yw_t.shape\n",
      "156/35: X_b[:10]\n",
      "156/36: Xb_t[:10]\n",
      "156/37: Xw_t[:10]\n",
      "156/38:\n",
      "rfs = [rf0, rf25, rf50, rf75, rf100]\n",
      "for i in range(len(rfs)):\n",
      "    print(i*25)\n",
      "    print(\"black: \" + rfs[i].score(Xb_t, yb_t))\n",
      "    print(\"white: \" + rfs[i].score(Xw_t, yw_t))\n",
      "156/39:\n",
      "rfs = [rf0, rf25, rf50, rf75, rf100]\n",
      "for i in range(len(rfs)):\n",
      "    print(i*25)\n",
      "    print(\"black:\", rfs[i].score(Xb_t, yb_t))\n",
      "    print(\"white:\", rfs[i].score(Xw_t, yw_t))\n",
      "156/40:\n",
      "count = 0\n",
      "for i in range(n):\n",
      "    if facefiles[i].split('_')[3] == 0:\n",
      "        count += 1\n",
      "count\n",
      "156/41:\n",
      "count = 0\n",
      "for i in range(n):\n",
      "    if face_filenames0[i].split('_')[3] == 0:\n",
      "        count += 1\n",
      "count\n",
      "156/42:\n",
      "count = 0\n",
      "for i in range(n):\n",
      "    if face_filenames1[i].split('_')[3] == 0:\n",
      "        count += 1\n",
      "count\n",
      "156/43:\n",
      "count = 0\n",
      "for i in range(n):\n",
      "    if face_filenames25[i].split('_')[3] == 0:\n",
      "        count += 1\n",
      "count\n",
      "156/44:\n",
      "count = 0\n",
      "for i in range(n):\n",
      "    if face_filenames25[i].split('_')[2] == 0:\n",
      "        count += 1\n",
      "count\n",
      "156/45:\n",
      "count = 0\n",
      "for i in range(n):\n",
      "    if int(face_filenames25[i].split('_')[2]) == 0:\n",
      "        count += 1\n",
      "count\n",
      "156/46: print n\n",
      "156/47: n\n",
      "156/48:\n",
      "count = 0\n",
      "for i in range(n):\n",
      "    if int(face_filenames50[i].split('_')[2]) == 0:\n",
      "        count += 1\n",
      "count\n",
      "156/49:\n",
      "count = 0\n",
      "for i in range(n):\n",
      "    if int(face_filenames60[i].split('_')[2]) == 0:\n",
      "        count += 1\n",
      "count\n",
      "156/50:\n",
      "count = 0\n",
      "for i in range(n):\n",
      "    if int(face_filenames75[i].split('_')[2]) == 0:\n",
      "        count += 1\n",
      "count\n",
      "156/51:\n",
      "count = 0\n",
      "for i in range(n):\n",
      "    if int(face_filenames100[i].split('_')[2]) == 0:\n",
      "        count += 1\n",
      "count\n",
      "156/52:\n",
      "count = 0\n",
      "for i in range(n):\n",
      "    if int(face_filenames0[i].split('_')[2]) == 0:\n",
      "        count += 1\n",
      "count\n",
      "156/53:\n",
      "count = 0\n",
      "for i in range(n):\n",
      "    if int(face_filenames0[i].split('_')[3]) == 0:\n",
      "        count += 1\n",
      "count\n",
      "156/54:\n",
      "count = 0\n",
      "for i in range(n):\n",
      "    if int(face_filenames1[i].split('_')[3]) == 0:\n",
      "        count += 1\n",
      "count\n",
      "156/55:\n",
      "count = 0\n",
      "for i in range(n):\n",
      "    if int(face_filenames25[i].split('_')[3]) == 0:\n",
      "        count += 1\n",
      "count\n",
      "156/56:\n",
      "count = 0\n",
      "for i in range(n):\n",
      "    if int(face_filenames50[i].split('_')[3]) == 0:\n",
      "        count += 1\n",
      "count\n",
      "156/57:\n",
      "count = 0\n",
      "for i in range(n):\n",
      "    if int(face_filenames75[i].split('_')[3]) == 0:\n",
      "        count += 1\n",
      "count\n",
      "156/58:\n",
      "count = 0\n",
      "for i in range(n):\n",
      "    if int(face_filenames100[i].split('_')[3]) == 0:\n",
      "        count += 1\n",
      "count\n",
      "156/59:\n",
      "count = 0\n",
      "for i in range(n):\n",
      "    if int(face_filenames0[i].split('_')[3]) == 0:\n",
      "        count += 1\n",
      "count\n",
      "156/60:\n",
      "count = 0\n",
      "for i in range(n):\n",
      "    if int(face_filenames25[i].split('_')[3]) == 0:\n",
      "        count += 1\n",
      "count\n",
      "156/61:\n",
      "count = 0\n",
      "for i in range(n):\n",
      "    if int(face_filenames50[i].split('_')[3]) == 0:\n",
      "        count += 1\n",
      "count\n",
      "156/62:\n",
      "count = 0\n",
      "for i in range(n):\n",
      "    if int(face_filenames75[i].split('_')[3]) == 0:\n",
      "        count += 1\n",
      "count\n",
      "156/63:\n",
      "count = 0\n",
      "for i in range(n):\n",
      "    if int(face_filenames100[i].split('_')[3]) == 0:\n",
      "        count += 1\n",
      "count\n",
      "156/64: face_filenames0[300]\n",
      "156/65: proportion_b(face_filenames0)\n",
      "156/66:\n",
      "def proportion_b(facenames):\n",
      "    count = 0\n",
      "    for i in range(n):\n",
      "        if int(face_filenames0[i].split('_')[3]) == 0:\n",
      "            count += 1\n",
      "    return count / n\n",
      "156/67:\n",
      "def proportion_b(facenames):\n",
      "    count = 0\n",
      "    for i in range(n):\n",
      "        if int(facenames[i].split('_')[3]) == 0:\n",
      "            count += 1\n",
      "    return count / n\n",
      "156/68: proportion_b(face_filenames0)\n",
      "156/69:\n",
      "for f in [face_filenames0, face_filenames25, face_filenames50, face_filenames75, face_filenames100]:\n",
      "proportion_b(f)\n",
      "156/70:\n",
      "for f in [face_filenames0, face_filenames25, face_filenames50, face_filenames75, face_filenames100]:\n",
      "    proportion_b(f)\n",
      "156/71:\n",
      "for f in [face_filenames0, face_filenames25, face_filenames50, face_filenames75, face_filenames100]:\n",
      "    print(proportion_b(f))\n",
      "156/72: dill.dump_session('01_05_env.db')\n",
      "157/1:\n",
      "import os\n",
      "import cv2\n",
      "from time import time\n",
      "import numpy as np\n",
      "import matplotlib.pyplot as plt\n",
      "from glob import glob\n",
      "import random\n",
      "from dask import delayed\n",
      "157/2:\n",
      "from sklearn.ensemble import RandomForestClassifier\n",
      "from sklearn.model_selection import train_test_split\n",
      "from sklearn.metrics import roc_auc_score\n",
      "from sklearn.linear_model import LogisticRegression\n",
      "157/3:\n",
      "from skimage.transform import integral_image\n",
      "from skimage.feature import haar_like_feature\n",
      "from skimage.feature import haar_like_feature_coord\n",
      "from skimage.feature import draw_haar_like_feature\n",
      "157/4: import dill\n",
      "157/5:\n",
      "t_start = time()\n",
      "dill.load_session('01_05_env.db')\n",
      "time() - t_start\n",
      "157/6: X0.shape\n",
      "157/7:\n",
      "# Train a random forest classifier and assess its performance\n",
      "rf0 = RandomForestClassifier(n_estimators=1000, max_depth=None,\n",
      "                             max_features=100, n_jobs=-1, random_state=0)\n",
      "t_start = time()\n",
      "rf0.fit(X0, y)\n",
      "time() - t_start\n",
      "157/8: print(rf0.score(X_t,y_t))\n",
      "157/9:\n",
      "# Train a random forest classifier and assess its performance\n",
      "rf25 = RandomForestClassifier(n_estimators=1000, max_depth=None,\n",
      "                              max_features=100, n_jobs=-1, random_state=0)\n",
      "t_start = time()\n",
      "rf25.fit(X25, y)\n",
      "time() - t_start\n",
      "157/10: print(rf25.score(X_t,y_t))\n",
      "157/11:\n",
      "# Train a random forest classifier and assess its performance\n",
      "rf50 = RandomForestClassifier(n_estimators=1000, max_depth=None,\n",
      "                              max_features=100, n_jobs=-1, random_state=0)\n",
      "t_start = time()\n",
      "rf50.fit(X50, y)\n",
      "time() - t_start\n",
      "157/12: print(rf50.score(X_t,y_t))\n",
      "157/13:\n",
      "# Train a random forest classifier and assess its performance\n",
      "rf75 = RandomForestClassifier(n_estimators=1000, max_depth=None,\n",
      "                              max_features=100, n_jobs=-1, random_state=0)\n",
      "t_start = time()\n",
      "rf75.fit(X75, y)\n",
      "time() - t_start\n",
      "157/14: print(rf75.score(X_t,y_t))\n",
      "157/15:\n",
      "# Train a random forest classifier and assess its performance\n",
      "rf100 = RandomForestClassifier(n_estimators=1000, max_depth=None,\n",
      "                             n_jobs=-1, random_state=0)\n",
      "t_start = time()\n",
      "rf100.fit(X100, y)\n",
      "time() - t_start\n",
      "157/16: print(rf100.score(X_t,y_t))\n",
      "157/17:\n",
      "rfs = [rf0, rf25, rf50, rf75, rf100]\n",
      "for i in range(len(rfs)):\n",
      "    print(i*25)\n",
      "    print(\"black:\", rfs[i].score(Xb_t, yb_t))\n",
      "    print(\"white:\", rfs[i].score(Xw_t, yw_t))\n",
      "157/18:\n",
      "# Extract all possible features\n",
      "feature_coord, feature_type = \\\n",
      "    haar_like_feature_coord(width=36, height=36,\n",
      "                            feature_type=feature_types)\n",
      "157/19:\n",
      "# Sort features in order of importance and plot the six most significant\n",
      "idx_sorted = np.argsort(rf0.feature_importances_)[::-1]\n",
      "157/20:\n",
      "fig, axes = plt.subplots(3, 2)\n",
      "for idx, ax in enumerate(axes.ravel()):\n",
      "    image = images0[0]\n",
      "    image = draw_haar_like_feature(image, 0, 0,\n",
      "                                   images.shape[2],\n",
      "                                   images.shape[1],\n",
      "                                   [feature_coord[idx_sorted[idx]]])\n",
      "    ax.imshow(image)\n",
      "    ax.set_xticks([])\n",
      "    ax.set_yticks([])\n",
      "\n",
      "fig.suptitle('The most important features')\n",
      "157/21:\n",
      "fig, axes = plt.subplots(3, 2)\n",
      "for idx, ax in enumerate(axes.ravel()):\n",
      "    image = images0[0]\n",
      "    image = draw_haar_like_feature(image, 0, 0,\n",
      "                                   images0.shape[2],\n",
      "                                   images0.shape[1],\n",
      "                                   [feature_coord[idx_sorted[idx]]])\n",
      "    ax.imshow(image)\n",
      "    ax.set_xticks([])\n",
      "    ax.set_yticks([])\n",
      "\n",
      "fig.suptitle('The most important features')\n",
      "157/22:\n",
      "fig, axes = plt.subplots(3, 2)\n",
      "images0 = np.array(images0)\n",
      "for idx, ax in enumerate(axes.ravel()):\n",
      "    image = images0[0]\n",
      "    image = draw_haar_like_feature(image, 0, 0,\n",
      "                                   images0.shape[2],\n",
      "                                   images0.shape[1],\n",
      "                                   [feature_coord[idx_sorted[idx]]])\n",
      "    ax.imshow(image)\n",
      "    ax.set_xticks([])\n",
      "    ax.set_yticks([])\n",
      "\n",
      "fig.suptitle('The most important features')\n",
      "157/23: images0[0].shape\n",
      "157/24:\n",
      "fig, axes = plt.subplots(3, 2)\n",
      "images0 = np.array([cv2.resize(im, (36,36)) for im in images0])\n",
      "for idx, ax in enumerate(axes.ravel()):\n",
      "    image = images0[0]\n",
      "    image = draw_haar_like_feature(image, 0, 0,\n",
      "                                   images0.shape[2],\n",
      "                                   images0.shape[1],\n",
      "                                   [feature_coord[idx_sorted[idx]]])\n",
      "    ax.imshow(image)\n",
      "    ax.set_xticks([])\n",
      "    ax.set_yticks([])\n",
      "\n",
      "fig.suptitle('The most important features')\n",
      "157/25:\n",
      "fig, axes = plt.subplots(5, 5)\n",
      "images0 = np.array([cv2.resize(im, (36,36)) for im in images0])\n",
      "for idx, ax in enumerate(axes.ravel()):\n",
      "    image = images0[0]\n",
      "    image = draw_haar_like_feature(image, 0, 0,\n",
      "                                   images0.shape[2],\n",
      "                                   images0.shape[1],\n",
      "                                   [feature_coord[idx_sorted[idx]]])\n",
      "    ax.imshow(image)\n",
      "    ax.set_xticks([])\n",
      "    ax.set_yticks([])\n",
      "\n",
      "fig.suptitle('The most important features')\n",
      "157/26:\n",
      "fig, axes = plt.subplots(4, 4)\n",
      "images0 = np.array([cv2.resize(im, (36,36)) for im in images0])\n",
      "for idx, ax in enumerate(axes.ravel()):\n",
      "    image = images0[0]\n",
      "    image = draw_haar_like_feature(image, 0, 0,\n",
      "                                   images0.shape[2],\n",
      "                                   images0.shape[1],\n",
      "                                   [feature_coord[idx_sorted[idx]]])\n",
      "    ax.imshow(image)\n",
      "    ax.set_xticks([])\n",
      "    ax.set_yticks([])\n",
      "\n",
      "fig.suptitle('The most important features')\n",
      "157/27: auc_full_features = roc_auc_score(y_t, rf0.predict_proba(X_t)[:, 1])\n",
      "157/28: auc_full_features = roc_auc_score(y_t, rf0.predict_proba(X_t)[:, 1])\n",
      "157/29:\n",
      "def gettop(ims, rf):\n",
      "    ims = np.array(ims)\n",
      "    idx_sorted = np.argsort(rf.feature_importances_)[::-1]\n",
      "    for idx, ax in enumerate(axes.ravel()):\n",
      "        image = ims[0]\n",
      "        image = draw_haar_like_feature(image, 0, 0,\n",
      "                                       ims.shape[2],\n",
      "                                       ims.shape[1],\n",
      "                                       [feature_coord[idx_sorted[idx]]])\n",
      "        ax.imshow(image)\n",
      "        ax.set_xticks([])\n",
      "        ax.set_yticks([])\n",
      "\n",
      "    fig.suptitle('The most important features')\n",
      "157/30:\n",
      "def gettop(ims, rf):\n",
      "    ims = np.array([cv2.resize(im, (36,36)) for im in ims])\n",
      "    idx_sorted = np.argsort(rf.feature_importances_)[::-1]\n",
      "    for idx, ax in enumerate(axes.ravel()):\n",
      "        image = ims[0]\n",
      "        image = draw_haar_like_feature(image, 0, 0,\n",
      "                                       ims.shape[2],\n",
      "                                       ims.shape[1],\n",
      "                                       [feature_coord[idx_sorted[idx]]])\n",
      "        ax.imshow(image)\n",
      "        ax.set_xticks([])\n",
      "        ax.set_yticks([])\n",
      "\n",
      "    fig.suptitle('The most important features')\n",
      "157/31: gettop(images0, rf0)\n",
      "157/32:\n",
      "gettop(images0, rf0)\n",
      "plt.show()\n",
      "157/33: fig.show()\n",
      "157/34:\n",
      "def gettop(ims, rf):\n",
      "    ims = np.array([cv2.resize(im, (36,36)) for im in ims])\n",
      "    idx_sorted = np.argsort(rf.feature_importances_)[::-1]\n",
      "    for idx, ax in enumerate(axes.ravel()):\n",
      "        image = ims[0]\n",
      "        image = draw_haar_like_feature(image, 0, 0,\n",
      "                                       ims.shape[2],\n",
      "                                       ims.shape[1],\n",
      "                                       [feature_coord[idx_sorted[idx]]])\n",
      "        ax.imshow(image)\n",
      "        ax.set_xticks([])\n",
      "        ax.set_yticks([])\n",
      "\n",
      "    fig.suptitle('The most important features')\n",
      "    ax.show()\n",
      "157/35: gettop(images0, rf0)\n",
      "157/36:\n",
      "ims = getimgs(face_filenames25)\n",
      "rf = rf25\n",
      "ims = np.array([cv2.resize(im, (36,36)) for im in ims])\n",
      "idx_sorted = np.argsort(rf.feature_importances_)[::-1]\n",
      "for idx, ax in enumerate(axes.ravel()):\n",
      "    image = ims[0]\n",
      "    image = draw_haar_like_feature(image, 0, 0,\n",
      "                                   ims.shape[2],\n",
      "                                   ims.shape[1],\n",
      "                                   [feature_coord[idx_sorted[idx]]])\n",
      "    ax.imshow(image)\n",
      "    ax.set_xticks([])\n",
      "    ax.set_yticks([])\n",
      "157/37:\n",
      "def gettop(ims, rf):\n",
      "    fig, axes = plt.subplots(4, 4)\n",
      "    ims = np.array([cv2.resize(im, (36,36)) for im in ims])\n",
      "    idx_sorted = np.argsort(rf.feature_importances_)[::-1]\n",
      "    for idx, ax in enumerate(axes.ravel()):\n",
      "        image = ims[0]\n",
      "        image = draw_haar_like_feature(image, 0, 0,\n",
      "                                       ims.shape[2],\n",
      "                                       ims.shape[1],\n",
      "                                       [feature_coord[idx_sorted[idx]]])\n",
      "        ax.imshow(image)\n",
      "        ax.set_xticks([])\n",
      "        ax.set_yticks([])\n",
      "\n",
      "    fig.suptitle('The most important features')\n",
      "    ax.show()\n",
      "157/38:\n",
      "def gettop(ims, rf):\n",
      "    fig, axes = plt.subplots(4, 4)\n",
      "    ims = np.array([cv2.resize(im, (36,36)) for im in ims])\n",
      "    idx_sorted = np.argsort(rf.feature_importances_)[::-1]\n",
      "    for idx, ax in enumerate(axes.ravel()):\n",
      "        image = ims[0]\n",
      "        image = draw_haar_like_feature(image, 0, 0,\n",
      "                                       ims.shape[2],\n",
      "                                       ims.shape[1],\n",
      "                                       [feature_coord[idx_sorted[idx]]])\n",
      "        ax.imshow(image)\n",
      "        ax.set_xticks([])\n",
      "        ax.set_yticks([])\n",
      "\n",
      "    fig.suptitle('The most important features')\n",
      "157/39:\n",
      "def gettop(fnames, rf):\n",
      "    ims = getimgs(fnames)\n",
      "    fig, axes = plt.subplots(4, 4)\n",
      "    ims = np.array([cv2.resize(im, (36,36)) for im in ims])\n",
      "    idx_sorted = np.argsort(rf.feature_importances_)[::-1]\n",
      "    for idx, ax in enumerate(axes.ravel()):\n",
      "        image = ims[0]\n",
      "        image = draw_haar_like_feature(image, 0, 0,\n",
      "                                       ims.shape[2],\n",
      "                                       ims.shape[1],\n",
      "                                       [feature_coord[idx_sorted[idx]]])\n",
      "        ax.imshow(image)\n",
      "        ax.set_xticks([])\n",
      "        ax.set_yticks([])\n",
      "\n",
      "    fig.suptitle('The most important features')\n",
      "157/40: gettop(face_filenames25, rf25)\n",
      "157/41:\n",
      "i = cv2.imread(face_filenames25[0], cv2.IMREAD_GRAYSCALE)\n",
      "plt.imshow(i, cmap='gray')\n",
      "157/42:\n",
      "i = cv2.imread(face_filenames25[4000], cv2.IMREAD_GRAYSCALE)\n",
      "plt.imshow(i, cmap='gray')\n",
      "157/43:\n",
      "i = cv2.imread(face_filenames25[4020], cv2.IMREAD_GRAYSCALE)\n",
      "plt.imshow(i, cmap='gray')\n",
      "157/44:\n",
      "i = cv2.imread(face_filenames25[4019], cv2.IMREAD_GRAYSCALE)\n",
      "plt.imshow(i, cmap='gray')\n",
      "157/45:\n",
      "i = cv2.imread(face_filenames25[5], cv2.IMREAD_GRAYSCALE)\n",
      "plt.imshow(i, cmap='gray')\n",
      "157/46:\n",
      "i = cv2.imread(face_filenames25[2], cv2.IMREAD_GRAYSCALE)\n",
      "plt.imshow(i, cmap='gray')\n",
      "157/47:\n",
      "i = cv2.imread(face_filenames25[3], cv2.IMREAD_GRAYSCALE)\n",
      "plt.imshow(i, cmap='gray')\n",
      "157/48:\n",
      "i = cv2.imread(face_filenames50[0], cv2.IMREAD_GRAYSCALE)\n",
      "plt.imshow(i, cmap='gray')\n",
      "157/49:\n",
      "i = cv2.imread(face_filenames75[0], cv2.IMREAD_GRAYSCALE)\n",
      "plt.imshow(i, cmap='gray')\n",
      "157/50:\n",
      "i = cv2.imread(face_filenames100[0], cv2.IMREAD_GRAYSCALE)\n",
      "plt.imshow(i, cmap='gray')\n",
      "157/51:\n",
      "i = cv2.imread(face_filenames100[5], cv2.IMREAD_GRAYSCALE)\n",
      "plt.imshow(i, cmap='gray')\n",
      "157/52:\n",
      "i = cv2.imread(face_filenames0[5], cv2.IMREAD_GRAYSCALE)\n",
      "plt.imshow(i, cmap='gray')\n",
      "157/53:\n",
      "i = cv2.imread(face_filenames25[5], cv2.IMREAD_GRAYSCALE)\n",
      "plt.imshow(i, cmap='gray')\n",
      "157/54:\n",
      "i = cv2.imread(face_filenames50[5], cv2.IMREAD_GRAYSCALE)\n",
      "plt.imshow(i, cmap='gray')\n",
      "157/55:\n",
      "i = cv2.imread(face_filenames75[5], cv2.IMREAD_GRAYSCALE)\n",
      "plt.imshow(i, cmap='gray')\n",
      "157/56:\n",
      "def gettop(fnames, rf):\n",
      "    ims = getimgs(fnames)\n",
      "    fig, axes = plt.subplots(4, 4)\n",
      "    ims = np.array([cv2.resize(im, (36,36)) for im in ims])\n",
      "    idx_sorted = np.argsort(rf.feature_importances_)[::-1]\n",
      "    for idx, ax in enumerate(axes.ravel()):\n",
      "        image = ims[5]\n",
      "        image = draw_haar_like_feature(image, 0, 0,\n",
      "                                       ims.shape[2],\n",
      "                                       ims.shape[1],\n",
      "                                       [feature_coord[idx_sorted[idx]]])\n",
      "        ax.imshow(image)\n",
      "        ax.set_xticks([])\n",
      "        ax.set_yticks([])\n",
      "\n",
      "    fig.suptitle('The most important features')\n",
      "157/57: gettop(face_filenames0, rf0)\n",
      "157/58: gettop(face_filenames25, rf25)\n",
      "157/59: gettop(face_filenames50, rf50)\n",
      "157/60: gettop(face_filenames25, rf25)\n",
      "157/61: gettop(face_filenames75, rf75)\n",
      "157/62: gettop(face_filenames100, rf100)\n",
      "157/63:\n",
      "def gettop(fnames, rf):\n",
      "    ims = getimgs(fnames)\n",
      "    fig, axes = plt.subplots(4, 4, figsize=(20,20))\n",
      "    ims = np.array([cv2.resize(im, (36,36)) for im in ims])\n",
      "    # Sort features in order of importance and plot the six most significant\n",
      "    idx_sorted = np.argsort(rf.feature_importances_)[::-1]\n",
      "    for idx, ax in enumerate(axes.ravel()):\n",
      "        image = ims[5]\n",
      "        image = draw_haar_like_feature(image, 0, 0,\n",
      "                                       ims.shape[2],\n",
      "                                       ims.shape[1],\n",
      "                                       [feature_coord[idx_sorted[idx]]])\n",
      "        ax.imshow(image)\n",
      "        ax.set_xticks([])\n",
      "        ax.set_yticks([])\n",
      "\n",
      "    fig.suptitle('The most important features')\n",
      "157/64:\n",
      "def gettop(fnames, rf):\n",
      "    ims = getimgs(fnames)\n",
      "    fig, axes = plt.subplots(5, 5, figsize=(20,20))\n",
      "    ims = np.array([cv2.resize(im, (36,36)) for im in ims])\n",
      "    # Sort features in order of importance and plot the six most significant\n",
      "    idx_sorted = np.argsort(rf.feature_importances_)[::-1]\n",
      "    for idx, ax in enumerate(axes.ravel()):\n",
      "        image = ims[5]\n",
      "        image = draw_haar_like_feature(image, 0, 0,\n",
      "                                       ims.shape[2],\n",
      "                                       ims.shape[1],\n",
      "                                       [feature_coord[idx_sorted[idx]]])\n",
      "        ax.imshow(image)\n",
      "        ax.set_xticks([])\n",
      "        ax.set_yticks([])\n",
      "\n",
      "    fig.suptitle('The most important features')\n",
      "157/65: gettop(face_filenames0, rf0)\n",
      "157/66:\n",
      "def gettop(fnames, rf):\n",
      "    ims = getimgs(fnames)\n",
      "    fig, axes = plt.subplots(4, 5, figsize=(20,20))\n",
      "    ims = np.array([cv2.resize(im, (36,36)) for im in ims])\n",
      "    # Sort features in order of importance and plot the six most significant\n",
      "    idx_sorted = np.argsort(rf.feature_importances_)[::-1]\n",
      "    for idx, ax in enumerate(axes.ravel()):\n",
      "        image = ims[5]\n",
      "        image = draw_haar_like_feature(image, 0, 0,\n",
      "                                       ims.shape[2],\n",
      "                                       ims.shape[1],\n",
      "                                       [feature_coord[idx_sorted[idx]]])\n",
      "        ax.imshow(image)\n",
      "        ax.set_xticks([])\n",
      "        ax.set_yticks([])\n",
      "\n",
      "    fig.suptitle('The most important features')\n",
      "157/67: gettop(face_filenames0, rf0)\n",
      "157/68: gettop(face_filenames25, rf25)\n",
      "157/69: gettop(face_filenames50, rf50)\n",
      "157/70: gettop(face_filenames75, rf75)\n",
      "157/71: gettop(face_filenames100, rf100)\n",
      "157/72:\n",
      "def getimgs(facefiles, n):\n",
      "    faces = []\n",
      "    for i in range(n):\n",
      "        faces.append(cv2.imread(facefiles[i], cv2.IMREAD_GRAYSCALE))\n",
      "    return faces + nonfaces\n",
      "158/1:\n",
      "import os\n",
      "import cv2\n",
      "from time import time\n",
      "import numpy as np\n",
      "import matplotlib.pyplot as plt\n",
      "from glob import glob\n",
      "import random\n",
      "from dask import delayed\n",
      "158/2:\n",
      "from sklearn.ensemble import RandomForestClassifier\n",
      "from sklearn.model_selection import train_test_split\n",
      "from sklearn.metrics import roc_auc_score\n",
      "from sklearn.linear_model import LogisticRegression\n",
      "158/3:\n",
      "from skimage.transform import integral_image\n",
      "from skimage.feature import haar_like_feature\n",
      "from skimage.feature import haar_like_feature_coord\n",
      "from skimage.feature import draw_haar_like_feature\n",
      "158/4: import dill\n",
      "158/5:\n",
      "b_inds = []\n",
      "w_inds = []\n",
      "for i in range(len(test_face_names)):\n",
      "    if int(test_face_names[i].split('_')[2]) is 0: # race is white\n",
      "        w_inds.append(i)\n",
      "    else:\n",
      "        b_inds.append(i)\n",
      "158/6:\n",
      "test_face_names = sorted(glob(os.path.join('data/tests', '*.jpg')))\n",
      "n_test = len(test_face_names)\n",
      "158/7:\n",
      "b_inds = []\n",
      "w_inds = []\n",
      "for i in range(len(test_face_names)):\n",
      "    if int(test_face_names[i].split('_')[2]) is 0: # race is white\n",
      "        w_inds.append(i)\n",
      "    else:\n",
      "        b_inds.append(i)\n",
      "158/8:\n",
      "test_nonface_names = sorted(glob(os.path.join('data/testing_nonfaces', '*.jpg')))\n",
      "n_nonfaces_test = len(test_nonface_names)\n",
      "158/9:\n",
      "faces_t = []\n",
      "for i in range(n_test):\n",
      "    faces_t.append(cv2.imread(test_face_names[i], cv2.IMREAD_GRAYSCALE))\n",
      "158/10:\n",
      "nonfaces_t = []\n",
      "for i in range(n_test):\n",
      "    # Read a random nonface file\n",
      "    j = random.randint(0, n_nonfaces_test - 1)\n",
      "    nonface = cv2.imread(test_nonface_names[j], cv2.IMREAD_GRAYSCALE)\n",
      "    \n",
      "    wsize = random.randint(36, min(nonface.shape[0], nonface.shape[1]))\n",
      "    row = random.randint(0, nonface.shape[0]-wsize)\n",
      "    col = random.randint(0, nonface.shape[1]-wsize)\n",
      "    crop = nonface[row:row+wsize, col:col+wsize]\n",
      "\n",
      "    # Resize to be the right size\n",
      "    crop = cv2.resize(crop, (200, 200))\n",
      "    nonfaces_t.append(crop)\n",
      "158/11:\n",
      "images_t = faces_t + nonfaces_t\n",
      "len(images_t)\n",
      "158/12:\n",
      "face_filenames0 = sorted(glob(os.path.join('data/train_0', '*.jpg')))\n",
      "num_face_filenames0 = len(face_filenames0)\n",
      "num_face_filenames0\n",
      "158/13:\n",
      "face_filenames25 = sorted(glob(os.path.join('data/train_25', '*.jpg')))\n",
      "num_face_filenames25 = len(face_filenames25)\n",
      "num_face_filenames25\n",
      "158/14:\n",
      "face_filenames50 = sorted(glob(os.path.join('data/train_50', '*.jpg')))\n",
      "num_face_filenames50 = len(face_filenames50)\n",
      "num_face_filenames50\n",
      "158/15:\n",
      "face_filenames75 = sorted(glob(os.path.join('data/train_75', '*.jpg')))\n",
      "num_face_filenames75 = len(face_filenames75)\n",
      "num_face_filenames75\n",
      "158/16:\n",
      "face_filenames100 = sorted(glob(os.path.join('data/train_100', '*.jpg')))\n",
      "num_face_filenames100 = len(face_filenames100)\n",
      "num_face_filenames100\n",
      "158/17:\n",
      "nonface_filenames = sorted(glob(os.path.join('data/training_nonfaces', '*.jpg')))\n",
      "num_nonface_filenames = len(nonface_filenames)\n",
      "num_nonface_filenames\n",
      "158/18: n = 100\n",
      "158/19:\n",
      "@delayed\n",
      "def extract_feature_image(img, feature_type, feature_coord=None):\n",
      "    \"\"\"Extract the haar feature for the current image\"\"\"\n",
      "    img = cv2.resize(img, (36,36))\n",
      "    ii = integral_image(img)\n",
      "    return haar_like_feature(ii, 0, 0, ii.shape[0], ii.shape[1],\n",
      "                             feature_type=feature_type,\n",
      "                             feature_coord=feature_coord)\n",
      "158/20:\n",
      "def getx(facefiles, n):\n",
      "    ims = getimgs(facefiles, n)\n",
      "    X = delayed(extract_feature_image(img, feature_types) for img in ims)\n",
      "    return np.array(X.compute(scheduler='threads'))\n",
      "158/21:\n",
      "def getimgs(facefiles, n):\n",
      "    faces = []\n",
      "    for i in range(n):\n",
      "        faces.append(cv2.imread(facefiles[i], cv2.IMREAD_GRAYSCALE))\n",
      "    return faces + nonfaces\n",
      "158/22:\n",
      "def getx(facefiles, n):\n",
      "    ims = getimgs(facefiles, n)\n",
      "    X = delayed(extract_feature_image(img, feature_types) for img in ims)\n",
      "    return np.array(X.compute(scheduler='threads'))\n",
      "158/23:\n",
      "for i in range(n):\n",
      "    print(face_filenames0[i])\n",
      "158/24:\n",
      "for i in range(300):\n",
      "    print(test_face_names[i])\n",
      "158/25:\n",
      "t_start = time()\n",
      "Xf_n = getx(face_filenames0, n)\n",
      "time() - t_start\n",
      "158/26:\n",
      "nonfaces = []\n",
      "for i in range(n):\n",
      "    # Read a random nonface file\n",
      "    j = random.randint(0, num_nonface_filenames - 1)\n",
      "    nonface = cv2.imread(nonface_filenames[j], cv2.IMREAD_GRAYSCALE)\n",
      "    \n",
      "    wsize = random.rand int(50, min(nonface.shape[0], nonface.shape[1]))\n",
      "    row = random.randint(0, nonface.shape[0]-wsize)\n",
      "    col = random.randint(0, nonface.shape[1]-wsize)\n",
      "    crop = nonface[row:row+wsize, col:col+wsize]\n",
      "\n",
      "    # Resize to be the right size\n",
      "    crop = cv2.resize(crop, (200, 200))\n",
      "    nonfaces.append(crop)\n",
      "158/27:\n",
      "nonfaces = []\n",
      "for i in range(n):\n",
      "    # Read a random nonface file\n",
      "    j = random.randint(0, num_nonface_filenames - 1)\n",
      "    nonface = cv2.imread(nonface_filenames[j], cv2.IMREAD_GRAYSCALE)\n",
      "    \n",
      "    wsize = random.randint(50, min(nonface.shape[0], nonface.shape[1]))\n",
      "    row = random.randint(0, nonface.shape[0]-wsize)\n",
      "    col = random.randint(0, nonface.shape[1]-wsize)\n",
      "    crop = nonface[row:row+wsize, col:col+wsize]\n",
      "\n",
      "    # Resize to be the right size\n",
      "    crop = cv2.resize(crop, (200, 200))\n",
      "    nonfaces.append(crop)\n",
      "158/28:\n",
      "t_start = time()\n",
      "Xf_n = getx(face_filenames0, n)\n",
      "time() - t_start\n",
      "158/29: feature_types = ['type-2-x', 'type-2-y']\n",
      "158/30:\n",
      "t_start = time()\n",
      "Xf_n = getx(face_filenames0, n)\n",
      "time() - t_start\n",
      "158/31: feature_types = ['type-2-x', 'type-2-y', 'type-3-x', 'type-3-y']\n",
      "158/32:\n",
      "def getx(facefiles, n):\n",
      "    ims = getimgs(facefiles, n)\n",
      "    X = delayed(extract_feature_image(img, feature_types) for img in ims)\n",
      "    return np.array(X.compute(scheduler='threads'))\n",
      "158/33:\n",
      "t_start = time()\n",
      "Xf_n = getx(face_filenames0, n)\n",
      "time() - t_start\n",
      "158/34:\n",
      "def getimgs(facefiles, n):\n",
      "    faces = []\n",
      "    for i in range(n):\n",
      "        faces.append(cv2.imread(facefiles[i], cv2.IMREAD_GRAYSCALE))\n",
      "    return faces + nonfaces[:n]\n",
      "158/35: Xf_n.shape\n",
      "158/36: yf = [1]*100+[0]*300\n",
      "158/37:\n",
      "# Train a random forest classifier and assess its performance\n",
      "clf = RandomForestClassifier(n_estimators=1000, max_depth=None,\n",
      "                             n_jobs=-1, random_state=0)\n",
      "t_start = time()\n",
      "clf.fit(Xf_n, yf)\n",
      "time() - t_start\n",
      "158/38: yf = [1]*100+[0]*100\n",
      "158/39:\n",
      "# Train a random forest classifier and assess its performance\n",
      "clf = RandomForestClassifier(n_estimators=1000, max_depth=None,\n",
      "                             n_jobs=-1, random_state=0)\n",
      "t_start = time()\n",
      "clf.fit(Xf_n, yf)\n",
      "time() - t_start\n",
      "158/40: clf.score(X_t, y_t)\n",
      "158/41: X_t = getx(test_facenames, n)\n",
      "158/42:\n",
      "def getimgs(facefiles, nonfaces, n):\n",
      "    faces = []\n",
      "    for i in range(n):\n",
      "        faces.append(cv2.imread(facefiles[i], cv2.IMREAD_GRAYSCALE))\n",
      "    return faces + nonfaces[:n]\n",
      "158/43:\n",
      "def getimgs(facefiles, nonfaces, n):\n",
      "    faces = []\n",
      "    for i in range(n):\n",
      "        faces.append(cv2.imread(facefiles[i], cv2.IMREAD_GRAYSCALE))\n",
      "    return faces + nonfaces[:n]\n",
      "158/44:\n",
      "def getx(facefiles, nonfaces, n):\n",
      "    ims = getimgs(facefiles, nonfaces, n)\n",
      "    X = delayed(extract_feature_image(img, feature_types) for img in ims)\n",
      "    return np.array(X.compute(scheduler='threads'))\n",
      "158/45: clf.score(Xf_n, yf)\n",
      "158/46:\n",
      "t_start = time()\n",
      "dill.load_session('01_05_env.db')\n",
      "time() - t_start\n",
      "158/47: clf.score(X_t, y_t)\n",
      "158/48: clf\n",
      "158/49:\n",
      "# Train a random forest classifier and assess its performance\n",
      "clf = RandomForestClassifier(n_estimators=1000, max_depth=None,\n",
      "                             n_jobs=-1, random_state=0)\n",
      "t_start = time()\n",
      "clf.fit(Xf_n, yf)\n",
      "time() - t_start\n",
      "158/50: clf.score(X_t, y_t)\n",
      "158/51:\n",
      "def getimgs(facefiles, nonfaces, n):\n",
      "    faces = []\n",
      "    for i in range(n):\n",
      "        faces.append(cv2.imread(facefiles[i], cv2.IMREAD_GRAYSCALE))\n",
      "    return faces + nonfaces[:n]\n",
      "158/52: feature_types = ['type-2-x', 'type-2-y', 'type-3-x', 'type-3-y']\n",
      "158/53:\n",
      "Xf_t = delayed(extract_feature_image(img, feature_types) for img in images_t)\n",
      "Xf_t = np.array(Xf_t.compute(scheduler='threads'))\n",
      "158/54:\n",
      "X = delayed(extract_feature_image(img, feature_types) for img in images[:1])\n",
      "# Compute the result\n",
      "t_start = time()\n",
      "X = np.array(X.compute(scheduler='threads'))\n",
      "time() - t_start\n",
      "158/55:\n",
      "X = delayed(extract_feature_image(img, feature_types) for img in images0[:1])\n",
      "# Compute the result\n",
      "t_start = time()\n",
      "X = np.array(X.compute(scheduler='threads'))\n",
      "time() - t_start\n",
      "158/56: X.shape\n",
      "158/57: print(ii)\n",
      "158/58: print(feature_coord)\n",
      "158/59:\n",
      "# Extract all possible features\n",
      "feature_coord, feature_type = \\\n",
      "    haar_like_feature_coord(width=36, height=36,\n",
      "                            feature_type=feature_types)\n",
      "158/60: print(feature_coord)\n",
      "158/61: print(feature_coord.shape)\n",
      "158/62: feature_type\n",
      "158/63: feature_type.shape\n",
      "158/64: feature_type\n",
      "158/65: feature_type[0]\n",
      "158/66: feature_type[:20\n",
      "158/67: feature_type[:40]\n",
      "158/68:\n",
      "# To speed up the example, extract the two types of features only\n",
      "feature_types = ['type-2-x', 'type-2-y']\n",
      "\n",
      "# Build a computation graph using Dask. This allows the use of multiple\n",
      "# CPU cores later during the actual computation\n",
      "X = delayed(extract_feature_image(img, feature_types) for img in images)\n",
      "# Compute the result\n",
      "t_start = time()\n",
      "X = np.array(X.compute(scheduler='threads'))\n",
      "time_full_feature_comp = time() - t_start\n",
      "158/69:\n",
      "images = lfw_subset()\n",
      "# To speed up the example, extract the two types of features only\n",
      "feature_types = ['type-2-x', 'type-2-y']\n",
      "\n",
      "# Build a computation graph using Dask. This allows the use of multiple\n",
      "# CPU cores later during the actual computation\n",
      "X = delayed(extract_feature_image(img, feature_types) for img in images)\n",
      "# Compute the result\n",
      "t_start = time()\n",
      "X = np.array(X.compute(scheduler='threads'))\n",
      "time_full_feature_comp = time() - t_start\n",
      "158/70:\n",
      "from skimage.data import lfw_subset\n",
      "\n",
      "images = lfw_subset()\n",
      "# To speed up the example, extract the two types of features only\n",
      "feature_types = ['type-2-x', 'type-2-y']\n",
      "\n",
      "# Build a computation graph using Dask. This allows the use of multiple\n",
      "# CPU cores later during the actual computation\n",
      "X = delayed(extract_feature_image(img, feature_types) for img in images)\n",
      "# Compute the result\n",
      "t_start = time()\n",
      "X = np.array(X.compute(scheduler='threads'))\n",
      "time_full_feature_comp = time() - t_start\n",
      "159/1:\n",
      "import os\n",
      "import cv2\n",
      "from time import time\n",
      "import numpy as np\n",
      "import matplotlib.pyplot as plt\n",
      "from glob import glob\n",
      "import random\n",
      "from dask import delayed\n",
      "159/2:\n",
      "from sklearn.ensemble import RandomForestClassifier\n",
      "from sklearn.model_selection import train_test_split\n",
      "from sklearn.metrics import roc_auc_score\n",
      "from sklearn.linear_model import LogisticRegression\n",
      "159/3:\n",
      "from skimage.transform import integral_image\n",
      "from skimage.feature import haar_like_feature\n",
      "from skimage.feature import haar_like_feature_coord\n",
      "from skimage.feature import draw_haar_like_feature\n",
      "159/4: import dill\n",
      "159/5:\n",
      "test_face_names = sorted(glob(os.path.join('data/tests', '*.jpg')))\n",
      "n_test = len(test_face_names)\n",
      "159/6:\n",
      "b_inds = []\n",
      "w_inds = []\n",
      "for i in range(len(test_face_names)):\n",
      "    if int(test_face_names[i].split('_')[2]) is 0: # race is white\n",
      "        w_inds.append(i)\n",
      "    else:\n",
      "        b_inds.append(i)\n",
      "159/7:\n",
      "test_nonface_names = sorted(glob(os.path.join('data/testing_nonfaces', '*.jpg')))\n",
      "n_nonfaces_test = len(test_nonface_names)\n",
      "159/8:\n",
      "faces_t = []\n",
      "for i in range(n_test):\n",
      "    faces_t.append(cv2.imread(test_face_names[i], cv2.IMREAD_GRAYSCALE))\n",
      "159/9:\n",
      "nonfaces_t = []\n",
      "for i in range(n_test):\n",
      "    # Read a random nonface file\n",
      "    j = random.randint(0, n_nonfaces_test - 1)\n",
      "    nonface = cv2.imread(test_nonface_names[j], cv2.IMREAD_GRAYSCALE)\n",
      "    \n",
      "    wsize = random.randint(36, min(nonface.shape[0], nonface.shape[1]))\n",
      "    row = random.randint(0, nonface.shape[0]-wsize)\n",
      "    col = random.randint(0, nonface.shape[1]-wsize)\n",
      "    crop = nonface[row:row+wsize, col:col+wsize]\n",
      "\n",
      "    # Resize to be the right size\n",
      "    crop = cv2.resize(crop, (200, 200))\n",
      "    nonfaces_t.append(crop)\n",
      "159/10:\n",
      "images_t = faces_t + nonfaces_t\n",
      "len(images_t)\n",
      "159/11: feature_types = ['type-2-x', 'type-2-y', 'type-3-x', 'type-3-y']\n",
      "159/12:\n",
      "def getx(facefiles, nonfaces, n):\n",
      "    ims = getimgs(facefiles, nonfaces, n)\n",
      "    X = delayed(extract_feature_image(img, feature_types) for img in ims)\n",
      "    return np.array(X.compute(scheduler='threads'))\n",
      "159/13:\n",
      "@delayed\n",
      "def extract_feature_image(img, feature_type, feature_coord=None):\n",
      "    \"\"\"Extract the haar feature for the current image\"\"\"\n",
      "    img = cv2.resize(img, (36,36))\n",
      "    ii = integral_image(img)\n",
      "    return haar_like_feature(ii, 0, 0, ii.shape[0], ii.shape[1],\n",
      "                             feature_type=feature_type,\n",
      "                             feature_coord=feature_coord)\n",
      "159/14:\n",
      "@delayed\n",
      "def extract_feature_image(img, feature_type, feature_coord=None):\n",
      "    \"\"\"Extract the haar feature for the current image\"\"\"\n",
      "    img = cv2.resize(img, (36,36))\n",
      "    ii = integral_image(img)\n",
      "    return haar_like_feature(ii, 0, 0, ii.shape[0], ii.shape[1],\n",
      "                             feature_type=feature_type,\n",
      "                             feature_coord=feature_coord)\n",
      "159/15:\n",
      "Xf_t = delayed(extract_feature_image(img, feature_types) for img in images_t)\n",
      "Xf_t = np.array(Xf_t.compute(scheduler='threads'))\n",
      "159/16:\n",
      "t_start = time()\n",
      "Xf_t = delayed(extract_feature_image(img, feature_types) for img in images_t)\n",
      "Xf_t = np.array(Xf_t.compute(scheduler='threads'))\n",
      "time() - t_start\n",
      "159/17: y_t = np.array([1]*len(faces_t) + [0]*len(nonfaces_t))\n",
      "159/18: yf_t = np.array([1]*len(faces_t) + [0]*len(nonfaces_t))\n",
      "159/19:\n",
      "# Train a random forest classifier and assess its performance\n",
      "clf = RandomForestClassifier(n_estimators=1000, max_depth=None,\n",
      "                             n_jobs=-1, random_state=0)\n",
      "t_start = time()\n",
      "clf.fit(Xf_t, yf_t)\n",
      "time() - t_start\n",
      "159/20:\n",
      "cdf_feature_importances = np.cumsum(clf.feature_importances_[idx_sorted])\n",
      "cdf_feature_importances /= np.max(cdf_feature_importances)\n",
      "sig_feature_count = np.count_nonzero(cdf_feature_importances < 0.7)\n",
      "sig_feature_percent = round(sig_feature_count /\n",
      "                            len(cdf_feature_importances) * 100, 1)\n",
      "print(('{} features, or {}%, account for 70% of branch points in the '\n",
      "       'random forest.').format(sig_feature_count, sig_feature_percent))\n",
      "159/21:\n",
      "idx_sorted = np.argsort(clf.feature_importances_)[::-1]\n",
      "\n",
      "fig, axes = plt.subplots(3, 2)\n",
      "for idx, ax in enumerate(axes.ravel()):\n",
      "    image = cv2.resize(images_t[0], (36,36))\n",
      "    image = draw_haar_like_feature(image, 0, 0, 36, 36,\n",
      "                                   [feature_coord[idx_sorted[idx]]])\n",
      "    ax.imshow(image)\n",
      "    ax.set_xticks([])\n",
      "    ax.set_yticks([])\n",
      "\n",
      "fig.suptitle('The most important features')\n",
      "159/22:\n",
      "feature_coord, feature_type = \\\n",
      "    haar_like_feature_coord(width=36, height=36,\n",
      "                            feature_type=feature_types)\n",
      "159/23:\n",
      "idx_sorted = np.argsort(clf.feature_importances_)[::-1]\n",
      "\n",
      "fig, axes = plt.subplots(3, 2)\n",
      "for idx, ax in enumerate(axes.ravel()):\n",
      "    image = cv2.resize(images_t[0], (36,36))\n",
      "    image = draw_haar_like_feature(image, 0, 0, 36, 36,\n",
      "                                   [feature_coord[idx_sorted[idx]]])\n",
      "    ax.imshow(image)\n",
      "    ax.set_xticks([])\n",
      "    ax.set_yticks([])\n",
      "\n",
      "fig.suptitle('The most important features')\n",
      "159/24:\n",
      "cdf_feature_importances = np.cumsum(clf.feature_importances_[idx_sorted])\n",
      "cdf_feature_importances /= np.max(cdf_feature_importances)\n",
      "sig_feature_count = np.count_nonzero(cdf_feature_importances < 0.7)\n",
      "sig_feature_percent = round(sig_feature_count /\n",
      "                            len(cdf_feature_importances) * 100, 1)\n",
      "print(('{} features, or {}%, account for 70% of branch points in the '\n",
      "       'random forest.').format(sig_feature_count, sig_feature_percent))\n",
      "159/25:\n",
      "feature_coord_sel = feature_coord[idx_sorted[:sig_feature_count]]\n",
      "feature_type_sel = feature_type[idx_sorted[:sig_feature_count]]\n",
      "159/26:\n",
      "idx_sorted = np.argsort(clf.feature_importances_)[::-1]\n",
      "\n",
      "fig, axes = plt.subplots(5, 5)\n",
      "for idx, ax in enumerate(axes.ravel()):\n",
      "    image = cv2.resize(images_t[0], (36,36))\n",
      "    image = draw_haar_like_feature(image, 0, 0, 36, 36,\n",
      "                                   [feature_coord[idx_sorted[idx]]])\n",
      "    ax.imshow(image)\n",
      "    ax.set_xticks([])\n",
      "    ax.set_yticks([])\n",
      "\n",
      "fig.suptitle('The most important features')\n",
      "159/27: feature_types\n",
      "159/28: feature_coord.shape\n",
      "159/29: feature_coord\n",
      "159/30:\n",
      "idx_sorted = np.argsort(clf.feature_importances_)[::-1]\n",
      "\n",
      "fig, axes = plt.subplots(8,8)\n",
      "for idx, ax in enumerate(axes.ravel()):\n",
      "    image = cv2.resize(images_t[0], (36,36))\n",
      "    image = draw_haar_like_feature(image, 0, 0, 36, 36,\n",
      "                                   [feature_coord[idx_sorted[idx]]])\n",
      "    ax.imshow(image)\n",
      "    ax.set_xticks([])\n",
      "    ax.set_yticks([])\n",
      "\n",
      "fig.suptitle('The most important features')\n",
      "159/31:\n",
      "for i in range(100):\n",
      "    print(feature_coord[idx_sorted[i]].shape)\n",
      "159/32:\n",
      "for i in range(100):\n",
      "    print(len(feature_coord[idx_sorted[i]]))\n",
      "159/33:\n",
      "for i in range(1009):\n",
      "    if len(feature_coord[idx_sorted[i]]):\n",
      "    print(i)\n",
      "159/34:\n",
      "for i in range(1009):\n",
      "    if len(feature_coord[idx_sorted[i]]):\n",
      "        print(i)\n",
      "159/35:\n",
      "for i in range(1009):\n",
      "    if len(feature_coord[idx_sorted[i]]) > 2:\n",
      "        print(i)\n",
      "159/36:\n",
      "for i in range(1000000):\n",
      "    if len(feature_coord[idx_sorted[i]]) > 2:\n",
      "        print(i)\n",
      "159/37:\n",
      "for i in range(10000):\n",
      "    if len(feature_coord[idx_sorted[i]]) > 2:\n",
      "        print(i)\n",
      "159/38:\n",
      "for i in range(2500):\n",
      "    if len(feature_coord[idx_sorted[i]]) > 2:\n",
      "        print(i)\n",
      "159/39:\n",
      "for i in range(1500):\n",
      "    if len(feature_coord[idx_sorted[i]]) > 2:\n",
      "        print(i)\n",
      "159/40: dill.dump_session('01_06_env.db')\n",
      "159/41:\n",
      "ys = [i for i in range(0,1,0.05)]\n",
      "xs = []\n",
      "for i in xs:\n",
      "    cdf_feature_importances = np.cumsum(clf.feature_importances_[idx_sorted])\n",
      "    cdf_feature_importances /= np.max(cdf_feature_importances)\n",
      "    sig_feature_count = np.count_nonzero(cdf_feature_importances < i)\n",
      "    xs.append(sig_feature_count)\n",
      "159/42:\n",
      "ys = [i for i in range(0,1,0.05)]\n",
      "xs = []\n",
      "for i in range(len(ys)):\n",
      "    sig_feature_count = np.count_nonzero(cdf_feature_importances < ys[i])\n",
      "    xs.append(sig_feature_count)\n",
      "159/43:\n",
      "ys = [i/20 for i in range(0,200)]\n",
      "xs = []\n",
      "for i in range(len(ys)):\n",
      "    sig_feature_count = np.count_nonzero(cdf_feature_importances < ys[i])\n",
      "    xs.append(sig_feature_count)\n",
      "159/44: len(ys)\n",
      "159/45: plot(xs, ys)\n",
      "159/46: plt.plot(xs, ys)\n",
      "159/47: ys[199]\n",
      "159/48:\n",
      "ys = [i/200 for i in range(0,200)]\n",
      "xs = []\n",
      "for i in range(len(ys)):\n",
      "    sig_feature_count = np.count_nonzero(cdf_feature_importances < ys[i])\n",
      "    xs.append(sig_feature_count)\n",
      "159/49: plt.plot(xs, ys)\n",
      "159/50:\n",
      "plt.plot(xs, ys)\n",
      "plt.title(\"percent of branch points vs number of features\")\n",
      "plt.show()\n",
      "159/51:\n",
      "plt.plot(xs, ys)\n",
      "plt.title(\"% branch points vs number of features\")\n",
      "plt.show()\n",
      "159/52: np.count_nonzero(cdf_feature_importances < .8)\n",
      "159/53: np.count_nonzero(cdf_feature_importances < .9)\n",
      "159/54: np.count_nonzero(cdf_feature_importances < .85)\n",
      "159/55: np.count_nonzero(cdf_feature_importances < .8)\n",
      "159/56: np.count_nonzero(cdf_feature_importances < .82)\n",
      "159/57: n_features = 1800\n",
      "159/58:\n",
      "feature_coord_sel = feature_coord[idx_sorted[:n_features]]\n",
      "feature_type_sel = feature_type[idx_sorted[:n_features]]\n",
      "159/59: n_features2 = 900\n",
      "159/60:\n",
      "feature_coord_sel2 = feature_coord[idx_sorted[:n_features2]]\n",
      "feature_type_sel2 = feature_type[idx_sorted[:n_features2]]\n",
      "159/61: dill.dump_session('01_06_env.db')\n",
      "159/62:\n",
      "face_filenames0 = sorted(glob(os.path.join('data/train_0', '*.jpg')))\n",
      "num_face_filenames0 = len(face_filenames0)\n",
      "num_face_filenames0\n",
      "159/63:\n",
      "face_filenames25 = sorted(glob(os.path.join('data/train_25', '*.jpg')))\n",
      "num_face_filenames25 = len(face_filenames25)\n",
      "num_face_filenames25\n",
      "159/64:\n",
      "face_filenames50 = sorted(glob(os.path.join('data/train_50', '*.jpg')))\n",
      "num_face_filenames50 = len(face_filenames50)\n",
      "num_face_filenames50\n",
      "159/65:\n",
      "face_filenames75 = sorted(glob(os.path.join('data/train_75', '*.jpg')))\n",
      "num_face_filenames75 = len(face_filenames75)\n",
      "num_face_filenames75\n",
      "159/66:\n",
      "face_filenames100 = sorted(glob(os.path.join('data/train_100', '*.jpg')))\n",
      "num_face_filenames100 = len(face_filenames100)\n",
      "num_face_filenames100\n",
      "159/67:\n",
      "nonface_filenames = sorted(glob(os.path.join('data/training_nonfaces', '*.jpg')))\n",
      "num_nonface_filenames = len(nonface_filenames)\n",
      "num_nonface_filenames\n",
      "159/68:\n",
      "nonfaces = []\n",
      "for i in range(n):\n",
      "    # Read a random nonface file\n",
      "    j = random.randint(0, num_nonface_filenames - 1)\n",
      "    nonface = cv2.imread(nonface_filenames[j], cv2.IMREAD_GRAYSCALE)\n",
      "    \n",
      "    wsize = random.randint(50, min(nonface.shape[0], nonface.shape[1]))\n",
      "    row = random.randint(0, nonface.shape[0]-wsize)\n",
      "    col = random.randint(0, nonface.shape[1]-wsize)\n",
      "    crop = nonface[row:row+wsize, col:col+wsize]\n",
      "\n",
      "    # Resize to be the right size\n",
      "    crop = cv2.resize(crop, (200, 200))\n",
      "    nonfaces.append(crop)\n",
      "159/69: n = 4020\n",
      "159/70:\n",
      "nonfaces = []\n",
      "for i in range(n):\n",
      "    # Read a random nonface file\n",
      "    j = random.randint(0, num_nonface_filenames - 1)\n",
      "    nonface = cv2.imread(nonface_filenames[j], cv2.IMREAD_GRAYSCALE)\n",
      "    \n",
      "    wsize = random.randint(50, min(nonface.shape[0], nonface.shape[1]))\n",
      "    row = random.randint(0, nonface.shape[0]-wsize)\n",
      "    col = random.randint(0, nonface.shape[1]-wsize)\n",
      "    crop = nonface[row:row+wsize, col:col+wsize]\n",
      "\n",
      "    # Resize to be the right size\n",
      "    crop = cv2.resize(crop, (200, 200))\n",
      "    nonfaces.append(crop)\n",
      "159/71:\n",
      "def getx(facefiles, n):\n",
      "    ims = getimgs(facefiles, n)\n",
      "    X = delayed(extract_feature_image(img, feature_types) for img in ims)\n",
      "    return np.array(X.compute(scheduler='threads'))\n",
      "159/72:\n",
      "def getimgs(facefiles, nonfaces, n):\n",
      "    faces = []\n",
      "    for i in range(n):\n",
      "        faces.append(cv2.imread(facefiles[i], cv2.IMREAD_GRAYSCALE))\n",
      "    return faces + nonfaces[:n]\n",
      "159/73: nonfaces.shape\n",
      "159/74: len(nonfaces)\n",
      "159/75:\n",
      "def getx2(facefiles, n, feature_types, feature_coords):\n",
      "    ims = getimgs(facefiles, n)\n",
      "    X = delayed(extract_feature_image(img, feature_types, feature_coords) for img in ims)\n",
      "    return np.array(X.compute(scheduler='threads'))\n",
      "159/76: X0 = getx2(face_filenames0, n, feature_type_sel2, feature_coord_sel2)\n",
      "159/77: n, len(nonfaces)\n",
      "159/78:\n",
      "def getx2(facefiles, n, feature_types, feature_coords):\n",
      "    ims = getimgs(facefiles, nonfaces, n)\n",
      "    X = delayed(extract_feature_image(img, feature_types, feature_coords) for img in ims)\n",
      "    return np.array(X.compute(scheduler='threads'))\n",
      "159/79: X0 = getx2(face_filenames0, n, feature_type_sel2, feature_coord_sel2)\n",
      "159/80:\n",
      "t_start = time()\n",
      "X0 = getx2(face_filenames0, n, feature_type_sel2, feature_coord_sel2)\n",
      "time() - t_start\n",
      "159/81: X2.shape\n",
      "159/82: X0.shape\n",
      "159/83: from logistic import logistic_fit, logistic, logistic_prob\n",
      "159/84: y = np.array([1]*n + [0]*n)\n",
      "159/85: y.shape\n",
      "159/86:\n",
      "t_start = time()\n",
      "params = logistic_fit(X0, y, 0.001)\n",
      "time() - t_start\n",
      "159/87: Xt_900 = getx2(test_facenames, 300, feature_type_sel2, feature_coord_sel2)\n",
      "159/88: Xt_900 = getx2(test_face_names, 300, feature_type_sel2, feature_coord_sel2)\n",
      "159/89: Xt_900.shape\n",
      "159/90: yt_900 = np.array([1]*300 + [0]*300)\n",
      "159/91: predicted = logistic_prob(Xt_900, params)\n",
      "159/92: len(predicted)\n",
      "159/93: predicted\n",
      "159/94:\n",
      "correct = 0\n",
      "for i in range(600):\n",
      "    if predicted[i] >= 0.5 and y[i] == 1:\n",
      "        correct += 1\n",
      "    elif predicted[i] < 0.5 and y[i] == 0:\n",
      "        correct += 1\n",
      "correct/600\n",
      "159/95:\n",
      "# Train a random forest classifier and assess its performance\n",
      "clf = RandomForestClassifier(n_estimators=1000, max_depth=None,\n",
      "                             n_jobs=-1, random_state=0)\n",
      "t_start = time()\n",
      "clf.fit(X0, y)\n",
      "time() - t_start\n",
      "159/96: clf.score(Xt_900, yt_900)\n",
      "159/97: preds = [1 for p in predicted if p >= 0.5 else 0]\n",
      "159/98: preds = 1*predicted[p>0]\n",
      "159/99: preds = 1*predicted[predicted>0]\n",
      "159/100: preds\n",
      "159/101: preds = 1*predicted[predicted>=0.5]\n",
      "159/102: preds\n",
      "159/103:\n",
      "preds = []\n",
      "for p in predicted:\n",
      "    if p >= 0.5:\n",
      "        preds.append(1)\n",
      "    else:\n",
      "        preds.append(0)\n",
      "159/104: preds\n",
      "159/105: sum(abs(preds-y))/600\n",
      "159/106:\n",
      "correct = 0\n",
      "for i in range(600):\n",
      "    if predicted[i] >= 0.5 and yt_900[i] == 1:\n",
      "        correct += 1\n",
      "    elif predicted[i] < 0.5 and yt_900[i] == 0:\n",
      "        correct += 1\n",
      "correct/600\n",
      "159/107: sum(abs(preds-yt_900))/600\n",
      "159/108: lr = LogisticRegression().fit(X0, y)\n",
      "159/109:\n",
      "t_start = time()\n",
      "lr = LogisticRegression().fit(X0, y)\n",
      "time() - t_start\n",
      "159/110: lr.score(Xt_900, yt_900)\n",
      "159/111:\n",
      "t_start = time()\n",
      "X25 = getx2(face_filenames25, n, feature_type_sel2, feature_coord_sel2)\n",
      "time() - t_start\n",
      "159/112:\n",
      "t_start = time()\n",
      "X50 = getx2(face_filenames50, n, feature_type_sel2, feature_coord_sel2)\n",
      "time() - t_start\n",
      "159/113:\n",
      "t_start = time()\n",
      "X75 = getx2(face_filenames75, n, feature_type_sel2, feature_coord_sel2)\n",
      "time() - t_start\n",
      "159/114:\n",
      "t_start = time()\n",
      "X100 = getx2(face_filenames100, n, feature_type_sel2, feature_coord_sel2)\n",
      "time() - t_start\n",
      "160/1:\n",
      "import os\n",
      "import cv2\n",
      "from time import time\n",
      "import numpy as np\n",
      "import matplotlib.pyplot as plt\n",
      "from glob import glob\n",
      "import random\n",
      "from dask import delayed\n",
      "160/2:\n",
      "from sklearn.ensemble import RandomForestClassifier\n",
      "from sklearn.model_selection import train_test_split\n",
      "from sklearn.metrics import roc_auc_score\n",
      "from sklearn.linear_model import LogisticRegression\n",
      "160/3:\n",
      "from sklearn.ensemble import RandomForestClassifier, AdaBoostClassifier\n",
      "from sklearn.model_selection import train_test_split\n",
      "from sklearn.metrics import roc_auc_score\n",
      "from sklearn.linear_model import LogisticRegression\n",
      "160/4:\n",
      "from skimage.transform import integral_image\n",
      "from skimage.feature import haar_like_feature\n",
      "from skimage.feature import haar_like_feature_coord\n",
      "from skimage.feature import draw_haar_like_feature\n",
      "160/5: from logistic import logistic_fit, logistic, logistic_prob\n",
      "160/6: import dill\n",
      "160/7:\n",
      "def proportion_b(facenames):\n",
      "    count = 0\n",
      "    for i in range(n):\n",
      "        if int(facenames[i].split('_')[3]) == 0:\n",
      "            count += 1\n",
      "    return count / n\n",
      "160/8:\n",
      "@delayed\n",
      "def extract_feature_image(img, feature_type, feature_coord=None):\n",
      "    \"\"\"Extract the haar feature for the current image\"\"\"\n",
      "    img = cv2.resize(img, (36,36))\n",
      "    ii = integral_image(img)\n",
      "    return haar_like_feature(ii, 0, 0, ii.shape[0], ii.shape[1],\n",
      "                             feature_type=feature_type,\n",
      "                             feature_coord=feature_coord)\n",
      "160/9:\n",
      "for f in [face_filenames0, face_filenames25, face_filenames50, face_filenames75, face_filenames100]:\n",
      "    print(proportion_b(f))\n",
      "160/10:\n",
      "def getimgs(facefiles, nonfaces, n):\n",
      "    faces = []\n",
      "    for i in range(n):\n",
      "        faces.append(cv2.imread(facefiles[i], cv2.IMREAD_GRAYSCALE))\n",
      "    return faces + nonfaces[:n]\n",
      "160/11:\n",
      "def getx(facefiles, n):\n",
      "    ims = getimgs(facefiles, n)\n",
      "    X = delayed(extract_feature_image(img, feature_types) for img in ims)\n",
      "    return np.array(X.compute(scheduler='threads'))\n",
      "160/12:\n",
      "def getx2(facefiles, n, feature_types, feature_coords):\n",
      "    ims = getimgs(facefiles, nonfaces, n)\n",
      "    X = delayed(extract_feature_image(img, feature_types, feature_coords) for img in ims)\n",
      "    return np.array(X.compute(scheduler='threads'))\n",
      "160/13: n, len(nonfaces)\n",
      "160/14:\n",
      "test_face_names = sorted(glob(os.path.join('data/tests', '*.jpg')))\n",
      "n_test = len(test_face_names)\n",
      "160/15: test_face_names[0]\n",
      "160/16: test_face_names[0].split('/').split('_')\n",
      "160/17: test_face_names[0].split('/')\n",
      "160/18: test_face_names[0].split('/')[2].split('_')\n",
      "160/19: fnames50 = sorted(glob(os.path.join('data/train_50', '*.jpg')))\n",
      "160/20:\n",
      "a = [1,2,3]\n",
      "b = copy(a)\n",
      "160/21:\n",
      "a = [1,2,3]\n",
      "b = deepcopy(a)\n",
      "160/22:\n",
      "a = [1,2,3]\n",
      "b = a.copy()\n",
      "160/23: b[0] = 2\n",
      "160/24: a\n",
      "160/25: fnames0[0].split('/')[2].split('_')\n",
      "160/26: fnames50[0].split('/')[2].split('_')\n",
      "160/27:\n",
      "feature_fnames = []\n",
      "for i in range(len(test_face_names)):\n",
      "    f = test_face_names[0].split('/')[2].split('_')\n",
      "    for j in range(4020):\n",
      "        f2 = fnames50[j].split('/')[2].split('_')\n",
      "        if f2[0] == f[0] and f2[1] == f[1] and f2[2] == f[2]:\n",
      "            feature_fnames.append(fnames50[j])\n",
      "            del fnames50[j]\n",
      "160/28: len(feature_fnames)\n",
      "160/29:\n",
      "feature_fnames = []\n",
      "for i in range(len(test_face_names)):\n",
      "    f = test_face_names[0].split('/')[2].split('_')\n",
      "    for j in range(4020):\n",
      "        f2 = fnames50[j].split('/')[2].split('_')\n",
      "        print(f2, f)\n",
      "        if f2[0] == f[0] and f2[1] == f[1] and f2[2] == f[2]:\n",
      "            feature_fnames.append(fnames50[j])\n",
      "            del fnames50[j]\n",
      "160/30:\n",
      "feature_fnames = []\n",
      "for i in range(len(test_face_names)):\n",
      "    f = test_face_names[0].split('/')[2].split('_')\n",
      "    for j in range(4020):\n",
      "        f2 = fnames50[j].split('/')[2].split('_')\n",
      "        print(f2[0] == f[0])\n",
      "        if f2[0] == f[0] and f2[1] == f[1] and f2[2] == f[2]:\n",
      "            feature_fnames.append(fnames50[j])\n",
      "            del fnames50[j]\n",
      "160/31:\n",
      "feature_fnames = []\n",
      "for i in range(len(test_face_names)):\n",
      "    f = test_face_names[0].split('/')[2].split('_')\n",
      "    for j in range(4020):\n",
      "        f2 = fnames50[j].split('/')[2].split('_')\n",
      "        print(f2[0] is f[0])\n",
      "        if f2[0] == f[0] and f2[1] == f[1] and f2[2] == f[2]:\n",
      "            feature_fnames.append(fnames50[j])\n",
      "            del fnames50[j]\n",
      "160/32:\n",
      "feature_fnames = []\n",
      "for i in range(len(test_face_names)):\n",
      "    f = test_face_names[0].split('/')[2].split('_')\n",
      "    for j in range(4020):\n",
      "        f2 = fnames50[j].split('/')[2].split('_')\n",
      "        print(f2[0], f[0])\n",
      "        if f2[0] == f[0] and f2[1] == f[1] and f2[2] == f[2]:\n",
      "            feature_fnames.append(fnames50[j])\n",
      "            del fnames50[j]\n",
      "160/33:\n",
      "feature_fnames = []\n",
      "for i in range(len(test_face_names)):\n",
      "    f = test_face_names[0].split('/')[2].split('_')\n",
      "    for j in range(4020):\n",
      "        f2 = fnames50[j].split('/')[2].split('_')\n",
      "        print(f2[0] is f[0])\n",
      "        if f2[0] == f[0] and f2[1] == f[1] and f2[2] == f[2]:\n",
      "            feature_fnames.append(fnames50[j])\n",
      "            del fnames50[j]\n",
      "160/34:\n",
      "feature_fnames = []\n",
      "for i in range(len(test_face_names)):\n",
      "    f = test_face_names[0].split('/')[2].split('_')\n",
      "    for j in range(4020):\n",
      "        f2 = fnames50[j].split('/')[2].split('_')\n",
      "        print(int(f2[0]) ==  int(f[0]))\n",
      "        if f2[0] == f[0] and f2[1] == f[1] and f2[2] == f[2]:\n",
      "            feature_fnames.append(fnames50[j])\n",
      "            del fnames50[j]\n",
      "160/35:\n",
      "feature_fnames = []\n",
      "for i in range(len(test_face_names)):\n",
      "    f = test_face_names[0].split('/')[2].split('_')\n",
      "    for j in range(4020):\n",
      "        f2 = fnames50[j].split('/')[2].split('_')\n",
      "        if int(f2[0]) == int(f[0]) and int(f2[1]) == int(f[1]) /\n",
      "        and int(f2[2]) == int(f[2]):\n",
      "            feature_fnames.append(fnames50[j])\n",
      "            del fnames50[j]\n",
      "160/36:\n",
      "feature_fnames = []\n",
      "for i in range(len(test_face_names)):\n",
      "    f = test_face_names[0].split('/')[2].split('_')\n",
      "    for j in range(4020):\n",
      "        f2 = fnames50[j].split('/')[2].split('_')\n",
      "        if int(f2[0]) == int(f[0]) and int(f2[1]) == int(f[1]) \n",
      "        and int(f2[2]) == int(f[2]):\n",
      "            feature_fnames.append(fnames50[j])\n",
      "            del fnames50[j]\n",
      "160/37:\n",
      "feature_fnames = []\n",
      "for i in range(len(test_face_names)):\n",
      "    f = test_face_names[0].split('/')[2].split('_')\n",
      "    for j in range(4020):\n",
      "        f2 = fnames50[j].split('/')[2].split('_')\n",
      "        if int(f2[0]) == int(f[0]) and int(f2[1]) == int(f[1]) and int(f2[2]) == int(f[2]):\n",
      "            feature_fnames.append(fnames50[j])\n",
      "            del fnames50[j]\n",
      "160/38: len(feature_fnames)\n",
      "160/39:\n",
      "feature_fnames = []\n",
      "for i in range(len(test_face_names)):\n",
      "    f = test_face_names[i].split('/')[2].split('_')\n",
      "    for j in range(4020):\n",
      "        f2 = fnames50[j].split('/')[2].split('_')\n",
      "        if int(f2[0]) == int(f[0]) and int(f2[1]) == int(f[1]) and int(f2[2]) == int(f[2]):\n",
      "            feature_fnames.append(fnames50[j])\n",
      "            del fnames50[j]\n",
      "160/40: len(test_face_names)\n",
      "160/41: len(test_face_names), len(fnames50)\n",
      "160/42:\n",
      "feature_fnames = []\n",
      "for i in range(len(test_face_names)):\n",
      "    f = test_face_names[i].split('/')[2].split('_')\n",
      "    for j in range(len(fnames50)):\n",
      "        f2 = fnames50[j].split('/')[2].split('_')\n",
      "        if int(f2[0]) == int(f[0]) and int(f2[1]) == int(f[1]) and int(f2[2]) == int(f[2]):\n",
      "            feature_fnames.append(fnames50[j])\n",
      "            del fnames50[j]\n",
      "160/43:\n",
      "feature_fnames = []\n",
      "for i in range(len(test_face_names)):\n",
      "    f = test_face_names[i].split('/')[2].split('_')\n",
      "    j = 0\n",
      "    while j < len(fnames50):\n",
      "        f2 = fnames50[j].split('/')[2].split('_')\n",
      "        if int(f2[0]) == int(f[0]) and int(f2[1]) == int(f[1]) and int(f2[2]) == int(f[2]):\n",
      "            feature_fnames.append(fnames50[j])\n",
      "            del fnames50[j]\n",
      "160/44:\n",
      "feature_fnames = []\n",
      "for i in range(len(test_face_names)):\n",
      "    f = test_face_names[i].split('/')[2].split('_')\n",
      "    j = 0\n",
      "    while j < len(fnames50):\n",
      "        j += 1\n",
      "        f2 = fnames50[j].split('/')[2].split('_')\n",
      "        if int(f2[0]) == int(f[0]) and int(f2[1]) == int(f[1]) and int(f2[2]) == int(f[2]):\n",
      "            feature_fnames.append(fnames50[j])\n",
      "            del fnames50[j]\n",
      "160/45:\n",
      "feature_fnames = []\n",
      "for i in range(len(test_face_names)):\n",
      "    f = test_face_names[i].split('/')[2].split('_')\n",
      "    j = 0\n",
      "    while j < len(fnames50):\n",
      "        f2 = fnames50[j].split('/')[2].split('_')\n",
      "        if int(f2[0]) == int(f[0]) and int(f2[1]) == int(f[1]) and int(f2[2]) == int(f[2]):\n",
      "            feature_fnames.append(fnames50[j])\n",
      "            del fnames50[j]\n",
      "            break\n",
      "        j += 1\n",
      "160/46: len(feature_fnames)\n",
      "160/47:\n",
      "feature_fnames = []\n",
      "for i in range(len(test_face_names)):\n",
      "    f = test_face_names[i].split('/')[2].split('_')\n",
      "    j = 0\n",
      "    while j < len(fnames50):\n",
      "        f2 = fnames50[j].split('/')[2].split('_')\n",
      "        if int(f2[0]) == int(f[0]) and int(f2[1]) == int(f[1]) and int(f2[2]) == int(f[2]):\n",
      "            feature_fnames.append(fnames50[j])\n",
      "            del fnames50[j]\n",
      "            break\n",
      "        j += 1\n",
      "    if j == len(fnames50):\n",
      "        j = 0\n",
      "        while j < len(fnames0):\n",
      "            f2 = fnames0[j].split('/')[2].split('_')\n",
      "            if int(f2[0]) == int(f[0]) and int(f2[1]) == int(f[1]) and int(f2[2]) == int(f[2]):\n",
      "                feature_fnames.append(fnames0[j])\n",
      "                del fnames0[j]\n",
      "                break\n",
      "            j += 1\n",
      "160/48:\n",
      "fnames50 = sorted(glob(os.path.join('data/train_50', '*.jpg')))\n",
      "fnames0 = sorted(glob(os.path.join('data/train_0', '*.jpg')))\n",
      "fnames100 = sorted(glob(os.path.join('data/train_100', '*.jpg')))\n",
      "160/49:\n",
      "feature_fnames = []\n",
      "for i in range(len(test_face_names)):\n",
      "    f = test_face_names[i].split('/')[2].split('_')\n",
      "    j = 0\n",
      "    while j < len(fnames50):\n",
      "        f2 = fnames50[j].split('/')[2].split('_')\n",
      "        if int(f2[0]) == int(f[0]) and int(f2[1]) == int(f[1]) and int(f2[2]) == int(f[2]):\n",
      "            feature_fnames.append(fnames50[j])\n",
      "            del fnames50[j]\n",
      "            break\n",
      "        j += 1\n",
      "    if j == len(fnames50):\n",
      "        j = 0\n",
      "        while j < len(fnames0):\n",
      "            f2 = fnames0[j].split('/')[2].split('_')\n",
      "            if int(f2[0]) == int(f[0]) and int(f2[1]) == int(f[1]) and int(f2[2]) == int(f[2]):\n",
      "                feature_fnames.append(fnames0[j])\n",
      "                del fnames0[j]\n",
      "                break\n",
      "            j += 1\n",
      "160/50: len(feature_fnames)\n",
      "160/51:\n",
      "feature_fnames = []\n",
      "for i in range(len(test_face_names)):\n",
      "    f = test_face_names[i].split('/')[2].split('_')\n",
      "    j = 0\n",
      "    while j < len(fnames50):\n",
      "        f2 = fnames50[j].split('/')[2].split('_')\n",
      "        if int(f2[0]) == int(f[0]) and int(f2[1]) == int(f[1]) and int(f2[2]) == int(f[2]):\n",
      "            feature_fnames.append(fnames50[j])\n",
      "            del fnames50[j]\n",
      "            break\n",
      "        j += 1\n",
      "    if j == len(fnames50):\n",
      "        j = 0\n",
      "        while j < len(fnames0):\n",
      "            f2 = fnames0[j].split('/')[2].split('_')\n",
      "            if int(f2[0]) == int(f[0]) and int(f2[1]) == int(f[1]) and int(f2[2]) == int(f[2]):\n",
      "                feature_fnames.append(fnames0[j])\n",
      "                del fnames0[j]\n",
      "                break\n",
      "            j += 1\n",
      "    if j == len(fnames0):\n",
      "        j = 0\n",
      "        while j < len(fnames100):\n",
      "            f2 = fnames100[j].split('/')[2].split('_')\n",
      "            if int(f2[0]) == int(f[0]) and int(f2[1]) == int(f[1]) and int(f2[2]) == int(f[2]):\n",
      "                feature_fnames.append(fnames100[j])\n",
      "                del fnames100[j]\n",
      "                break\n",
      "            j += 1\n",
      "    if len(feature_fnames) < i+1:\n",
      "        print(f)\n",
      "160/52: len(feature_fnames)\n",
      "161/1:\n",
      "import os\n",
      "import cv2\n",
      "from time import time\n",
      "import numpy as np\n",
      "import matplotlib.pyplot as plt\n",
      "from glob import glob\n",
      "import random\n",
      "from dask import delayed\n",
      "161/2:\n",
      "from sklearn.ensemble import RandomForestClassifier, AdaBoostClassifier\n",
      "from sklearn.model_selection import train_test_split\n",
      "from sklearn.metrics import roc_auc_score\n",
      "from sklearn.linear_model import LogisticRegression\n",
      "161/3:\n",
      "from skimage.transform import integral_image\n",
      "from skimage.feature import haar_like_feature\n",
      "from skimage.feature import haar_like_feature_coord\n",
      "from skimage.feature import draw_haar_like_feature\n",
      "161/4: from logistic import logistic_fit, logistic, logistic_prob\n",
      "161/5: import dill\n",
      "161/6:\n",
      "def proportion_b(facenames):\n",
      "    count = 0\n",
      "    for i in range(n):\n",
      "        if int(facenames[i].split('_')[3]) == 0:\n",
      "            count += 1\n",
      "    return count / n\n",
      "161/7:\n",
      "@delayed\n",
      "def extract_feature_image(img, feature_type, feature_coord=None):\n",
      "    \"\"\"Extract the haar feature for the current image\"\"\"\n",
      "    img = cv2.resize(img, (36,36))\n",
      "    ii = integral_image(img)\n",
      "    return haar_like_feature(ii, 0, 0, ii.shape[0], ii.shape[1],\n",
      "                             feature_type=feature_type,\n",
      "                             feature_coord=feature_coord)\n",
      "161/8:\n",
      "def getimgs(facefiles, nonfaces, n):\n",
      "    faces = []\n",
      "    for i in range(n):\n",
      "        faces.append(cv2.imread(facefiles[i], cv2.IMREAD_GRAYSCALE))\n",
      "    return faces + nonfaces[:n]\n",
      "161/9:\n",
      "def getx(facefiles, n):\n",
      "    ims = getimgs(facefiles, n)\n",
      "    X = delayed(extract_feature_image(img, feature_types) for img in ims)\n",
      "    return np.array(X.compute(scheduler='threads'))\n",
      "161/10:\n",
      "def getx2(facefiles, n, feature_types, feature_coords):\n",
      "    ims = getimgs(facefiles, nonfaces, n)\n",
      "    X = delayed(extract_feature_image(img, feature_types, feature_coords) for img in ims)\n",
      "    return np.array(X.compute(scheduler='threads'))\n",
      "161/11:\n",
      "test_face_names = sorted(glob(os.path.join('data/tests', '*.jpg')))\n",
      "n_test = len(test_face_names)\n",
      "161/12:\n",
      "b_inds = []\n",
      "w_inds = []\n",
      "for i in range(len(test_face_names)):\n",
      "    if int(test_face_names[i].split('_')[2]) is 0: # race is white\n",
      "        w_inds.append(i)\n",
      "    else:\n",
      "        b_inds.append(i)\n",
      "161/13:\n",
      "test_nonface_names = sorted(glob(os.path.join('data/testing_nonfaces', '*.jpg')))\n",
      "n_nonfaces_test = len(test_nonface_names)\n",
      "161/14:\n",
      "faces_t = []\n",
      "for i in range(n_test):\n",
      "    faces_t.append(cv2.imread(test_face_names[i], cv2.IMREAD_GRAYSCALE))\n",
      "161/15:\n",
      "nonfaces_t = []\n",
      "for i in range(n_test):\n",
      "    # Read a random nonface file\n",
      "    j = random.randint(0, n_nonfaces_test - 1)\n",
      "    nonface = cv2.imread(test_nonface_names[j], cv2.IMREAD_GRAYSCALE)\n",
      "    \n",
      "    wsize = random.randint(36, min(nonface.shape[0], nonface.shape[1]))\n",
      "    row = random.randint(0, nonface.shape[0]-wsize)\n",
      "    col = random.randint(0, nonface.shape[1]-wsize)\n",
      "    crop = nonface[row:row+wsize, col:col+wsize]\n",
      "\n",
      "    # Resize to be the right size\n",
      "    crop = cv2.resize(crop, (200, 200))\n",
      "    nonfaces_t.append(crop)\n",
      "161/16:\n",
      "images_t = faces_t + nonfaces_t\n",
      "len(images_t)\n",
      "161/17: feature_types = ['type-2-x', 'type-2-y', 'type-3-x', 'type-3-y']\n",
      "161/18: dill.load_session('01_06_env.db')\n",
      "161/19: len(images_t)\n",
      "161/20: feature_tyoes\n",
      "161/21: feature_types\n",
      "161/22: Xf_t.shape\n",
      "161/23:\n",
      "# Train a random forest classifier and assess its performance\n",
      "clf = AdaBoostClassifier(n_estimators=1000, max_depth=None,\n",
      "                            n_jobs=-1, random_state=0)\n",
      "t_start = time()\n",
      "clf.fit(Xf_t, yf_t)\n",
      "time() - t_start\n",
      "161/24:\n",
      "# Train a random forest classifier and assess its performance\n",
      "clf = AdaBoostClassifier(n_estimators=1000, n_jobs=-1, random_state=0)\n",
      "t_start = time()\n",
      "clf.fit(Xf_t, yf_t)\n",
      "time() - t_start\n",
      "161/25:\n",
      "# Train a random forest classifier and assess its performance\n",
      "clf = AdaBoostClassifier(n_estimators=1000, random_state=0)\n",
      "t_start = time()\n",
      "clf.fit(Xf_t, yf_t)\n",
      "time() - t_start\n",
      "159/115:\n",
      "def proportion_b(facenames, n):\n",
      "    count = 0\n",
      "    for i in range(n):\n",
      "        if int(facenames[i].split('_')[3]) == 0:\n",
      "            count += 1\n",
      "    return count / n\n",
      "159/116:\n",
      "for f in [face_filenames0, face_filenames25, face_filenames50, face_filenames75, face_filenames100]:\n",
      "    print(proportion_b(f, n))\n",
      "159/117:\n",
      "idx_sorted = np.argsort(clf.feature_importances_)[::-1]\n",
      "\n",
      "fig, axes = plt.subplots(4,4, figsize=(20,20))\n",
      "for idx, ax in enumerate(axes.ravel()):\n",
      "    image = cv2.resize(images_t[0], (36,36))\n",
      "    image = draw_haar_like_feature(image, 0, 0, 36, 36,\n",
      "                                   [feature_coord[idx_sorted[idx]]])\n",
      "    ax.imshow(image)\n",
      "    ax.set_xticks([])\n",
      "    ax.set_yticks([])\n",
      "\n",
      "fig.suptitle('The most important features')\n",
      "159/118:\n",
      "idx_sorted = np.argsort(clf.feature_importances_)[::-1]\n",
      "\n",
      "fig, axes = plt.subplots(4,4\n",
      "for idx, ax in enumerate(axes.ravel()):\n",
      "    image = cv2.resize(images_t[0], (36,36))\n",
      "    image = draw_haar_like_feature(image, 0, 0, 36, 36,\n",
      "                                   [feature_coord[idx_sorted[idx]]])\n",
      "    ax.imshow(image)\n",
      "    ax.set_xticks([])\n",
      "    ax.set_yticks([])\n",
      "\n",
      "fig.suptitle('The most important features')\n",
      "159/119:\n",
      "idx_sorted = np.argsort(clf.feature_importances_)[::-1]\n",
      "\n",
      "fig, axes = plt.subplots(4,4)\n",
      "for idx, ax in enumerate(axes.ravel()):\n",
      "    image = cv2.resize(images_t[0], (36,36))\n",
      "    image = draw_haar_like_feature(image, 0, 0, 36, 36,\n",
      "                                   [feature_coord[idx_sorted[idx]]])\n",
      "    ax.imshow(image)\n",
      "    ax.set_xticks([])\n",
      "    ax.set_yticks([])\n",
      "\n",
      "fig.suptitle('The most important features')\n",
      "159/120: feature_coord.shape\n",
      "159/121: feature_coord[0]\n",
      "159/122: feature_coord\n",
      "159/123: Xf_t.shape\n",
      "159/124: clf\n",
      "159/125: yf_t.shape\n",
      "159/126:\n",
      "for i in range(1500):\n",
      "    if len(feature_coord[idx_sorted[i]]) > 2:\n",
      "        print(i)\n",
      "159/127:\n",
      "# Train a random forest classifier and assess its performance\n",
      "clf = RandomForestClassifier(n_estimators=1000, max_depth=None,\n",
      "                             n_jobs=-1, random_state=0)\n",
      "t_start = time()\n",
      "clf.fit(Xf_t, yf_t)\n",
      "time() - t_start\n",
      "159/128:\n",
      "for i in range(1500):\n",
      "    if len(feature_coord[idx_sorted[i]]) > 2:\n",
      "        print(i)\n",
      "159/129: idx_sorted = np.argsort(clf.feature_importances_)[::-1]\n",
      "159/130:\n",
      "for i in range(1500):\n",
      "    if len(feature_coord[idx_sorted[i]]) > 2:\n",
      "        print(i)\n",
      "159/131:\n",
      "fig, axes = plt.subplots(4,4)\n",
      "for idx, ax in enumerate(axes.ravel()):\n",
      "    image = cv2.resize(images_t[0], (36,36))\n",
      "    image = draw_haar_like_feature(image, 0, 0, 36, 36,\n",
      "                                   [feature_coord[idx_sorted[idx]]])\n",
      "    ax.imshow(image)\n",
      "    ax.set_xticks([])\n",
      "    ax.set_yticks([])\n",
      "\n",
      "fig.suptitle('The most important features')\n",
      "159/132:\n",
      "fig, axes = plt.subplots(4,4, figsize=(20,20))\n",
      "for idx, ax in enumerate(axes.ravel()):\n",
      "    image = cv2.resize(images_t[0], (36,36))\n",
      "    image = draw_haar_like_feature(image, 0, 0, 36, 36,\n",
      "                                   [feature_coord[idx_sorted[idx]]])\n",
      "    ax.imshow(image)\n",
      "    ax.set_xticks([])\n",
      "    ax.set_yticks([])\n",
      "\n",
      "fig.suptitle('The most important features')\n",
      "159/133:\n",
      "fig, axes = plt.subplots(4,1, figsize=(20,20))\n",
      "idxs = [1250,1325,1341,1403,1448]\n",
      "for idx, ax in enumerate(axes.ravel()):\n",
      "    image = cv2.resize(images_t[0], (36,36))\n",
      "    image = draw_haar_like_feature(image, 0, 0, 36, 36,\n",
      "                                   [feature_coord[idx_sorted[idxs[idx]]])\n",
      "    ax.imshow(image)\n",
      "    ax.set_xticks([])\n",
      "    ax.set_yticks([])\n",
      "\n",
      "fig.suptitle('The most important features')\n",
      "159/134:\n",
      "fig, axes = plt.subplots(4,1, figsize=(20,20))\n",
      "idxs = [1250,1325,1341,1403,1448]\n",
      "for idx, ax in enumerate(axes.ravel()):\n",
      "    image = cv2.resize(images_t[0], (36,36))\n",
      "    image = draw_haar_like_feature(image, 0, 0, 36, 36,\n",
      "                                   [feature_coord[idx_sorted[idxs[idx]]]})\n",
      "    ax.imshow(image)\n",
      "    ax.set_xticks([])\n",
      "    ax.set_yticks([])\n",
      "\n",
      "fig.suptitle('The most important features')\n",
      "159/135:\n",
      "fig, axes = plt.subplots(4,1, figsize=(20,20))\n",
      "idxs = [1250,1325,1341,1403,1448]\n",
      "for idx, ax in enumerate(axes.ravel()):\n",
      "    image = cv2.resize(images_t[0], (36,36))\n",
      "    image = draw_haar_like_feature(image, 0, 0, 36, 36,\n",
      "                                   [feature_coord[idx_sorted[idxs[idx]]]])\n",
      "    ax.imshow(image)\n",
      "    ax.set_xticks([])\n",
      "    ax.set_yticks([])\n",
      "\n",
      "fig.suptitle('The most important features')\n",
      "159/136:\n",
      "fig, axes = plt.subplots(1,4, figsize=(5,5))\n",
      "idxs = [1250,1325,1341,1403,1448]\n",
      "for idx, ax in enumerate(axes.ravel()):\n",
      "    image = cv2.resize(images_t[0], (36,36))\n",
      "    image = draw_haar_like_feature(image, 0, 0, 36, 36,\n",
      "                                   [feature_coord[idx_sorted[idxs[idx]]]])\n",
      "    ax.imshow(image)\n",
      "    ax.set_xticks([])\n",
      "    ax.set_yticks([])\n",
      "\n",
      "fig.suptitle('The most important features')\n",
      "159/137:\n",
      "fig, axes = plt.subplots(1,4)\n",
      "idxs = [1250,1325,1341,1403,1448]\n",
      "for idx, ax in enumerate(axes.ravel()):\n",
      "    image = cv2.resize(images_t[0], (36,36))\n",
      "    image = draw_haar_like_feature(image, 0, 0, 36, 36,\n",
      "                                   [feature_coord[idx_sorted[idxs[idx]]]])\n",
      "    ax.imshow(image)\n",
      "    ax.set_xticks([])\n",
      "    ax.set_yticks([])\n",
      "\n",
      "fig.suptitle('The most important features')\n",
      "159/138:\n",
      "fig, axes = plt.subplots(1,4, figsize=(10,10))\n",
      "idxs = [1250,1325,1341,1403,1448]\n",
      "for idx, ax in enumerate(axes.ravel()):\n",
      "    image = cv2.resize(images_t[0], (36,36))\n",
      "    image = draw_haar_like_feature(image, 0, 0, 36, 36,\n",
      "                                   [feature_coord[idx_sorted[idxs[idx]]]])\n",
      "    ax.imshow(image)\n",
      "    ax.set_xticks([])\n",
      "    ax.set_yticks([])\n",
      "\n",
      "fig.suptitle('The most important features')\n",
      "159/139:\n",
      "fig, axes = plt.subplots(1,4, figsize=(5,10))\n",
      "idxs = [1250,1325,1341,1403,1448]\n",
      "for idx, ax in enumerate(axes.ravel()):\n",
      "    image = cv2.resize(images_t[0], (36,36))\n",
      "    image = draw_haar_like_feature(image, 0, 0, 36, 36,\n",
      "                                   [feature_coord[idx_sorted[idxs[idx]]]])\n",
      "    ax.imshow(image)\n",
      "    ax.set_xticks([])\n",
      "    ax.set_yticks([])\n",
      "\n",
      "fig.suptitle('The most important features')\n",
      "159/140:\n",
      "fig, axes = plt.subplots(1,4, figsize=(10,5))\n",
      "idxs = [1250,1325,1341,1403,1448]\n",
      "for idx, ax in enumerate(axes.ravel()):\n",
      "    image = cv2.resize(images_t[0], (36,36))\n",
      "    image = draw_haar_like_feature(image, 0, 0, 36, 36,\n",
      "                                   [feature_coord[idx_sorted[idxs[idx]]]])\n",
      "    ax.imshow(image)\n",
      "    ax.set_xticks([])\n",
      "    ax.set_yticks([])\n",
      "\n",
      "fig.suptitle('The most important features')\n",
      "159/141:\n",
      "fig, axes = plt.subplots(1,4, figsize=(10,4))\n",
      "idxs = [1250,1325,1341,1403,1448]\n",
      "for idx, ax in enumerate(axes.ravel()):\n",
      "    image = cv2.resize(images_t[0], (36,36))\n",
      "    image = draw_haar_like_feature(image, 0, 0, 36, 36,\n",
      "                                   [feature_coord[idx_sorted[idxs[idx]]]])\n",
      "    ax.imshow(image)\n",
      "    ax.set_xticks([])\n",
      "    ax.set_yticks([])\n",
      "\n",
      "fig.suptitle('The most important features')\n",
      "159/142:\n",
      "fig, axes = plt.subplots(1,4, figsize=(12,4))\n",
      "idxs = [1250,1325,1341,1403,1448]\n",
      "for idx, ax in enumerate(axes.ravel()):\n",
      "    image = cv2.resize(images_t[0], (36,36))\n",
      "    image = draw_haar_like_feature(image, 0, 0, 36, 36,\n",
      "                                   [feature_coord[idx_sorted[idxs[idx]]]])\n",
      "    ax.imshow(image)\n",
      "    ax.set_xticks([])\n",
      "    ax.set_yticks([])\n",
      "\n",
      "fig.suptitle('The most important features')\n",
      "159/143:\n",
      "cdf_feature_importances = np.cumsum(clf.feature_importances_[idx_sorted])\n",
      "cdf_feature_importances /= np.max(cdf_feature_importances)\n",
      "sig_feature_count = np.count_nonzero(cdf_feature_importances < 0.7)\n",
      "sig_feature_percent = round(sig_feature_count /\n",
      "                            len(cdf_feature_importances) * 100, 1)\n",
      "print(('{} features, or {}%, account for 70% of branch points in the '\n",
      "       'random forest.').format(sig_feature_count, sig_feature_percent))\n",
      "159/144: len(feature_coord_sel)\n",
      "159/145: len(feature_coord_sel2)\n",
      "159/146:\n",
      "# Train a random forest classifier and assess its performance\n",
      "rf0 = RandomForestClassifier(n_estimators=1000, max_depth=None,\n",
      "                             n_jobs=-1, random_state=0)\n",
      "t_start = time()\n",
      "rf0.fit(X0, y)\n",
      "time() - t_start\n",
      "159/147: rf0.score(Xt_900, yt_900)\n",
      "159/148: X0_1800 = getx2(face_filenames0, n, feature_type_sel, feature_coord_sel)\n",
      "159/149:\n",
      "t_start = time()\n",
      "params_1800 = logistic_fit(X0_1800, y, 0.001)\n",
      "time() - t_start\n",
      "159/150:\n",
      "def getx2(facefiles, n, nonfaces, feature_types, feature_coords):\n",
      "    ims = getimgs(facefiles, nonfaces, n)\n",
      "    X = delayed(extract_feature_image(img, feature_types, feature_coords) for img in ims)\n",
      "    return np.array(X.compute(scheduler='threads'))\n",
      "159/151: Xt_900 = getx2(test_face_names, 300, nonfaces_t, feature_type_sel2, feature_coord_sel2)\n",
      "159/152: Xt_1800 = getx2(face_filenames0, n, nonfaces_t, feature_type_sel, feature_coord_sel)\n",
      "159/153: predicted_1800 = logistic_prob(Xt_1800, params_1800)\n",
      "159/154:\n",
      "def getacc(predicted, actual):\n",
      "    preds = []\n",
      "    for p in predicted:\n",
      "        if p >= 0.5:\n",
      "            preds.append(1)\n",
      "        else:\n",
      "            preds.append(0)\n",
      "    return 1 - sum(abs(preds-actual))/len(preds)\n",
      "159/155: predicted = logistic_prob(Xt_900, params)\n",
      "159/156: getacc(predicted, yt_900)\n",
      "159/157: getacc(predicted_1800, yt_900)\n",
      "159/158: Xt_1800.shape\n",
      "159/159: Xt_1800.shape, n\n",
      "159/160: Xt_1800.shape, n, Xt_900.shape, yt_900\n",
      "159/161: Xt_1800.shape, n, Xt_900.shape, yt_900.shape\n",
      "159/162: Xt_1800.shape, n, Xt_900.shape, yt_900.shape, X0.shape\n",
      "159/163: Xt_1800 = getx2(face_filenames0, 300, nonfaces_t, feature_type_sel, feature_coord_sel)\n",
      "159/164: Xt_1800.shape, n, Xt_900.shape, yt_900.shape, X0.shape\n",
      "159/165: getacc(predicted, yt_900)\n",
      "159/166: getacc(predicted_1800, yt_900)\n",
      "159/167: predicted_1800 = logistic_prob(Xt_1800, params_1800)\n",
      "159/168: getacc(predicted_1800, yt_900)\n",
      "159/169: Xt_1800.shape, n, Xt_900.shape, yt_900.shape, X0.shape, len(y)\n",
      "159/170: Xt_1800.shape, n, Xt_900.shape, yt_900.shape, X0.shape, len(y), len(y_t)\n",
      "159/171: getacc(predicted_1800, y_t)\n",
      "159/172: getacc(predicted, y_t)\n",
      "159/173: X25_1800 = getx2(face_filenames25, n, nonfaces, feature_type_sel, feature_coord_sel)\n",
      "159/174: X50_1800 = getx2(face_filenames50, n, nonfaces, feature_type_sel, feature_coord_sel)\n",
      "159/175: X75_1800 = getx2(face_filenames75, n, nonfaces, feature_type_sel, feature_coord_sel)\n",
      "159/176: X100_1800 = getx2(face_filenames100, n, nonfaces, feature_type_sel, feature_coord_sel)\n",
      "159/177:\n",
      "X0 = getx2(face_filenames0, n, nonfaces, feature_type_sel2, feature_coord_sel2)\n",
      "X25 = getx2(face_filenames25, n, nonfaces, feature_type_sel2, feature_coord_sel2)\n",
      "X50 = getx2(face_filenames50, n, nonfaces, feature_type_sel2, feature_coord_sel2)\n",
      "X75 = getx2(face_filenames75, n, nonfaces, feature_type_sel2, feature_coord_sel2)\n",
      "X100 = getx2(face_filenames100, n, nonfaces, feature_type_sel2, feature_coord_sel2)\n",
      "Xt = getx2(face_filenames0, 300, nonfaces_t, feature_type_sel2, feature_coord_sel2)\n",
      "159/178:\n",
      "t_start = time()\n",
      "params_1800 = logistic_fit(X0_1800, y, 0.001)\n",
      "predicted_1800 = logistic_prob(Xt_1800, params_1800)\n",
      "print(time() - t_start)\n",
      "print(getacc(predicted_1800, y_t))\n",
      "159/179:\n",
      "t_start = time()\n",
      "params = logistic_fit(X0, y, 0.001)\n",
      "predicted = logistic_prob(Xt_900, params)\n",
      "print(time() - t_start)\n",
      "print(getacc(predicted, y_t))\n",
      "159/180:\n",
      "t_start = time()\n",
      "params25 = logistic_fit(X25, y, 0.001)\n",
      "predicted25 = logistic_prob(Xt_900, params)\n",
      "print(time() - t_start)\n",
      "print(getacc(predicted25, y_t))\n",
      "159/181:\n",
      "t_start = time()\n",
      "params25_1800 = logistic_fit(X25_1800, y, 0.001)\n",
      "predicted25_1800 = logistic_prob(Xt_1800, params_1800)\n",
      "print(time() - t_start)\n",
      "print(getacc(predicted25_1800, y_t))\n",
      "159/182:\n",
      "t_start = time()\n",
      "params25 = logistic_fit(X25, y, 0.001)\n",
      "predicted25 = logistic_prob(Xt_900, params25)\n",
      "print(time() - t_start)\n",
      "print(getacc(predicted25, y_t))\n",
      "159/183:\n",
      "t_start = time()\n",
      "params25_1800 = logistic_fit(X25_1800, y, 0.001)\n",
      "predicted25_1800 = logistic_prob(Xt_1800, params25_1800)\n",
      "print(time() - t_start)\n",
      "print(getacc(predicted25_1800, y_t))\n",
      "159/184:\n",
      "t_start = time()\n",
      "params50 = logistic_fit(X25, y, 0.001)\n",
      "predicted50 = logistic_prob(Xt_900, params50)\n",
      "print(time() - t_start)\n",
      "print(getacc(predicted50, y_t))\n",
      "159/185:\n",
      "t_start = time()\n",
      "params50 = logistic_fit(X50, y, 0.001)\n",
      "predicted50 = logistic_prob(Xt_900, params50)\n",
      "print(time() - t_start)\n",
      "print(getacc(predicted50, y_t))\n",
      "159/186:\n",
      "t_start = time()\n",
      "params50_1800 = logistic_fit(X50_1800, y, 0.001)\n",
      "predicted50_1800 = logistic_prob(Xt_1800, params50_1800)\n",
      "print(time() - t_start)\n",
      "print(getacc(predicted50_1800, y_t))\n",
      "159/187:\n",
      "t_start = time()\n",
      "params75 = logistic_fit(X75, y, 0.001)\n",
      "predicted75 = logistic_prob(Xt_900, params75)\n",
      "print(time() - t_start)\n",
      "print(getacc(predicted75, y_t))\n",
      "159/188:\n",
      "t_start = time()\n",
      "params75_1800 = logistic_fit(X75_1800, y, 0.001)\n",
      "predicted75_1800 = logistic_prob(Xt_1800, params75_1800)\n",
      "print(time() - t_start)\n",
      "print(getacc(predicted75_1800, y_t))\n",
      "159/189:\n",
      "t_start = time()\n",
      "params100 = logistic_fit(X100, y, 0.001)\n",
      "predicted100 = logistic_prob(Xt_900, params100)\n",
      "print(time() - t_start)\n",
      "print(getacc(predicted100, y_t))\n",
      "159/190:\n",
      "t_start = time()\n",
      "params100_1800 = logistic_fit(X100_1800, y, 0.001)\n",
      "predicted100_1800 = logistic_prob(Xt_1800, params100_1800)\n",
      "print(time() - t_start)\n",
      "print(getacc(predicted100_1800, y_t))\n",
      "159/191:\n",
      "t_start = time()\n",
      "rf0 = RandomForestClassifier(n_estimators=1000, max_depth=None, n_jobs=-1, random_state=0)\n",
      "rf0.fit(X0, y)\n",
      "print(time() - t_start)\n",
      "print(rf0.score(Xt_900, y_t))\n",
      "159/192:\n",
      "t_start = time()\n",
      "rf0_1800 = RandomForestClassifier(n_estimators=1000, max_depth=None, n_jobs=-1, random_state=0)\n",
      "rf0_1800.fit(X0_1800, y)\n",
      "print(time() - t_start)\n",
      "print(rf0_1800.score(Xt_1800, y_t))\n",
      "159/193:\n",
      "t_start = time()\n",
      "rf25 = RandomForestClassifier(n_estimators=1000, max_depth=None, n_jobs=-1, random_state=0)\n",
      "rf25.fit(X25, y)\n",
      "print(time() - t_start)\n",
      "print(rf25.score(Xt_900, y_t))\n",
      "159/194:\n",
      "t_start = time()\n",
      "rf25_1800 = RandomForestClassifier(n_estimators=1000, max_depth=None, n_jobs=-1, random_state=0)\n",
      "rf25_1800.fit(X25_1800, y)\n",
      "print(time() - t_start)\n",
      "print(rf25_1800.score(Xt_1800, y_t))\n",
      "159/195:\n",
      "t_start = time()\n",
      "rf50 = RandomForestClassifier(n_estimators=1000, max_depth=None, n_jobs=-1, random_state=0)\n",
      "rf50.fit(X50, y)\n",
      "print(time() - t_start)\n",
      "print(rf50.score(Xt_900, y_t))\n",
      "159/196:\n",
      "t_start = time()\n",
      "rf50_1800 = RandomForestClassifier(n_estimators=1000, max_depth=None, n_jobs=-1, random_state=0)\n",
      "rf50_1800.fit(X50_1800, y)\n",
      "print(time() - t_start)\n",
      "print(rf50_1800.score(Xt_1800, y_t))\n",
      "159/197:\n",
      "t_start = time()\n",
      "rf75 = RandomForestClassifier(n_estimators=1000, max_depth=None, n_jobs=-1, random_state=0)\n",
      "rf75.fit(X75, y)\n",
      "print(time() - t_start)\n",
      "print(rf75.score(Xt_900, y_t))\n",
      "159/198:\n",
      "t_start = time()\n",
      "rf75_1800 = RandomForestClassifier(n_estimators=1000, max_depth=None, n_jobs=-1, random_state=0)\n",
      "rf75_1800.fit(X75_1800, y)\n",
      "print(time() - t_start)\n",
      "print(rf75_1800.score(Xt_1800, y_t))\n",
      "159/199:\n",
      "t_start = time()\n",
      "rf100 = RandomForestClassifier(n_estimators=1000, max_depth=None, n_jobs=-1, random_state=0)\n",
      "rf100.fit(X100, y)\n",
      "print(time() - t_start)\n",
      "print(rf100.score(Xt_900, y_t))\n",
      "159/200:\n",
      "t_start = time()\n",
      "rf100_1800 = RandomForestClassifier(n_estimators=1000, max_depth=None, n_jobs=-1, random_state=0)\n",
      "rf100_1800.fit(X100_1800, y)\n",
      "print(time() - t_start)\n",
      "print(rf100_1800.score(Xt_1800, y_t))\n",
      "159/201:\n",
      "# use b_inds and w_inds to select rows of X_t and y_t\n",
      "Xb_t = Xt_900[b_inds]\n",
      "Xb_t_1800 = Xt_1800[b_inds]\n",
      "yb_t = y_t[b_inds]\n",
      "Xw_t = Xt_900[w_inds]\n",
      "Xw_t_1800 = Xt_1800[w_inds]\n",
      "yw_t = y_t[w_inds]\n",
      "Xb_t.shape, Xb_t_1800.shape, yb_t.shape, Xw_t.shape, Xw_t_1800.shape, yw_t.shape\n",
      "159/202:\n",
      "def rfscores(rfs, Xt, Xbt, Xwt):\n",
      "    for i in range(len(rfs)):\n",
      "        print(i*25)\n",
      "        print(\"overall:\", rfs[i].score(Xt, y_t))\n",
      "        print(\"black:\", rfs[i].score(Xbt, yb_t))\n",
      "        print(\"white:\", rfs[i].score(Xwt, yw_t))\n",
      "159/203: rfscores([rf0, rf25, rf50, rf75, rf100], Xt_900, Xb_t, Xw_t)\n",
      "159/204: rfscores([rf0_1800, rf25_1800, rf50_1800, rf75_1800, rf100_1800], Xt_1800, Xb_t_1800, Xw_t_1800)\n",
      "159/205:\n",
      "def getscore(X, y, params):\n",
      "    predicted = logistic_prob(X, params)\n",
      "    return getacc(predicted, y)\n",
      "159/206:\n",
      "def logscores(logs, Xt, Xbt, Xwt):\n",
      "    for i in range(len(logs)):\n",
      "        print(i*25)\n",
      "        print(\"overall:\", getscore(Xt, y_t, logs[i]))\n",
      "        print(\"black:\", getscore(Xbt, yb_t, logs[i]))\n",
      "        print(\"white:\", getscore(Xwt, yw_t, logs[i]))\n",
      "159/207: logscores([params, params25, params50, params75, params100], Xt_900, Xb_t, Xw_t)\n",
      "159/208: logscores([params_1800, params25_1800, params50_1800, params75_1800, params100_1800], Xt_1800, Xb_t_1800, Xw_t_1800)\n",
      "159/209:\n",
      "def rfscores(rfs, Xt, Xbt, Xwt):\n",
      "    os = []\n",
      "    bs = []\n",
      "    ws = []\n",
      "    for i in range(len(rfs)):\n",
      "        print(i*25)\n",
      "        o = rfs[i].score(Xt, y_t)\n",
      "        b = rfs[i].score(Xbt, yb_t)\n",
      "        w = rfs[i].score(Xwt, yw_t)\n",
      "        print(\"overall:\", o)\n",
      "        print(\"black:\", b)\n",
      "        print(\"white:\", w)\n",
      "        os.append(o)\n",
      "        bs.append(b)\n",
      "        ws.append(w)\n",
      "    return os, bs, ws\n",
      "159/210:\n",
      "def logscores(logs, Xt, Xbt, Xwt):\n",
      "    os = []\n",
      "    bs = []\n",
      "    ws = []\n",
      "    for i in range(len(logs)):\n",
      "        print(i*25)\n",
      "        o = getscore(Xt, y_t, logs[i])\n",
      "        b = getscore(Xbt, yb_t, logs[i])\n",
      "        w = getscore(Xwt, yw_t, logs[i])\n",
      "        print(\"overall:\", o)\n",
      "        print(\"black:\", b)\n",
      "        print(\"white:\", w)\n",
      "        os.append(o)\n",
      "        bs.append(b)\n",
      "        ws.append(w)\n",
      "    return os, bs, ws\n",
      "159/211: os_l, bs_l, ws_l = logscores([params, params25, params50, params75, params100], Xt_900, Xb_t, Xw_t)\n",
      "159/212: os_l2, bs_l2, ws_l2 = logscores([params_1800, params25_1800, params50_1800, params75_1800, params100_1800], Xt_1800, Xb_t_1800, Xw_t_1800)\n",
      "159/213: os_r, bs_r, ws_r = rfscores([rf0, rf25, rf50, rf75, rf100], Xt_900, Xb_t, Xw_t)\n",
      "159/214: os_r2, bs_r2, ws_r2 = rfscores([rf0_1800, rf25_1800, rf50_1800, rf75_1800, rf100_1800], Xt_1800, Xb_t_1800, Xw_t_1800)\n",
      "159/215: plt.plot([0, 25, 50, 75, 100], os_r2)\n",
      "159/216:\n",
      "plt.plot([0, 25, 50, 75, 100], os_r2)\n",
      "plt.plot([0, 25, 50, 75, 100], bs_r2)\n",
      "plt.plot([0, 25, 50, 75, 100], ws_r2)\n",
      "159/217:\n",
      "plt.plot([0, 25, 50, 75, 100], os_r2, label=\"o\")\n",
      "plt.plot([0, 25, 50, 75, 100], bs_r2, label=\"b\")\n",
      "plt.plot([0, 25, 50, 75, 100], ws_r2, label=\"w\")\n",
      "plt.legend()\n",
      "159/218:\n",
      "plt.plot([0, 25, 50, 75, 100], os_r, label=\"o\")\n",
      "plt.plot([0, 25, 50, 75, 100], bs_r, label=\"b\")\n",
      "plt.plot([0, 25, 50, 75, 100], ws_r, label=\"w\")\n",
      "plt.legend()\n",
      "159/219:\n",
      "plt.plot([0, 25, 50, 75, 100], os_l2, label=\"o\")\n",
      "plt.plot([0, 25, 50, 75, 100], bs_l2, label=\"b\")\n",
      "plt.plot([0, 25, 50, 75, 100], ws_l2, label=\"w\")\n",
      "plt.legend()\n",
      "159/220:\n",
      "plt.plot([0, 25, 50, 75, 100], os_l, label=\"o\")\n",
      "plt.plot([0, 25, 50, 75, 100], bs_l, label=\"b\")\n",
      "plt.plot([0, 25, 50, 75, 100], ws_l, label=\"w\")\n",
      "plt.legend()\n",
      "159/221:\n",
      "def gettop(fnames, rf):\n",
      "    ims = getimgs(fnames)\n",
      "    fig, axes = plt.subplots(4, 5, figsize=(20,20))\n",
      "    ims = np.array([cv2.resize(im, (36,36)) for im in ims])\n",
      "    # Sort features in order of importance and plot the six most significant\n",
      "    idx_sorted = np.argsort(rf.feature_importances_)[::-1]\n",
      "    for idx, ax in enumerate(axes.ravel()):\n",
      "        image = ims[5]\n",
      "        image = draw_haar_like_feature(image, 0, 0,\n",
      "                                       ims.shape[2],\n",
      "                                       ims.shape[1],\n",
      "                                       [feature_coord[idx_sorted[idx]]])\n",
      "        ax.imshow(image)\n",
      "        ax.set_xticks([])\n",
      "        ax.set_yticks([])\n",
      "\n",
      "    fig.suptitle('The most important features')\n",
      "159/222: gettop(face_filenames0, rf0)\n",
      "159/223: len(feature_coord_sel)\n",
      "159/224: len(feature_coord_sel2)\n",
      "159/225: len(feature_coord_sel2), len(feature_type_sel2)\n",
      "159/226: print(feature_coord_sel900)\n",
      "159/227: print(feature_coord_sel2)\n",
      "159/228: Xt\n",
      "159/229: Xt.shape\n",
      "159/230: Xt\n",
      "159/231: import csv\n",
      "159/232: import json\n",
      "159/233: json.dumps(Xt)\n",
      "159/234: np.savetxt(\"X_haar.csv\", Xt, delimiter=\",\")\n",
      "159/235: b_inds\n",
      "159/236: np.savetxt(\"b_inds.csv\", b_inds, delimiter=\",\")\n",
      "159/237: np.savetxt(\"w_inds.csv\", w_inds, delimiter=\",\")\n",
      "159/238: b_inds\n",
      "159/239: w_inds\n",
      "159/240: np.savetxt(\"b_inds.csv\", np.array(b_inds), delimiter=\",\")\n",
      "159/241: Xb_t\n",
      "159/242: Xb_t.shape\n",
      "159/243:\n",
      "def gettop(fnames, rf):\n",
      "    im = cv2.resize(cv2.imread(fnames[i], cv2.IMREAD_GRAYSCALE),(36,36))\n",
      "    fig, axes = plt.subplots(4, 5, figsize=(20,20))\n",
      "    # Sort features in order of importance and plot the six most significant\n",
      "    idx_sorted = np.argsort(rf.feature_importances_)[::-1]\n",
      "    for idx, ax in enumerate(axes.ravel()):\n",
      "        image = draw_haar_like_feature(im, 0, 0, 36, 36,\n",
      "                                       [feature_coord[idx_sorted[idx]]])\n",
      "        ax.imshow(image)\n",
      "        ax.set_xticks([])\n",
      "        ax.set_yticks([])\n",
      "\n",
      "    fig.suptitle('The most important features')\n",
      "159/244: gettop(face_filenames0, rf0)\n",
      "159/245:\n",
      "def gettop(fnames, rf):\n",
      "    im = cv2.resize(cv2.imread(fnames[i], cv2.IMREAD_GRAYSCALE),(36,36))\n",
      "    fig, axes = plt.subplots(4, 4, figsize=(20,20))\n",
      "    # Sort features in order of importance and plot the six most significant\n",
      "    idx_sorted = np.argsort(rf.feature_importances_)[::-1]\n",
      "    for idx, ax in enumerate(axes.ravel()):\n",
      "        image = draw_haar_like_feature(im, 0, 0, 36, 36,\n",
      "                                       [feature_coord[idx_sorted[idx]]])\n",
      "        ax.imshow(image)\n",
      "        ax.set_xticks([])\n",
      "        ax.set_yticks([])\n",
      "\n",
      "    fig.suptitle('The most important features')\n",
      "159/246:\n",
      "def gettop(fnames, rf):\n",
      "    im = cv2.resize(cv2.imread(fnames[i], cv2.IMREAD_GRAYSCALE),(36,36))\n",
      "    fig, axes = plt.subplots(4, 4, figsize=(20,20))\n",
      "    # Sort features in order of importance and plot the six most significant\n",
      "    idx_sorted = np.argsort(rf.feature_importances_)[::-1]\n",
      "    for idx, ax in enumerate(axes.ravel()):\n",
      "        image = draw_haar_like_feature(im, 0, 0, 36, 36,\n",
      "                                       [feature_coord[idx_sorted[idx]]])\n",
      "        ax.imshow(image)\n",
      "        ax.set_xticks([])\n",
      "        ax.set_yticks([])\n",
      "\n",
      "    fig.suptitle('The most important features')\n",
      "159/247: gettop(face_filenames0, rf0)\n",
      "159/248: feature_coord\n",
      "159/249:\n",
      "def gettop(fnames, rf):\n",
      "    im = cv2.resize(cv2.imread(fnames[i], cv2.IMREAD_GRAYSCALE),(36,36))\n",
      "    fig, axes = plt.subplots(4, 4, figsize=(20,20))\n",
      "    # Sort features in order of importance and plot the six most significant\n",
      "    idx_sorted = np.argsort(rf0.feature_importances_)[::-1]\n",
      "    for idx, ax in enumerate(axes.ravel()):\n",
      "        image = draw_haar_like_feature(im, 0, 0, 36, 36,\n",
      "                                       [feature_coord[idx_sorted[idx]]])\n",
      "        ax.imshow(image)\n",
      "        ax.set_xticks([])\n",
      "        ax.set_yticks([])\n",
      "\n",
      "    fig.suptitle('The most important features')\n",
      "159/250: gettop(face_filenames0, rf0)\n",
      "159/251:\n",
      "def gettop(fnames, rf):\n",
      "    im = cv2.resize(cv2.imread(fnames[i], cv2.IMREAD_GRAYSCALE),(36,36))\n",
      "    fig, axes = plt.subplots(4, 4, figsize=(20,20))\n",
      "    # Sort features in order of importance and plot the six most significant\n",
      "    idx_sorted = np.argsort(rf25.feature_importances_)[::-1]\n",
      "    for idx, ax in enumerate(axes.ravel()):\n",
      "        image = draw_haar_like_feature(im, 0, 0, 36, 36,\n",
      "                                       [feature_coord[idx_sorted[idx]]])\n",
      "        ax.imshow(image)\n",
      "        ax.set_xticks([])\n",
      "        ax.set_yticks([])\n",
      "\n",
      "    fig.suptitle('The most important features')\n",
      "159/252: gettop(face_filenames0, rf0)\n",
      "159/253: len(feature_coord)\n",
      "159/254:\n",
      "def gettop(fnames, rf):\n",
      "    im = cv2.resize(cv2.imread(fnames[i], cv2.IMREAD_GRAYSCALE),(36,36))\n",
      "    fig, axes = plt.subplots(4, 4, figsize=(20,20))\n",
      "    # Sort features in order of importance and plot the six most significant\n",
      "    idx_sorted = np.argsort(rf25.feature_importances_)[::-1]\n",
      "    for idx, ax in enumerate(axes.ravel()):\n",
      "        image = draw_haar_like_feature(im, 0, 0, 36, 36,\n",
      "                                       [feature_coord_sel[idx_sorted[idx]]])\n",
      "        ax.imshow(image)\n",
      "        ax.set_xticks([])\n",
      "        ax.set_yticks([])\n",
      "\n",
      "    fig.suptitle('The most important features')\n",
      "159/255: gettop(face_filenames0, rf0)\n",
      "159/256: len(feature_coord_sel)\n",
      "159/257:\n",
      "# Extract all possible features\n",
      "feature_coord, feature_type = feature_coord_sel2, feature_type_sel2\n",
      "159/258:\n",
      "def gettop(fnames, rf):\n",
      "    im = cv2.resize(cv2.imread(fnames[i], cv2.IMREAD_GRAYSCALE),(36,36))\n",
      "    fig, axes = plt.subplots(4, 4, figsize=(20,20))\n",
      "    # Sort features in order of importance and plot the six most significant\n",
      "    idx_sorted = np.argsort(rf.feature_importances_)[::-1]\n",
      "    for idx, ax in enumerate(axes.ravel()):\n",
      "        image = draw_haar_like_feature(im, 0, 0, 36, 36,\n",
      "                                       [feature_coord_sel[idx_sorted[idx]]])\n",
      "        ax.imshow(image)\n",
      "        ax.set_xticks([])\n",
      "        ax.set_yticks([])\n",
      "\n",
      "    fig.suptitle('The most important features')\n",
      "159/259: gettop(face_filenames0, rf0)\n",
      "159/260: gettop(face_filenames25, rf25)\n",
      "159/261: gettop(face_filenames50, rf50)\n",
      "159/262: gettop(face_filenames75, rf75)\n",
      "159/263: gettop(face_filenames100, rf100)\n",
      "159/264: gettop(face_filenames100, rf100)\n",
      "159/265:\n",
      "def gettop(fnames, rf):\n",
      "    im = cv2.resize(cv2.imread(fnames[i], cv2.IMREAD_GRAYSCALE),(36,36))\n",
      "    fig, axes = plt.subplots(8,8, figsize=(20,20))\n",
      "    # Sort features in order of importance and plot the six most significant\n",
      "    idx_sorted = np.argsort(rf.feature_importances_)[::-1]\n",
      "    for idx, ax in enumerate(axes.ravel()):\n",
      "        image = draw_haar_like_feature(im, 0, 0, 36, 36,\n",
      "                                       [feature_coord_sel[idx_sorted[idx]]])\n",
      "        ax.imshow(image)\n",
      "        ax.set_xticks([])\n",
      "        ax.set_yticks([])\n",
      "\n",
      "    fig.suptitle('The most important features')\n",
      "159/266: gettop(face_filenames0, rf0)\n",
      "159/267:\n",
      "def gettop(fnames, rf):\n",
      "    im = cv2.resize(cv2.imread(fnames[i], cv2.IMREAD_GRAYSCALE),(36,36))\n",
      "    fig, axes = plt.subplots(8,8, figsize=(20,20))\n",
      "    # Sort features in order of importance and plot the six most significant\n",
      "    idx_sorted = np.argsort(rf.feature_importances_)[::-1]\n",
      "    for idx, ax in enumerate(axes.ravel()):\n",
      "        image = draw_haar_like_feature(im, 0, 0, 36, 36,\n",
      "                                       [feature_coord_sel[idx_sorted[-idx]]])\n",
      "        ax.imshow(image)\n",
      "        ax.set_xticks([])\n",
      "        ax.set_yticks([])\n",
      "\n",
      "    fig.suptitle('The most important features')\n",
      "159/268: gettop(face_filenames0, rf0)\n",
      "159/269:\n",
      "def gettop(fnames, rf):\n",
      "    im = cv2.resize(cv2.imread(fnames[i], cv2.IMREAD_GRAYSCALE),(36,36))\n",
      "    fig, axes = plt.subplots(8,8, figsize=(20,20))\n",
      "    # Sort features in order of importance and plot the six most significant\n",
      "    idx_sorted = np.argsort(rf.feature_importances_)[::-1]\n",
      "    for idx, ax in enumerate(axes.ravel()):\n",
      "        image = draw_haar_like_feature(im, 0, 0, 36, 36,\n",
      "                                       [feature_coord_sel[idx_sorted[idx]]])\n",
      "        ax.imshow(image)\n",
      "        ax.set_xticks([])\n",
      "        ax.set_yticks([])\n",
      "\n",
      "    fig.suptitle('The most important features')\n",
      "159/270: feature_type\n",
      "159/271: gettop(face_filenames0, rf0)\n",
      "159/272:\n",
      "def gettop(fnames, rf):\n",
      "    im = cv2.resize(cv2.imread(fnames[i], cv2.IMREAD_GRAYSCALE),(36,36))\n",
      "    fig, axes = plt.subplots(8,8, figsize=(20,20))\n",
      "    # Sort features in order of importance and plot the six most significant\n",
      "    idx_sorted = np.argsort(rf.feature_importances_)[::-1]\n",
      "    for idx, ax in enumerate(axes.ravel()):\n",
      "        image = draw_haar_like_feature(im, 0, 0, 36, 36,\n",
      "                                       [feature_coord_sel[idx_sorted[idx]]])\n",
      "        ax.imshow(image)\n",
      "        ax.set_xticks([])\n",
      "        ax.set_yticks([])\n",
      "\n",
      "    fig.suptitle('The most important features')\n",
      "159/273: gettop(face_filenames0, rf0)\n",
      "159/274:\n",
      "def gettop(fnames, rf):\n",
      "    im = cv2.resize(cv2.imread(fnames[i], cv2.IMREAD_GRAYSCALE),(36,36))\n",
      "    fig, axes = plt.subplots(7,7, figsize=(20,20))\n",
      "    # Sort features in order of importance and plot the six most significant\n",
      "    idx_sorted = np.argsort(rf.feature_importances_)[::-1]\n",
      "    for idx, ax in enumerate(axes.ravel()):\n",
      "        image = draw_haar_like_feature(im, 0, 0, 36, 36,\n",
      "                                       [feature_coord_sel[idx_sorted[idx]]])\n",
      "        ax.imshow(image)\n",
      "        ax.set_xticks([])\n",
      "        ax.set_yticks([])\n",
      "\n",
      "    fig.suptitle('The most important features')\n",
      "159/275: gettop(face_filenames0, rf0)\n",
      "159/276: gettop(face_filenames25, rf25)\n",
      "159/277: gettop(face_filenames50, rf50)\n",
      "159/278: gettop(face_filenames75, rf75)\n",
      "159/279: gettop(face_filenames100, rf100)\n",
      "159/280: gettop(face_filenames0, rf0_1800)\n",
      "159/281: gettop(face_filenames0, rf0)\n",
      "159/282: gettop(face_filenames0, rf0)\n",
      "159/283: os_l, bs_l, ws_l = logscores([params, params25, params50, params75, params100], Xt_900, Xb_t, Xw_t)\n",
      "159/284: os_l, bs_l, ws_l = logscores([params, params25, params50, params75, params100], Xt_900, Xb_t, Xw_t)\n",
      "159/285: os_l, bs_l, ws_l = logscores([params, params25, params50, params75, params100], Xt_900, Xb_t, Xw_t)\n",
      "159/286: os_l, bs_l, ws_l = logscores([params, params25, params50, params75, params100], Xt_900, Xb_t, Xw_t)\n",
      "159/287:\n",
      "t_start = time()\n",
      "params = logistic_fit(X0, y, 0.001)\n",
      "predicted = logistic_prob(Xt_900, params)\n",
      "print(time() - t_start)\n",
      "print(getacc(predicted, y_t))\n",
      "159/288:\n",
      "t_start = time()\n",
      "params25 = logistic_fit(X25, y, 0.001)\n",
      "predicted25 = logistic_prob(Xt_900, params25)\n",
      "print(time() - t_start)\n",
      "print(getacc(predicted25, y_t))\n",
      "159/289:\n",
      "t_start = time()\n",
      "params50 = logistic_fit(X50, y, 0.001)\n",
      "predicted50 = logistic_prob(Xt_900, params50)\n",
      "print(time() - t_start)\n",
      "print(getacc(predicted50, y_t))\n",
      "159/290:\n",
      "t_start = time()\n",
      "params75 = logistic_fit(X75, y, 0.001)\n",
      "predicted75 = logistic_prob(Xt_900, params75)\n",
      "print(time() - t_start)\n",
      "print(getacc(predicted75, y_t))\n",
      "159/291:\n",
      "t_start = time()\n",
      "params100 = logistic_fit(X100, y, 0.001)\n",
      "predicted100 = logistic_prob(Xt_900, params100)\n",
      "print(time() - t_start)\n",
      "print(getacc(predicted100, y_t))\n",
      "159/292: os_l, bs_l, ws_l = logscores([params, params25, params50, params75, params100], Xt_900, Xb_t, Xw_t)\n",
      "159/293:\n",
      "plt.plot([0, 25, 50, 75, 100], os_l, label=\"o\")\n",
      "plt.plot([0, 25, 50, 75, 100], bs_l, label=\"b\")\n",
      "plt.plot([0, 25, 50, 75, 100], ws_l, label=\"w\")\n",
      "plt.legend()\n",
      "159/294: from logistic import logistic_fit, logistic, logistic_prob, temp\n",
      "159/295: from logistic import logistic_fit, logistic, logistic_prob\n",
      "159/296: from logistic import logistic_fit, logistic, logistic_prob\n",
      "159/297: logistic_fit(1)\n",
      "159/298: logistic(1)\n",
      "159/299:\n",
      "t_start = time()\n",
      "rf0_1800 = RandomForestClassifier(n_estimators=1000, max_depth=None, n_jobs=-1, random_state=0)\n",
      "rf0_1800.fit(X0_1800, y)\n",
      "print(time() - t_start)\n",
      "print(rf0_1800.score(Xt_1800, y_t))\n",
      "159/300: os_r2, bs_r2, ws_r2 = rfscores([rf0_1800, rf25_1800, rf50_1800, rf75_1800, rf100_1800], Xt_1800, Xb_t_1800, Xw_t_1800)\n",
      "159/301:\n",
      "t_start = time()\n",
      "rf100_1800 = RandomForestClassifier(n_estimators=1000, max_depth=None, n_jobs=-1, random_state=0)\n",
      "rf100_1800.fit(X100_1800, y)\n",
      "print(time() - t_start)\n",
      "print(rf100_1800.score(Xt_1800, y_t))\n",
      "159/302:\n",
      "plt.plot([0, 25, 50, 75, 100], os_r2, label=\"o\")\n",
      "plt.plot([0, 25, 50, 75, 100], bs_r2, label=\"b\")\n",
      "plt.plot([0, 25, 50, 75, 100], ws_r2, label=\"w\")\n",
      "plt.legend()\n",
      "159/303: os_r2, bs_r2, ws_r2 = rfscores([rf0_1800, rf25_1800, rf50_1800, rf75_1800, rf100_1800], Xt_1800, Xb_t_1800, Xw_t_1800)\n",
      "159/304:\n",
      "nonfaces = []\n",
      "for i in range(n):\n",
      "    # Read a random nonface file\n",
      "    j = random.randint(0, num_test_nonfaces - 1)\n",
      "    nonface = cv2.imread(test_nonfaces[j], cv2.IMREAD_GRAYSCALE)\n",
      "    \n",
      "    wsize = random.randint(50, min(nonface.shape[0], nonface.shape[1]))\n",
      "    row = random.randint(0, nonface.shape[0]-wsize)\n",
      "    col = random.randint(0, nonface.shape[1]-wsize)\n",
      "    crop = nonface[row:row+wsize, col:col+wsize]\n",
      "\n",
      "    # Resize to be the right size\n",
      "    crop = cv2.resize(crop, (200, 200))\n",
      "    nonfaces.append(crop)\n",
      "159/305:\n",
      "nonfaces = []\n",
      "for i in range(n):\n",
      "    # Read a random nonface file\n",
      "    j = random.randint(0, len(test_nonfaces) - 1)\n",
      "    nonface = cv2.imread(test_nonfaces[j], cv2.IMREAD_GRAYSCALE)\n",
      "    \n",
      "    wsize = random.randint(50, min(nonface.shape[0], nonface.shape[1]))\n",
      "    row = random.randint(0, nonface.shape[0]-wsize)\n",
      "    col = random.randint(0, nonface.shape[1]-wsize)\n",
      "    crop = nonface[row:row+wsize, col:col+wsize]\n",
      "\n",
      "    # Resize to be the right size\n",
      "    crop = cv2.resize(crop, (200, 200))\n",
      "    nonfaces.append(crop)\n",
      "159/306:\n",
      "nonfaces = []\n",
      "for i in range(n):\n",
      "    # Read a random nonface file\n",
      "    j = random.randint(0, len(test_nonface_names) - 1)\n",
      "    nonface = cv2.imread(test_nonface_names[j], cv2.IMREAD_GRAYSCALE)\n",
      "    \n",
      "    wsize = random.randint(50, min(nonface.shape[0], nonface.shape[1]))\n",
      "    row = random.randint(0, nonface.shape[0]-wsize)\n",
      "    col = random.randint(0, nonface.shape[1]-wsize)\n",
      "    crop = nonface[row:row+wsize, col:col+wsize]\n",
      "\n",
      "    # Resize to be the right size\n",
      "    crop = cv2.resize(crop, (200, 200))\n",
      "    nonfaces.append(crop)\n",
      "159/307:\n",
      "nonfaces = []\n",
      "for i in range(n):\n",
      "    # Read a random nonface file\n",
      "    j = random.randint(0, len(test_nonface_names) - 1)\n",
      "    nonface = cv2.imread(test_nonface_names[j], cv2.IMREAD_GRAYSCALE)\n",
      "    \n",
      "    wsize = random.randint(36, min(nonface.shape[0], nonface.shape[1]))\n",
      "    row = random.randint(0, nonface.shape[0]-wsize)\n",
      "    col = random.randint(0, nonface.shape[1]-wsize)\n",
      "    crop = nonface[row:row+wsize, col:col+wsize]\n",
      "\n",
      "    # Resize to be the right size\n",
      "    crop = cv2.resize(crop, (200, 200))\n",
      "    nonfaces.append(crop)\n",
      "159/308:\n",
      "X0 = getx2(face_filenames0, n, nonfaces, feature_type_sel2, feature_coord_sel2)\n",
      "X25 = getx2(face_filenames25, n, nonfaces, feature_type_sel2, feature_coord_sel2)\n",
      "X50 = getx2(face_filenames50, n, nonfaces, feature_type_sel2, feature_coord_sel2)\n",
      "X75 = getx2(face_filenames75, n, nonfaces, feature_type_sel2, feature_coord_sel2)\n",
      "X100 = getx2(face_filenames100, n, nonfaces, feature_type_sel2, feature_coord_sel2)\n",
      "Xt = getx2(face_filenames0, 300, nonfaces_t, feature_type_sel2, feature_coord_sel2)\n",
      "159/309:\n",
      "t_start = time()\n",
      "params = logistic_fit(X0, y, 0.001)\n",
      "predicted = logistic_prob(Xt_900, params)\n",
      "print(time() - t_start)\n",
      "print(getacc(predicted, y_t))\n",
      "159/310:\n",
      "t_start = time()\n",
      "params25 = logistic_fit(X25, y, 0.001)\n",
      "predicted25 = logistic_prob(Xt_900, params25)\n",
      "print(time() - t_start)\n",
      "print(getacc(predicted25, y_t))\n",
      "159/311:\n",
      "t_start = time()\n",
      "params50 = logistic_fit(X50, y, 0.001)\n",
      "predicted50 = logistic_prob(Xt_900, params50)\n",
      "print(time() - t_start)\n",
      "print(getacc(predicted50, y_t))\n",
      "159/312:\n",
      "t_start = time()\n",
      "params75 = logistic_fit(X75, y, 0.001)\n",
      "predicted75 = logistic_prob(Xt_900, params75)\n",
      "print(time() - t_start)\n",
      "print(getacc(predicted75, y_t))\n",
      "159/313:\n",
      "t_start = time()\n",
      "params100 = logistic_fit(X100, y, 0.001)\n",
      "predicted100 = logistic_prob(Xt_900, params100)\n",
      "print(time() - t_start)\n",
      "print(getacc(predicted100, y_t))\n",
      "159/314: os_l, bs_l, ws_l = logscores([params, params25, params50, params75, params100], Xt_900, Xb_t, Xw_t)\n",
      "159/315:\n",
      "plt.plot([0, 25, 50, 75, 100], os_l, label=\"o\")\n",
      "plt.plot([0, 25, 50, 75, 100], bs_l, label=\"b\")\n",
      "plt.plot([0, 25, 50, 75, 100], ws_l, label=\"w\")\n",
      "plt.legend()\n",
      "159/316:\n",
      "nonfaces = []\n",
      "for i in range(n):\n",
      "    # Read a random nonface file\n",
      "    j = random.randint(0, len() - 1)\n",
      "    nonface = cv2.imread(nonface_filenames[j], cv2.IMREAD_GRAYSCALE)\n",
      "    \n",
      "    wsize = random.randint(36, min(nonface.shape[0], nonface.shape[1]))\n",
      "    row = random.randint(0, nonface.shape[0]-wsize)\n",
      "    col = random.randint(0, nonface.shape[1]-wsize)\n",
      "    crop = nonface[row:row+wsize, col:col+wsize]\n",
      "\n",
      "    # Resize to be the right size\n",
      "    crop = cv2.resize(crop, (200, 200))\n",
      "    nonfaces.append(crop)\n",
      "159/317:\n",
      "nonfaces = []\n",
      "for i in range(n):\n",
      "    # Read a random nonface file\n",
      "    j = random.randint(0, len(nonface_filenames) - 1)\n",
      "    nonface = cv2.imread(nonface_filenames[j], cv2.IMREAD_GRAYSCALE)\n",
      "    wsize = random.randint(36, min(nonface.shape[0], nonface.shape[1]))\n",
      "    row = random.randint(0, nonface.shape[0]-wsize)\n",
      "    col = random.randint(0, nonface.shape[1]-wsize)\n",
      "    crop = nonface[row:row+wsize, col:col+wsize]\n",
      "\n",
      "    # Resize to be the right size\n",
      "    crop = cv2.resize(crop, (200, 200))\n",
      "    nonfaces.append(crop)\n",
      "159/318:\n",
      "t_start = time()\n",
      "params = logistic_fit(X0, y, 0.001)\n",
      "predicted = logistic_prob(Xt_900, params)\n",
      "print(time() - t_start)\n",
      "print(getacc(predicted, y_t))\n",
      "159/319:\n",
      "t_start = time()\n",
      "params25 = logistic_fit(X25, y, 0.001)\n",
      "predicted25 = logistic_prob(Xt_900, params25)\n",
      "print(time() - t_start)\n",
      "print(getacc(predicted25, y_t))\n",
      "159/320:\n",
      "t_start = time()\n",
      "params50 = logistic_fit(X50, y, 0.001)\n",
      "predicted50 = logistic_prob(Xt_900, params50)\n",
      "print(time() - t_start)\n",
      "print(getacc(predicted50, y_t))\n",
      "159/321:\n",
      "t_start = time()\n",
      "params75 = logistic_fit(X75, y, 0.001)\n",
      "predicted75 = logistic_prob(Xt_900, params75)\n",
      "print(time() - t_start)\n",
      "print(getacc(predicted75, y_t))\n",
      "159/322:\n",
      "t_start = time()\n",
      "params100 = logistic_fit(X100, y, 0.001)\n",
      "predicted100 = logistic_prob(Xt_900, params100)\n",
      "print(time() - t_start)\n",
      "print(getacc(predicted100, y_t))\n",
      "159/323: os_l, bs_l, ws_l = logscores([params, params25, params50, params75, params100], Xt_900, Xb_t, Xw_t)\n",
      "159/324:\n",
      "plt.plot([0, 25, 50, 75, 100], os_l, label=\"o\")\n",
      "plt.plot([0, 25, 50, 75, 100], bs_l, label=\"b\")\n",
      "plt.plot([0, 25, 50, 75, 100], ws_l, label=\"w\")\n",
      "plt.legend()\n",
      "159/325:\n",
      "# use b_inds and w_inds to select rows of X_t and y_t\n",
      "Xb_t = Xt_900[b_inds]\n",
      "yb_t = y_t[b_inds]\n",
      "Xw_t = Xt_900[w_inds]\n",
      "yw_t = y_t[w_inds]\n",
      "Xb_t.shape, Xb_t_1800.shape, yb_t.shape, Xw_t.shape, Xw_t_1800.shape, yw_t.shape\n",
      "159/326:\n",
      "t_start = time()\n",
      "rf100_ = RandomForestClassifier(n_estimators=1000, max_depth=None, n_jobs=-1, random_state=0)\n",
      "rf100_.fit(X100, y)\n",
      "print(time() - t_start)\n",
      "print(rf100_.score(Xt_900, y_t))\n",
      "159/327:\n",
      "t_start = time()\n",
      "rf75_ = RandomForestClassifier(n_estimators=1000, max_depth=None, n_jobs=-1, random_state=0)\n",
      "rf75_.fit(X75, y)\n",
      "print(time() - t_start)\n",
      "print(rf75_.score(Xt_900, y_t))\n",
      "159/328:\n",
      "t_start = time()\n",
      "rf50_ = RandomForestClassifier(n_estimators=1000, max_depth=None, n_jobs=-1, random_state=0)\n",
      "rf50_.fit(X50, y)\n",
      "print(time() - t_start)\n",
      "print(rf50_.score(Xt_900, y_t))\n",
      "159/329:\n",
      "t_start = time()\n",
      "rf25_ = RandomForestClassifier(n_estimators=1000, max_depth=None, n_jobs=-1, random_state=0)\n",
      "rf25_.fit(X25, y)\n",
      "print(time() - t_start)\n",
      "print(rf25_.score(Xt_900, y_t))\n",
      "159/330:\n",
      "t_start = time()\n",
      "rf0_ = RandomForestClassifier(n_estimators=1000, max_depth=None, n_jobs=-1, random_state=0)\n",
      "rf0_.fit(X0, y)\n",
      "print(time() - t_start)\n",
      "print(rf0_.score(Xt_900, y_t))\n",
      "159/331: os_r_, bs_r_, ws_r_ = rfscores([rf0_, rf25_, rf50_, rf75_, rf100_], Xt_900, Xb_t, Xw_t)\n",
      "159/332:\n",
      "plt.plot([0, 25, 50, 75, 100], os_r_, label=\"o\")\n",
      "plt.plot([0, 25, 50, 75, 100], bs_r_, label=\"b\")\n",
      "plt.plot([0, 25, 50, 75, 100], ws_r_, label=\"w\")\n",
      "plt.legend()\n",
      "159/333: gettop(face_filenames0, rf0_)\n",
      "159/334: gettop(face_filenames100, rf100_)\n",
      "159/335: gettop(face_filenames75, rf75_)\n",
      "159/336: gettop(face_filenames50, rf50_)\n",
      "159/337: gettop(face_filenames25, rf25_)\n",
      "159/338:\n",
      "def gettop(fnames, rf):\n",
      "    im = cv2.resize(cv2.imread(fnames[i], cv2.IMREAD_GRAYSCALE),(36,36))\n",
      "    fig, axes = plt.subplots(3,4, figsize=(20,20))\n",
      "    # Sort features in order of importance and plot the six most significant\n",
      "    idx_sorted = np.argsort(rf.feature_importances_)[::-1]\n",
      "    for idx, ax in enumerate(axes.ravel()):\n",
      "        image = draw_haar_like_feature(im, 0, 0, 36, 36,\n",
      "                                       [feature_coord_sel[idx_sorted[idx]]])\n",
      "        ax.imshow(image)\n",
      "        ax.set_xticks([])\n",
      "        ax.set_yticks([])\n",
      "\n",
      "    fig.suptitle('The most important features')\n",
      "159/339: gettop(face_filenames0, rf0_)\n",
      "159/340:\n",
      "def gettop(fnames, rf):\n",
      "    im = cv2.resize(cv2.imread(fnames[i], cv2.IMREAD_GRAYSCALE),(36,36))\n",
      "    fig, axes = plt.subplots(3,4, figsize=(15,20))\n",
      "    # Sort features in order of importance and plot the six most significant\n",
      "    idx_sorted = np.argsort(rf.feature_importances_)[::-1]\n",
      "    for idx, ax in enumerate(axes.ravel()):\n",
      "        image = draw_haar_like_feature(im, 0, 0, 36, 36,\n",
      "                                       [feature_coord_sel[idx_sorted[idx]]])\n",
      "        ax.imshow(image)\n",
      "        ax.set_xticks([])\n",
      "        ax.set_yticks([])\n",
      "\n",
      "    fig.suptitle('The most important features')\n",
      "159/341: gettop(face_filenames0, rf0_)\n",
      "159/342:\n",
      "def gettop(fnames, rf):\n",
      "    im = cv2.resize(cv2.imread(fnames[i], cv2.IMREAD_GRAYSCALE),(36,36))\n",
      "    fig, axes = plt.subplots(3,4, figsize=(16,8))\n",
      "    # Sort features in order of importance and plot the six most significant\n",
      "    idx_sorted = np.argsort(rf.feature_importances_)[::-1]\n",
      "    for idx, ax in enumerate(axes.ravel()):\n",
      "        image = draw_haar_like_feature(im, 0, 0, 36, 36,\n",
      "                                       [feature_coord_sel[idx_sorted[idx]]])\n",
      "        ax.imshow(image)\n",
      "        ax.set_xticks([])\n",
      "        ax.set_yticks([])\n",
      "\n",
      "    fig.suptitle('The most important features')\n",
      "159/343: gettop(face_filenames0, rf0_)\n",
      "159/344: gettop(face_filenames25, rf25_)\n",
      "159/345: gettop(face_filenames50, rf50_)\n",
      "159/346: gettop(face_filenames75, rf75_)\n",
      "159/347: gettop(face_filenames100, rf100_)\n",
      "159/348:\n",
      "def gettop(fnames, rf):\n",
      "    im = cv2.resize(cv2.imread(fnames[0], cv2.IMREAD_GRAYSCALE),(36,36))\n",
      "    fig, axes = plt.subplots(3,4, figsize=(16,8))\n",
      "    # Sort features in order of importance and plot the six most significant\n",
      "    idx_sorted = np.argsort(rf.feature_importances_)[::-1]\n",
      "    for idx, ax in enumerate(axes.ravel()):\n",
      "        image = draw_haar_like_feature(im, 0, 0, 36, 36,\n",
      "                                       [feature_coord_sel[idx_sorted[idx]]])\n",
      "        ax.imshow(image)\n",
      "        ax.set_xticks([])\n",
      "        ax.set_yticks([])\n",
      "\n",
      "    fig.suptitle('The most important features')\n",
      "159/349: gettop(face_filenames0, rf0_)\n",
      "159/350: gettop(face_filenames25, rf25_)\n",
      "159/351:\n",
      "def gettop(fnames, rf):\n",
      "    im = cv2.resize(cv2.imread(fnames[1], cv2.IMREAD_GRAYSCALE),(36,36))\n",
      "    fig, axes = plt.subplots(3,4, figsize=(16,8))\n",
      "    # Sort features in order of importance and plot the six most significant\n",
      "    idx_sorted = np.argsort(rf.feature_importances_)[::-1]\n",
      "    for idx, ax in enumerate(axes.ravel()):\n",
      "        image = draw_haar_like_feature(im, 0, 0, 36, 36,\n",
      "                                       [feature_coord_sel[idx_sorted[idx]]])\n",
      "        ax.imshow(image)\n",
      "        ax.set_xticks([])\n",
      "        ax.set_yticks([])\n",
      "\n",
      "    fig.suptitle('The most important features')\n",
      "159/352: gettop(face_filenames0, rf0_)\n",
      "159/353: gettop(face_filenames25, rf25_)\n",
      "159/354:\n",
      "def gettop(fnames, rf):\n",
      "    im = cv2.resize(cv2.imread(fnames[5], cv2.IMREAD_GRAYSCALE),(36,36))\n",
      "    fig, axes = plt.subplots(3,4, figsize=(16,8))\n",
      "    # Sort features in order of importance and plot the six most significant\n",
      "    idx_sorted = np.argsort(rf.feature_importances_)[::-1]\n",
      "    for idx, ax in enumerate(axes.ravel()):\n",
      "        image = draw_haar_like_feature(im, 0, 0, 36, 36,\n",
      "                                       [feature_coord_sel[idx_sorted[idx]]])\n",
      "        ax.imshow(image)\n",
      "        ax.set_xticks([])\n",
      "        ax.set_yticks([])\n",
      "\n",
      "    fig.suptitle('The most important features')\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "159/355: gettop(face_filenames0, rf0_)\n",
      "159/356: gettop(face_filenames25, rf25_)\n",
      "159/357: gettop(face_filenames50, rf50_)\n",
      "159/358: gettop(face_filenames75, rf75_)\n",
      "159/359:\n",
      "t_start = time()\n",
      "params = logistic_fit(X0, y, 0.0001)\n",
      "predicted = logistic_prob(Xt_900, params)\n",
      "print(time() - t_start)\n",
      "print(getacc(predicted, y_t))\n",
      "159/360: Xt[:300]\n",
      "159/361: Xt[:300].shape\n",
      "159/362: Xb_t.shape\n",
      "159/363: faces_t.shape\n",
      "159/364: len(faces_t)\n",
      "159/365: faces_t[b_inds]\n",
      "159/366: np.array(faces_t)[b_inds]\n",
      "159/367: np.array(faces_t)[b_inds].shape\n",
      "159/368: plt.imshow(faces_t[0])\n",
      "159/369: plt.imshow(faces_t[0], cmap='gray')\n",
      "159/370: sum(np.array(faces_t)[b_inds]).shape\n",
      "159/371: plt.imshow(sum(np.array(faces_t)[b_inds])/200/200, cmap='gray')\n",
      "159/372: plt.imshow(sum(np.array(faces_t)[b_inds])/150, cmap='gray')\n",
      "159/373: plt.imshow(sum(np.array(faces_t)[w_inds])/150, cmap='gray')\n",
      "159/374: plt.imshow(sum(np.array(faces_t)[b_inds[0,1,2]])/3)\n",
      "159/375: plt.imshow(sum(np.array(faces_t)[b_inds[0,1,2]]))\n",
      "159/376: plt.imshow(sum(np.array(faces_t)[b_inds[0:2]]))\n",
      "159/377: plt.imshow(sum(np.array(faces_t)[b_inds[0:2]]), cmap='gray')\n",
      "159/378: plt.imshow(sum(np.array(faces_t)[b_inds[0:1]]), cmap='gray')\n",
      "159/379: np.array(faces_t)[b_inds[0]]\n",
      "159/380: img = np.array(faces_t)[b_inds[0]]\n",
      "159/381:\n",
      "hsv = cv2.cvtColor(img, cv2.COLOR_BGR2HSV)\n",
      "_, _, v = cv2.split(hsv)\n",
      "v\n",
      "159/382:\n",
      "hsv = cv2.cvtColor(test_face_names[0], cv2.COLOR_BGR2HSV)\n",
      "_, _, v = cv2.split(hsv)\n",
      "v\n",
      "159/383:\n",
      "hsv = cv2.cvtColor(img, cv2.COLOR_BGR2HSV)\n",
      "_, _, v = cv2.split(hsv)\n",
      "v\n",
      "159/384:\n",
      "\n",
      "hsv = cv2.cvtColor(img, cv2.COLOR_GRAY2HSV)\n",
      "_, _, v = cv2.split(hsv)\n",
      "v\n",
      "159/385:\n",
      "\n",
      "hsv = cv2.cvtColor(img, cv2.COLOR_GRAY2BGR)\n",
      "_, _, v = cv2.split(hsv)\n",
      "v\n",
      "159/386:\n",
      "hsv = cv2.cvtColor(cv2.cvtColor(img, cv2.COLOR_GRAY2BGR), cv2.COLOR_BGR2HSV)\n",
      "_, _, v = cv2.split(hsv)\n",
      "v\n",
      "159/387:\n",
      "hsv = cv2.cvtColor(cv2.cvtColor(img, cv2.COLOR_GRAY2BGR), cv2.COLOR_BGR2HSV)\n",
      "_, _, v = cv2.split(hsv)\n",
      "v\n",
      "159/388:\n",
      "hsv = cv2.cvtColor(cv2.cvtColor(img, cv2.COLOR_GRAY2BGR), cv2.COLOR_BGR2HSV)\n",
      "_, _, v = cv2.split(hsv)\n",
      "mean(v)\n",
      "159/389:\n",
      "hsv = cv2.cvtColor(cv2.cvtColor(img, cv2.COLOR_GRAY2BGR), cv2.COLOR_BGR2HSV)\n",
      "_, _, v = cv2.split(hsv)\n",
      "np.mean(v)\n",
      "159/390:\n",
      "hsv = cv2.cvtColor(cv2.cvtColor(img, cv2.COLOR_GRAY2BGR), cv2.COLOR_BGR2HSV)\n",
      "_, _, v = cv2.split(hsv)\n",
      "np.median(v)\n",
      "159/391: img = np.array(faces_t)[w_inds[0]]\n",
      "159/392:\n",
      "hsv = cv2.cvtColor(cv2.cvtColor(img, cv2.COLOR_GRAY2BGR), cv2.COLOR_BGR2HSV)\n",
      "_, _, v = cv2.split(hsv)\n",
      "np.median(v)\n",
      "159/393: plt.imshow(sum(np.array(faces_t)[w_inds[0]]), cmap='gray')\n",
      "159/394: plt.imshow(sum(np.array(faces_t)[b_inds[0]]), cmap='gray')\n",
      "159/395: plt.imshow(np.array(faces_t)[b_inds[0]], cmap='gray')\n",
      "159/396: plt.imshow(np.array(faces_t)[w_inds[0]], cmap='gray')\n",
      "159/397: plt.imshow(np.array(faces_t)[b_inds[0]], cmap='gray')\n",
      "159/398: plt.imshow(np.array(faces_t)[b_inds[0]][50:150,50:150], cmap='gray')\n",
      "159/399: plt.imshow(np.array(faces_t)[b_inds[0]][25:175,25:175], cmap='gray')\n",
      "159/400: plt.imshow(np.array(faces_t)[b_inds[0]][50:150,50:150], cmap='gray')\n",
      "159/401: plt.imshow(np.array(faces_t)[w_inds[0]][50:150,50:150], cmap='gray')\n",
      "159/402:\n",
      "# average black face image brightness/intensity:\n",
      "vs_b = []\n",
      "vs_b2 = []\n",
      "for b in b_inds:\n",
      "    img = np.array(faces_t)[b][50:150,50:150]\n",
      "    hsv = cv2.cvtColor(cv2.cvtColor(img, cv2.COLOR_GRAY2BGR), cv2.COLOR_BGR2HSV)\n",
      "    _, _, v = cv2.split(hsv)\n",
      "    vs_b.append(np.mean(v))\n",
      "    vs_b2.append(np.median(v))\n",
      "159/403: mean(vs_b), median(vs_b2)\n",
      "159/404: np.mean(vs_b), np.median(vs_b2)\n",
      "159/405:\n",
      "# average white face image brightness/intensity:\n",
      "vs_w = []\n",
      "vs_w2 = []\n",
      "for w in w_inds:\n",
      "    img = np.array(faces_t)[w][50:150,50:150]\n",
      "    hsv = cv2.cvtColor(cv2.cvtColor(img, cv2.COLOR_GRAY2BGR), cv2.COLOR_BGR2HSV)\n",
      "    _, _, v = cv2.split(hsv)\n",
      "    vs_w.append(np.mean(v))\n",
      "    vs_w2.append(np.median(v))\n",
      "159/406: np.mean(vs_w), np.median(vs_w2)\n",
      "159/407:\n",
      "# average white face image brightness/intensity:\n",
      "vs_w = []\n",
      "vs_w2 = []\n",
      "for w in w_inds:\n",
      "    img = np.array(faces_t)[w]\n",
      "    hsv = cv2.cvtColor(cv2.cvtColor(img, cv2.COLOR_GRAY2BGR), cv2.COLOR_BGR2HSV)\n",
      "    _, _, v = cv2.split(hsv)\n",
      "    vs_w.append(np.mean(v))\n",
      "    vs_w2.append(np.median(v))\n",
      "159/408: np.mean(vs_w), np.median(vs_w2)\n",
      "159/409:\n",
      "# average black face image brightness/intensity:\n",
      "vs_b = []\n",
      "vs_b2 = []\n",
      "for b in b_inds:\n",
      "    img = np.array(faces_t)[b]#[50:150,50:150]\n",
      "    hsv = cv2.cvtColor(cv2.cvtColor(img, cv2.COLOR_GRAY2BGR), cv2.COLOR_BGR2HSV)\n",
      "    _, _, v = cv2.split(hsv)\n",
      "    vs_b.append(np.mean(v))\n",
      "    vs_b2.append(np.median(v))\n",
      "159/410: np.mean(vs_b), np.median(vs_b2)\n",
      "159/411:\n",
      "t_start = time()\n",
      "params = logistic_fit(X0, y, 0.01)\n",
      "predicted = logistic_prob(Xt_900, params)\n",
      "print(time() - t_start)\n",
      "print(getacc(predicted, y_t))\n",
      "159/412:\n",
      "t_start = time()\n",
      "params25 = logistic_fit(X25, y, 0.01)\n",
      "predicted25 = logistic_prob(Xt_900, params25)\n",
      "print(time() - t_start)\n",
      "print(getacc(predicted25, y_t))\n",
      "159/413:\n",
      "t_start = time()\n",
      "params50 = logistic_fit(X50, y, 0.01)\n",
      "predicted50 = logistic_prob(Xt_900, params50)\n",
      "print(time() - t_start)\n",
      "print(getacc(predicted50, y_t))\n",
      "159/414:\n",
      "t_start = time()\n",
      "params75 = logistic_fit(X75, y, 0.01)\n",
      "predicted75 = logistic_prob(Xt_900, params75)\n",
      "print(time() - t_start)\n",
      "print(getacc(predicted75, y_t))\n",
      "159/415:\n",
      "t_start = time()\n",
      "params100 = logistic_fit(X100, y, 0.01)\n",
      "predicted100 = logistic_prob(Xt_900, params100)\n",
      "print(time() - t_start)\n",
      "print(getacc(predicted100, y_t))\n",
      "159/416: os_l, bs_l, ws_l = logscores([params, params25, params50, params75, params100], Xt_900, Xb_t, Xw_t)\n",
      "159/417:\n",
      "plt.plot([0, 25, 50, 75, 100], os_l, label=\"o\")\n",
      "plt.plot([0, 25, 50, 75, 100], bs_l, label=\"b\")\n",
      "plt.plot([0, 25, 50, 75, 100], ws_l, label=\"w\")\n",
      "plt.legend()\n",
      "159/418:\n",
      "t_start = time()\n",
      "params = logistic_fit(X0, y, 0.005)\n",
      "predicted = logistic_prob(Xt_900, params)\n",
      "print(time() - t_start)\n",
      "print(getacc(predicted, y_t))\n",
      "159/419:\n",
      "t_start = time()\n",
      "params25 = logistic_fit(X25, y, 0.005)\n",
      "predicted25 = logistic_prob(Xt_900, params25)\n",
      "print(time() - t_start)\n",
      "print(getacc(predicted25, y_t))\n",
      "159/420:\n",
      "t_start = time()\n",
      "params50 = logistic_fit(X50, y, 0.005)\n",
      "predicted50 = logistic_prob(Xt_900, params50)\n",
      "print(time() - t_start)\n",
      "print(getacc(predicted50, y_t))\n",
      "159/421:\n",
      "t_start = time()\n",
      "params75 = logistic_fit(X75, y, 0.005)\n",
      "predicted75 = logistic_prob(Xt_900, params75)\n",
      "print(time() - t_start)\n",
      "print(getacc(predicted75, y_t))\n",
      "159/422:\n",
      "t_start = time()\n",
      "params100 = logistic_fit(X100, y, 0.005)\n",
      "predicted100 = logistic_prob(Xt_900, params100)\n",
      "print(time() - t_start)\n",
      "print(getacc(predicted100, y_t))\n",
      "159/423: os_l, bs_l, ws_l = logscores([params, params25, params50, params75, params100], Xt_900, Xb_t, Xw_t)\n",
      "159/424:\n",
      "plt.plot([0, 25, 50, 75, 100], os_l, label=\"o\")\n",
      "plt.plot([0, 25, 50, 75, 100], bs_l, label=\"b\")\n",
      "plt.plot([0, 25, 50, 75, 100], ws_l, label=\"w\")\n",
      "plt.legend()\n",
      "159/425:\n",
      "# average black face image brightness/intensity:\n",
      "vs_b = []\n",
      "hs_b = []\n",
      "ss_b = []\n",
      "for b in b_inds:\n",
      "    img = np.array(faces_t)[b]\n",
      "    hsv = cv2.cvtColor(cv2.cvtColor(img, cv2.COLOR_GRAY2BGR), cv2.COLOR_BGR2HSV)\n",
      "    h, s, v = cv2.split(hsv)\n",
      "    vs_b.append(np.median(v))\n",
      "    ss_b.append(np.median(s))\n",
      "    hs_b.append(np.median(h))\n",
      "159/426: np.median(vs_b), np.median(hs_b), np.median(ss_b)\n",
      "159/427: np.median(vs_w), np.median(hs_w), np.median(ss_w)\n",
      "159/428:\n",
      "# average white face image brightness/intensity:\n",
      "vs_w = []\n",
      "hs_w = []\n",
      "ss_w = []\n",
      "for w in w_inds:\n",
      "    img = np.array(faces_t)[w]\n",
      "    hsv = cv2.cvtColor(cv2.cvtColor(img, cv2.COLOR_GRAY2BGR), cv2.COLOR_BGR2HSV)\n",
      "    h, s, v = cv2.split(hsv)\n",
      "    vs_w.append(np.median(v))\n",
      "    ss_w.append(np.median(s))\n",
      "    hs_w.append(np.median(h))\n",
      "159/429: np.median(vs_w), np.median(hs_w), np.median(ss_w)\n",
      "159/430: np.median(vs_b), np.mean(hs_b), np.median(ss_b)\n",
      "159/431:\n",
      "# average black face image brightness/intensity:\n",
      "vs_b = []\n",
      "hs_b = []\n",
      "ss_b = []\n",
      "for b in b_inds:\n",
      "    img = np.array(faces_t)[b]\n",
      "    hsv = cv2.cvtColor(cv2.cvtColor(img, cv2.COLOR_GRAY2BGR), cv2.COLOR_BGR2HSV)\n",
      "    h, s, v = cv2.split(hsv)\n",
      "    vs_b.append(np.median(v))\n",
      "    ss_b.append(np.mean(s))\n",
      "    hs_b.append(np.median(h))\n",
      "159/432: np.median(vs_b), np.mean(hs_b), np.median(ss_b)\n",
      "159/433:\n",
      "# average black face image brightness/intensity:\n",
      "vs_b = []\n",
      "vs_b2 = []\n",
      "for b in b_inds:\n",
      "    img = np.array(faces_t)[b][50:150,50:150]\n",
      "    hsv = cv2.cvtColor(cv2.cvtColor(img, cv2.COLOR_GRAY2BGR), cv2.COLOR_BGR2HSV)\n",
      "    _, _, v = cv2.split(hsv)\n",
      "    vs_b.append(np.mean(v))\n",
      "    vs_b2.append(np.median(v))\n",
      "159/434: np.mean(vs_b), np.median(vs_b2)\n",
      "159/435:\n",
      "# average white face image brightness/intensity:\n",
      "vs_w = []\n",
      "vs_w2 = []\n",
      "for w in w_inds:\n",
      "    img = np.array(faces_t)[w][50:150,50:150]\n",
      "    hsv = cv2.cvtColor(cv2.cvtColor(img, cv2.COLOR_GRAY2BGR), cv2.COLOR_BGR2HSV)\n",
      "    _, _, v = cv2.split(hsv)\n",
      "    vs_w.append(np.mean(v))\n",
      "    vs_w2.append(np.median(v))\n",
      "159/436: np.mean(vs_w), np.median(vs_w2)\n",
      "159/437:\n",
      "# average black face image brightness/intensity:\n",
      "vs_b = []\n",
      "vs_b2 = []\n",
      "for b in b_inds:\n",
      "    img = np.array(faces_t)[b]\n",
      "    hsv = cv2.cvtColor(cv2.cvtColor(img, cv2.COLOR_GRAY2BGR), cv2.COLOR_BGR2HSV)\n",
      "    _, _, v = cv2.split(hsv)\n",
      "    vs_b.append(np.mean(v))\n",
      "    vs_b2.append(np.median(v))\n",
      "159/438: np.mean(vs_b), np.median(vs_b2)\n",
      "159/439:\n",
      "# average white face image brightness/intensity:\n",
      "vs_w = []\n",
      "vs_w2 = []\n",
      "for w in w_inds:\n",
      "    img = np.array(faces_t)[w][50:150,50:150]\n",
      "    hsv = cv2.cvtColor(cv2.cvtColor(img, cv2.COLOR_GRAY2BGR), cv2.COLOR_BGR2HSV)\n",
      "    _, _, v = cv2.split(hsv)\n",
      "    vs_w.append(np.mean(v))\n",
      "    vs_w2.append(np.median(v))\n",
      "159/440:\n",
      "# average white face image brightness/intensity:\n",
      "vs_w = []\n",
      "vs_w2 = []\n",
      "for w in w_inds:\n",
      "    img = np.array(faces_t)[w]\n",
      "    hsv = cv2.cvtColor(cv2.cvtColor(img, cv2.COLOR_GRAY2BGR), cv2.COLOR_BGR2HSV)\n",
      "    _, _, v = cv2.split(hsv)\n",
      "    vs_w.append(np.mean(v))\n",
      "    vs_w2.append(np.median(v))\n",
      "159/441: np.mean(vs_w), np.median(vs_w2)\n",
      "159/442: plt.hist(vs_w)\n",
      "159/443: plt.hist(vs_b)\n",
      "159/444:\n",
      "# average white face image brightness/intensity:\n",
      "vs_w = []\n",
      "vs_w2 = []\n",
      "stds_w = []\n",
      "for w in w_inds:\n",
      "    img = np.array(faces_t)[w]\n",
      "    hsv = cv2.cvtColor(cv2.cvtColor(img, cv2.COLOR_GRAY2BGR), cv2.COLOR_BGR2HSV)\n",
      "    _, _, v = cv2.split(hsv)\n",
      "    vs_w.append(np.mean(v))\n",
      "    vs_w2.append(np.median(v))\n",
      "    stds_w.append(np.std(v))\n",
      "159/445: np.mean(vs_w), np.median(vs_w2), np.mean(stds_w), np.median(stds_w)\n",
      "159/446: plt.hist(stds_w)\n",
      "159/447: X50.shape\n",
      "159/448: X50[:4020].shape\n",
      "159/449:\n",
      "# average black face image brightness/intensity:\n",
      "vs_b = []\n",
      "vs_b2 = []\n",
      "stds_b = []\n",
      "for b in b_inds:\n",
      "    img = np.array(faces_t)[b]\n",
      "    hsv = cv2.cvtColor(cv2.cvtColor(img, cv2.COLOR_GRAY2BGR), cv2.COLOR_BGR2HSV)\n",
      "    _, _, v = cv2.split(hsv)\n",
      "    vs_b.append(np.mean(v))\n",
      "    vs_b2.append(np.median(v))\n",
      "    stds_b.append(np.std(v))\n",
      "159/450: np.mean(vs_b), np.median(vs_b2), np.mean(stds_b), np.median(stds_b)\n",
      "159/451: plt.hist(stds_b)\n",
      "159/452: plt.imshow(np.array(faces_t)[w_inds[0]], cmap='gray')\n",
      "159/453: plt.imshow(np.array(faces_t)[b_inds[0]], cmap='gray')\n",
      "159/454: hsv.shape\n",
      "159/455: hsv[2[.shape\n",
      "159/456: hsv[2].shape\n",
      "159/457: hsv[0].shape\n",
      "159/458: hsv[1].shape\n",
      "159/459: hsv\n",
      "159/460: hsv[:,:,0]\n",
      "159/461: hsv[:,:,2]\n",
      "159/462: hsv[:,:,2] = 125+47.5*(hsv[:,:,2]-np.mean(hsv[:,:,2]))/np.std(hsv[:,:,2])\n",
      "159/463: plt.imshow(cv2.cvtColor(hsv, cv2.COLOR_HSV2BGR))\n",
      "159/464: hsv\n",
      "159/465: img = np.array(faces_t)[w_inds[0]]\n",
      "159/466:\n",
      "hsv = cv2.cvtColor(cv2.cvtColor(img, cv2.COLOR_GRAY2BGR), cv2.COLOR_BGR2HSV)\n",
      "_, _, v = cv2.split(hsv)\n",
      "np.median(v)\n",
      "159/467: hsv\n",
      "159/468: plt.imshow(cv2.cvtColor(hsv, cv2.COLOR_HSV2BGR))\n",
      "159/469: hsv[:,:,2]\n",
      "159/470: hsv[:,:,2] = 125+47.5*(hsv[:,:,2]-np.mean(hsv[:,:,2]))/np.std(hsv[:,:,2])\n",
      "159/471: hsv[:,:,2]\n",
      "159/472: plt.imshow(cv2.cvtColor(hsv, cv2.COLOR_HSV2BGR))\n",
      "159/473: img = np.array(faces_t)[w_inds[0]]\n",
      "159/474:\n",
      "hsv = cv2.cvtColor(cv2.cvtColor(img, cv2.COLOR_GRAY2BGR), cv2.COLOR_BGR2HSV)\n",
      "_, _, v = cv2.split(hsv)\n",
      "np.median(v)\n",
      "159/475: np.mean(hsv[:,:,2]), np.std(hsv[:,:,2])\n",
      "159/476: (60-163.36)/39.54\n",
      "159/477: 125+47.5*((60-163.36)/39.54)\n",
      "159/478: 125+47.5*((0-163.36)/39.54)\n",
      "159/479: hsv[:,:,2] = 125+47.5*(hsv[:,:,2]-np.mean(hsv[:,:,2]))/np.std(hsv[:,:,2])\n",
      "159/480: np.mean(hsv[:,:,2]), np.std(hsv[:,:,2])\n",
      "159/481:\n",
      "hsv = cv2.cvtColor(cv2.cvtColor(img, cv2.COLOR_GRAY2BGR), cv2.COLOR_BGR2HSV)\n",
      "_, _, v = cv2.split(hsv)\n",
      "np.median(v)\n",
      "159/482: hsv[:,:,2]\n",
      "159/483: np.mean(hsv[:,:,2]), np.std(hsv[:,:,2])\n",
      "159/484: hsv[:,:,2] = min(max(0, 125+47.5*(hsv[:,:,2]-np.mean(hsv[:,:,2]))/np.std(hsv[:,:,2])), 255)\n",
      "159/485: hsv[:,:,2] = 125+47.5*(hsv[:,:,2]-np.mean(hsv[:,:,2]))/np.std(hsv[:,:,2])\n",
      "159/486: hsv[hsv < 0] = 0\n",
      "159/487: plt.imshow(cv2.cvtColor(hsv, cv2.COLOR_HSV2BGR))\n",
      "159/488: hsv[:,:,2] = max(0, 125+47.5*(hsv[:,:,2]-np.mean(hsv[:,:,2]))/np.std(hsv[:,:,2]))\n",
      "159/489: hsv[:,:,2] = np.max(0, 125+47.5*(hsv[:,:,2]-np.mean(hsv[:,:,2]))/np.std(hsv[:,:,2]))\n",
      "159/490: hsv[hsv < 0]\n",
      "159/491: hsv[0,0,2]\n",
      "159/492: hsv\n",
      "159/493:\n",
      "hsv = cv2.cvtColor(cv2.cvtColor(img, cv2.COLOR_GRAY2BGR), cv2.COLOR_BGR2HSV)\n",
      "_, _, v = cv2.split(hsv)\n",
      "np.median(v)\n",
      "159/494: hsv[:,:,2] = np.max(0, 125+47.5*(hsv[:,:,2]-np.mean(hsv[:,:,2]))/np.std(hsv[:,:,2]))\n",
      "159/495: hsv[:,:,2] = 125+47.5*(hsv[:,:,2]-np.mean(hsv[:,:,2]))/np.std(hsv[:,:,2])\n",
      "159/496: 125+47.5*((255-163.36)/39.54)\n",
      "159/497: 125+47.5*((255-0)/10)\n",
      "159/498: hsv[:,:,2].astype(uint32)\n",
      "159/499: hsv[:,:,2].astype(np.uint32)\n",
      "159/500:\n",
      "hsv = cv2.cvtColor(cv2.cvtColor(img, cv2.COLOR_GRAY2BGR), cv2.COLOR_BGR2HSV)\n",
      "_, _, v = cv2.split(hsv)\n",
      "np.median(v)\n",
      "159/501: hsv[:,:,2].astype(np.uint32)\n",
      "159/502: hsv[:,:,2] = 125+47.5*(hsv[:,:,2]-np.mean(hsv[:,:,2]))/np.std(hsv[:,:,2])\n",
      "159/503: hsv[hsv<0]\n",
      "159/504:\n",
      "hsv = cv2.cvtColor(cv2.cvtColor(img, cv2.COLOR_GRAY2BGR), cv2.COLOR_BGR2HSV)\n",
      "_, _, v = cv2.split(hsv)\n",
      "np.median(v)\n",
      "159/505: hsv = hsv.astype(np.uint32)\n",
      "159/506: hsv[:,:,32]\n",
      "159/507: hsv[:,:,2]\n",
      "159/508: np.mean(hsv[:,:,2]), np.std(hsv[:,:,2])\n",
      "159/509: hsv[:,:,2] = 125+47.5*(hsv[:,:,2]-np.mean(hsv[:,:,2]))/np.std(hsv[:,:,2])\n",
      "159/510: hsv[hsv<0]\n",
      "159/511: hsv[:,:,2\n",
      "159/512: hsv[:,:,2]\n",
      "159/513: hsv[:,:,2] = 125+47.5*(35-163)/40\n",
      "159/514: 125+47.5*(35-163)/40\n",
      "159/515:\n",
      "hsv = cv2.cvtColor(cv2.cvtColor(img, cv2.COLOR_GRAY2BGR), cv2.COLOR_BGR2HSV)\n",
      "_, _, v = cv2.split(hsv)\n",
      "np.median(v)\n",
      "159/516: hsv = hsv.astype(np.int32)\n",
      "159/517: hsv[:,:,2]\n",
      "159/518: hsv[:,:,2] = 125+47.5*(hsv[:,:,2]-np.mean(hsv[:,:,2]))/np.std(hsv[:,:,2])\n",
      "159/519: hsv[:,:,2]\n",
      "159/520: hsv[hsv<0]\n",
      "159/521: hsv[hsv>255]\n",
      "159/522: hsv[hsv>254] = 2254\n",
      "159/523: hsv[hsv<0] = 0\n",
      "159/524: hsv[:,:,2]\n",
      "159/525: plt.imshow(cv2.cvtColor(hsv, cv2.COLOR_HSV2BGR))\n",
      "159/526: plt.imshow(cv2.cvtColor(hsv.astype(np.uint8), cv2.COLOR_HSV2BGR))\n",
      "159/527: np.mean(hsv[:,:,2])\n",
      "159/528: cv2.cvtColor(hsv.astype(np.uint8), cv2.COLOR_HSV2BGR)\n",
      "159/529: cv2.cvtColor(hsv.astype(np.uint8), cv2.COLOR_HSV2BGR).shape\n",
      "159/530: cv2.cvtColor(cv2.cvtColor(hsv.astype(np.uint8), cv2.COLOR_HSV2BGR), cv2.COLOR_BGR2GRAY)\n",
      "159/531: cv2.cvtColor(cv2.cvtColor(hsv.astype(np.uint8), cv2.COLOR_HSV2BGR), cv2.COLOR_BGR2GRAY).shape\n",
      "159/532: temp = cv2.cvtColor(cv2.cvtColor(hsv.astype(np.uint8), cv2.COLOR_HSV2BGR), cv2.COLOR_BGR2GRAY)\n",
      "159/533: plt.imshow(temp, cmap='gray')\n",
      "159/534: plt.imshow(np.array(faces_t)[w_inds[0]], cmap='gray')\n",
      "159/535: plt.imshow(np.array(faces_t)[w_inds[1]], cmap='gray')\n",
      "159/536: plt.imshow(np.array(faces_t)[w_inds[2]], cmap='gray')\n",
      "159/537: plt.imshow(np.array(faces_t)[w_inds[3]], cmap='gray')\n",
      "159/538: hsv[hsv>254] = 254\n",
      "159/539:\n",
      "def normalize(img):\n",
      "    hsv = cv2.cvtColor(cv2.cvtColor(img, cv2.COLOR_GRAY2BGR), cv2.COLOR_BGR2HSV)\n",
      "    _, _, v = cv2.split(hsv)\n",
      "    m, s = np.mean(hsv[:,:,2]), np.std(hsv[:,:,2])\n",
      "    hsv[:,:,2] = 125+47.5*(hsv[:,:,2]-m)/s\n",
      "    hsv[hsv>254] = 254\n",
      "    hsv[hsv<0] = 0\n",
      "    return cv2.cvtColor(cv2.cvtColor(hsv.astype(np.uint8), cv2.COLOR_HSV2BGR), cv2.COLOR_BGR2GRAY)\n",
      "159/540: plt.imshow(normalize(np.array(faces_t)[w_inds[3]]))\n",
      "159/541: plt.imshow(normalize(np.array(faces_t)[w_inds[3]]), cmap='gray')\n",
      "159/542: plt.imshow(np.array(faces_t)[w_inds[4]], cmap='gray')\n",
      "159/543: plt.imshow(np.array(faces_t)[w_inds[5]], cmap='gray')\n",
      "159/544: plt.imshow(normalize(np.array(faces_t)[w_inds[5]]), cmap='gray')\n",
      "159/545: plt.imshow(normalize(np.array(faces_t)[b_inds[5]]), cmap='gray')\n",
      "159/546: plt.imshow(normalize(np.array(faces_t)[b_inds[2]]), cmap='gray')\n",
      "159/547: plt.imshow(normalize(np.array(faces_t)[b_inds[1]]), cmap='gray')\n",
      "159/548: plt.imshow(np.array(faces_t)[b_inds[1]], cmap='gray')\n",
      "159/549:\n",
      "def normalize(img):\n",
      "    hsv = cv2.cvtColor(cv2.cvtColor(img, cv2.COLOR_GRAY2BGR), cv2.COLOR_BGR2HSV)\n",
      "    _, _, v = cv2.split(hsv)\n",
      "    m, s = np.mean(hsv[:,:,2]), np.std(hsv[:,:,2])\n",
      "    hsv[:,:,2] = 125+47.5*(hsv[:,:,2]-m)/s\n",
      "    hsv[hsv>255] = 255\n",
      "    hsv[hsv<0] = 0\n",
      "    return cv2.cvtColor(cv2.cvtColor(hsv.astype(np.uint8), cv2.COLOR_HSV2BGR), cv2.COLOR_BGR2GRAY)\n",
      "159/550:\n",
      "def normalize(img):\n",
      "    hsv = cv2.cvtColor(cv2.cvtColor(img, cv2.COLOR_GRAY2BGR), cv2.COLOR_BGR2HSV)\n",
      "    hsv = hsv.astype(np.int32)\n",
      "    _, _, v = cv2.split(hsv)\n",
      "    m, s = np.mean(hsv[:,:,2]), np.std(hsv[:,:,2])\n",
      "    hsv[:,:,2] = 125+47.5*(hsv[:,:,2]-m)/s\n",
      "    hsv[hsv>255] = 255\n",
      "    hsv[hsv<0] = 0\n",
      "    return cv2.cvtColor(cv2.cvtColor(hsv.astype(np.uint8), cv2.COLOR_HSV2BGR), cv2.COLOR_BGR2GRAY)\n",
      "159/551: plt.imshow(normalize(np.array(faces_t)[b_inds[1]]), cmap='gray')\n",
      "159/552:\n",
      "def normalize(img):\n",
      "    hsv = cv2.cvtColor(cv2.cvtColor(img, cv2.COLOR_GRAY2BGR), cv2.COLOR_BGR2HSV)\n",
      "    print(np.mean(hsv[:,:,2]))\n",
      "    hsv = hsv.astype(np.int32)\n",
      "    _, _, v = cv2.split(hsv)\n",
      "    m, s = np.mean(hsv[:,:,2]), np.std(hsv[:,:,2])\n",
      "    hsv[:,:,2] = 125+47.5*(hsv[:,:,2]-m)/s\n",
      "    hsv[hsv>255] = 255\n",
      "    hsv[hsv<0] = 0\n",
      "    print(np.mean(hsv[:,:,2]))\n",
      "    return cv2.cvtColor(cv2.cvtColor(hsv.astype(np.uint8), cv2.COLOR_HSV2BGR), cv2.COLOR_BGR2GRAY)\n",
      "159/553: plt.imshow(normalize(np.array(faces_t)[b_inds[1]]), cmap='gray')\n",
      "159/554: plt.imshow(normalize(np.array(faces_t)[b_inds[2]]), cmap='gray')\n",
      "159/555: plt.imshow(np.array(faces_t)[b_inds[2]], cmap='gray')\n",
      "159/556:\n",
      "def normalize(img):\n",
      "    hsv = cv2.cvtColor(cv2.cvtColor(img, cv2.COLOR_GRAY2BGR), cv2.COLOR_BGR2HSV)\n",
      "    print(np.mean(hsv[:,:,2]))\n",
      "    hsv = hsv.astype(np.int32)\n",
      "    _, _, v = cv2.split(hsv)\n",
      "    m, s = np.mean(hsv[:,:,2]), np.std(hsv[:,:,2])\n",
      "    hsv[:,:,2] = 125+47.5*(hsv[:,:,2]-m)/s\n",
      "    hsv[hsv>255] = 255\n",
      "    hsv[hsv<0] = 0\n",
      "    print(np.mean(hsv[:,:,2]))\n",
      "    return cv2.cvtColor(hsv.astype(np.uint8), cv2.COLOR_HSV2GRAY)\n",
      "159/557: plt.imshow(normalize(np.array(faces_t)[b_inds[2]]), cmap='gray')\n",
      "159/558:\n",
      "def normalize(img):\n",
      "    hsv = cv2.cvtColor(cv2.cvtColor(img, cv2.COLOR_GRAY2BGR), cv2.COLOR_BGR2HSV)\n",
      "    print(np.mean(hsv[:,:,2]))\n",
      "    hsv = hsv.astype(np.int32)\n",
      "    _, _, v = cv2.split(hsv)\n",
      "    m, s = np.mean(hsv[:,:,2]), np.std(hsv[:,:,2])\n",
      "    hsv[:,:,2] = 125+47.5*(hsv[:,:,2]-m)/s\n",
      "    hsv[hsv>255] = 255\n",
      "    hsv[hsv<0] = 0\n",
      "    print(np.mean(hsv[:,:,2]))\n",
      "    return cv2.cvtColor(hsv.astype(np.uint8), cv2.COLOR_HSV2BGR)\n",
      "159/559: plt.imshow(normalize(np.array(faces_t)[b_inds[2]]), cmap='gray')\n",
      "159/560: cv2.cvtColor(hsv.astype(np.uint8), cv2.COLOR_HSV2BGR)\n",
      "159/561: plt.imshow(normalize(np.array(faces_t)[b_inds[10]]), cmap='gray')\n",
      "159/562: plt.imshow(np.array(faces_t)[b_inds[10]], cmap='gray')\n",
      "159/563:\n",
      "def normalize(img):\n",
      "    hsv = cv2.cvtColor(cv2.cvtColor(img, cv2.COLOR_GRAY2BGR), cv2.COLOR_BGR2HSV)\n",
      "    print(np.mean(hsv[:,:,2]))\n",
      "    hsv = hsv.astype(np.int32)\n",
      "    _, _, v = cv2.split(hsv)\n",
      "    m, s = np.mean(hsv[:,:,2]), np.std(hsv[:,:,2])\n",
      "    hsv[:,:,2] = 125+25*(hsv[:,:,2]-m)/s\n",
      "    hsv[hsv>255] = 255\n",
      "    hsv[hsv<0] = 0\n",
      "    print(np.mean(hsv[:,:,2]))\n",
      "    return cv2.cvtColor(hsv.astype(np.uint8), cv2.COLOR_HSV2BGR)\n",
      "159/564: plt.imshow(normalize(np.array(faces_t)[b_inds[10]]), cmap='gray')\n",
      "159/565: plt.imshow(np.array(faces_t)[b_inds[10]], cmap='gray')\n",
      "159/566: plt.imshow(normalize(np.array(faces_t)[w_inds[10]]), cmap='gray')\n",
      "159/567: plt.imshow(np.array(faces_t)[w_inds[10]], cmap='gray')\n",
      "159/568: plt.imshow(normalize(np.array(faces_t)[w_inds[0]]), cmap='gray')\n",
      "159/569: plt.imshow(np.array(faces_t)[w_inds[0]], cmap='gray')\n",
      "159/570:\n",
      "plt.subplot(2,1,1)\n",
      "plt.imshow(np.array(faces_t)[w_inds[0]], cmap='gray')\n",
      "plt.subplot(2,1,2)\n",
      "plt.imshow(normalize(np.array(faces_t)[w_inds[0]]), cmap='gray')\n",
      "159/571:\n",
      "plt.subplot(1,2,1)\n",
      "plt.imshow(np.array(faces_t)[w_inds[0]], cmap='gray')\n",
      "plt.subplot(1.2,2)\n",
      "plt.imshow(normalize(np.array(faces_t)[w_inds[0]]), cmap='gray')\n",
      "159/572:\n",
      "plt.subplot(1,2,1)\n",
      "plt.imshow(np.array(faces_t)[w_inds[0]], cmap='gray')\n",
      "plt.subplot(1,2,2)\n",
      "plt.imshow(normalize(np.array(faces_t)[w_inds[0]]), cmap='gray')\n",
      "159/573:\n",
      "plt.subplot(1,2,1)\n",
      "plt.imshow(np.array(faces_t)[b_inds[0]], cmap='gray')\n",
      "plt.subplot(1,2,2)\n",
      "plt.imshow(normalize(np.array(faces_t)[b_inds[0]]), cmap='gray')\n",
      "159/574:\n",
      "def normalize(img):\n",
      "    hsv = cv2.cvtColor(cv2.cvtColor(img, cv2.COLOR_GRAY2BGR), cv2.COLOR_BGR2HSV)\n",
      "    print(np.mean(hsv[:,:,2]))\n",
      "    hsv = hsv.astype(np.int32)\n",
      "    _, _, v = cv2.split(hsv)\n",
      "    m, s = np.mean(hsv[:,:,2]), np.std(hsv[:,:,2])\n",
      "    hsv[:,:,2] = 130+25*(hsv[:,:,2]-m)/s\n",
      "    hsv[hsv>255] = 255\n",
      "    hsv[hsv<0] = 0\n",
      "    print(np.mean(hsv[:,:,2]))\n",
      "    return cv2.cvtColor(hsv.astype(np.uint8), cv2.COLOR_HSV2BGR)\n",
      "159/575:\n",
      "plt.subplot(1,2,1)\n",
      "plt.imshow(np.array(faces_t)[b_inds[0]], cmap='gray')\n",
      "plt.subplot(1,2,2)\n",
      "plt.imshow(normalize(np.array(faces_t)[b_inds[0]]), cmap='gray')\n",
      "159/576:\n",
      "plt.subplot(1,2,1)\n",
      "plt.imshow(np.array(faces_t)[w_inds[0]], cmap='gray')\n",
      "plt.subplot(1,2,2)\n",
      "plt.imshow(normalize(np.array(faces_t)[w_inds[0]]), cmap='gray')\n",
      "159/577:\n",
      "def normalize(img):\n",
      "    hsv = cv2.cvtColor(cv2.cvtColor(img, cv2.COLOR_GRAY2BGR), cv2.COLOR_BGR2HSV)\n",
      "    print(np.mean(hsv[:,:,2]))\n",
      "    hsv = hsv.astype(np.int32)\n",
      "    _, _, v = cv2.split(hsv)\n",
      "    m, s = np.mean(hsv[:,:,2]), np.std(hsv[:,:,2])\n",
      "    hsv[:,:,2] = 135+30*(hsv[:,:,2]-m)/s\n",
      "    hsv[hsv>255] = 255\n",
      "    hsv[hsv<0] = 0\n",
      "    print(np.mean(hsv[:,:,2]))\n",
      "    return cv2.cvtColor(hsv.astype(np.uint8), cv2.COLOR_HSV2BGR)\n",
      "159/578:\n",
      "plt.subplot(1,2,1)\n",
      "plt.imshow(np.array(faces_t)[b_inds[0]], cmap='gray')\n",
      "plt.subplot(1,2,2)\n",
      "plt.imshow(normalize(np.array(faces_t)[b_inds[0]]), cmap='gray')\n",
      "159/579:\n",
      "plt.subplot(1,2,1)\n",
      "plt.imshow(np.array(faces_t)[w_inds[0]], cmap='gray')\n",
      "plt.subplot(1,2,2)\n",
      "plt.imshow(normalize(np.array(faces_t)[w_inds[0]]), cmap='gray')\n",
      "159/580:\n",
      "plt.subplot(1,2,1)\n",
      "plt.imshow(np.array(faces_t)[b_inds[1]], cmap='gray')\n",
      "plt.subplot(1,2,2)\n",
      "plt.imshow(normalize(np.array(faces_t)[b_inds[1]]), cmap='gray')\n",
      "159/581:\n",
      "plt.subplot(1,2,1)\n",
      "plt.imshow(np.array(faces_t)[b_inds[6]], cmap='gray')\n",
      "plt.subplot(1,2,2)\n",
      "plt.imshow(normalize(np.array(faces_t)[b_inds[6]]), cmap='gray')\n",
      "159/582:\n",
      "plt.subplot(1,2,1)\n",
      "plt.imshow(np.array(faces_t)[b_inds[2]], cmap='gray')\n",
      "plt.subplot(1,2,2)\n",
      "plt.imshow(normalize(np.array(faces_t)[b_inds[2]]), cmap='gray')\n",
      "159/583:\n",
      "def normalize(img):\n",
      "    hsv = cv2.cvtColor(cv2.cvtColor(img, cv2.COLOR_GRAY2BGR), cv2.COLOR_BGR2HSV)\n",
      "    print(np.mean(hsv[:,:,2]))\n",
      "    hsv = hsv.astype(np.int32)\n",
      "    _, _, v = cv2.split(hsv)\n",
      "    m, s = np.mean(hsv[:,:,2]), np.std(hsv[:,:,2])\n",
      "    hsv[:,:,2] = 140+35*(hsv[:,:,2]-m)/s\n",
      "    hsv[hsv>255] = 255\n",
      "    hsv[hsv<0] = 0\n",
      "    print(np.mean(hsv[:,:,2]))\n",
      "    return cv2.cvtColor(hsv.astype(np.uint8), cv2.COLOR_HSV2BGR)\n",
      "159/584:\n",
      "plt.subplot(1,2,1)\n",
      "plt.imshow(np.array(faces_t)[b_inds[2]], cmap='gray')\n",
      "plt.subplot(1,2,2)\n",
      "plt.imshow(normalize(np.array(faces_t)[b_inds[2]]), cmap='gray')\n",
      "159/585:\n",
      "def normalize(img):\n",
      "    hsv = cv2.cvtColor(cv2.cvtColor(img, cv2.COLOR_GRAY2BGR), cv2.COLOR_BGR2HSV)\n",
      "    print(np.mean(hsv[:,:,2]))\n",
      "    hsv = hsv.astype(np.int32)\n",
      "    _, _, v = cv2.split(hsv)\n",
      "    m, s = np.mean(hsv[:,:,2]), np.std(hsv[:,:,2])\n",
      "    hsv[:,:,2] = 150+35*(hsv[:,:,2]-m)/s\n",
      "    hsv[hsv>255] = 255\n",
      "    hsv[hsv<0] = 0\n",
      "    print(np.mean(hsv[:,:,2]))\n",
      "    return cv2.cvtColor(hsv.astype(np.uint8), cv2.COLOR_HSV2BGR)\n",
      "159/586:\n",
      "plt.subplot(1,2,1)\n",
      "plt.imshow(np.array(faces_t)[b_inds[2]], cmap='gray')\n",
      "plt.subplot(1,2,2)\n",
      "plt.imshow(normalize(np.array(faces_t)[b_inds[2]]), cmap='gray')\n",
      "159/587:\n",
      "plt.subplot(1,2,1)\n",
      "plt.imshow(np.array(faces_t)[w_inds[0]], cmap='gray')\n",
      "plt.subplot(1,2,2)\n",
      "plt.imshow(normalize(np.array(faces_t)[w_inds[0]]), cmap='gray')\n",
      "159/588:\n",
      "def normalize(img):\n",
      "    hsv = cv2.cvtColor(cv2.cvtColor(img, cv2.COLOR_GRAY2BGR), cv2.COLOR_BGR2HSV)\n",
      "    print(np.mean(hsv[:,:,2]))\n",
      "    hsv = hsv.astype(np.int32)\n",
      "    _, _, v = cv2.split(hsv)\n",
      "    m, s = np.mean(hsv[:,:,2]), np.std(hsv[:,:,2])\n",
      "    hsv[:,:,2] = 125+47*(hsv[:,:,2]-m)/s\n",
      "    hsv[hsv>255] = 255\n",
      "    hsv[hsv<0] = 0\n",
      "    print(np.mean(hsv[:,:,2]))\n",
      "    return cv2.cvtColor(hsv.astype(np.uint8), cv2.COLOR_HSV2BGR)\n",
      "159/589:\n",
      "plt.subplot(1,2,1)\n",
      "plt.imshow(np.array(faces_t)[b_inds[2]], cmap='gray')\n",
      "plt.subplot(1,2,2)\n",
      "plt.imshow(normalize(np.array(faces_t)[b_inds[2]]), cmap='gray')\n",
      "159/590:\n",
      "plt.subplot(1,2,1)\n",
      "plt.imshow(np.array(faces_t)[w_inds[0]], cmap='gray')\n",
      "plt.subplot(1,2,2)\n",
      "plt.imshow(normalize(np.array(faces_t)[w_inds[0]]), cmap='gray')\n",
      "159/591:\n",
      "def normalize(img):\n",
      "    hsv = cv2.cvtColor(cv2.cvtColor(img, cv2.COLOR_GRAY2BGR), cv2.COLOR_BGR2HSV)\n",
      "    print(np.mean(hsv[:,:,2]))\n",
      "    hsv = hsv.astype(np.int32)\n",
      "    _, _, v = cv2.split(hsv)\n",
      "    m, s = np.mean(hsv[:,:,2]), np.std(hsv[:,:,2])\n",
      "    hsv[:,:,2] = 125+47*(hsv[:,:,2]-m)/s\n",
      "    hsv[hsv>255] = 255\n",
      "    hsv[hsv<0] = 0\n",
      "    print(np.mean(hsv[:,:,2]))\n",
      "    return cv2.cvtColor(cv2.cvtColor(hsv.astype(np.uint8), cv2.COLOR_HSV2BGR), cv2.COLOR_BGR2GRAY))\n",
      "159/592:\n",
      "def normalize(img):\n",
      "    hsv = cv2.cvtColor(cv2.cvtColor(img, cv2.COLOR_GRAY2BGR), cv2.COLOR_BGR2HSV)\n",
      "    print(np.mean(hsv[:,:,2]))\n",
      "    hsv = hsv.astype(np.int32)\n",
      "    _, _, v = cv2.split(hsv)\n",
      "    m, s = np.mean(hsv[:,:,2]), np.std(hsv[:,:,2])\n",
      "    hsv[:,:,2] = 125+47*(hsv[:,:,2]-m)/s\n",
      "    hsv[hsv>255] = 255\n",
      "    hsv[hsv<0] = 0\n",
      "    print(np.mean(hsv[:,:,2]))\n",
      "    return cv2.cvtColor(cv2.cvtColor(hsv.astype(np.uint8), cv2.COLOR_HSV2BGR), cv2.COLOR_BGR2GRAY)\n",
      "159/593:\n",
      "plt.subplot(1,2,1)\n",
      "plt.imshow(np.array(faces_t)[b_inds[2]], cmap='gray')\n",
      "plt.subplot(1,2,2)\n",
      "plt.imshow(normalize(np.array(faces_t)[b_inds[2]]), cmap='gray')\n",
      "159/594:\n",
      "plt.subplot(1,2,1)\n",
      "plt.imshow(np.array(faces_t)[w_inds[0]], cmap='gray')\n",
      "plt.subplot(1,2,2)\n",
      "plt.imshow(normalize(np.array(faces_t)[w_inds[0]]), cmap='gray')\n",
      "159/595:\n",
      "def normalize(img):\n",
      "    hsv = cv2.cvtColor(cv2.cvtColor(img, cv2.COLOR_GRAY2BGR), cv2.COLOR_BGR2HSV)\n",
      "    print(np.mean(hsv[:,:,2]))\n",
      "    hsv = hsv.astype(np.int32)\n",
      "    _, _, v = cv2.split(hsv)\n",
      "    m, s = np.mean(hsv[:,:,2]), np.std(hsv[:,:,2])\n",
      "    hsv[:,:,2] = 138+47*(hsv[:,:,2]-m)/s\n",
      "    hsv[hsv>255] = 255\n",
      "    hsv[hsv<0] = 0\n",
      "    print(np.mean(hsv[:,:,2]))\n",
      "    return cv2.cvtColor(cv2.cvtColor(hsv.astype(np.uint8), cv2.COLOR_HSV2BGR), cv2.COLOR_BGR2GRAY)\n",
      "159/596:\n",
      "plt.subplot(1,2,1)\n",
      "plt.imshow(np.array(faces_t)[b_inds[2]], cmap='gray')\n",
      "plt.subplot(1,2,2)\n",
      "plt.imshow(normalize(np.array(faces_t)[b_inds[2]]), cmap='gray')\n",
      "159/597:\n",
      "plt.subplot(1,2,1)\n",
      "plt.imshow(np.array(faces_t)[w_inds[0]], cmap='gray')\n",
      "plt.subplot(1,2,2)\n",
      "plt.imshow(normalize(np.array(faces_t)[w_inds[0]]), cmap='gray')\n",
      "159/598:\n",
      "def normalize(img):\n",
      "    hsv = cv2.cvtColor(cv2.cvtColor(img, cv2.COLOR_GRAY2BGR), cv2.COLOR_BGR2HSV)\n",
      "    print(np.mean(hsv[:,:,2]))\n",
      "    hsv = hsv.astype(np.int32)\n",
      "    _, _, v = cv2.split(hsv)\n",
      "    m, s = np.mean(hsv[:,:,2]), np.std(hsv[:,:,2])\n",
      "    hsv[:,:,2] = 138+37*(hsv[:,:,2]-m)/s\n",
      "    hsv[hsv>255] = 255\n",
      "    hsv[hsv<0] = 0\n",
      "    print(np.mean(hsv[:,:,2]))\n",
      "    return cv2.cvtColor(cv2.cvtColor(hsv.astype(np.uint8), cv2.COLOR_HSV2BGR), cv2.COLOR_BGR2GRAY)\n",
      "159/599:\n",
      "plt.subplot(1,2,1)\n",
      "plt.imshow(np.array(faces_t)[b_inds[2]], cmap='gray')\n",
      "plt.subplot(1,2,2)\n",
      "plt.imshow(normalize(np.array(faces_t)[b_inds[2]]), cmap='gray')\n",
      "159/600:\n",
      "plt.subplot(1,2,1)\n",
      "plt.imshow(np.array(faces_t)[w_inds[0]], cmap='gray')\n",
      "plt.subplot(1,2,2)\n",
      "plt.imshow(normalize(np.array(faces_t)[w_inds[0]]), cmap='gray')\n",
      "159/601:\n",
      "def normalize(img):\n",
      "    hsv = cv2.cvtColor(cv2.cvtColor(img, cv2.COLOR_GRAY2BGR), cv2.COLOR_BGR2HSV)\n",
      "    print(np.mean(hsv[:,:,2]))\n",
      "    hsv = hsv.astype(np.int32)\n",
      "    _, _, v = cv2.split(hsv)\n",
      "    m, s = np.mean(hsv[:,:,2]), np.std(hsv[:,:,2])\n",
      "    hsv[:,:,2] = 200+37*(hsv[:,:,2]-m)/s\n",
      "    hsv[hsv>255] = 255\n",
      "    hsv[hsv<0] = 0\n",
      "    print(np.mean(hsv[:,:,2]))\n",
      "    return cv2.cvtColor(cv2.cvtColor(hsv.astype(np.uint8), cv2.COLOR_HSV2BGR), cv2.COLOR_BGR2GRAY)\n",
      "159/602:\n",
      "plt.subplot(1,2,1)\n",
      "plt.imshow(np.array(faces_t)[b_inds[2]], cmap='gray')\n",
      "plt.subplot(1,2,2)\n",
      "plt.imshow(normalize(np.array(faces_t)[b_inds[2]]), cmap='gray')\n",
      "159/603:\n",
      "def normalize(img):\n",
      "    hsv = cv2.cvtColor(cv2.cvtColor(img, cv2.COLOR_GRAY2BGR), cv2.COLOR_BGR2HSV)\n",
      "    print(np.mean(hsv[:,:,2]))\n",
      "    hsv = hsv.astype(np.int32)\n",
      "    _, _, v = cv2.split(hsv)\n",
      "    m, s = np.mean(hsv[:,:,2]), np.std(hsv[:,:,2])\n",
      "    hsv[:,:,2] = 250+37*(hsv[:,:,2]-m)/s\n",
      "    hsv[hsv>255] = 255\n",
      "    hsv[hsv<0] = 0\n",
      "    print(np.mean(hsv[:,:,2]))\n",
      "    return cv2.cvtColor(cv2.cvtColor(hsv.astype(np.uint8), cv2.COLOR_HSV2BGR), cv2.COLOR_BGR2GRAY)\n",
      "159/604:\n",
      "plt.subplot(1,2,1)\n",
      "plt.imshow(np.array(faces_t)[b_inds[2]], cmap='gray')\n",
      "plt.subplot(1,2,2)\n",
      "plt.imshow(normalize(np.array(faces_t)[b_inds[2]]), cmap='gray')\n",
      "159/605:\n",
      "def normalize(img):\n",
      "    hsv = cv2.cvtColor(cv2.cvtColor(img, cv2.COLOR_GRAY2BGR), cv2.COLOR_BGR2HSV)\n",
      "    print(np.mean(hsv[:,:,2]))\n",
      "    hsv = hsv.astype(np.int32)\n",
      "    _, _, v = cv2.split(hsv)\n",
      "    m, s = np.mean(hsv[:,:,2]), np.std(hsv[:,:,2])\n",
      "    hsv[:,:,2] = 100+37*(hsv[:,:,2]-m)/s\n",
      "    hsv[hsv>255] = 255\n",
      "    hsv[hsv<0] = 0\n",
      "    print(np.mean(hsv[:,:,2]))\n",
      "    return cv2.cvtColor(cv2.cvtColor(hsv.astype(np.uint8), cv2.COLOR_HSV2BGR), cv2.COLOR_BGR2GRAY)\n",
      "159/606:\n",
      "plt.subplot(1,2,1)\n",
      "plt.imshow(np.array(faces_t)[b_inds[2]], cmap='gray')\n",
      "plt.subplot(1,2,2)\n",
      "plt.imshow(normalize(np.array(faces_t)[b_inds[2]]), cmap='gray')\n",
      "159/607:\n",
      "plt.subplot(1,2,1)\n",
      "plt.imshow(np.array(faces_t)[w_inds[0]], cmap='gray')\n",
      "plt.subplot(1,2,2)\n",
      "plt.imshow(normalize(np.array(faces_t)[w_inds[0]]), cmap='gray')\n",
      "159/608: sum(hsv[:,:,0])\n",
      "159/609: sum(hsv[:,:,1])\n",
      "159/610: plt.imshow(hsv)\n",
      "159/611: plt.imshow(hsv, cmap='grau')\n",
      "159/612: plt.imshow(hsv, cmap='gray')\n",
      "159/613: plt.imshow(hsv[:,:,2], cmap='gray')\n",
      "159/614: img\n",
      "159/615:\n",
      "def normalize(img):\n",
      "    img = img.astype(np.int32)\n",
      "    m, s = np.mean(img), np.std(img)\n",
      "    img = 100+37*(img-m)/s\n",
      "    img[img>255] = 255\n",
      "    img[img<0] = 0\n",
      "    print(np.mean(img[:,:,2]))\n",
      "    return img\n",
      "159/616:\n",
      "plt.subplot(1,2,1)\n",
      "plt.imshow(np.array(faces_t)[b_inds[2]], cmap='gray')\n",
      "plt.subplot(1,2,2)\n",
      "plt.imshow(normalize(np.array(faces_t)[b_inds[2]]), cmap='gray')\n",
      "159/617:\n",
      "def normalize(img):\n",
      "    img = img.astype(np.int32)\n",
      "    m, s = np.mean(img), np.std(img)\n",
      "    img = 100+37*(img-m)/s\n",
      "    img[img>255] = 255\n",
      "    img[img<0] = 0\n",
      "    print(np.mean(img))\n",
      "    return img\n",
      "159/618:\n",
      "plt.subplot(1,2,1)\n",
      "plt.imshow(np.array(faces_t)[b_inds[2]], cmap='gray')\n",
      "plt.subplot(1,2,2)\n",
      "plt.imshow(normalize(np.array(faces_t)[b_inds[2]]), cmap='gray')\n",
      "159/619:\n",
      "plt.subplot(1,2,1)\n",
      "plt.imshow(np.array(faces_t)[w_inds[0]], cmap='gray')\n",
      "plt.subplot(1,2,2)\n",
      "plt.imshow(normalize(np.array(faces_t)[w_inds[0]]), cmap='gray')\n",
      "159/620:\n",
      "def normalize(img):\n",
      "    img = img.astype(np.int32)\n",
      "    m, s = np.mean(img), np.std(img)\n",
      "    img = 125+30*(img-m)/s\n",
      "    img[img>255] = 255\n",
      "    img[img<0] = 0\n",
      "    print(np.mean(img))\n",
      "    return img\n",
      "159/621:\n",
      "plt.subplot(1,2,1)\n",
      "plt.imshow(np.array(faces_t)[b_inds[2]], cmap='gray')\n",
      "plt.subplot(1,2,2)\n",
      "plt.imshow(normalize(np.array(faces_t)[b_inds[2]]), cmap='gray')\n",
      "159/622:\n",
      "plt.subplot(1,2,1)\n",
      "plt.imshow(np.array(faces_t)[w_inds[0]], cmap='gray')\n",
      "plt.subplot(1,2,2)\n",
      "plt.imshow(normalize(np.array(faces_t)[w_inds[0]]), cmap='gray')\n",
      "159/623:\n",
      "def normalize(img):\n",
      "    img = img.astype(np.int32)\n",
      "    print(np.mean(img))\n",
      "    print(img)\n",
      "    m, s = np.mean(img), np.std(img)\n",
      "    img = 125+30*(img-m)/s\n",
      "    img[img>255] = 255\n",
      "    img[img<0] = 0\n",
      "    print(np.mean(img))\n",
      "    print(img)\n",
      "    return img\n",
      "159/624:\n",
      "plt.subplot(1,2,1)\n",
      "plt.imshow(np.array(faces_t)[b_inds[2]], cmap='gray')\n",
      "plt.subplot(1,2,2)\n",
      "plt.imshow(normalize(np.array(faces_t)[b_inds[2]]), cmap='gray')\n",
      "159/625:\n",
      "def normalize(img):\n",
      "    img = img.astype(np.int32)\n",
      "    print(np.mean(img))\n",
      "    print(img)\n",
      "    m, s = np.mean(img), np.std(img)\n",
      "    img = 125+10*(img-m)/s\n",
      "    img[img>255] = 255\n",
      "    img[img<0] = 0\n",
      "    print(np.mean(img))\n",
      "    print(img)\n",
      "    return img\n",
      "159/626:\n",
      "def normalize(img):\n",
      "    img = img.astype(np.int32)\n",
      "    print(np.mean(img))\n",
      "    m, s = np.mean(img), np.std(img)\n",
      "    img = 125+10*(img-m)/s\n",
      "    img[img>255] = 255\n",
      "    img[img<0] = 0\n",
      "    print(np.mean(img))\n",
      "    return img\n",
      "159/627:\n",
      "plt.subplot(1,2,1)\n",
      "plt.imshow(np.array(faces_t)[b_inds[2]], cmap='gray')\n",
      "plt.subplot(1,2,2)\n",
      "plt.imshow(normalize(np.array(faces_t)[b_inds[2]]), cmap='gray')\n",
      "159/628:\n",
      "plt.subplot(1,2,1)\n",
      "plt.imshow(np.array(faces_t)[w_inds[0]], cmap='gray')\n",
      "plt.subplot(1,2,2)\n",
      "plt.imshow(normalize(np.array(faces_t)[w_inds[0]]), cmap='gray')\n",
      "159/629:\n",
      "def normalize(img):\n",
      "    img = img.astype(np.int32)\n",
      "    print(np.mean(img))\n",
      "    m, s = np.mean(img), np.std(img)\n",
      "    img = 125+0*(img-m)/s\n",
      "    img[img>255] = 255\n",
      "    img[img<0] = 0\n",
      "    print(np.mean(img))\n",
      "    return img\n",
      "159/630:\n",
      "plt.subplot(1,2,1)\n",
      "plt.imshow(np.array(faces_t)[b_inds[2]], cmap='gray')\n",
      "plt.subplot(1,2,2)\n",
      "plt.imshow(normalize(np.array(faces_t)[b_inds[2]]), cmap='gray')\n",
      "159/631:\n",
      "def normalize(img):\n",
      "    img = img.astype(np.int32)\n",
      "    print(np.mean(img))\n",
      "    m, s = np.mean(img), np.std(img)\n",
      "    img = 125+10*(img-m)/s\n",
      "    img[img>255] = 255\n",
      "    img[img<0] = 0\n",
      "    print(np.mean(img))\n",
      "    return img\n",
      "159/632:\n",
      "plt.subplot(1,2,1)\n",
      "plt.imshow(np.array(faces_t)[b_inds[2]], cmap='gray')\n",
      "plt.subplot(1,2,2)\n",
      "plt.imshow(normalize(np.array(faces_t)[b_inds[2]]), cmap='gray')\n",
      "159/633:\n",
      "def normalize(img):\n",
      "    img = img.astype(np.int32)\n",
      "    print(np.mean(img))\n",
      "    m, s = np.mean(img), np.std(img)\n",
      "    img = 125+5*(img-m)/s\n",
      "    img[img>255] = 255\n",
      "    img[img<0] = 0\n",
      "    print(np.mean(img))\n",
      "    return img\n",
      "159/634:\n",
      "plt.subplot(1,2,1)\n",
      "plt.imshow(np.array(faces_t)[b_inds[2]], cmap='gray')\n",
      "plt.subplot(1,2,2)\n",
      "plt.imshow(normalize(np.array(faces_t)[b_inds[2]]), cmap='gray')\n",
      "159/635:\n",
      "def normalize(img):\n",
      "    img = img.astype(np.int32)\n",
      "    print(np.mean(img))\n",
      "    m, s = np.mean(img), np.std(img)\n",
      "    img = 125+1*(img-m)/s\n",
      "    img[img>255] = 255\n",
      "    img[img<0] = 0\n",
      "    print(np.mean(img))\n",
      "    return img\n",
      "159/636:\n",
      "plt.subplot(1,2,1)\n",
      "plt.imshow(np.array(faces_t)[b_inds[2]], cmap='gray')\n",
      "plt.subplot(1,2,2)\n",
      "plt.imshow(normalize(np.array(faces_t)[b_inds[2]]), cmap='gray')\n",
      "159/637: plt.imshow(np.array(faces_t)[b_inds[2]]+50, cmap='gray')\n",
      "159/638: plt.imshow(np.array(faces_t)[b_inds[2]]+10, cmap='gray')\n",
      "159/639:\n",
      "def normalize(img):\n",
      "    img = img.astype(np.int32)\n",
      "    print(np.mean(img))\n",
      "    m, s = np.mean(img), np.std(img)\n",
      "    img = 125+1*(img-m)/s\n",
      "#     img[img>255] = 255\n",
      "#     img[img<0] = 0\n",
      "    img = (img - np.min(img))/(np.max(img) - np.min(img))*255\n",
      "    print(np.mean(img))\n",
      "    return img\n",
      "159/640:\n",
      "plt.subplot(1,2,1)\n",
      "plt.imshow(np.array(faces_t)[b_inds[2]], cmap='gray')\n",
      "plt.subplot(1,2,2)\n",
      "plt.imshow(normalize(np.array(faces_t)[b_inds[2]]), cmap='gray')\n",
      "159/641:\n",
      "plt.subplot(1,2,1)\n",
      "plt.imshow(np.array(faces_t)[w_inds[0]], cmap='gray')\n",
      "plt.subplot(1,2,2)\n",
      "plt.imshow(normalize(np.array(faces_t)[w_inds[0]]), cmap='gray')\n",
      "159/642:\n",
      "def normalize(img):\n",
      "    img = img.astype(np.int32)\n",
      "    print(np.mean(img))\n",
      "    m, s = np.mean(img), np.std(img)\n",
      "    img = 125+1*(img-m)/s\n",
      "    img[img>255] = 255\n",
      "    img[img<0] = 0\n",
      "    print(np.mean(img))\n",
      "    return img\n",
      "159/643:\n",
      "plt.subplot(1,2,1)\n",
      "plt.imshow(np.array(faces_t)[b_inds[2]], cmap='gray')\n",
      "plt.subplot(1,2,2)\n",
      "plt.imshow(normalize(np.array(faces_t)[b_inds[2]]), cmap='gray')\n",
      "159/644:\n",
      "plt.subplot(1,2,1)\n",
      "plt.imshow(np.array(faces_t)[w_inds[0]], cmap='gray')\n",
      "plt.subplot(1,2,2)\n",
      "plt.imshow(normalize(np.array(faces_t)[w_inds[0]]), cmap='gray')\n",
      "159/645:\n",
      "def normalize(img):\n",
      "    img = img.astype(np.int32)\n",
      "    print(np.mean(img))\n",
      "    m, s = np.mean(img), np.std(img)\n",
      "    img = 125+47*(img-m)/s\n",
      "    img[img>255] = 255\n",
      "    img[img<0] = 0\n",
      "    print(np.mean(img))\n",
      "    return img\n",
      "159/646:\n",
      "plt.subplot(1,2,1)\n",
      "plt.imshow(np.array(faces_t)[b_inds[2]], cmap='gray')\n",
      "plt.subplot(1,2,2)\n",
      "plt.imshow(normalize(np.array(faces_t)[b_inds[2]]), cmap='gray')\n",
      "159/647:\n",
      "plt.subplot(1,2,1)\n",
      "plt.imshow(np.array(faces_t)[w_inds[0]], cmap='gray')\n",
      "plt.subplot(1,2,2)\n",
      "plt.imshow(normalize(np.array(faces_t)[w_inds[0]]), cmap='gray')\n",
      "159/648:\n",
      "def normalize(img):\n",
      "    img = img.astype(np.int32)\n",
      "    print(np.mean(img))\n",
      "    m, s = np.mean(img), np.std(img)\n",
      "    img[50:150,50:150] = 125+47*(img[50:150,50:150]-m)/s\n",
      "    img[img>255] = 255\n",
      "    img[img<0] = 0\n",
      "    print(np.mean(img))\n",
      "    return img\n",
      "159/649:\n",
      "plt.subplot(1,2,1)\n",
      "plt.imshow(np.array(faces_t)[b_inds[2]], cmap='gray')\n",
      "plt.subplot(1,2,2)\n",
      "plt.imshow(normalize(np.array(faces_t)[b_inds[2]]), cmap='gray')\n",
      "159/650:\n",
      "plt.subplot(1,2,1)\n",
      "plt.imshow(np.array(faces_t)[w_inds[0]], cmap='gray')\n",
      "plt.subplot(1,2,2)\n",
      "plt.imshow(normalize(np.array(faces_t)[w_inds[0]]), cmap='gray')\n",
      "159/651:\n",
      "def normalize(img):\n",
      "    img = img.astype(np.int32)\n",
      "    print(np.mean(img))\n",
      "    m, s = np.mean(img), np.std(img)\n",
      "    img = 125+47*(img-m)/s\n",
      "    img[img>255] = 255\n",
      "    img[img<0] = 0\n",
      "    print(np.mean(img))\n",
      "    return img\n",
      "159/652:\n",
      "plt.subplot(1,2,1)\n",
      "plt.imshow(np.array(faces_t)[b_inds[2]], cmap='gray')\n",
      "plt.subplot(1,2,2)\n",
      "plt.imshow(normalize(np.array(faces_t)[b_inds[2]]), cmap='gray')\n",
      "159/653:\n",
      "plt.subplot(1,2,1)\n",
      "plt.imshow(np.array(faces_t)[w_inds[0]], cmap='gray')\n",
      "plt.subplot(1,2,2)\n",
      "plt.imshow(normalize(np.array(faces_t)[w_inds[0]]), cmap='gray')\n",
      "159/654: plt.hist(stds_w)\n",
      "159/655:\n",
      "def pipeline(f):\n",
      "    def getx3(facefiles, n, nonfaces, feature_types, feature_coords):\n",
      "        ims = [f(i) for i in getimgs(facefiles, nonfaces, n)]\n",
      "        X = delayed(extract_feature_image(img, feature_types, feature_coords) for img in ims)\n",
      "        return np.array(X.compute(scheduler='threads'))\n",
      "    X0 = getx3(face_filenames0, n, nonfaces, feature_type_sel2, feature_coord_sel2)\n",
      "    X25 = getx3(face_filenames25, n, nonfaces, feature_type_sel2, feature_coord_sel2)\n",
      "    X50 = getx3(face_filenames50, n, nonfaces, feature_type_sel2, feature_coord_sel2)\n",
      "    X75 = getx3(face_filenames75, n, nonfaces, feature_type_sel2, feature_coord_sel2)\n",
      "    X100 = getx3(face_filenames100, n, nonfaces, feature_type_sel2, feature_coord_sel2)\n",
      "    Xt = getx3(face_filenames0, 300, nonfaces_t, feature_type_sel2, feature_coord_sel2)\n",
      "    \n",
      "    \n",
      "    rf0 = RandomForestClassifier(n_estimators=1000, max_depth=None, n_jobs=-1, random_state=0)\n",
      "    rf0.fit(X0, y)\n",
      "    print(rf0.score(Xt_900, y_t))\n",
      "    rf25 = RandomForestClassifier(n_estimators=1000, max_depth=None, n_jobs=-1, random_state=0)\n",
      "    rf25.fit(X25, y)\n",
      "    print(rf25.score(Xt_900, y_t))\n",
      "    rf50 = RandomForestClassifier(n_estimators=1000, max_depth=None, n_jobs=-1, random_state=0)\n",
      "    rf50.fit(X50, y)\n",
      "    print(rf50.score(Xt_900, y_t))\n",
      "    rf75 = RandomForestClassifier(n_estimators=1000, max_depth=None, n_jobs=-1, random_state=0)\n",
      "    rf75.fit(X75, y)\n",
      "    print(rf75.score(Xt_900, y_t))\n",
      "    rf100 = RandomForestClassifier(n_estimators=1000, max_depth=None, n_jobs=-1, random_state=0)\n",
      "    rf100.fit(X100, y)\n",
      "    print(rf100.score(Xt_900, y_t))\n",
      "    \n",
      "    os_r, bs_r, ws_r = rfscores([rf0, rf25, rf50, rf75, rf100], Xt_900, Xb_t, Xw_t)\n",
      "    \n",
      "    plt.plot([0, 25, 50, 75, 100], os_r, label=\"o\")\n",
      "    plt.plot([0, 25, 50, 75, 100], bs_r, label=\"b\")\n",
      "    plt.plot([0, 25, 50, 75, 100], ws_r, label=\"w\")\n",
      "    plt.legend()\n",
      "    plt.show()\n",
      "159/656: pipeline(normalize)\n",
      "159/657:\n",
      "def normalize(img):\n",
      "    img = img.astype(np.int32)\n",
      "#     print(np.mean(img))\n",
      "    m, s = np.mean(img), np.std(img)\n",
      "    img = 125+47*(img-m)/s\n",
      "    img[img>255] = 255\n",
      "    img[img<0] = 0\n",
      "#     print(np.mean(img))\n",
      "    return img\n",
      "159/658:\n",
      "def normalize(img):\n",
      "    img = img.astype(np.int32)\n",
      "#     print(np.mean(img))\n",
      "    m, s = np.mean(img), np.std(img)\n",
      "    img = 125+47*(img-m)/s\n",
      "    img[img>255] = 255\n",
      "    img[img<0] = 0\n",
      "#     print(np.mean(img))\n",
      "    return img\n",
      "159/659: pipeline(normalize)\n",
      "159/660:\n",
      "def pipeline(f):\n",
      "    def getx3(facefiles, n, nonfaces, feature_types, feature_coords):\n",
      "        ims = [f(i) for i in getimgs(facefiles, nonfaces, n)]\n",
      "        X = delayed(extract_feature_image(img, feature_types, feature_coords) for img in ims)\n",
      "        return np.array(X.compute(scheduler='threads'))\n",
      "    X0 = getx3(face_filenames0, n, nonfaces, feature_type_sel2, feature_coord_sel2)\n",
      "    X25 = getx3(face_filenames25, n, nonfaces, feature_type_sel2, feature_coord_sel2)\n",
      "    X50 = getx3(face_filenames50, n, nonfaces, feature_type_sel2, feature_coord_sel2)\n",
      "    X75 = getx3(face_filenames75, n, nonfaces, feature_type_sel2, feature_coord_sel2)\n",
      "    X100 = getx3(face_filenames100, n, nonfaces, feature_type_sel2, feature_coord_sel2)\n",
      "    Xt = getx3(test_face_names, 300, nonfaces_t, feature_type_sel2, feature_coord_sel2)\n",
      "    \n",
      "    \n",
      "    rf0 = RandomForestClassifier(n_estimators=1000, max_depth=None, n_jobs=-1, random_state=0)\n",
      "    rf0.fit(X0, y)\n",
      "    print(rf0.score(Xt_900, y_t))\n",
      "    rf25 = RandomForestClassifier(n_estimators=1000, max_depth=None, n_jobs=-1, random_state=0)\n",
      "    rf25.fit(X25, y)\n",
      "    print(rf25.score(Xt_900, y_t))\n",
      "    rf50 = RandomForestClassifier(n_estimators=1000, max_depth=None, n_jobs=-1, random_state=0)\n",
      "    rf50.fit(X50, y)\n",
      "    print(rf50.score(Xt_900, y_t))\n",
      "    rf75 = RandomForestClassifier(n_estimators=1000, max_depth=None, n_jobs=-1, random_state=0)\n",
      "    rf75.fit(X75, y)\n",
      "    print(rf75.score(Xt_900, y_t))\n",
      "    rf100 = RandomForestClassifier(n_estimators=1000, max_depth=None, n_jobs=-1, random_state=0)\n",
      "    rf100.fit(X100, y)\n",
      "    print(rf100.score(Xt_900, y_t))\n",
      "    \n",
      "    os_r, bs_r, ws_r = rfscores([rf0, rf25, rf50, rf75, rf100], Xt_900, Xb_t, Xw_t)\n",
      "    \n",
      "    plt.plot([0, 25, 50, 75, 100], os_r, label=\"o\")\n",
      "    plt.plot([0, 25, 50, 75, 100], bs_r, label=\"b\")\n",
      "    plt.plot([0, 25, 50, 75, 100], ws_r, label=\"w\")\n",
      "    plt.legend()\n",
      "    plt.show()\n",
      "159/661:\n",
      "def pipeline(f):\n",
      "    def getx3(facefiles, n, nonfaces, feature_types, feature_coords):\n",
      "        ims = [f(i) for i in getimgs(facefiles, nonfaces, n)]\n",
      "        X = delayed(extract_feature_image(img, feature_types, feature_coords) for img in ims)\n",
      "        return np.array(X.compute(scheduler='threads'))\n",
      "    X0 = getx3(face_filenames0, n, nonfaces, feature_type_sel2, feature_coord_sel2)\n",
      "    print(X0.shape)\n",
      "    X25 = getx3(face_filenames25, n, nonfaces, feature_type_sel2, feature_coord_sel2)\n",
      "    print(X25.shape)\n",
      "    X50 = getx3(face_filenames50, n, nonfaces, feature_type_sel2, feature_coord_sel2)\n",
      "    print(X50.shape)\n",
      "    X75 = getx3(face_filenames75, n, nonfaces, feature_type_sel2, feature_coord_sel2)\n",
      "    print(X75.shape)\n",
      "    X100 = getx3(face_filenames100, n, nonfaces, feature_type_sel2, feature_coord_sel2)\n",
      "    print(X100.shape)\n",
      "    Xt = getx3(test_face_names, 300, nonfaces_t, feature_type_sel2, feature_coord_sel2)\n",
      "    print(Xt.shape)\n",
      "    \n",
      "    rf0 = RandomForestClassifier(n_estimators=1000, max_depth=None, n_jobs=-1, random_state=0)\n",
      "    rf0.fit(X0, y)\n",
      "    print(rf0.score(Xt_900, y_t))\n",
      "    rf25 = RandomForestClassifier(n_estimators=1000, max_depth=None, n_jobs=-1, random_state=0)\n",
      "    rf25.fit(X25, y)\n",
      "    print(rf25.score(Xt_900, y_t))\n",
      "    rf50 = RandomForestClassifier(n_estimators=1000, max_depth=None, n_jobs=-1, random_state=0)\n",
      "    rf50.fit(X50, y)\n",
      "    print(rf50.score(Xt_900, y_t))\n",
      "    rf75 = RandomForestClassifier(n_estimators=1000, max_depth=None, n_jobs=-1, random_state=0)\n",
      "    rf75.fit(X75, y)\n",
      "    print(rf75.score(Xt_900, y_t))\n",
      "    rf100 = RandomForestClassifier(n_estimators=1000, max_depth=None, n_jobs=-1, random_state=0)\n",
      "    rf100.fit(X100, y)\n",
      "    print(rf100.score(Xt_900, y_t))\n",
      "    \n",
      "    os_r, bs_r, ws_r = rfscores([rf0, rf25, rf50, rf75, rf100], Xt_900, Xb_t, Xw_t)\n",
      "    \n",
      "    plt.plot([0, 25, 50, 75, 100], os_r, label=\"o\")\n",
      "    plt.plot([0, 25, 50, 75, 100], bs_r, label=\"b\")\n",
      "    plt.plot([0, 25, 50, 75, 100], ws_r, label=\"w\")\n",
      "    plt.legend()\n",
      "    plt.show()\n",
      "159/662: pipeline(normalize)\n",
      "159/663:\n",
      "def pipeline(f):\n",
      "    def getx3(facefiles, n, nonfaces, feature_types, feature_coords):\n",
      "        ims = [f(i) for i in getimgs(facefiles, nonfaces, n)]\n",
      "        X = delayed(extract_feature_image(img, feature_types, feature_coords) for img in ims)\n",
      "        return np.array(X.compute(scheduler='threads'))\n",
      "    t_start = time()\n",
      "    X0 = getx3(face_filenames0, n, nonfaces, feature_type_sel2, feature_coord_sel2)\n",
      "    print(X0.shape)\n",
      "    X25 = getx3(face_filenames25, n, nonfaces, feature_type_sel2, feature_coord_sel2)\n",
      "    print(X25.shape)\n",
      "    X50 = getx3(face_filenames50, n, nonfaces, feature_type_sel2, feature_coord_sel2)\n",
      "    print(X50.shape)\n",
      "    X75 = getx3(face_filenames75, n, nonfaces, feature_type_sel2, feature_coord_sel2)\n",
      "    print(X75.shape)\n",
      "    X100 = getx3(face_filenames100, n, nonfaces, feature_type_sel2, feature_coord_sel2)\n",
      "    print(X100.shape)\n",
      "    Xt = getx3(test_face_names, 300, nonfaces_t, feature_type_sel2, feature_coord_sel2)\n",
      "    print(Xt.shape)\n",
      "    \n",
      "    rf0 = RandomForestClassifier(n_estimators=1000, max_depth=None, n_jobs=-1, random_state=0)\n",
      "    rf0.fit(X0, y)\n",
      "    print(rf0.score(Xt_900, y_t))\n",
      "    rf25 = RandomForestClassifier(n_estimators=1000, max_depth=None, n_jobs=-1, random_state=0)\n",
      "    rf25.fit(X25, y)\n",
      "    print(rf25.score(Xt_900, y_t))\n",
      "    rf50 = RandomForestClassifier(n_estimators=1000, max_depth=None, n_jobs=-1, random_state=0)\n",
      "    rf50.fit(X50, y)\n",
      "    print(rf50.score(Xt_900, y_t))\n",
      "    rf75 = RandomForestClassifier(n_estimators=1000, max_depth=None, n_jobs=-1, random_state=0)\n",
      "    rf75.fit(X75, y)\n",
      "    print(rf75.score(Xt_900, y_t))\n",
      "    rf100 = RandomForestClassifier(n_estimators=1000, max_depth=None, n_jobs=-1, random_state=0)\n",
      "    rf100.fit(X100, y)\n",
      "    print(rf100.score(Xt_900, y_t))\n",
      "    \n",
      "    os_r, bs_r, ws_r = rfscores([rf0, rf25, rf50, rf75, rf100], Xt_900, Xb_t, Xw_t)\n",
      "    \n",
      "    plt.plot([0, 25, 50, 75, 100], os_r, label=\"o\")\n",
      "    plt.plot([0, 25, 50, 75, 100], bs_r, label=\"b\")\n",
      "    plt.plot([0, 25, 50, 75, 100], ws_r, label=\"w\")\n",
      "    plt.legend()\n",
      "    plt.show()\n",
      "    \n",
      "    print(time()-t_start)\n",
      "159/664: pipeline(normalize)\n",
      "159/665: pipeline(lambda x: x)\n",
      "159/666: sharpen_kernel = np.array([0, -1, 0, -1, 5, -1, 0, -1, 0])\n",
      "159/667: im = np.array(faces_t)[b_inds[0]]\n",
      "159/668: im_sharp = ndimage.convolve(im, sharpen_kernel, mode='nearest')\n",
      "159/669: from scipy import ndimage\n",
      "159/670: im_sharp = ndimage.convolve(im, sharpen_kernel, mode='nearest')\n",
      "159/671: im_sharp = cv2.filter2D(src=im, kernel=sharpen_kernel)\n",
      "159/672: im_sharp = cv2.filter2D(src=im, ddepth=src.depth(), kernel=sharpen_kernel)\n",
      "159/673: im_sharp = cv2.filter2D(src=im, ddepth=im.depth(), kernel=sharpen_kernel)\n",
      "159/674: im_sharp = cv2.filter2D(src=im, ddepth=-1, kernel=sharpen_kernel)\n",
      "159/675: plt.imshow(im_sharp, cmap='gray')\n",
      "159/676:\n",
      "def sharpen(im):\n",
      "    return cv2.filter2D(src=im, ddepth=-1, kernel=sharpen_kernel)\n",
      "159/677: pipeline(sharpen)\n",
      "159/678: pipeline(sharpen)\n",
      "159/679:\n",
      "def filteredGradient(im, sigma):\n",
      "    def gprime(x,s):\n",
      "        return (-1/(np.sqrt(2*np.pi)*s**3))*x*np.exp(-x**2/(2*s**2))\n",
      "\n",
      "    # Gaussian of x, stdev s\n",
      "    def g(x,s):\n",
      "        return (1/(np.sqrt(2*np.pi)*s))*np.exp(-x**2/(2*s**2))\n",
      "\n",
      "    # First derivative of Gaussian - kernel with halfwidth halfw, stdev s\n",
      "    def gprimekernel(halfw,s):\n",
      "        k = [gprime(x,s) for x in np.linspace(-halfw, halfw, 2*halfw)]\n",
      "        return k\n",
      "\n",
      "    # Gaussian - kernel with halfwidth halfw, stdev s\n",
      "    def gkernel(halfw, s):\n",
      "        k = [g(x,s) for x in np.linspace(-halfw, halfw, 2*halfw)]\n",
      "        return k/np.sum(k)\n",
      "\n",
      "    halfw = 3*sigma\n",
      "\n",
      "    # create filters\n",
      "    x = np.expand_dims(gprimekernel(halfw,sigma),0)\n",
      "    xg = np.expand_dims(gkernel(halfw,sigma),0)\n",
      "    y = np.expand_dims(gprimekernel(halfw,sigma),1)\n",
      "    yg = np.expand_dims(gkernel(halfw,sigma),1)\n",
      "\n",
      "    # convolve\n",
      "    Fx = cv2.filter2D(im, -1, x)\n",
      "    Fx = cv2.filter2D(Fx, -1, xg)\n",
      "    Fy = cv2.filter2D(im, -1, y)\n",
      "    Fy = cv2.filter2D(Fy, -1, yg)\n",
      "\n",
      "    return Fx, Fy\n",
      "159/680:\n",
      "def gradient_strength(im):\n",
      "    Fx, Fy = filteredGradient(im, 1)\n",
      "    return np.array(np.sqrt(Fx**2 + Fy**2))\n",
      "159/681:\n",
      "def gradient_orientation(im):\n",
      "    Fx, Fy = filteredGradient(im, 1)\n",
      "    Fx = (np.isclose(Fx,0))*1e-14 + (1-np.isclose(Fx,0))*Fx\n",
      "    # get orientation in radians between 0 and pi\n",
      "    D = np.arctan(np.divide(Fy, Fx))\n",
      "    D = (D < 0)*(D+np.pi) + (D >= 0)*D\n",
      "    return D\n",
      "159/682: pipeline(gradient_strength)\n",
      "159/683: gradient_strength(im)\n",
      "159/684: plt.imshow(gradient_strength(im))\n",
      "159/685: np.min(gradient_strength(im))\n",
      "159/686: np.max(gradient_strength(im))\n",
      "159/687: from sklearn.preprocessing import MinMaxScaler\n",
      "159/688: MinMaxScaler((0,255)).fit_transform(gradient_strength(im))\n",
      "159/689: gradient_strength(im)\n",
      "159/690: MinMaxScaler((0,255)).fit_transform(gradient_strength(im)).astype(uint8)\n",
      "159/691: MinMaxScaler((0,255)).fit_transform(gradient_strength(im)).astype(np.uint8)\n",
      "159/692: imm = MinMaxScaler((0,255)).fit_transform(gradient_strength(im)).astype(np.uint8)\n",
      "159/693: plt.imshow(imm, cmap='gray')\n",
      "159/694:\n",
      "def transform(im):\n",
      "    return (255*(im - np.min(im))/(np.max(im)-np.min(im))).astype(np.uint8)\n",
      "159/695: plt.imshow(transform(im), cmap='gray')\n",
      "159/696:\n",
      "def transform(im):\n",
      "    return (255*(im - np.min(im, 0))/(np.max(im)-np.min(im))).astype(np.uint8)\n",
      "159/697: gradient_strength(im)\n",
      "159/698: plt.imshow(transform(im), cmap='gray')\n",
      "159/699:\n",
      "def transform(im):\n",
      "    return (255*(im - np.min(im, 0))/(np.max(im, 0)-np.min(im, 0))).astype(np.uint8)\n",
      "159/700: gradient_strength(im)\n",
      "159/701: plt.imshow(transform(im), cmap='gray')\n",
      "159/702:\n",
      "def transform(im):\n",
      "    return np.array(255*(im - np.min(im, 0))/(np.max(im, 0)-np.min(im, 0))).astype(np.uint8)\n",
      "159/703: gradient_strength(im)\n",
      "159/704: plt.imshow(transform(im), cmap='gray')\n",
      "159/705:\n",
      "def transform(im):\n",
      "    return np.array(255*(im - np.min(im, axis=0))/(np.max(im, 0)-np.min(im, 0))).astype(np.uint8)\n",
      "159/706: gradient_strength(im)\n",
      "159/707: plt.imshow(transform(im), cmap='gray')\n",
      "159/708:\n",
      "def transform(im):\n",
      "    return np.array(255*(im - np.min(im, axis=0))/(np.max(im, axis=0)-np.min(im, 0))).astype(np.uint8)\n",
      "159/709: gradient_strength(im)\n",
      "159/710: plt.imshow(transform(im), cmap='gray')\n",
      "159/711:\n",
      "def transform(im):\n",
      "    return np.array(255*(im - np.min(im, axis=0))/(np.max(im, axis=0)-np.min(im, axis=0))).astype(np.uint8)\n",
      "159/712: gradient_strength(im)\n",
      "159/713: plt.imshow(transform(im), cmap='gray')\n",
      "159/714:\n",
      "def transform(im):\n",
      "    return MinMaxScaler((0,255)).fit_transform(gradient_strength(im)).astype(np.uint8)\n",
      "159/715: gradient_strength(im)\n",
      "159/716: plt.imshow(transform(im), cmap='gray')\n",
      "159/717:\n",
      "def transform(im):\n",
      "    return MinMaxScaler((0,255)).fit_transform(gradient_direction(im)).astype(np.uint8)\n",
      "159/718: gradient_strength(im)\n",
      "159/719: plt.imshow(transform(im), cmap='gray')\n",
      "159/720:\n",
      "def transform(im):\n",
      "    return MinMaxScaler((0,255)).fit_transform(gradient_orientation(im)).astype(np.uint8)\n",
      "159/721: gradient_strength(im)\n",
      "159/722: plt.imshow(transform(im), cmap='gray')\n",
      "159/723:\n",
      "def transform(im, f):\n",
      "    return MinMaxScaler((0,255)).fit_transform(f(im)).astype(np.uint8)\n",
      "159/724:\n",
      "def transform(im, f):\n",
      "    return MinMaxScaler((0,255)).fit_transform(im).astype(np.uint8)\n",
      "159/725:\n",
      "def gradient_orientation(im):\n",
      "    Fx, Fy = filteredGradient(im, 1)\n",
      "    Fx = (np.isclose(Fx,0))*1e-14 + (1-np.isclose(Fx,0))*Fx\n",
      "    # get orientation in radians between 0 and pi\n",
      "    D = np.arctan(np.divide(Fy, Fx))\n",
      "    D = (D < 0)*(D+np.pi) + (D >= 0)*D\n",
      "    return transform(D)\n",
      "159/726:\n",
      "def gradient_strength(im):\n",
      "    Fx, Fy = filteredGradient(im, 1)\n",
      "    return transform(np.array(np.sqrt(Fx**2 + Fy**2)))\n",
      "159/727: pipeline(gradient_strength)\n",
      "159/728:\n",
      "def transform(im):\n",
      "    return MinMaxScaler((0,255)).fit_transform(im).astype(np.uint8)\n",
      "159/729:\n",
      "def gradient_strength(im):\n",
      "    Fx, Fy = filteredGradient(im, 1)\n",
      "    return transform(np.array(np.sqrt(Fx**2 + Fy**2)))\n",
      "159/730:\n",
      "def gradient_orientation(im):\n",
      "    Fx, Fy = filteredGradient(im, 1)\n",
      "    Fx = (np.isclose(Fx,0))*1e-14 + (1-np.isclose(Fx,0))*Fx\n",
      "    # get orientation in radians between 0 and pi\n",
      "    D = np.arctan(np.divide(Fy, Fx))\n",
      "    D = (D < 0)*(D+np.pi) + (D >= 0)*D\n",
      "    return transform(D)\n",
      "159/731: pipeline(gradient_strength)\n",
      "159/732: pipeline(gradient_orientation)\n",
      "159/733: plt.imshow(im)\n",
      "159/734: plt.imshow(gradient_strength(im))/\n",
      "159/735: plt.imshow(gradient_strength(im))\n",
      "159/736: skintones = [int(face_filenames50[i].split('_')[3]) for i in range(n)]\n",
      "159/737:\n",
      "rfskin = RandomForestClassifier(n_estimators=1000, max_depth=None, n_jobs=-1, random_state=0)\n",
      "rfskin.fit(X50[:4020], skintones)\n",
      "159/738: skintones_t = [int(test_face_names[i].split('_')[3]) for i in range(300)]]\n",
      "159/739: skintones_t = [int(test_face_names[i].split('_')[3]) for i in range(300)]\n",
      "159/740: test_face_names[2]\n",
      "159/741: face_filenames0[2]\n",
      "159/742: skintones_t = [int(test_face_names[i].split('_')[2]) for i in range(300)]\n",
      "159/743: rfskin.score(X_t[:300], skintones_t)\n",
      "159/744: rfskin.score(Xt_900[:300], skintones_t)\n",
      "159/745: gettop(test_face_names, rfskin)\n",
      "159/746: rf0.classes_\n",
      "159/747: predictions0 = rf0.predict(Xt_900)\n",
      "159/748:\n",
      "for i in range(300):\n",
      "    if predictions0[i] != yt_900[i]:\n",
      "        print i\n",
      "159/749:\n",
      "for i in range(300):\n",
      "    if predictions0[i] != yt_900[i]:\n",
      "        plt.imshow(test_face_names[i], cmap='gray')\n",
      "159/750:\n",
      "for i in range(300):\n",
      "    if predictions0[i] != yt_900[i]:\n",
      "        plt.imshow(cv2.imread(test_face_names[i], cv2.IMREAD_GRAYSCALE), cmap='gray')\n",
      "159/751:\n",
      "count = 0\n",
      "for i in range(300):\n",
      "    if predictions0[i] != yt_900[i]:\n",
      "        count += 1\n",
      "        plt.imshow(cv2.imread(test_face_names[i], cv2.IMREAD_GRAYSCALE), cmap='gray')\n",
      "159/752: count\n",
      "159/753:\n",
      "plt.subplots((4,4))\n",
      "for i in range(300):\n",
      "    if predictions0[i] != yt_900[i]:\n",
      "        count += 1\n",
      "        plt.imshow(cv2.imread(test_face_names[i], cv2.IMREAD_GRAYSCALE), cmap='gray')\n",
      "159/754:\n",
      "plt.subplots(4,4)\n",
      "for i in range(300):\n",
      "    if predictions0[i] != yt_900[i]:\n",
      "        count += 1\n",
      "        plt.imshow(cv2.imread(test_face_names[i], cv2.IMREAD_GRAYSCALE), cmap='gray')\n",
      "159/755:\n",
      "plt.subplots(4,4,figsize=(16,16))\n",
      "count = 0\n",
      "for i in range(300):\n",
      "    if predictions0[i] != yt_900[i]:\n",
      "        count += 1\n",
      "        plt.subplot(4,4,count)\n",
      "        plt.imshow(cv2.imread(test_face_names[i], cv2.IMREAD_GRAYSCALE), cmap='gray')\n",
      "159/756:\n",
      "plt.subplots(4,4,figsize=(16,16))\n",
      "count = 0\n",
      "for i in range(300,600):\n",
      "    if predictions0[i] != yt_900[i]:\n",
      "        count += 1\n",
      "        plt.subplot(4,4,count)\n",
      "        plt.imshow(cv2.imread(nonfaces_t[i], cv2.IMREAD_GRAYSCALE), cmap='gray')\n",
      "count\n",
      "159/757:\n",
      "plt.subplots(4,4,figsize=(16,16))\n",
      "count = 0\n",
      "for i in range(300,600):\n",
      "    print(i)\n",
      "    if predictions0[i] != yt_900[i]:\n",
      "        count += 1\n",
      "        plt.subplot(4,4,count)\n",
      "        plt.imshow(cv2.imread(nonfaces_t[i], cv2.IMREAD_GRAYSCALE), cmap='gray')\n",
      "count\n",
      "159/758:\n",
      "count = 0\n",
      "for i in range(300,600):\n",
      "    if predictions0[i] != yt_900[i]:\n",
      "        count += 1\n",
      "count\n",
      "159/759:\n",
      "predictions50 = rf50.predict(Xt_900)\n",
      "plt.subplots(4,4,figsize=(16,16))\n",
      "count = 0\n",
      "for i in range(300):\n",
      "    if predictions50[i] != yt_900[i]:\n",
      "        count += 1\n",
      "        plt.subplot(4,4,count)\n",
      "        plt.imshow(cv2.imread(test_face_names[i], cv2.IMREAD_GRAYSCALE), cmap='gray')\n",
      "159/760:\n",
      "predictions50 = rf50.predict(Xt_900)\n",
      "plt.subplots(4,2,figsize=(16,16))\n",
      "count = 0\n",
      "for i in range(300):\n",
      "    if predictions50[i] != yt_900[i]:\n",
      "        count += 1\n",
      "        plt.subplot(4,4,count)\n",
      "        plt.imshow(cv2.imread(test_face_names[i], cv2.IMREAD_GRAYSCALE), cmap='gray')\n",
      "159/761:\n",
      "predictions50 = rf50.predict(Xt_900)\n",
      "plt.subplots(4,2,figsize=(8,16))\n",
      "count = 0\n",
      "for i in range(300):\n",
      "    if predictions50[i] != yt_900[i]:\n",
      "        count += 1\n",
      "        plt.subplot(4,4,count)\n",
      "        plt.imshow(cv2.imread(test_face_names[i], cv2.IMREAD_GRAYSCALE), cmap='gray')\n",
      "159/762:\n",
      "predictions50 = rf50.predict(Xt_900)\n",
      "plt.subplots(4,2,figsize=(16,8))\n",
      "count = 0\n",
      "for i in range(300):\n",
      "    if predictions50[i] != yt_900[i]:\n",
      "        count += 1\n",
      "        plt.subplot(4,4,count)\n",
      "        plt.imshow(cv2.imread(test_face_names[i], cv2.IMREAD_GRAYSCALE), cmap='gray')\n",
      "159/763:\n",
      "predictions50 = rf50.predict(Xt_900)\n",
      "plt.subplots(4,2,figsize=(16,8))\n",
      "count = 0\n",
      "for i in range(300):\n",
      "    if predictions50[i] != yt_900[i]:\n",
      "        count += 1\n",
      "        plt.subplot(4,2,count)\n",
      "        plt.imshow(cv2.imread(test_face_names[i], cv2.IMREAD_GRAYSCALE), cmap='gray')\n",
      "159/764:\n",
      "predictions50 = rf50.predict(Xt_900)\n",
      "plt.subplots(3,2,figsize=(16,8))\n",
      "count = 0\n",
      "for i in range(300):\n",
      "    if predictions50[i] != yt_900[i]:\n",
      "        count += 1\n",
      "        plt.subplot(3,2,count)\n",
      "        plt.imshow(cv2.imread(test_face_names[i], cv2.IMREAD_GRAYSCALE), cmap='gray')\n",
      "159/765:\n",
      "predictions50 = rf50.predict(Xt_900)\n",
      "plt.subplots(3,2,figsize=(8,8))\n",
      "count = 0\n",
      "for i in range(300):\n",
      "    if predictions50[i] != yt_900[i]:\n",
      "        count += 1\n",
      "        plt.subplot(3,2,count)\n",
      "        plt.imshow(cv2.imread(test_face_names[i], cv2.IMREAD_GRAYSCALE), cmap='gray')\n",
      "159/766:\n",
      "predictions50 = rf50.predict(Xt_900)\n",
      "plt.subplots(2,3,figsize=(8,8))\n",
      "count = 0\n",
      "for i in range(300):\n",
      "    if predictions50[i] != yt_900[i]:\n",
      "        count += 1\n",
      "        plt.subplot(2,3,count)\n",
      "        plt.imshow(cv2.imread(test_face_names[i], cv2.IMREAD_GRAYSCALE), cmap='gray')\n",
      "159/767:\n",
      "predictions50 = rf50.predict(Xt_900)\n",
      "plt.subplots(2,3,figsize=(8,6))\n",
      "count = 0\n",
      "for i in range(300):\n",
      "    if predictions50[i] != yt_900[i]:\n",
      "        count += 1\n",
      "        plt.subplot(2,3,count)\n",
      "        plt.imshow(cv2.imread(test_face_names[i], cv2.IMREAD_GRAYSCALE), cmap='gray')\n",
      "159/768:\n",
      "count = 0\n",
      "for i in range(300,600):\n",
      "    if predictions50[i] != yt_900[i]:\n",
      "        count += 1\n",
      "count\n",
      "159/769:\n",
      "predictions100 = rf100.predict(Xt_900)\n",
      "plt.subplots(2,3,figsize=(8,6))\n",
      "count = 0\n",
      "for i in range(300):\n",
      "    if predictions100[i] != yt_900[i]:\n",
      "        count += 1\n",
      "        plt.subplot(2,3,count)\n",
      "        plt.imshow(cv2.imread(test_face_names[i], cv2.IMREAD_GRAYSCALE), cmap='gray')\n",
      "159/770:\n",
      "count = 0\n",
      "for i in range(300,600):\n",
      "    if predictions100[i] != yt_900[i]:\n",
      "        count += 1\n",
      "count\n",
      "159/771:\n",
      "count = 0\n",
      "plt.subplots(2,3,figsize=(8,6))\n",
      "for i in range(300,600):\n",
      "    if predictions100[i] != yt_900[i]:\n",
      "        count += 1\n",
      "        plt.subplot(2,3,count)\n",
      "        plt.imshow(cv2.imread(nonfaces_t[i], cv2.IMREAD_GRAYSCALE), cmap='gray')\n",
      "count\n",
      "159/772:\n",
      "count = 0\n",
      "plt.subplots(2,3,figsize=(8,6))\n",
      "for i in range(300,600):\n",
      "    if predictions100[i] != yt_900[i]:\n",
      "        count += 1\n",
      "        plt.subplot(2,3,count)\n",
      "        plt.imshow(cv2.imread(nonfaces_t[i-300], cv2.IMREAD_GRAYSCALE), cmap='gray')\n",
      "count\n",
      "159/773:\n",
      "count = 0\n",
      "plt.subplots(2,3,figsize=(8,6))\n",
      "for i in range(300,600):\n",
      "    if predictions100[i] != yt_900[i]:\n",
      "        count += 1\n",
      "        plt.subplot(2,3,count)\n",
      "        plt.imshow(nonfaces_t[i-300], cmap='gray')\n",
      "count\n",
      "159/774:\n",
      "count = 0\n",
      "plt.subplots(2,3,figsize=(8,6))\n",
      "for i in range(300,600):\n",
      "    if predictions100[i] != yt_900[i]:\n",
      "        count += 1\n",
      "        plt.subplot(2,3,count)\n",
      "        print(i)\n",
      "        plt.imshow(nonfaces_t[i-300], cmap='gray')\n",
      "count\n",
      "159/775:\n",
      "count = 0\n",
      "plt.subplots(2,3,figsize=(8,6))\n",
      "for i in range(350,500):\n",
      "    if predictions100[i] != yt_900[i]:\n",
      "        count += 1\n",
      "        plt.subplot(2,3,count)\n",
      "        plt.imshow(nonfaces_t[i-300], cmap='gray')\n",
      "count\n",
      "159/776:\n",
      "count = 0\n",
      "plt.subplots(2,2,figsize=(8,6))\n",
      "for i in range(350,500):\n",
      "    if predictions100[i] != yt_900[i]:\n",
      "        count += 1\n",
      "        plt.subplot(2,2,count)\n",
      "        plt.imshow(nonfaces_t[i-300], cmap='gray')\n",
      "count\n",
      "159/777: X.append(np.transpose([0]*4020))\n",
      "159/778: X0.append(np.transpose([0]*4020))\n",
      "159/779: X0.concatenate(np.transpose([0]*4020))\n",
      "159/780: np.concatenate([1,1], [2])\n",
      "159/781: np.concatenate(np.array([1,1]), np.array([2]))\n",
      "159/782: np.concatenate((np.array([1,1]), np.array([2])))\n",
      "159/783: np.concatenate((np.array([[1,1],[2,2]]), np.array([[2],[2]])))\n",
      "159/784: np.array([[1,1],[2,2]]).shape\n",
      "159/785: np.array([[1],[2]]).shape\n",
      "159/786: np.concatenate((np.array([[1,1],[2,2]]), np.array([[2],[2]])),axis=1)\n",
      "159/787: np.concatenate((np.array([[1,1],[2,2]]), np.array([[3],[3]])),axis=1)\n",
      "159/788:\n",
      "def pipeline(f, n):\n",
      "    def getx3(facefiles, n, nonfaces, feature_types, feature_coords):\n",
      "        ims = [f(i) for i in getimgs(facefiles, nonfaces, n)]\n",
      "        X = delayed(extract_feature_image(img, feature_types, feature_coords) for img in ims)\n",
      "        return np.array(X.compute(scheduler='threads'))\n",
      "    t_start = time()\n",
      "    X0 = getx3(face_filenames0, n, nonfaces, feature_type_sel2, feature_coord_sel2)\n",
      "    print(X0.shape)\n",
      "    X25 = getx3(face_filenames25, n, nonfaces, feature_type_sel2, feature_coord_sel2)\n",
      "    print(X25.shape)\n",
      "    X50 = getx3(face_filenames50, n, nonfaces, feature_type_sel2, feature_coord_sel2)\n",
      "    print(X50.shape)\n",
      "    X75 = getx3(face_filenames75, n, nonfaces, feature_type_sel2, feature_coord_sel2)\n",
      "    print(X75.shape)\n",
      "    X100 = getx3(face_filenames100, n, nonfaces, feature_type_sel2, feature_coord_sel2)\n",
      "    print(X100.shape)\n",
      "    Xt = getx3(test_face_names, 300, nonfaces_t, feature_type_sel2, feature_coord_sel2)\n",
      "    print(Xt.shape)\n",
      "    \n",
      "    os_rs = []\n",
      "    bs_rs = []\n",
      "    ws_rs = []\n",
      "    for i in range(n):\n",
      "        rf0 = RandomForestClassifier(n_estimators=1000, max_depth=None, n_jobs=-1, random_state=0)\n",
      "        rf0.fit(X0, y)\n",
      "        print(rf0.score(Xt_900, y_t))\n",
      "        rf25 = RandomForestClassifier(n_estimators=1000, max_depth=None, n_jobs=-1, random_state=0)\n",
      "        rf25.fit(X25, y)\n",
      "        print(rf25.score(Xt_900, y_t))\n",
      "        rf50 = RandomForestClassifier(n_estimators=1000, max_depth=None, n_jobs=-1, random_state=0)\n",
      "        rf50.fit(X50, y)\n",
      "        print(rf50.score(Xt_900, y_t))\n",
      "        rf75 = RandomForestClassifier(n_estimators=1000, max_depth=None, n_jobs=-1, random_state=0)\n",
      "        rf75.fit(X75, y)\n",
      "        print(rf75.score(Xt_900, y_t))\n",
      "        rf100 = RandomForestClassifier(n_estimators=1000, max_depth=None, n_jobs=-1, random_state=0)\n",
      "        rf100.fit(X100, y)\n",
      "        print(rf100.score(Xt_900, y_t))\n",
      "\n",
      "        os_r, bs_r, ws_r = rfscores([rf0, rf25, rf50, rf75, rf100], Xt_900, Xb_t, Xw_t)\n",
      "        os_rs.append(os_r)\n",
      "        bs_rs.append(bs_r)\n",
      "        ws_rs.append(ws_r)\n",
      "    \n",
      "    plt.plot([0, 25, 50, 75, 100], os_r, label=\"o\")\n",
      "    plt.plot([0, 25, 50, 75, 100], bs_r, label=\"b\")\n",
      "    plt.plot([0, 25, 50, 75, 100], ws_r, label=\"w\")\n",
      "    plt.legend()\n",
      "    plt.show()\n",
      "    \n",
      "    print(time()-t_start)\n",
      "    return os_rs, bs_rs, ws_rs\n",
      "159/789: os_rs, bs_rs, ws_rs = pipeline(lambda x: x)\n",
      "159/790: os_rs, bs_rs, ws_rs = pipeline(lambda x: x, 2)\n",
      "159/791: X0.shape\n",
      "159/792: y.shape\n",
      "159/793: rf0\n",
      "159/794: n\n",
      "159/795:\n",
      "def pipeline(f, times):\n",
      "    def getx3(facefiles, n, nonfaces, feature_types, feature_coords):\n",
      "        ims = [f(i) for i in getimgs(facefiles, nonfaces, n)]\n",
      "        X = delayed(extract_feature_image(img, feature_types, feature_coords) for img in ims)\n",
      "        return np.array(X.compute(scheduler='threads'))\n",
      "    t_start = time()\n",
      "    X0 = getx3(face_filenames0, n, nonfaces, feature_type_sel2, feature_coord_sel2)\n",
      "    print(X0.shape)\n",
      "    X25 = getx3(face_filenames25, n, nonfaces, feature_type_sel2, feature_coord_sel2)\n",
      "    print(X25.shape)\n",
      "    X50 = getx3(face_filenames50, n, nonfaces, feature_type_sel2, feature_coord_sel2)\n",
      "    print(X50.shape)\n",
      "    X75 = getx3(face_filenames75, n, nonfaces, feature_type_sel2, feature_coord_sel2)\n",
      "    print(X75.shape)\n",
      "    X100 = getx3(face_filenames100, n, nonfaces, feature_type_sel2, feature_coord_sel2)\n",
      "    print(X100.shape)\n",
      "    Xt = getx3(test_face_names, 300, nonfaces_t, feature_type_sel2, feature_coord_sel2)\n",
      "    print(Xt.shape)\n",
      "    \n",
      "    os_rs = []\n",
      "    bs_rs = []\n",
      "    ws_rs = []\n",
      "    for i in range(times):\n",
      "        rf0 = RandomForestClassifier(n_estimators=1000, max_depth=None, n_jobs=-1, random_state=0)\n",
      "        rf0.fit(X0, y)\n",
      "        print(rf0.score(Xt_900, y_t))\n",
      "        rf25 = RandomForestClassifier(n_estimators=1000, max_depth=None, n_jobs=-1, random_state=0)\n",
      "        rf25.fit(X25, y)\n",
      "        print(rf25.score(Xt_900, y_t))\n",
      "        rf50 = RandomForestClassifier(n_estimators=1000, max_depth=None, n_jobs=-1, random_state=0)\n",
      "        rf50.fit(X50, y)\n",
      "        print(rf50.score(Xt_900, y_t))\n",
      "        rf75 = RandomForestClassifier(n_estimators=1000, max_depth=None, n_jobs=-1, random_state=0)\n",
      "        rf75.fit(X75, y)\n",
      "        print(rf75.score(Xt_900, y_t))\n",
      "        rf100 = RandomForestClassifier(n_estimators=1000, max_depth=None, n_jobs=-1, random_state=0)\n",
      "        rf100.fit(X100, y)\n",
      "        print(rf100.score(Xt_900, y_t))\n",
      "\n",
      "        os_r, bs_r, ws_r = rfscores([rf0, rf25, rf50, rf75, rf100], Xt_900, Xb_t, Xw_t)\n",
      "        os_rs.append(os_r)\n",
      "        bs_rs.append(bs_r)\n",
      "        ws_rs.append(ws_r)\n",
      "    \n",
      "    plt.plot([0, 25, 50, 75, 100], os_r, label=\"o\")\n",
      "    plt.plot([0, 25, 50, 75, 100], bs_r, label=\"b\")\n",
      "    plt.plot([0, 25, 50, 75, 100], ws_r, label=\"w\")\n",
      "    plt.legend()\n",
      "    plt.show()\n",
      "    \n",
      "    print(time()-t_start)\n",
      "    return os_rs, bs_rs, ws_rs\n",
      "159/796:\n",
      "def pipeline(f, times):\n",
      "    def getx3(facefiles, n, nonfaces, feature_types, feature_coords):\n",
      "        ims = [f(i) for i in getimgs(facefiles, nonfaces, n)]\n",
      "        X = delayed(extract_feature_image(img, feature_types, feature_coords) for img in ims)\n",
      "        return np.array(X.compute(scheduler='threads'))\n",
      "    t_start = time()\n",
      "    X0 = getx3(face_filenames0, n, nonfaces, feature_type_sel2, feature_coord_sel2)\n",
      "    print(X0.shape)\n",
      "    X25 = getx3(face_filenames25, n, nonfaces, feature_type_sel2, feature_coord_sel2)\n",
      "    print(X25.shape)\n",
      "    X50 = getx3(face_filenames50, n, nonfaces, feature_type_sel2, feature_coord_sel2)\n",
      "    print(X50.shape)\n",
      "    X75 = getx3(face_filenames75, n, nonfaces, feature_type_sel2, feature_coord_sel2)\n",
      "    print(X75.shape)\n",
      "    X100 = getx3(face_filenames100, n, nonfaces, feature_type_sel2, feature_coord_sel2)\n",
      "    print(X100.shape)\n",
      "    Xt = getx3(test_face_names, 300, nonfaces_t, feature_type_sel2, feature_coord_sel2)\n",
      "    print(Xt.shape)\n",
      "    \n",
      "    os_rs = []\n",
      "    bs_rs = []\n",
      "    ws_rs = []\n",
      "    for i in range(times):\n",
      "        rf0 = RandomForestClassifier(n_estimators=1000, max_depth=None, n_jobs=-1, random_state=0)\n",
      "        rf0.fit(X0, y)\n",
      "        print(rf0.score(Xt_900, y_t))\n",
      "        rf25 = RandomForestClassifier(n_estimators=1000, max_depth=None, n_jobs=-1, random_state=0)\n",
      "        rf25.fit(X25, y)\n",
      "        print(rf25.score(Xt_900, y_t))\n",
      "        rf50 = RandomForestClassifier(n_estimators=1000, max_depth=None, n_jobs=-1, random_state=0)\n",
      "        rf50.fit(X50, y)\n",
      "        print(rf50.score(Xt_900, y_t))\n",
      "        rf75 = RandomForestClassifier(n_estimators=1000, max_depth=None, n_jobs=-1, random_state=0)\n",
      "        rf75.fit(X75, y)\n",
      "        print(rf75.score(Xt_900, y_t))\n",
      "        rf100 = RandomForestClassifier(n_estimators=1000, max_depth=None, n_jobs=-1, random_state=0)\n",
      "        rf100.fit(X100, y)\n",
      "        print(rf100.score(Xt_900, y_t))\n",
      "\n",
      "        os_r, bs_r, ws_r = rfscores([rf0, rf25, rf50, rf75, rf100], Xt_900, Xb_t, Xw_t)\n",
      "        os_rs.append(os_r)\n",
      "        bs_rs.append(bs_r)\n",
      "        ws_rs.append(ws_r)\n",
      "    \n",
      "    plt.plot([0, 25, 50, 75, 100], os_r, label=\"o\")\n",
      "    plt.plot([0, 25, 50, 75, 100], bs_r, label=\"b\")\n",
      "    plt.plot([0, 25, 50, 75, 100], ws_r, label=\"w\")\n",
      "    plt.legend()\n",
      "    plt.show()\n",
      "    \n",
      "    print(time()-t_start)\n",
      "    return os_rs, bs_rs, ws_rs\n",
      "159/797: n\n",
      "159/798: os_rs, bs_rs, ws_rs = pipeline(lambda x: x, 2)\n",
      "159/799: plt.plot([0,25,50,75,100],np.mean(np.array(os_rs),axis=0)\n",
      "159/800: plt.plot([0,25,50,75,100],np.mean(np.array(os_rs),axis=0))\n",
      "159/801:\n",
      "plt.plot([0,25,50,75,100],np.mean(np.array(os_rs),axis=0))\n",
      "plt.plot([0,25,50,75,100],np.mean(np.array(bs_rs),axis=0))\n",
      "plt.plot([0,25,50,75,100],np.mean(np.array(ws_rs),axis=0))\n",
      "159/802:\n",
      "plt.plot([0,25,50,75,100],np.mean(np.array(os_rs),axis=0))\n",
      "plt.plot([0,25,50,75,100],np.mean(np.array(bs_rs),axis=0))\n",
      "plt.plot([0,25,50,75,100],np.mean(np.array(ws_rs),axis=0))\n",
      "plt.plot([0,25,50,75,100],np.mean(np.array(os_rs),axis=0)+np.std(np.array(os_rs),axis=0)+1))\n",
      "159/803: np.mean(np.array(os_rs),axis=0)+np.std(np.array(os_rs),axis=0)+1\n",
      "159/804: np.mean(np.array(os_rs),axis=0)+np.std(np.array(os_rs),axis=0)\n",
      "159/805: np.mean(np.array(os_rs),axis=0)+np.std(np.array(os_rs),axis=0)+1\n",
      "159/806:\n",
      "plt.plot([0,25,50,75,100],np.mean(np.array(os_rs),axis=0))\n",
      "plt.plot([0,25,50,75,100],np.mean(np.array(bs_rs),axis=0))\n",
      "plt.plot([0,25,50,75,100],np.mean(np.array(ws_rs),axis=0))\n",
      "plt.plot([0,25,50,75,100], np.array(np.mean(np.array(os_rs),axis=0)+np.std(np.array(os_rs),axis=0)+1)),axis=0,':')\n",
      "159/807:\n",
      "plt.plot([0,25,50,75,100],np.mean(np.array(os_rs),axis=0))\n",
      "plt.plot([0,25,50,75,100],np.mean(np.array(bs_rs),axis=0))\n",
      "plt.plot([0,25,50,75,100],np.mean(np.array(ws_rs),axis=0))\n",
      "plt.plot([0,25,50,75,100], np.array(np.mean(np.array(os_rs),axis=0)+np.std(np.array(os_rs),axis=0))),axis=0,':')\n",
      "159/808:\n",
      "plt.plot([0,25,50,75,100],np.mean(np.array(os_rs),axis=0))\n",
      "plt.plot([0,25,50,75,100],np.mean(np.array(bs_rs),axis=0))\n",
      "plt.plot([0,25,50,75,100],np.mean(np.array(ws_rs),axis=0))\n",
      "plt.plot([0,25,50,75,100], np.array(np.mean(np.array(os_rs),axis=0)+np.std(np.array(os_rs),axis=0)),axis=0,':')\n",
      "159/809:\n",
      "plt.plot([0,25,50,75,100],np.mean(np.array(os_rs),axis=0))\n",
      "plt.plot([0,25,50,75,100],np.mean(np.array(bs_rs),axis=0))\n",
      "plt.plot([0,25,50,75,100],np.mean(np.array(ws_rs),axis=0))\n",
      "plt.plot([0,25,50,75,100], np.array(np.mean(np.array(os_rs),axis=0)+np.std(np.array(os_rs),axis=0)),':')\n",
      "159/810:\n",
      "plt.plot([0,25,50,75,100],np.mean(np.array(os_rs),axis=0))\n",
      "plt.plot([0,25,50,75,100],np.mean(np.array(bs_rs),axis=0))\n",
      "plt.plot([0,25,50,75,100],np.mean(np.array(ws_rs),axis=0))\n",
      "plt.plot([0,25,50,75,100], np.array(np.mean(np.array(os_rs),axis=0)+np.std(np.array(os_rs),axis=0)+1),':')\n",
      "159/811:\n",
      "plt.plot([0,25,50,75,100],np.mean(np.array(os_rs),axis=0))\n",
      "plt.plot([0,25,50,75,100],np.mean(np.array(bs_rs),axis=0))\n",
      "plt.plot([0,25,50,75,100],np.mean(np.array(ws_rs),axis=0))\n",
      "plt.plot([0,25,50,75,100], np.array(np.mean(np.array(os_rs),axis=0)+np.std(np.array(os_rs),axis=0)+.01),':')\n",
      "159/812: os_rs, bs_rs, ws_rs = pipeline(lambda x: x, 10)\n",
      "159/813: os_rsn, bs_rsn, ws_rsn = pipeline(normalize, 10)\n",
      "159/814: os_rss, bs_rss, ws_rss = pipeline(sharpen, 10)\n",
      "159/815:\n",
      "def plots(os_rs, bs_rs, ws_rs):\n",
      "    plt.plot([0,25,50,75,100],np.mean(np.array(os_rs),axis=0),label='o')\n",
      "    plt.plot([0,25,50,75,100],np.mean(np.array(bs_rs),axis=0),label='b')\n",
      "    plt.plot([0,25,50,75,100],np.mean(np.array(ws_rs),axis=0),label='w')\n",
      "    plt.plot([0,25,50,75,100], np.array(np.mean(np.array(os_rs),axis=0)+np.std(np.array(os_rs),axis=0)),':',label='o+')\n",
      "    plt.plot([0,25,50,75,100], np.array(np.mean(np.array(os_rs),axis=0)-np.std(np.array(os_rs),axis=0)),':',label='o-')\n",
      "    plt.plot([0,25,50,75,100], np.array(np.mean(np.array(bs_rs),axis=0)+np.std(np.array(bs_rs),axis=0)),':',label='o+')\n",
      "    plt.plot([0,25,50,75,100], np.array(np.mean(np.array(bs_rs),axis=0)-np.std(np.array(bs_rs),axis=0)),':',label='o-')\n",
      "    plt.plot([0,25,50,75,100], np.array(np.mean(np.array(ws_rs),axis=0)+np.std(np.array(ws_rs),axis=0)),':',label='o+')\n",
      "    plt.plot([0,25,50,75,100], np.array(np.mean(np.array(ws_rs),axis=0)-np.std(np.array(ws_rs),axis=0)),':',label='o-')\n",
      "    plt.legend()\n",
      "    plt.show()\n",
      "159/816: plots(os_rs, bs_rs, ws_rs)\n",
      "159/817: plots(os_rsn, bs_rsn, ws_rsn)\n",
      "159/818: plots(os_rss, bs_rss, ws_rss)\n",
      "159/819:\n",
      "def regenerate_nonfaces():\n",
      "    nonfaces = []\n",
      "    for i in range(n):\n",
      "        # Read a random nonface file\n",
      "        j = random.randint(0, len(nonface_filenames) - 1)\n",
      "        nonface = cv2.imread(nonface_filenames[j], cv2.IMREAD_GRAYSCALE)\n",
      "        wsize = random.randint(36, min(nonface.shape[0], nonface.shape[1]))\n",
      "        row = random.randint(0, nonface.shape[0]-wsize)\n",
      "        col = random.randint(0, nonface.shape[1]-wsize)\n",
      "        crop = nonface[row:row+wsize, col:col+wsize]\n",
      "\n",
      "        # Resize to be the right size\n",
      "        crop = cv2.resize(crop, (200, 200))\n",
      "        nonfaces.append(crop)\n",
      "    nonfaces_t = []\n",
      "    for i in range(n_test):\n",
      "        # Read a random nonface file\n",
      "        j = random.randint(0, n_nonfaces_test - 1)\n",
      "        nonface = cv2.imread(test_nonface_names[j], cv2.IMREAD_GRAYSCALE)\n",
      "\n",
      "        wsize = random.randint(36, min(nonface.shape[0], nonface.shape[1]))\n",
      "        row = random.randint(0, nonface.shape[0]-wsize)\n",
      "        col = random.randint(0, nonface.shape[1]-wsize)\n",
      "        crop = nonface[row:row+wsize, col:col+wsize]\n",
      "\n",
      "        # Resize to be the right size\n",
      "        crop = cv2.resize(crop, (200, 200))\n",
      "        nonfaces_t.append(crop)\n",
      "    return nonfaces, nonfaces_t\n",
      "159/820:\n",
      "def pipeline(f, times):\n",
      "    def getx3(facefiles, n, nonfaces, feature_types, feature_coords):\n",
      "        ims = [f(i) for i in getimgs(facefiles, nonfaces, n)]\n",
      "        X = delayed(extract_feature_image(img, feature_types, feature_coords) for img in ims)\n",
      "        return np.array(X.compute(scheduler='threads'))\n",
      "    t_start = time()\n",
      "    X0 = getx3(face_filenames0, n, nonfaces, feature_type_sel2, feature_coord_sel2)\n",
      "    print(X0.shape)\n",
      "    X25 = getx3(face_filenames25, n, nonfaces, feature_type_sel2, feature_coord_sel2)\n",
      "    print(X25.shape)\n",
      "    X50 = getx3(face_filenames50, n, nonfaces, feature_type_sel2, feature_coord_sel2)\n",
      "    print(X50.shape)\n",
      "    X75 = getx3(face_filenames75, n, nonfaces, feature_type_sel2, feature_coord_sel2)\n",
      "    print(X75.shape)\n",
      "    X100 = getx3(face_filenames100, n, nonfaces, feature_type_sel2, feature_coord_sel2)\n",
      "    print(X100.shape)\n",
      "    Xt = getx3(test_face_names, 300, nonfaces_t, feature_type_sel2, feature_coord_sel2)\n",
      "    print(Xt.shape)\n",
      "    \n",
      "    os_rs = []\n",
      "    bs_rs = []\n",
      "    ws_rs = []\n",
      "    for i in range(times):\n",
      "        rf0 = RandomForestClassifier(n_estimators=1000, max_depth=None, n_jobs=-1, random_state=0)\n",
      "        rf0.fit(X0, y)\n",
      "        print(rf0.score(Xt_900, y_t))\n",
      "        rf25 = RandomForestClassifier(n_estimators=1000, max_depth=None, n_jobs=-1, random_state=0)\n",
      "        rf25.fit(X25, y)\n",
      "        print(rf25.score(Xt_900, y_t))\n",
      "        rf50 = RandomForestClassifier(n_estimators=1000, max_depth=None, n_jobs=-1, random_state=0)\n",
      "        rf50.fit(X50, y)\n",
      "        print(rf50.score(Xt_900, y_t))\n",
      "        rf75 = RandomForestClassifier(n_estimators=1000, max_depth=None, n_jobs=-1, random_state=0)\n",
      "        rf75.fit(X75, y)\n",
      "        print(rf75.score(Xt_900, y_t))\n",
      "        rf100 = RandomForestClassifier(n_estimators=1000, max_depth=None, n_jobs=-1, random_state=0)\n",
      "        rf100.fit(X100, y)\n",
      "        print(rf100.score(Xt_900, y_t))\n",
      "\n",
      "        os_r, bs_r, ws_r = rfscores([rf0, rf25, rf50, rf75, rf100], Xt_900, Xb_t, Xw_t)\n",
      "        os_rs.append(os_r)\n",
      "        bs_rs.append(bs_r)\n",
      "        ws_rs.append(ws_r)\n",
      "        nonfaces, nonfaces_t = regenerate_nonfaces()\n",
      "    \n",
      "    plt.plot([0, 25, 50, 75, 100], os_r, label=\"o\")\n",
      "    plt.plot([0, 25, 50, 75, 100], bs_r, label=\"b\")\n",
      "    plt.plot([0, 25, 50, 75, 100], ws_r, label=\"w\")\n",
      "    plt.legend()\n",
      "    plt.show()\n",
      "    \n",
      "    print(time()-t_start)\n",
      "    return os_rs, bs_rs, ws_rs\n",
      "159/821: os_rs, bs_rs, ws_rs = pipeline(lambda x: x, 10)\n",
      "159/822:\n",
      "def pipeline(f, times):\n",
      "    def getx3(facefiles, n, nonfaces, feature_types, feature_coords):\n",
      "        ims = [f(i) for i in getimgs(facefiles, nonfaces, n)]\n",
      "        X = delayed(extract_feature_image(img, feature_types, feature_coords) for img in ims)\n",
      "        return np.array(X.compute(scheduler='threads'))\n",
      "    nonfaces, nonfaces_t = nonfaces, nonfaces_t\n",
      "    t_start = time()\n",
      "    X0 = getx3(face_filenames0, n, nonfaces, feature_type_sel2, feature_coord_sel2)\n",
      "    print(X0.shape)\n",
      "    X25 = getx3(face_filenames25, n, nonfaces, feature_type_sel2, feature_coord_sel2)\n",
      "    print(X25.shape)\n",
      "    X50 = getx3(face_filenames50, n, nonfaces, feature_type_sel2, feature_coord_sel2)\n",
      "    print(X50.shape)\n",
      "    X75 = getx3(face_filenames75, n, nonfaces, feature_type_sel2, feature_coord_sel2)\n",
      "    print(X75.shape)\n",
      "    X100 = getx3(face_filenames100, n, nonfaces, feature_type_sel2, feature_coord_sel2)\n",
      "    print(X100.shape)\n",
      "    Xt = getx3(test_face_names, 300, nonfaces_t, feature_type_sel2, feature_coord_sel2)\n",
      "    print(Xt.shape)\n",
      "    \n",
      "    os_rs = []\n",
      "    bs_rs = []\n",
      "    ws_rs = []\n",
      "    for i in range(times):\n",
      "        rf0 = RandomForestClassifier(n_estimators=1000, max_depth=None, n_jobs=-1, random_state=0)\n",
      "        rf0.fit(X0, y)\n",
      "        print(rf0.score(Xt_900, y_t))\n",
      "        rf25 = RandomForestClassifier(n_estimators=1000, max_depth=None, n_jobs=-1, random_state=0)\n",
      "        rf25.fit(X25, y)\n",
      "        print(rf25.score(Xt_900, y_t))\n",
      "        rf50 = RandomForestClassifier(n_estimators=1000, max_depth=None, n_jobs=-1, random_state=0)\n",
      "        rf50.fit(X50, y)\n",
      "        print(rf50.score(Xt_900, y_t))\n",
      "        rf75 = RandomForestClassifier(n_estimators=1000, max_depth=None, n_jobs=-1, random_state=0)\n",
      "        rf75.fit(X75, y)\n",
      "        print(rf75.score(Xt_900, y_t))\n",
      "        rf100 = RandomForestClassifier(n_estimators=1000, max_depth=None, n_jobs=-1, random_state=0)\n",
      "        rf100.fit(X100, y)\n",
      "        print(rf100.score(Xt_900, y_t))\n",
      "\n",
      "        os_r, bs_r, ws_r = rfscores([rf0, rf25, rf50, rf75, rf100], Xt_900, Xb_t, Xw_t)\n",
      "        os_rs.append(os_r)\n",
      "        bs_rs.append(bs_r)\n",
      "        ws_rs.append(ws_r)\n",
      "        nonfaces, nonfaces_t = regenerate_nonfaces()\n",
      "    \n",
      "    plt.plot([0, 25, 50, 75, 100], os_r, label=\"o\")\n",
      "    plt.plot([0, 25, 50, 75, 100], bs_r, label=\"b\")\n",
      "    plt.plot([0, 25, 50, 75, 100], ws_r, label=\"w\")\n",
      "    plt.legend()\n",
      "    plt.show()\n",
      "    \n",
      "    print(time()-t_start)\n",
      "    return os_rs, bs_rs, ws_rs\n",
      "159/823: os_rs, bs_rs, ws_rs = pipeline(lambda x: x, 10)\n",
      "159/824:\n",
      "def pipeline(f, times):\n",
      "    def getx3(facefiles, n, nonfaces, feature_types, feature_coords):\n",
      "        ims = [f(i) for i in getimgs(facefiles, nonfaces, n)]\n",
      "        X = delayed(extract_feature_image(img, feature_types, feature_coords) for img in ims)\n",
      "        return np.array(X.compute(scheduler='threads'))\n",
      "    nonfaces, nonfaces_t = regenerate_nonfaces()\n",
      "    t_start = time()\n",
      "    X0 = getx3(face_filenames0, n, nonfaces, feature_type_sel2, feature_coord_sel2)\n",
      "    print(X0.shape)\n",
      "    X25 = getx3(face_filenames25, n, nonfaces, feature_type_sel2, feature_coord_sel2)\n",
      "    print(X25.shape)\n",
      "    X50 = getx3(face_filenames50, n, nonfaces, feature_type_sel2, feature_coord_sel2)\n",
      "    print(X50.shape)\n",
      "    X75 = getx3(face_filenames75, n, nonfaces, feature_type_sel2, feature_coord_sel2)\n",
      "    print(X75.shape)\n",
      "    X100 = getx3(face_filenames100, n, nonfaces, feature_type_sel2, feature_coord_sel2)\n",
      "    print(X100.shape)\n",
      "    Xt = getx3(test_face_names, 300, nonfaces_t, feature_type_sel2, feature_coord_sel2)\n",
      "    print(Xt.shape)\n",
      "    \n",
      "    os_rs = []\n",
      "    bs_rs = []\n",
      "    ws_rs = []\n",
      "    for i in range(times):\n",
      "        rf0 = RandomForestClassifier(n_estimators=1000, max_depth=None, n_jobs=-1, random_state=0)\n",
      "        rf0.fit(X0, y)\n",
      "        print(rf0.score(Xt_900, y_t))\n",
      "        rf25 = RandomForestClassifier(n_estimators=1000, max_depth=None, n_jobs=-1, random_state=0)\n",
      "        rf25.fit(X25, y)\n",
      "        print(rf25.score(Xt_900, y_t))\n",
      "        rf50 = RandomForestClassifier(n_estimators=1000, max_depth=None, n_jobs=-1, random_state=0)\n",
      "        rf50.fit(X50, y)\n",
      "        print(rf50.score(Xt_900, y_t))\n",
      "        rf75 = RandomForestClassifier(n_estimators=1000, max_depth=None, n_jobs=-1, random_state=0)\n",
      "        rf75.fit(X75, y)\n",
      "        print(rf75.score(Xt_900, y_t))\n",
      "        rf100 = RandomForestClassifier(n_estimators=1000, max_depth=None, n_jobs=-1, random_state=0)\n",
      "        rf100.fit(X100, y)\n",
      "        print(rf100.score(Xt_900, y_t))\n",
      "\n",
      "        os_r, bs_r, ws_r = rfscores([rf0, rf25, rf50, rf75, rf100], Xt_900, Xb_t, Xw_t)\n",
      "        os_rs.append(os_r)\n",
      "        bs_rs.append(bs_r)\n",
      "        ws_rs.append(ws_r)\n",
      "        nonfaces, nonfaces_t = regenerate_nonfaces()\n",
      "    \n",
      "    plt.plot([0, 25, 50, 75, 100], os_r, label=\"o\")\n",
      "    plt.plot([0, 25, 50, 75, 100], bs_r, label=\"b\")\n",
      "    plt.plot([0, 25, 50, 75, 100], ws_r, label=\"w\")\n",
      "    plt.legend()\n",
      "    plt.show()\n",
      "    \n",
      "    print(time()-t_start)\n",
      "    return os_rs, bs_rs, ws_rs\n",
      "159/825: os_rs, bs_rs, ws_rs = pipeline(lambda x: x, 10)\n",
      "159/826: print(regenerate_nonfaces()-regenerate_nonfaces())\n",
      "159/827: n1, n2 = regenerate_nonfaces()\n",
      "159/828: n3, n4 = regenerate_nonfaces()\n",
      "159/829: n1 - n3\n",
      "159/830: np.array(n1) - np.array(n3)\n",
      "159/831:\n",
      "for i in n1:\n",
      "    plt.imshow(i)\n",
      "159/832: i\n",
      "159/833: # look at most important features as selected by randomforest\n",
      "159/834: from hog36 import hog36\n",
      "159/835: hog0 = np.array([hog36(im, 9, True) for im in getimgs(face_filenames0, nonfaces, n)])\n",
      "159/836: hog36(im, 9, True)\n",
      "159/837: im.shape\n",
      "159/838: hog0 = np.array([hog36(cv2.resize(im,(36,36)), 9, True) for im in getimgs(face_filenames0, nonfaces, n)])\n",
      "159/839: hog0.shape\n",
      "159/840: X0_hog = np.concatenate(X0, hog0)\n",
      "159/841: X0_hog = np.concatenate((X0, hog0),axis=1)\n",
      "159/842: X0_hog.shape\n",
      "159/843: X0h = np.concatenate((X0, hog0),axis=1)\n",
      "159/844: X0h.shape\n",
      "159/845:\n",
      "hog0 = np.array([hog36(cv2.resize(im,(36,36)), 9, True) for im in getimgs(face_filenames0, nonfaces, n)])\n",
      "X0h = np.concatenate((X0, hog0),axis=1)\n",
      "159/846:\n",
      "hog25 = np.array([hog36(cv2.resize(im,(36,36)), 9, True) for im in getimgs(face_filenames0, nonfaces, n)])\n",
      "X25h = np.concatenate((X25, hog25),axis=1)\n",
      "159/847:\n",
      "hog25 = np.array([hog36(cv2.resize(im,(36,36)), 9, True) for im in getimgs(face_filenames25, nonfaces, n)])\n",
      "X25h = np.concatenate((X25, hog25),axis=1)\n",
      "159/848:\n",
      "hog50 = np.array([hog36(cv2.resize(im,(36,36)), 9, True) for im in getimgs(face_filenames50, nonfaces, n)])\n",
      "X50h = np.concatenate((X50, hog50),axis=1)\n",
      "159/849:\n",
      "hog75 = np.array([hog36(cv2.resize(im,(36,36)), 9, True) for im in getimgs(face_filenames75, nonfaces, n)])\n",
      "X75h = np.concatenate((X75, hog75),axis=1)\n",
      "159/850:\n",
      "hog100 = np.array([hog36(cv2.resize(im,(36,36)), 9, True) for im in getimgs(face_filenames100, nonfaces, n)])\n",
      "X100h = np.concatenate((X100, hog100),axis=1)\n",
      "159/851:\n",
      "hogt = np.array([hog36(cv2.resize(im,(36,36)), 9, True) for im in getimgs(test_file_names, nonfaces_t, n)])\n",
      "Xt_900h = np.concatenate((Xt_900, hogt),axis=1)\n",
      "159/852:\n",
      "hogt = np.array([hog36(cv2.resize(im,(36,36)), 9, True) for im in getimgs(test_face_names, nonfaces_t, n)])\n",
      "Xt_900h = np.concatenate((Xt_900, hogt),axis=1)\n",
      "159/853:\n",
      "hogt = np.array([hog36(cv2.resize(im,(36,36)), 9, True) for im in getimgs(test_face_names, nonfaces_t, 300)])\n",
      "Xt_900h = np.concatenate((Xt_900, hogt),axis=1)\n",
      "159/854:\n",
      "rf0h = RandomForestClassifier(n_estimators=1000, max_depth=None, n_jobs=-1, random_state=0)\n",
      "rf0h.fit(X0h, y)\n",
      "print(rf0h.score(Xt_900h, y_t))\n",
      "rf25h = RandomForestClassifier(n_estimators=1000, max_depth=None, n_jobs=-1, random_state=0)\n",
      "rf25h.fit(X25h, y)\n",
      "print(rf25h.score(Xt_900h, y_t))\n",
      "rf50h = RandomForestClassifier(n_estimators=1000, max_depth=None, n_jobs=-1, random_state=0)\n",
      "rf50h.fit(X50h, y)\n",
      "print(rf50h.score(Xt_900h, y_t))\n",
      "rf75h = RandomForestClassifier(n_estimators=1000, max_depth=None, n_jobs=-1, random_state=0)\n",
      "rf75h.fit(X75h, y)\n",
      "print(rf75h.score(Xt_900h, y_t))\n",
      "rf100h = RandomForestClassifier(n_estimators=1000, max_depth=None, n_jobs=-1, random_state=0)\n",
      "rf100h.fit(X100h, y)\n",
      "print(rf100h.score(Xt_900h, y_t))\n",
      "\n",
      "os_rh, bs_rh, ws_rh = rfscores([rf0h, rf25h, rf50h, rf75h, rf100h], Xt_900h, Xb_t, Xw_t)\n",
      "159/855:\n",
      "rf0h = RandomForestClassifier(n_estimators=1000, max_depth=None, n_jobs=-1, random_state=0)\n",
      "rf0h.fit(X0h, y)\n",
      "print(rf0h.score(Xt_900h, y_t))\n",
      "rf25h = RandomForestClassifier(n_estimators=1000, max_depth=None, n_jobs=-1, random_state=0)\n",
      "rf25h.fit(X25h, y)\n",
      "print(rf25h.score(Xt_900h, y_t))\n",
      "rf50h = RandomForestClassifier(n_estimators=1000, max_depth=None, n_jobs=-1, random_state=0)\n",
      "rf50h.fit(X50h, y)\n",
      "print(rf50h.score(Xt_900h, y_t))\n",
      "rf75h = RandomForestClassifier(n_estimators=1000, max_depth=None, n_jobs=-1, random_state=0)\n",
      "rf75h.fit(X75h, y)\n",
      "print(rf75h.score(Xt_900h, y_t))\n",
      "rf100h = RandomForestClassifier(n_estimators=1000, max_depth=None, n_jobs=-1, random_state=0)\n",
      "rf100h.fit(X100h, y)\n",
      "print(rf100h.score(Xt_900h, y_t))\n",
      "\n",
      "os_rh, bs_rh, ws_rh = rfscores([rf0h, rf25h, rf50h, rf75h, rf100h], Xt_900h, Xt_900h[b_inds], Xt_900h[w_inds])\n",
      "159/856:\n",
      "plt.plot([0, 25, 50, 75, 100], os_rh, label=\"o\")\n",
      "plt.plot([0, 25, 50, 75, 100], bs_rh, label=\"b\")\n",
      "plt.plot([0, 25, 50, 75, 100], ws_rh, label=\"w\")\n",
      "plt.legend()\n",
      "plt.show()\n",
      "159/857: # top features\n",
      "159/858: idx_sortedh = np.argsort(rf75h.feature_importances_)[::-1]\n",
      "159/859: sorted(rf75h.feature_importances_)\n",
      "159/860: zip(idx_sortedh, sorted(rf75h.feature_importances_, reverse=True))\n",
      "159/861: print(zip(idx_sortedh, sorted(rf75h.feature_importances_, reverse=True)))\n",
      "159/862: zip(idx_sortedh, sorted(rf75h.feature_importances_, reverse=True))[:10]\n",
      "159/863: print([x + \" \" + y for x,y in zip(idx_sortedh, sorted(rf75h.feature_importances_, reverse=True))])\n",
      "159/864: list(zip(idx_sortedh, sorted(rf75h.feature_importances_, reverse=True)))\n",
      "159/865: list(zip(idx_sortedh, sorted(rf75h.feature_importances_, reverse=True)))[:20]\n",
      "159/866: list(zip(idx_sortedh, sorted(rf75h.feature_importances_, reverse=True)))[:50]\n",
      "159/867:\n",
      "feats = list(zip(idx_sortedh, sorted(rf75h.feature_importances_, reverse=True)))\n",
      "feats[feats>900]\n",
      "159/868:\n",
      "feats = list(zip(idx_sortedh, sorted(rf75h.feature_importances_, reverse=True)))\n",
      "i = 0\n",
      "for f in feats:\n",
      "    if f[0] >= 900:\n",
      "        print(i)\n",
      "    i += 1\n",
      "159/869:\n",
      "feats = list(zip(idx_sortedh, sorted(rf75h.feature_importances_, reverse=True)))\n",
      "i = 0\n",
      "for f in feats:\n",
      "    if f[0] >= 900:\n",
      "        print(i)\n",
      "        break\n",
      "    i += 1\n",
      "159/870:\n",
      "def pipeline(f, times):\n",
      "    def getx3(facefiles, n, nonfaces, feature_types, feature_coords):\n",
      "        ims = [f(i) for i in getimgs(facefiles, nonfaces, n)]\n",
      "        X = delayed(extract_feature_image(img, feature_types, feature_coords) for img in ims)\n",
      "        return np.array(X.compute(scheduler='threads'))\n",
      "    \n",
      "    t_start = time()\n",
      "    \n",
      "    os_rs = []\n",
      "    bs_rs = []\n",
      "    ws_rs = []\n",
      "    for i in range(times):\n",
      "        nonfaces, nonfaces_t = regenerate_nonfaces()\n",
      "        \n",
      "        X0 = getx3(face_filenames0, n, nonfaces, feature_type_sel2, feature_coord_sel2)\n",
      "        print(X0.shape)\n",
      "        X25 = getx3(face_filenames25, n, nonfaces, feature_type_sel2, feature_coord_sel2)\n",
      "        print(X25.shape)\n",
      "        X50 = getx3(face_filenames50, n, nonfaces, feature_type_sel2, feature_coord_sel2)\n",
      "        print(X50.shape)\n",
      "        X75 = getx3(face_filenames75, n, nonfaces, feature_type_sel2, feature_coord_sel2)\n",
      "        print(X75.shape)\n",
      "        X100 = getx3(face_filenames100, n, nonfaces, feature_type_sel2, feature_coord_sel2)\n",
      "        print(X100.shape)\n",
      "        Xt = getx3(test_face_names, 300, nonfaces_t, feature_type_sel2, feature_coord_sel2)\n",
      "        print(Xt.shape)\n",
      "        \n",
      "        rf0 = RandomForestClassifier(n_estimators=1000, max_depth=None, n_jobs=-1, random_state=0)\n",
      "        rf0.fit(X0, y)\n",
      "        print(rf0.score(Xt_900, y_t))\n",
      "        rf25 = RandomForestClassifier(n_estimators=1000, max_depth=None, n_jobs=-1, random_state=0)\n",
      "        rf25.fit(X25, y)\n",
      "        print(rf25.score(Xt_900, y_t))\n",
      "        rf50 = RandomForestClassifier(n_estimators=1000, max_depth=None, n_jobs=-1, random_state=0)\n",
      "        rf50.fit(X50, y)\n",
      "        print(rf50.score(Xt_900, y_t))\n",
      "        rf75 = RandomForestClassifier(n_estimators=1000, max_depth=None, n_jobs=-1, random_state=0)\n",
      "        rf75.fit(X75, y)\n",
      "        print(rf75.score(Xt_900, y_t))\n",
      "        rf100 = RandomForestClassifier(n_estimators=1000, max_depth=None, n_jobs=-1, random_state=0)\n",
      "        rf100.fit(X100, y)\n",
      "        print(rf100.score(Xt_900, y_t))\n",
      "\n",
      "        os_r, bs_r, ws_r = rfscores([rf0, rf25, rf50, rf75, rf100], Xt_900, Xb_t, Xw_t)\n",
      "        os_rs.append(os_r)\n",
      "        bs_rs.append(bs_r)\n",
      "        ws_rs.append(ws_r)\n",
      "    \n",
      "    plt.plot([0, 25, 50, 75, 100], os_r, label=\"o\")\n",
      "    plt.plot([0, 25, 50, 75, 100], bs_r, label=\"b\")\n",
      "    plt.plot([0, 25, 50, 75, 100], ws_r, label=\"w\")\n",
      "    plt.legend()\n",
      "    plt.show()\n",
      "    \n",
      "    print(time()-t_start)\n",
      "    return os_rs, bs_rs, ws_rs\n",
      "159/871: os_rs, bs_rs, ws_rs = pipeline(lambda x: x, 10)\n",
      "159/872:\n",
      "def plots(os_rs, bs_rs, ws_rs):\n",
      "    plt.plot([0,25,50,75,100],np.mean(np.array(os_rs),axis=0),label='o')\n",
      "    plt.plot([0,25,50,75,100],np.mean(np.array(bs_rs),axis=0),label='b')\n",
      "    plt.plot([0,25,50,75,100],np.mean(np.array(ws_rs),axis=0),label='w')\n",
      "    plt.plot([0,25,50,75,100], np.array(np.mean(np.array(os_rs),axis=0)+np.std(np.array(os_rs),axis=0)),':',label='o+')\n",
      "    plt.plot([0,25,50,75,100], np.array(np.mean(np.array(os_rs),axis=0)-np.std(np.array(os_rs),axis=0)),':',label='o-')\n",
      "    plt.plot([0,25,50,75,100], np.array(np.mean(np.array(bs_rs),axis=0)+np.std(np.array(bs_rs),axis=0)),':',label='o+')\n",
      "    plt.plot([0,25,50,75,100], np.array(np.mean(np.array(bs_rs),axis=0)-np.std(np.array(bs_rs),axis=0)),':',label='o-')\n",
      "    plt.plot([0,25,50,75,100], np.array(np.mean(np.array(ws_rs),axis=0)+np.std(np.array(ws_rs),axis=0)),':',label='o+')\n",
      "    plt.plot([0,25,50,75,100], np.array(np.mean(np.array(ws_rs),axis=0)-np.std(np.array(ws_rs),axis=0)),':',label='o-')\n",
      "    plt.legend()\n",
      "    plt.show()\n",
      "159/873: plots(os_rs, bs_rs, ws_rs)\n",
      "159/874:\n",
      "plt.subplot(1,2,1)\n",
      "plt.imshow(np.array(faces_t)[w_inds[0]], cmap='gray')\n",
      "plt.subplot(1,2,2)\n",
      "plt.imshow(sharpen(np.array(faces_t)[w_inds[0]]), cmap='gray')\n",
      "159/875:\n",
      "plt.subplot(1,2,1)\n",
      "plt.imshow(np.array(faces_t)[w_inds[0]], cmap='gray')\n",
      "plt.subplot(1,2,2)\n",
      "plt.imshow(gradient_strength(np.array(faces_t)[w_inds[0]]), cmap='gray')\n",
      "159/876:\n",
      "plt.subplot(1,2,1)\n",
      "plt.imshow(np.array(faces_t)[b_inds[0]], cmap='gray')\n",
      "plt.subplot(1,2,2)\n",
      "plt.imshow(sharpen(np.array(faces_t)[b_inds[0]]), cmap='gray')\n",
      "159/877:\n",
      "plt.subplot(1,2,1)\n",
      "plt.imshow(np.array(faces_t)[b_inds[0]], cmap='gray')\n",
      "plt.subplot(1,2,2)\n",
      "plt.imshow(normalize(np.array(faces_t)[b_inds[0]]), cmap='gray')\n",
      "159/878:\n",
      "plt.subplot(1,2,1)\n",
      "plt.imshow(np.array(faces_t)[b_inds[0]], cmap='gray')\n",
      "plt.subplot(1,2,2)\n",
      "plt.imshow(gradient_strength(np.array(faces_t)[b_inds[0]]), cmap='gray')\n",
      "159/879:\n",
      "feats = list(zip(idx_sortedh, sorted(rf75h.feature_importances_, reverse=True)))\n",
      "i = 0\n",
      "for f in feats:\n",
      "    if f[0] >= 250:\n",
      "        print(i)\n",
      "#         break\n",
      "    i += 1\n",
      "159/880:\n",
      "feats = list(zip(idx_sortedh, sorted(rf75h.feature_importances_, reverse=True)))\n",
      "i = 0\n",
      "for f in feats:\n",
      "    if f[0] >= 250:\n",
      "        print(f)\n",
      "#         break\n",
      "    i += 1\n",
      "159/881:\n",
      "feats = list(zip(idx_sortedh, sorted(rf75h.feature_importances_, reverse=True)))\n",
      "i = 0\n",
      "for f in feats:\n",
      "    if i >= 250:\n",
      "        print(f)\n",
      "#         break\n",
      "    i += 1\n",
      "159/882:\n",
      "feats = list(zip(idx_sortedh, sorted(rf75h.feature_importances_, reverse=True)))\n",
      "i = 0\n",
      "for f in feats:\n",
      "    print(f)\n",
      "    if i >= 250:\n",
      "        print(f)\n",
      "#         break\n",
      "    i += 1\n",
      "159/883:\n",
      "feats = list(zip(idx_sortedh, sorted(rf75h.feature_importances_, reverse=True)))\n",
      "i = 0\n",
      "for f in feats:\n",
      "    if f[0] >= 900:\n",
      "        print(i)\n",
      "        break\n",
      "    i += 1\n",
      "159/884: hog0.shape\n",
      "159/885:\n",
      "rfskinh = RandomForestClassifier(n_estimators=1000, max_depth=None, n_jobs=-1, random_state=0)\n",
      "rfskinh.fit(hog50[:4020], skintones)\n",
      "159/886: rfskinh.score(hogt, skintones_t)\n",
      "159/887: hog50.shape, hogt.shape\n",
      "159/888: rfskinh.score(hogt[:300], skintones_t)\n",
      "159/889:\n",
      "def plots(os_rs, bs_rs, ws_rs):\n",
      "    plt.plot([0,25,50,75,100],np.mean(np.array(os_rs),axis=0),label='o')\n",
      "    plt.plot([0,25,50,75,100],np.mean(np.array(bs_rs),axis=0),label='b')\n",
      "    plt.plot([0,25,50,75,100],np.mean(np.array(ws_rs),axis=0),label='w')\n",
      "    plt.plot([0,25,50,75,100], np.array(np.mean(np.array(os_rs),axis=0)+np.std(np.array(os_rs),axis=0)),':',label='o+')\n",
      "    plt.plot([0,25,50,75,100], np.array(np.mean(np.array(os_rs),axis=0)-np.std(np.array(os_rs),axis=0)),':',label='o-')\n",
      "    plt.plot([0,25,50,75,100], np.array(np.mean(np.array(bs_rs),axis=0)+np.std(np.array(bs_rs),axis=0)),':',label='b+')\n",
      "    plt.plot([0,25,50,75,100], np.array(np.mean(np.array(bs_rs),axis=0)-np.std(np.array(bs_rs),axis=0)),':',label='b-')\n",
      "    plt.plot([0,25,50,75,100], np.array(np.mean(np.array(ws_rs),axis=0)+np.std(np.array(ws_rs),axis=0)),':',label='w+')\n",
      "    plt.plot([0,25,50,75,100], np.array(np.mean(np.array(ws_rs),axis=0)-np.std(np.array(ws_rs),axis=0)),':',label='w-')\n",
      "    plt.legend()\n",
      "    plt.show()\n",
      "159/890: os_rs, bs_rs, ws_rs = pipeline(lambda x: x, 10)\n",
      "159/891: pipeline(sharpen)\n",
      "159/892: plots(os_rs, bs_rs, ws_rs)\n",
      "159/893: plots(os_rsn, bs_rsn, ws_rsn)\n",
      "159/894: pipeline(sharpen, 10)\n",
      "159/895: pipeline(normalize, 10)\n",
      "159/896:\n",
      "os_rsn, bs_rsn, ws_rsn = ([[0.9683333333333334,\n",
      "   0.9816666666666667,\n",
      "   0.985,\n",
      "   0.9883333333333333,\n",
      "   0.9783333333333334],\n",
      "  [0.9716666666666667,\n",
      "   0.9883333333333333,\n",
      "   0.985,\n",
      "   0.9883333333333333,\n",
      "   0.9783333333333334],\n",
      "  [0.9716666666666667, 0.985, 0.985, 0.9883333333333333, 0.9783333333333334],\n",
      "  [0.97, 0.985, 0.9816666666666667, 0.9866666666666667, 0.9783333333333334],\n",
      "  [0.97, 0.985, 0.9766666666666667, 0.9783333333333334, 0.9766666666666667],\n",
      "  [0.97, 0.985, 0.98, 0.9816666666666667, 0.98],\n",
      "  [0.9716666666666667,\n",
      "   0.9866666666666667,\n",
      "   0.98,\n",
      "   0.9816666666666667,\n",
      "   0.9783333333333334],\n",
      "  [0.9683333333333334,\n",
      "   0.9833333333333333,\n",
      "   0.9816666666666667,\n",
      "   0.9816666666666667,\n",
      "   0.98],\n",
      "  [0.9716666666666667,\n",
      "   0.9866666666666667,\n",
      "   0.9766666666666667,\n",
      "   0.98,\n",
      "   0.9783333333333334],\n",
      "  [0.9716666666666667, 0.985, 0.98, 0.9816666666666667, 0.9783333333333334]],\n",
      " [[0.9, 0.9466666666666667, 0.96, 0.98, 0.98],\n",
      "  [0.9066666666666666, 0.9666666666666667, 0.96, 0.98, 0.98],\n",
      "  [0.9066666666666666, 0.96, 0.9666666666666667, 0.98, 0.98],\n",
      "  [0.9, 0.9533333333333334, 0.9533333333333334, 0.98, 0.98],\n",
      "  [0.9066666666666666, 0.9533333333333334, 0.96, 0.98, 0.98],\n",
      "  [0.9, 0.96, 0.9666666666666667, 0.98, 0.98],\n",
      "  [0.9133333333333333, 0.9666666666666667, 0.9666666666666667, 0.98, 0.98],\n",
      "  [0.8933333333333333, 0.9466666666666667, 0.9533333333333334, 0.98, 0.98],\n",
      "  [0.9133333333333333, 0.9666666666666667, 0.96, 0.98, 0.98],\n",
      "  [0.9133333333333333, 0.96, 0.9666666666666667, 0.98, 0.98]],\n",
      " [[0.9866666666666667,\n",
      "   0.9933333333333333,\n",
      "   0.9933333333333333,\n",
      "   0.9933333333333333,\n",
      "   0.9933333333333333],\n",
      "  [0.9866666666666667,\n",
      "   0.9933333333333333,\n",
      "   0.9933333333333333,\n",
      "   0.9933333333333333,\n",
      "   0.9933333333333333],\n",
      "  [0.9866666666666667,\n",
      "   0.9933333333333333,\n",
      "   0.9933333333333333,\n",
      "   0.9933333333333333,\n",
      "   0.9933333333333333],\n",
      "  [0.9866666666666667,\n",
      "   0.9933333333333333,\n",
      "   0.9933333333333333,\n",
      "   0.9933333333333333,\n",
      "   0.9933333333333333],\n",
      "  [0.9866666666666667,\n",
      "   0.9933333333333333,\n",
      "   0.9933333333333333,\n",
      "   0.9866666666666667,\n",
      "   0.9933333333333333],\n",
      "  [0.9866666666666667,\n",
      "   0.9933333333333333,\n",
      "   0.9933333333333333,\n",
      "   0.9933333333333333,\n",
      "   0.9933333333333333],\n",
      "  [0.9866666666666667,\n",
      "   0.9933333333333333,\n",
      "   0.9933333333333333,\n",
      "   0.9933333333333333,\n",
      "   0.9933333333333333],\n",
      "  [0.9866666666666667,\n",
      "   0.9933333333333333,\n",
      "   0.9933333333333333,\n",
      "   0.9933333333333333,\n",
      "   0.9933333333333333],\n",
      "  [0.9866666666666667,\n",
      "   0.9933333333333333,\n",
      "   0.9933333333333333,\n",
      "   0.9933333333333333,\n",
      "   0.9933333333333333],\n",
      "  [0.9866666666666667,\n",
      "   0.9933333333333333,\n",
      "   0.9933333333333333,\n",
      "   0.9933333333333333,\n",
      "   0.9933333333333333]])\n",
      "159/897: plots(os_rsn, bs_rsn, ws_rsn)\n",
      "159/898: pipeline(normalize, 1)\n",
      "159/899: os_rsn, bs_rsn, ws_rsn = pipeline(normalize, 10)\n",
      "159/900:\n",
      "def normalize(img):\n",
      "    img = img.astype(np.int32)\n",
      "#     print(np.mean(img))\n",
      "    m, s = np.mean(img), np.std(img)\n",
      "    img = 125+47*(img-m)/max(1,s)\n",
      "    img[img>255] = 255\n",
      "    img[img<0] = 0\n",
      "#     print(np.mean(img))\n",
      "    return img\n",
      "159/901: os_rsn, bs_rsn, ws_rsn = pipeline(normalize, 10)\n",
      "159/902: plots(os_rsn, bs_rsn, ws_rsn)\n",
      "159/903: X0\n",
      "159/904: X0.shape\n",
      "159/905: X0\n",
      "159/906:\n",
      "def getims2(facefiles):\n",
      "    faces = []\n",
      "    for i in range(n):\n",
      "        faces.append(cv2.imread(facefiles[i], cv2.IMREAD_GRAYSCALE))\n",
      "    return np.array([i.flatten() for i in faces])\n",
      "159/907: eigs50 = getims2(face_filenames50)\n",
      "159/908: eigs50.shape\n",
      "159/909: np.sum(eigs50).shape\n",
      "159/910: np.sum(eigs50)\n",
      "159/911: np.sum(eigs50, axis=0)\n",
      "159/912: np.sum(eigs50, axis=0).shape\n",
      "159/913: np.sum(eigs50, axis=0)/len(eigs50).shape\n",
      "159/914: len(eigs50)\n",
      "159/915: (np.sum(eigs50, axis=0)/len(eigs50)).shape\n",
      "159/916: (np.sum(eigs50, axis=0)/len(eigs50))\n",
      "159/917: eigs50\n",
      "159/918: mean50 = (np.sum(eigs50, axis=0)/len(eigs50))\n",
      "159/919: eigs50 = eigs50 - mean50\n",
      "159/920: eigs50\n",
      "159/921: eigs50.shape\n",
      "159/922: eigs50 = getims2(face_filenames50)\n",
      "159/923: mean50 = (np.sum(eigs50, axis=0)/len(eigs50))\n",
      "159/924: mean50\n",
      "159/925: eigs50 = eigs50 - mean50\n",
      "159/926: eigs50\n",
      "159/927: cov = np.transpose(eigs50)*eigs50\n",
      "159/928: cov = np.multiply(np.transpose(eigs50), eigs50)\n",
      "159/929: [1,2]*[[1,2],[3,4]]\n",
      "159/930: np.array([1,2])*np.array([[1,2],[3,4]])\n",
      "159/931: np.transpose(eigs50)\n",
      "159/932: np.transpose(eigs50).shape\n",
      "159/933: np.array([1,2])*np.array([[1,2],[3,4],[5,6]])\n",
      "159/934: np.transpose(eigs50)\n",
      "159/935: eigs50\n",
      "159/936: eigs50.shape\n",
      "159/937: cov = np.dot(np.transpose(eigs50), eigs50)\n",
      "163/1:\n",
      "t_start = time()\n",
      "cov = np.dot(np.transpose(eigs50), eigs50)\n",
      "time() - t_start\n",
      "163/2:\n",
      "import os\n",
      "import cv2\n",
      "from time import time\n",
      "import numpy as np\n",
      "import matplotlib.pyplot as plt\n",
      "from glob import glob\n",
      "import random\n",
      "from dask import delayed\n",
      "163/3:\n",
      "from sklearn.ensemble import RandomForestClassifier\n",
      "from sklearn.model_selection import train_test_split\n",
      "from sklearn.metrics import roc_auc_score\n",
      "from sklearn.linear_model import LogisticRegression\n",
      "163/4:\n",
      "from skimage.transform import integral_image\n",
      "from skimage.feature import haar_like_feature\n",
      "from skimage.feature import haar_like_feature_coord\n",
      "from skimage.feature import draw_haar_like_feature\n",
      "163/5: import dill\n",
      "163/6:\n",
      "face_filenames0 = sorted(glob(os.path.join('data/train_0', '*.jpg')))\n",
      "num_face_filenames0 = len(face_filenames0)\n",
      "num_face_filenames0\n",
      "163/7:\n",
      "face_filenames25 = sorted(glob(os.path.join('data/train_25', '*.jpg')))\n",
      "num_face_filenames25 = len(face_filenames25)\n",
      "num_face_filenames25\n",
      "163/8:\n",
      "face_filenames50 = sorted(glob(os.path.join('data/train_50', '*.jpg')))\n",
      "num_face_filenames50 = len(face_filenames50)\n",
      "num_face_filenames50\n",
      "163/9:\n",
      "face_filenames75 = sorted(glob(os.path.join('data/train_75', '*.jpg')))\n",
      "num_face_filenames75 = len(face_filenames75)\n",
      "num_face_filenames75\n",
      "163/10:\n",
      "face_filenames100 = sorted(glob(os.path.join('data/train_100', '*.jpg')))\n",
      "num_face_filenames100 = len(face_filenames100)\n",
      "num_face_filenames100\n",
      "163/11:\n",
      "nonface_filenames = sorted(glob(os.path.join('data/training_nonfaces', '*.jpg')))\n",
      "num_nonface_filenames = len(nonface_filenames)\n",
      "num_nonface_filenames\n",
      "163/12:\n",
      "test_face_names = sorted(glob(os.path.join('data/tests', '*.jpg')))\n",
      "n_test = len(test_face_names)\n",
      "163/13:\n",
      "b_inds = []\n",
      "w_inds = []\n",
      "for i in range(len(test_face_names)):\n",
      "    if int(test_face_names[i].split('_')[2]) is 0: # race is white\n",
      "        w_inds.append(i)\n",
      "    else:\n",
      "        b_inds.append(i)\n",
      "163/14:\n",
      "test_nonface_names = sorted(glob(os.path.join('data/testing_nonfaces', '*.jpg')))\n",
      "n_nonfaces_test = len(test_nonface_names)\n",
      "163/15:\n",
      "faces_t = []\n",
      "for i in range(n_test):\n",
      "    faces_t.append(cv2.imread(test_face_names[i], cv2.IMREAD_GRAYSCALE))\n",
      "163/16:\n",
      "nonfaces_t = []\n",
      "for i in range(n_test):\n",
      "    # Read a random nonface file\n",
      "    j = random.randint(0, n_nonfaces_test - 1)\n",
      "    nonface = cv2.imread(test_nonface_names[j], cv2.IMREAD_GRAYSCALE)\n",
      "    \n",
      "    wsize = random.randint(36, min(nonface.shape[0], nonface.shape[1]))\n",
      "    row = random.randint(0, nonface.shape[0]-wsize)\n",
      "    col = random.randint(0, nonface.shape[1]-wsize)\n",
      "    crop = nonface[row:row+wsize, col:col+wsize]\n",
      "\n",
      "    # Resize to be the right size\n",
      "    crop = cv2.resize(crop, (200, 200))\n",
      "    nonfaces_t.append(crop)\n",
      "163/17:\n",
      "images_t = faces_t + nonfaces_t\n",
      "len(images_t)\n",
      "163/18:\n",
      "t_start = time()\n",
      "cov = np.dot(np.transpose(eigs50), eigs50)\n",
      "time() - t_start\n",
      "163/19:\n",
      "def getims2(facefiles):\n",
      "    faces = []\n",
      "    for i in range(n):\n",
      "        faces.append(cv2.imread(facefiles[i], cv2.IMREAD_GRAYSCALE))\n",
      "    return np.array([i.flatten() for i in faces])\n",
      "163/20: eigs50 = getims2(face_filenames50)\n",
      "163/21: n = 4020\n",
      "163/22:\n",
      "def getims2(facefiles):\n",
      "    faces = []\n",
      "    for i in range(n):\n",
      "        faces.append(cv2.imread(facefiles[i], cv2.IMREAD_GRAYSCALE))\n",
      "    return np.array([i.flatten() for i in faces])\n",
      "163/23: eigs50 = getims2(face_filenames50)\n",
      "163/24: eigs50.shape\n",
      "163/25: eigs50\n",
      "163/26: (np.sum(eigs50, axis=0)/len(eigs50)).shape\n",
      "163/27: mean50 = (np.sum(eigs50, axis=0)/len(eigs50))\n",
      "163/28: mean50\n",
      "163/29: eigs50 = eigs50 - mean50\n",
      "163/30: eigs50\n",
      "163/31: eigs50.shape\n",
      "163/32:\n",
      "t_start = time()\n",
      "cov = np.dot(np.transpose(eigs50), eigs50)\n",
      "time() - t_start\n",
      "164/1: n = 300\n",
      "164/2: n = 4020\n",
      "164/3:\n",
      "def getims2(facefiles):\n",
      "    faces = []\n",
      "    for i in range(n):\n",
      "        faces.append(cv2.imread(facefiles[i], cv2.IMREAD_GRAYSCALE))\n",
      "    return np.array([cv2.resize(i,(36,36)).flatten() for i in faces])\n",
      "164/4: eigs50 = getims2(face_filenames50)\n",
      "164/5:\n",
      "import os\n",
      "import cv2\n",
      "from time import time\n",
      "import numpy as np\n",
      "import matplotlib.pyplot as plt\n",
      "from glob import glob\n",
      "import random\n",
      "from dask import delayed\n",
      "164/6:\n",
      "from sklearn.ensemble import RandomForestClassifier\n",
      "from sklearn.model_selection import train_test_split\n",
      "from sklearn.metrics import roc_auc_score\n",
      "from sklearn.linear_model import LogisticRegression\n",
      "164/7:\n",
      "from skimage.transform import integral_image\n",
      "from skimage.feature import haar_like_feature\n",
      "from skimage.feature import haar_like_feature_coord\n",
      "from skimage.feature import draw_haar_like_feature\n",
      "164/8: import dill\n",
      "164/9:\n",
      "test_face_names = sorted(glob(os.path.join('data/tests', '*.jpg')))\n",
      "n_test = len(test_face_names)\n",
      "164/10:\n",
      "b_inds = []\n",
      "w_inds = []\n",
      "for i in range(len(test_face_names)):\n",
      "    if int(test_face_names[i].split('_')[2]) is 0: # race is white\n",
      "        w_inds.append(i)\n",
      "    else:\n",
      "        b_inds.append(i)\n",
      "164/11:\n",
      "test_nonface_names = sorted(glob(os.path.join('data/testing_nonfaces', '*.jpg')))\n",
      "n_nonfaces_test = len(test_nonface_names)\n",
      "164/12:\n",
      "faces_t = []\n",
      "for i in range(n_test):\n",
      "    faces_t.append(cv2.imread(test_face_names[i], cv2.IMREAD_GRAYSCALE))\n",
      "164/13:\n",
      "nonfaces_t = []\n",
      "for i in range(n_test):\n",
      "    # Read a random nonface file\n",
      "    j = random.randint(0, n_nonfaces_test - 1)\n",
      "    nonface = cv2.imread(test_nonface_names[j], cv2.IMREAD_GRAYSCALE)\n",
      "    \n",
      "    wsize = random.randint(36, min(nonface.shape[0], nonface.shape[1]))\n",
      "    row = random.randint(0, nonface.shape[0]-wsize)\n",
      "    col = random.randint(0, nonface.shape[1]-wsize)\n",
      "    crop = nonface[row:row+wsize, col:col+wsize]\n",
      "\n",
      "    # Resize to be the right size\n",
      "    crop = cv2.resize(crop, (200, 200))\n",
      "    nonfaces_t.append(crop)\n",
      "164/14:\n",
      "images_t = faces_t + nonfaces_t\n",
      "len(images_t)\n",
      "164/15:\n",
      "face_filenames0 = sorted(glob(os.path.join('data/train_0', '*.jpg')))\n",
      "num_face_filenames0 = len(face_filenames0)\n",
      "num_face_filenames0\n",
      "164/16:\n",
      "face_filenames25 = sorted(glob(os.path.join('data/train_25', '*.jpg')))\n",
      "num_face_filenames25 = len(face_filenames25)\n",
      "num_face_filenames25\n",
      "164/17:\n",
      "face_filenames50 = sorted(glob(os.path.join('data/train_50', '*.jpg')))\n",
      "num_face_filenames50 = len(face_filenames50)\n",
      "num_face_filenames50\n",
      "164/18:\n",
      "face_filenames75 = sorted(glob(os.path.join('data/train_75', '*.jpg')))\n",
      "num_face_filenames75 = len(face_filenames75)\n",
      "num_face_filenames75\n",
      "164/19:\n",
      "face_filenames100 = sorted(glob(os.path.join('data/train_100', '*.jpg')))\n",
      "num_face_filenames100 = len(face_filenames100)\n",
      "num_face_filenames100\n",
      "164/20:\n",
      "nonface_filenames = sorted(glob(os.path.join('data/training_nonfaces', '*.jpg')))\n",
      "num_nonface_filenames = len(nonface_filenames)\n",
      "num_nonface_filenames\n",
      "164/21: eigs50 = getims2(face_filenames50)\n",
      "164/22: eigs50.shape\n",
      "164/23: eigs50\n",
      "164/24: (np.sum(eigs50, axis=0)/len(eigs50)).shape\n",
      "164/25: mean50 = (np.sum(eigs50, axis=0)/len(eigs50))\n",
      "164/26: mean50\n",
      "164/27: eigs50 = eigs50 - mean50\n",
      "164/28: eigs50\n",
      "164/29: eigs50.shape\n",
      "164/30:\n",
      "t_start = time()\n",
      "cov = np.dot(np.transpose(eigs50), eigs50)\n",
      "time() - t_start\n",
      "164/31: cov.shape\n",
      "164/32: u, s, vh = np.linalg.svd(cov)\n",
      "164/33: u.shape, s.shape, vh.shape\n",
      "164/34: s\n",
      "164/35: plt.plot(s)\n",
      "164/36: plt.plot(s[:200])\n",
      "164/37: plt.plot(s[:100])\n",
      "164/38: plt.plot(s[:50])\n",
      "164/39: plt.plot(s[:100])\n",
      "164/40: s\n",
      "164/41: cumsums = np.cumsum(s)\n",
      "164/42: cumsums\n",
      "164/43:\n",
      "for i in range(0,60,5):\n",
      "    print s[i]\n",
      "164/44:\n",
      "for i in range(0,60,5):\n",
      "    print(s[i])\n",
      "164/45:\n",
      "for i in range(0,80,5):\n",
      "    print(s[i])\n",
      "164/46:\n",
      "for i in range(0,200,5):\n",
      "    print(s[i])\n",
      "164/47:\n",
      "for i in range(0,30,5):\n",
      "    print(s[i])\n",
      "164/48:\n",
      "for i in range(0,40,5):\n",
      "    print(s[i])\n",
      "164/49:\n",
      "for i in range(0,60,5):\n",
      "    print(s[i])\n",
      "164/50:\n",
      "for i in range(0,80,5):\n",
      "    print(s[i])\n",
      "164/51:\n",
      "for i in range(1,80):\n",
      "    print(s[i]/s[i-1])\n",
      "164/52:\n",
      "for i in range(1,80):\n",
      "    print((s[i]-s[i-1])/s[i-1])\n",
      "164/53:\n",
      "for i in range(1,80):\n",
      "    print((s[i-1]-s[i])/s[i-1])\n",
      "164/54:\n",
      "for i in range(1,50):\n",
      "    print((s[i-1]-s[i])/s[i-1])\n",
      "164/55:\n",
      "for i in range(1,25):\n",
      "    print((s[i-1]-s[i])/s[i-1])\n",
      "164/56:\n",
      "for i in range(1,40):\n",
      "    print((s[i-1]-s[i])/s[i-1])\n",
      "164/57: u, v = np.linalg.eigh(cov)\n",
      "164/58: u2,v2 = np.linalg.eig(cov)\n",
      "164/59: u, u2, v, v2\n",
      "164/60: u, u2, v.shape, v2.shape\n",
      "164/61: u, u2, v, v2\n",
      "164/62: u, v = np.linalg.eig(cov)\n",
      "164/63: w, v = np.linalg.eigh(cov)\n",
      "164/64: w\n",
      "164/65: v[:,1295]\n",
      "164/66: v[:,1295].shape\n",
      "164/67: v[:,1295]\n",
      "164/68: plt.imshow(v[:,1295])\n",
      "164/69: plt.imshow(v[:,1295].reshape(36,36))\n",
      "164/70: plt.imshow(v[:,1295].reshape(36,36), cmap='gray')\n",
      "164/71: plt.imshow(v[:,1294].reshape(36,36), cmap='gray')\n",
      "164/72:\n",
      "fig, axes = plt.subplots(1,4, figsize=(12,4))\n",
      "for idx, ax in enumerate(axes.ravel()):\n",
      "    image = v[:,1296-idx]\n",
      "    ax.imshow(image)\n",
      "    ax.set_xticks([])\n",
      "    ax.set_yticks([])\n",
      "\n",
      "fig.suptitle('The most important features')\n",
      "164/73:\n",
      "fig, axes = plt.subplots(1,4, figsize=(12,4))\n",
      "for idx, ax in enumerate(axes.ravel()):\n",
      "    image = v[:,1296-idx-1]\n",
      "    ax.imshow(image)\n",
      "    ax.set_xticks([])\n",
      "    ax.set_yticks([])\n",
      "\n",
      "fig.suptitle('The most important features')\n",
      "164/74:\n",
      "fig, axes = plt.subplots(1,4, figsize=(12,4))\n",
      "for idx, ax in enumerate(axes.ravel()):\n",
      "    image = v[:,1296-idx-1]\n",
      "    ax.imshow(image.reshape(36,36))\n",
      "    ax.set_xticks([])\n",
      "    ax.set_yticks([])\n",
      "\n",
      "fig.suptitle('The most important features')\n",
      "164/75:\n",
      "fig, axes = plt.subplots(2,4, figsize=(12,4))\n",
      "for idx, ax in enumerate(axes.ravel()):\n",
      "    image = v[:,1296-idx-1]\n",
      "    ax.imshow(image.reshape(36,36))\n",
      "    ax.set_xticks([])\n",
      "    ax.set_yticks([])\n",
      "\n",
      "fig.suptitle('The most important features')\n",
      "164/76:\n",
      "fig, axes = plt.subplots(5,5, figsize=(20,20))\n",
      "for idx, ax in enumerate(axes.ravel()):\n",
      "    image = v[:,1296-idx-1]\n",
      "    ax.imshow(image.reshape(36,36))\n",
      "    ax.set_xticks([])\n",
      "    ax.set_yticks([])\n",
      "\n",
      "fig.suptitle('The most important features')\n",
      "164/77:\n",
      "fig, axes = plt.subplots(5,5, figsize=(20,20))\n",
      "for idx, ax in enumerate(axes.ravel()):\n",
      "    image = v[:,1296-idx-1]\n",
      "    ax.imshow(image.reshape(36,36), cmap='gray')\n",
      "    ax.set_xticks([])\n",
      "    ax.set_yticks([])\n",
      "\n",
      "fig.suptitle('The most important features')\n",
      "164/78:\n",
      "fig, axes = plt.subplots(8,8, figsize=(20,20))\n",
      "for idx, ax in enumerate(axes.ravel()):\n",
      "    image = v[:,1296-idx-1]\n",
      "    ax.imshow(image.reshape(36,36), cmap='gray')\n",
      "    ax.set_xticks([])\n",
      "    ax.set_yticks([])\n",
      "\n",
      "fig.suptitle('The most important features')\n",
      "164/79: v[:,1]\n",
      "164/80: plt.imshow(v[:,0].reshape(36,36), cmap='gray')\n",
      "164/81: plt.imshow(v[:,1000].reshape(36,36), cmap='gray')\n",
      "164/82: plt.imshow(v[:,1500].reshape(36,36), cmap='gray')\n",
      "164/83: plt.imshow(v[:,1200].reshape(36,36), cmap='gray')\n",
      "164/84: plt.imshow(v[:,1100].reshape(36,36), cmap='gray')\n",
      "164/85: plt.imshow(v[:,1150].reshape(36,36), cmap='gray')\n",
      "164/86: plt.imshow(v[:,1196].reshape(36,36), cmap='gray')\n",
      "164/87: plt.imshow(v[:,1190].reshape(36,36), cmap='gray')\n",
      "164/88:\n",
      "fig, axes = plt.subplots(5,5, figsize=(20,20))\n",
      "for idx, ax in enumerate(axes.ravel()):\n",
      "    image = v[:,1296-idx-1]\n",
      "    ax.imshow(image.reshape(36,36), cmap='gray')\n",
      "    ax.set_xticks([])\n",
      "    ax.set_yticks([])\n",
      "\n",
      "fig.suptitle('The most important features')\n",
      "164/89:\n",
      "fig, axes = plt.subplots(4,4, figsize=(16,16))\n",
      "for idx, ax in enumerate(axes.ravel()):\n",
      "    image = v[:,1296-idx-1]\n",
      "    ax.imshow(image.reshape(36,36), cmap='gray')\n",
      "    ax.set_xticks([])\n",
      "    ax.set_yticks([])\n",
      "\n",
      "fig.suptitle('The most important features')\n",
      "164/90:\n",
      "fig, axes = plt.subplots(2,4, figsize=(16,16))\n",
      "for idx, ax in enumerate(axes.ravel()):\n",
      "    image = v[:,1296-idx-1]\n",
      "    ax.imshow(image.reshape(36,36), cmap='gray')\n",
      "    ax.set_xticks([])\n",
      "    ax.set_yticks([])\n",
      "\n",
      "fig.suptitle('The most important features')\n",
      "164/91:\n",
      "fig, axes = plt.subplots(2,4, figsize=(16,4))\n",
      "for idx, ax in enumerate(axes.ravel()):\n",
      "    image = v[:,1296-idx-1]\n",
      "    ax.imshow(image.reshape(36,36), cmap='gray')\n",
      "    ax.set_xticks([])\n",
      "    ax.set_yticks([])\n",
      "\n",
      "fig.suptitle('The most important features')\n",
      "164/92:\n",
      "fig, axes = plt.subplots(2,4, figsize=(16,6))\n",
      "for idx, ax in enumerate(axes.ravel()):\n",
      "    image = v[:,1296-idx-1]\n",
      "    ax.imshow(image.reshape(36,36), cmap='gray')\n",
      "    ax.set_xticks([])\n",
      "    ax.set_yticks([])\n",
      "\n",
      "fig.suptitle('The most important features')\n",
      "164/93: plt.imshow(mean50)\n",
      "164/94: plt.imshow(mean50.reshape(36,36))\n",
      "164/95: plt.imshow(mean50.reshape(36,36), cmap='gray')\n",
      "164/96: norm(v[:,1290])\n",
      "164/97: np.linalg/norm(v[:,1290])\n",
      "164/98: np.linalg.norm(v[:,1290])\n",
      "164/99: np.linalg.norm(v[:,10])\n",
      "164/100:\n",
      "for i in images_t:\n",
      "    print(i)\n",
      "164/101: images_t.shape\n",
      "164/102: len(images_t)\n",
      "164/103: images_t[0]\n",
      "164/104: plt.imshow(images_t[0], cmap='gray')\n",
      "164/105: images_t[0].flatten()-mean50\n",
      "164/106: cv.resize(images_t[0],(36,36)).flatten()-mean50\n",
      "164/107: cv2.resize(images_t[0],(36,36)).flatten()-mean50\n",
      "164/108: phi = cv2.resize(images_t[0],(36,36)).flatten()-mean50\n",
      "164/109: topeigs50 = [v[:,1296-i-1] for i in range(50)]\n",
      "164/110: phi_hat = sum([np.dot(e, phi)*e for e in topeigs50])\n",
      "164/111: phi_hat.shape\n",
      "164/112: np.linalg.norm(phi_hat - phi)\n",
      "164/113:\n",
      "dists50 = []\n",
      "for i in range(len(images_t)):\n",
      "    phi = cv2.resize(images_t[i],(36,36)).flatten()-mean50\n",
      "    phi_hat = sum([np.dot(e, phi)*e for e in topeigs50])\n",
      "    dist = np.linalg.norm(phi_hat - phi)\n",
      "    dists50.append(dist)\n",
      "164/114: plt.hist(dists50)\n",
      "164/115:\n",
      "def getacc(predicted, actual):\n",
      "    preds = []\n",
      "    for p in predicted:\n",
      "        if p >= 0.5:\n",
      "            preds.append(1)\n",
      "        else:\n",
      "            preds.append(0)\n",
      "    return 1 - sum(abs(preds-actual))/len(preds)\n",
      "164/116:\n",
      "predictions = []\n",
      "for i in range(len(images_t)):\n",
      "    phi = cv2.resize(images_t[i],(36,36)).flatten()-mean50\n",
      "    phi_hat = sum([np.dot(e, phi)*e for e in topeigs50])\n",
      "    dist = np.linalg.norm(phi_hat - phi)\n",
      "    if dist >= 870:\n",
      "        predictions.append(0)\n",
      "    else:\n",
      "        predictions.append(1)\n",
      "getacc(predictions, y_t)\n",
      "164/117: y_t = [1]*150 + [0]*150\n",
      "164/118:\n",
      "predictions = []\n",
      "for i in range(len(images_t)):\n",
      "    phi = cv2.resize(images_t[i],(36,36)).flatten()-mean50\n",
      "    phi_hat = sum([np.dot(e, phi)*e for e in topeigs50])\n",
      "    dist = np.linalg.norm(phi_hat - phi)\n",
      "    if dist >= 870:\n",
      "        predictions.append(0)\n",
      "    else:\n",
      "        predictions.append(1)\n",
      "getacc(predictions, y_t)\n",
      "164/119: y_t = np.array([1]*150 + [0]*150)\n",
      "164/120:\n",
      "predictions = []\n",
      "for i in range(len(images_t)):\n",
      "    phi = cv2.resize(images_t[i],(36,36)).flatten()-mean50\n",
      "    phi_hat = sum([np.dot(e, phi)*e for e in topeigs50])\n",
      "    dist = np.linalg.norm(phi_hat - phi)\n",
      "    if dist >= 870:\n",
      "        predictions.append(0)\n",
      "    else:\n",
      "        predictions.append(1)\n",
      "getacc(np.array(predictions), y_t)\n",
      "164/121: images_t.shape\n",
      "164/122: len(images_t)\n",
      "164/123:\n",
      "predictions = []\n",
      "for i in range(len(images_t)):\n",
      "    phi = cv2.resize(images_t[i],(36,36)).flatten()-mean50\n",
      "    phi_hat = sum([np.dot(e, phi)*e for e in topeigs50])\n",
      "    dist = np.linalg.norm(phi_hat - phi)\n",
      "    if dist >= 870:\n",
      "        predictions.append(0)\n",
      "    else:\n",
      "        predictions.append(1)\n",
      "164/124: len(predictions)\n",
      "164/125: getacc(np.array(predictions), y_t)\n",
      "164/126: 1 - sum(abs(predictions-actual))/len(preds)\n",
      "164/127: 1 - sum(abs(predictions-actual))/len(predictions)\n",
      "164/128: 1 - sum(abs(predictions-y_t))/len(predictions)\n",
      "164/129: predictions-y_t\n",
      "164/130: y_t = np.array([1]*300 + [0]*300)\n",
      "164/131: getacc(predictions, y_t)\n",
      "164/132:\n",
      "def eigacc(t):\n",
      "    predictions = []\n",
      "    for i in range(len(images_t)):\n",
      "        phi = cv2.resize(images_t[i],(36,36)).flatten()-mean50\n",
      "        phi_hat = sum([np.dot(e, phi)*e for e in topeigs50])\n",
      "        dist = np.linalg.norm(phi_hat - phi)\n",
      "        if dist >= 870:\n",
      "            predictions.append(0)\n",
      "        else:\n",
      "            predictions.append(1)\n",
      "    return getacc(predictions, y_t)\n",
      "164/133:\n",
      "def eigacc(t):\n",
      "    predictions = []\n",
      "    for i in range(len(images_t)):\n",
      "        phi = cv2.resize(images_t[i],(36,36)).flatten()-mean50\n",
      "        phi_hat = sum([np.dot(e, phi)*e for e in topeigs50])\n",
      "        dist = np.linalg.norm(phi_hat - phi)\n",
      "        if dist >= t:\n",
      "            predictions.append(0)\n",
      "        else:\n",
      "            predictions.append(1)\n",
      "    return getacc(predictions, y_t)\n",
      "164/134:\n",
      "for i in range(600,1000,20):\n",
      "    print(eigacc(i))\n",
      "164/135: topeigs50 = [v[:,1296-i-1] for i in range(100)]\n",
      "164/136:\n",
      "def eigacc(t):\n",
      "    predictions = []\n",
      "    for i in range(len(images_t)):\n",
      "        phi = cv2.resize(images_t[i],(36,36)).flatten()-mean50\n",
      "        phi_hat = sum([np.dot(e, phi)*e for e in topeigs50])\n",
      "        dist = np.linalg.norm(phi_hat - phi)\n",
      "        if dist >= t:\n",
      "            predictions.append(0)\n",
      "        else:\n",
      "            predictions.append(1)\n",
      "    return getacc(predictions, y_t)\n",
      "164/137:\n",
      "for i in range(600,1000,20):\n",
      "    print(eigacc(i))\n",
      "164/138: topeigs50 = [v[:,1296-i-1] for i in range(40)]\n",
      "164/139: phi_hat = sum([np.dot(e, phi)*e for e in topeigs50])\n",
      "164/140: np.linalg.norm(phi_hat - phi)\n",
      "164/141:\n",
      "dists50 = []\n",
      "for i in range(len(images_t)):\n",
      "    phi = cv2.resize(images_t[i],(36,36)).flatten()-mean50\n",
      "    phi_hat = sum([np.dot(e, phi)*e for e in topeigs50])\n",
      "    dist = np.linalg.norm(phi_hat - phi)\n",
      "    dists50.append(dist)\n",
      "164/142: plt.hist(dists50)\n",
      "164/143:\n",
      "def eigacc(t):\n",
      "    predictions = []\n",
      "    for i in range(len(images_t)):\n",
      "        phi = cv2.resize(images_t[i],(36,36)).flatten()-mean50\n",
      "        phi_hat = sum([np.dot(e, phi)*e for e in topeigs50])\n",
      "        dist = np.linalg.norm(phi_hat - phi)\n",
      "        if dist >= t:\n",
      "            predictions.append(0)\n",
      "        else:\n",
      "            predictions.append(1)\n",
      "    return getacc(predictions, y_t)\n",
      "164/144:\n",
      "for i in range(600,1000,20):\n",
      "    print(eigacc(i))\n",
      "164/145: topeigs50 = [v[:,1296-i-1] for i in range(30)]\n",
      "164/146: phi_hat = sum([np.dot(e, phi)*e for e in topeigs50])\n",
      "164/147: np.linalg.norm(phi_hat - phi)\n",
      "164/148:\n",
      "dists50 = []\n",
      "for i in range(len(images_t)):\n",
      "    phi = cv2.resize(images_t[i],(36,36)).flatten()-mean50\n",
      "    phi_hat = sum([np.dot(e, phi)*e for e in topeigs50])\n",
      "    dist = np.linalg.norm(phi_hat - phi)\n",
      "    dists50.append(dist)\n",
      "164/149: plt.hist(dists50)\n",
      "164/150: y_t = np.array([1]*300 + [0]*300)\n",
      "164/151:\n",
      "def eigacc(t):\n",
      "    predictions = []\n",
      "    for i in range(len(images_t)):\n",
      "        phi = cv2.resize(images_t[i],(36,36)).flatten()-mean50\n",
      "        phi_hat = sum([np.dot(e, phi)*e for e in topeigs50])\n",
      "        dist = np.linalg.norm(phi_hat - phi)\n",
      "        if dist >= t:\n",
      "            predictions.append(0)\n",
      "        else:\n",
      "            predictions.append(1)\n",
      "    return getacc(predictions, y_t)\n",
      "164/152:\n",
      "for i in range(600,1000,20):\n",
      "    print(eigacc(i))\n",
      "164/153: topeigs50 = [v[:,1296-i-1] for i in range(20)]\n",
      "164/154: phi_hat = sum([np.dot(e, phi)*e for e in topeigs50])\n",
      "164/155: np.linalg.norm(phi_hat - phi)\n",
      "164/156:\n",
      "dists50 = []\n",
      "for i in range(len(images_t)):\n",
      "    phi = cv2.resize(images_t[i],(36,36)).flatten()-mean50\n",
      "    phi_hat = sum([np.dot(e, phi)*e for e in topeigs50])\n",
      "    dist = np.linalg.norm(phi_hat - phi)\n",
      "    dists50.append(dist)\n",
      "164/157: plt.hist(dists50)\n",
      "164/158:\n",
      "def eigacc(t):\n",
      "    predictions = []\n",
      "    for i in range(len(images_t)):\n",
      "        phi = cv2.resize(images_t[i],(36,36)).flatten()-mean50\n",
      "        phi_hat = sum([np.dot(e, phi)*e for e in topeigs50])\n",
      "        dist = np.linalg.norm(phi_hat - phi)\n",
      "        if dist >= t:\n",
      "            predictions.append(0)\n",
      "        else:\n",
      "            predictions.append(1)\n",
      "    return getacc(predictions, y_t)\n",
      "164/159:\n",
      "for i in range(600,1200,20):\n",
      "    print(eigacc(i))\n",
      "164/160: topeigs50 = [v[:,1296-i-1] for i in range(30)]\n",
      "164/161: phi_hat = sum([np.dot(e, phi)*e for e in topeigs50])\n",
      "164/162: np.linalg.norm(phi_hat - phi)\n",
      "164/163:\n",
      "dists50 = []\n",
      "for i in range(len(images_t)):\n",
      "    phi = cv2.resize(images_t[i],(36,36)).flatten()-mean50\n",
      "    phi_hat = sum([np.dot(e, phi)*e for e in topeigs50])\n",
      "    dist = np.linalg.norm(phi_hat - phi)\n",
      "    dists50.append(dist)\n",
      "164/164: plt.hist(dists50)\n",
      "164/165: y_t = np.array([1]*300 + [0]*300)\n",
      "164/166:\n",
      "def eigacc(t):\n",
      "    predictions = []\n",
      "    for i in range(len(images_t)):\n",
      "        phi = cv2.resize(images_t[i],(36,36)).flatten()-mean50\n",
      "        phi_hat = sum([np.dot(e, phi)*e for e in topeigs50])\n",
      "        dist = np.linalg.norm(phi_hat - phi)\n",
      "        if dist >= t:\n",
      "            predictions.append(0)\n",
      "        else:\n",
      "            predictions.append(1)\n",
      "    return getacc(predictions, y_t)\n",
      "164/167:\n",
      "for i in range(600,1200,20):\n",
      "    print(eigacc(i))\n",
      "164/168:\n",
      "eigaccs = [eigacc(i) for i in range(600,1200,20)]\n",
      "np.max(eigaccs), np.argmax(eigaccs)\n",
      "164/169:\n",
      "eigaccs = [(eigacc(i), i) for i in range(600,1200,20)]\n",
      "np.max(eigaccs)\n",
      "164/170:\n",
      "eigaccs = [eigacc(i) for i in range(600,1200,20)]\n",
      "np.max(eigaccs), np.argmax(eigaccs)*20+600\n",
      "164/171: np.argmax([1,0])\n",
      "164/172: phi = cv2.resize(images_t[0],(36,36)).flatten()-mean50\n",
      "164/173: topeigs50 = [v[:,1296-i-1] for i in range(30)]\n",
      "164/174: phi_hat = sum([np.dot(e, phi)*e for e in topeigs50])\n",
      "164/175: np.linalg.norm(phi_hat - phi)\n",
      "164/176: plt.plot(w[:100])\n",
      "164/177: plt.plot(s[:100])\n",
      "164/178: plt.plot(w)\n",
      "164/179: plt.plot(sorted(w, reverse))\n",
      "164/180: plt.plot(sorted(w, reverse=True))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "164/181: plt.plot(sorted(w, reverse=True)[:100])\n",
      "164/182:\n",
      "def eigacc(t, m, topeigs):\n",
      "    predictions = []\n",
      "    for i in range(len(images_t)):\n",
      "        phi = cv2.resize(images_t[i],(36,36)).flatten()-m\n",
      "        phi_hat = sum([np.dot(e, phi)*e for e in topeigs])\n",
      "        dist = np.linalg.norm(phi_hat - phi)\n",
      "        if dist >= t:\n",
      "            predictions.append(0)\n",
      "        else:\n",
      "            predictions.append(1)\n",
      "    return getacc(predictions, y_t)\n",
      "164/183: eigacc(1000, mean50, topeigs50)\n",
      "164/184:\n",
      "def eigfaces(face_filenames):\n",
      "    eigs = getims2(face_filenames)\n",
      "    m = (np.sum(eigs, axis=0)/len(eigs))\n",
      "    eigs = eigs - m\n",
      "    plt.imshow(.reshape(36,36), cmap='gray')\n",
      "    \n",
      "    cov = np.dot(np.transpose(eigs), eigs)\n",
      "    w, v = np.linalg.eigh(cov)\n",
      "    \n",
      "    plt.plot(sorted(w, reverse=True)[:100])\n",
      "    topeigs = [v[:,1296-i-1] for i in range(30)]\n",
      "    \n",
      "    fig, axes = plt.subplots(2,4, figsize=(16,6))\n",
      "    for idx, ax in enumerate(axes.ravel()):\n",
      "        image = v[:,1296-idx-1]\n",
      "        ax.imshow(image.reshape(36,36), cmap='gray')\n",
      "        ax.set_xticks([])\n",
      "        ax.set_yticks([])\n",
      "    \n",
      "    dists = []\n",
      "    for i in range(len(images_t)):\n",
      "        phi = cv2.resize(images_t[i],(36,36)).flatten()-m\n",
      "        phi_hat = sum([np.dot(e, phi)*e for e in topeigs])\n",
      "        dist = np.linalg.norm(phi_hat - phi)\n",
      "        dists.append(dist)\n",
      "    plt.hist(dists)\n",
      "    \n",
      "    return m, topeigs, w, v\n",
      "164/185:\n",
      "def eigfaces(face_filenames):\n",
      "    eigs = getims2(face_filenames)\n",
      "    m = (np.sum(eigs, axis=0)/len(eigs))\n",
      "    eigs = eigs - m\n",
      "    plt.imshow(m.reshape(36,36), cmap='gray')\n",
      "    \n",
      "    cov = np.dot(np.transpose(eigs), eigs)\n",
      "    w, v = np.linalg.eigh(cov)\n",
      "    \n",
      "    plt.plot(sorted(w, reverse=True)[:100])\n",
      "    topeigs = [v[:,1296-i-1] for i in range(30)]\n",
      "    \n",
      "    fig, axes = plt.subplots(2,4, figsize=(16,6))\n",
      "    for idx, ax in enumerate(axes.ravel()):\n",
      "        image = v[:,1296-idx-1]\n",
      "        ax.imshow(image.reshape(36,36), cmap='gray')\n",
      "        ax.set_xticks([])\n",
      "        ax.set_yticks([])\n",
      "    \n",
      "    dists = []\n",
      "    for i in range(len(images_t)):\n",
      "        phi = cv2.resize(images_t[i],(36,36)).flatten()-m\n",
      "        phi_hat = sum([np.dot(e, phi)*e for e in topeigs])\n",
      "        dist = np.linalg.norm(phi_hat - phi)\n",
      "        dists.append(dist)\n",
      "    plt.hist(dists)\n",
      "    \n",
      "    return m, topeigs, w, v\n",
      "164/186: m0, topeigs0, w0, v0 = eigfaces(face_filenames0)\n",
      "164/187:\n",
      "def eigfaces(face_filenames):\n",
      "    eigs = getims2(face_filenames)\n",
      "    m = (np.sum(eigs, axis=0)/len(eigs))\n",
      "    eigs = eigs - m\n",
      "    plt.imshow(m.reshape(36,36), cmap='gray')\n",
      "    \n",
      "    cov = np.dot(np.transpose(eigs), eigs)\n",
      "    w, v = np.linalg.eigh(cov)\n",
      "    \n",
      "    plt.plot(sorted(w, reverse=True)[:100])\n",
      "    topeigs = [v[:,1296-i-1] for i in range(30)]\n",
      "    \n",
      "    fig, axes = plt.subplots(2,4, figsize=(16,6))\n",
      "    for idx, ax in enumerate(axes.ravel()):\n",
      "        image = v[:,1296-idx-1]\n",
      "        ax.imshow(image.reshape(36,36), cmap='gray')\n",
      "        ax.set_xticks([])\n",
      "        ax.set_yticks([])\n",
      "    \n",
      "    dists = []\n",
      "    for i in range(len(images_t)):\n",
      "        phi = cv2.resize(images_t[i],(36,36)).flatten()-m\n",
      "        phi_hat = sum([np.dot(e, phi)*e for e in topeigs])\n",
      "        dist = np.linalg.norm(phi_hat - phi)\n",
      "        dists.append(dist)\n",
      "    \n",
      "    return m, topeigs, w, v, dists\n",
      "164/188:\n",
      "t_start = time()\n",
      "m0, topeigs0, w0, v0, dists0 = eigfaces(face_filenames0)\n",
      "time() - t_start\n",
      "164/189:\n",
      "def eigfaces(face_filenames):\n",
      "    eigs = getims2(face_filenames)\n",
      "    m = (np.sum(eigs, axis=0)/len(eigs))\n",
      "    eigs = eigs - m\n",
      "    plt.imshow(m.reshape(36,36), cmap='gray')\n",
      "    \n",
      "    cov = np.dot(np.transpose(eigs), eigs)\n",
      "    w, v = np.linalg.eigh(cov)\n",
      "    \n",
      "    plt.plot(sorted(w, reverse=True)[:100])\n",
      "    plt.show()\n",
      "    topeigs = [v[:,1296-i-1] for i in range(30)]\n",
      "    \n",
      "    fig, axes = plt.subplots(2,4, figsize=(16,6))\n",
      "    for idx, ax in enumerate(axes.ravel()):\n",
      "        image = v[:,1296-idx-1]\n",
      "        ax.imshow(image.reshape(36,36), cmap='gray')\n",
      "        ax.set_xticks([])\n",
      "        ax.set_yticks([])\n",
      "    \n",
      "    dists = []\n",
      "    for i in range(len(images_t)):\n",
      "        phi = cv2.resize(images_t[i],(36,36)).flatten()-m\n",
      "        phi_hat = sum([np.dot(e, phi)*e for e in topeigs])\n",
      "        dist = np.linalg.norm(phi_hat - phi)\n",
      "        dists.append(dist)\n",
      "    \n",
      "    return m, topeigs, w, v, dists\n",
      "164/190:\n",
      "t_start = time()\n",
      "m0, topeigs0, w0, v0, dists0 = eigfaces(face_filenames0)\n",
      "time() - t_start\n",
      "164/191:\n",
      "def eigfaces(face_filenames):\n",
      "    eigs = getims2(face_filenames)\n",
      "    m = (np.sum(eigs, axis=0)/len(eigs))\n",
      "    eigs = eigs - m\n",
      "    plt.imshow(m.reshape(36,36), cmap='gray')\n",
      "    \n",
      "    cov = np.dot(np.transpose(eigs), eigs)\n",
      "    w, v = np.linalg.eigh(cov)\n",
      "    \n",
      "    plt.plot(sorted(w, reverse=True))\n",
      "    plt.show()\n",
      "    topeigs = [v[:,1296-i-1] for i in range(30)]\n",
      "    \n",
      "    fig, axes = plt.subplots(2,4, figsize=(16,6))\n",
      "    for idx, ax in enumerate(axes.ravel()):\n",
      "        image = v[:,1296-idx-1]\n",
      "        ax.imshow(image.reshape(36,36), cmap='gray')\n",
      "        ax.set_xticks([])\n",
      "        ax.set_yticks([])\n",
      "    \n",
      "    dists = []\n",
      "    for i in range(len(images_t)):\n",
      "        phi = cv2.resize(images_t[i],(36,36)).flatten()-m\n",
      "        phi_hat = sum([np.dot(e, phi)*e for e in topeigs])\n",
      "        dist = np.linalg.norm(phi_hat - phi)\n",
      "        dists.append(dist)\n",
      "    \n",
      "    return m, topeigs, w, v, dists\n",
      "164/192:\n",
      "t_start = time()\n",
      "m0, topeigs0, w0, v0, dists0 = eigfaces(face_filenames0)\n",
      "time() - t_start\n",
      "164/193:\n",
      "def eigfaces(face_filenames):\n",
      "    eigs = getims2(face_filenames)\n",
      "    m = (np.sum(eigs, axis=0)/len(eigs))\n",
      "    eigs = eigs - m\n",
      "    plt.imshow(m.reshape(36,36), cmap='gray')\n",
      "    \n",
      "    cov = np.dot(np.transpose(eigs), eigs)\n",
      "    w, v = np.linalg.eigh(cov)\n",
      "    \n",
      "    topeigs = [v[:,1296-i-1] for i in range(30)]\n",
      "    \n",
      "    fig, axes = plt.subplots(2,4, figsize=(16,6))\n",
      "    for idx, ax in enumerate(axes.ravel()):\n",
      "        image = v[:,1296-idx-1]\n",
      "        ax.imshow(image.reshape(36,36), cmap='gray')\n",
      "        ax.set_xticks([])\n",
      "        ax.set_yticks([])\n",
      "    \n",
      "    dists = []\n",
      "    for i in range(len(images_t)):\n",
      "        phi = cv2.resize(images_t[i],(36,36)).flatten()-m\n",
      "        phi_hat = sum([np.dot(e, phi)*e for e in topeigs])\n",
      "        dist = np.linalg.norm(phi_hat - phi)\n",
      "        dists.append(dist)\n",
      "    \n",
      "    return m, topeigs, w, v, dists\n",
      "164/194:\n",
      "t_start = time()\n",
      "m0, topeigs0, w0, v0, dists0 = eigfaces(face_filenames0)\n",
      "time() - t_start\n",
      "164/195: plt.plot(sorted(w0, reverse=True)[:100])\n",
      "164/196: plt.hist(dists0)\n",
      "164/197:\n",
      "eigaccs0 = [eigacc(i, m0, topeigs0) for i in range(600,1200,20)]\n",
      "np.max(eigaccs0), np.argmax(eigaccs0)*20+600\n",
      "164/198:\n",
      "t_start = time()\n",
      "m100, topeigs100, w100, v100, dists100 = eigfaces(face_filenames100)\n",
      "time() - t_start\n",
      "164/199:\n",
      "eigaccs100 = [eigacc(i, m100, topeigs100) for i in range(600,1200,20)]\n",
      "np.max(eigaccs100), np.argmax(eigaccs100)*20+600\n",
      "164/200:\n",
      "def eigacc(t, m, topeigs, xt, yt):\n",
      "    predictions = []\n",
      "    for i in range(len(xt)):\n",
      "        phi = cv2.resize(xt[i],(36,36)).flatten()-m\n",
      "        phi_hat = sum([np.dot(e, phi)*e for e in topeigs])\n",
      "        dist = np.linalg.norm(phi_hat - phi)\n",
      "        if dist >= t:\n",
      "            predictions.append(0)\n",
      "        else:\n",
      "            predictions.append(1)\n",
      "    return getacc(predictions, yt)\n",
      "164/201: eigacc(1000, mean50, topeigs50, images_t, y_T)\n",
      "164/202: eigacc(1000, mean50, topeigs50, images_t, y_t)\n",
      "164/203:\n",
      "eigaccs0 = [eigacc(i, m0, topeigs0, images_t, y_t) for i in range(600,1200,20)]\n",
      "np.max(eigaccs0), np.argmax(eigaccs0)*20+600\n",
      "164/204:\n",
      "eigaccs100 = [eigacc(i, m100, topeigs100, images_t, y_t) for i in range(600,1200,20)]\n",
      "np.max(eigaccs100), np.argmax(eigaccs100)*20+600\n",
      "164/205:\n",
      "eigaccs100 = [eigacc(i, m100, topeigs100, images_t[b_inds], y_t[b_inds]) for i in range(600,1200,20)]\n",
      "np.max(eigaccs100), np.argmax(eigaccs100)*20+600\n",
      "164/206: y_t[b_inds]\n",
      "164/207: y_t[w_inds]\n",
      "164/208: images_t\n",
      "164/209: images_t = np.array(images_t)\n",
      "164/210:\n",
      "eigaccs100 = [eigacc(i, m100, topeigs100, images_t[b_inds], y_t[b_inds]) for i in range(600,1200,20)]\n",
      "np.max(eigaccs100), np.argmax(eigaccs100)*20+600\n",
      "164/211:\n",
      "eigaccs100b = [eigacc(i, m100, topeigs100, images_t[b_inds], y_t[b_inds]) for i in range(600,1200,20)]\n",
      "np.max(eigaccs100b), np.argmax(eigaccs100b)*20+600\n",
      "164/212:\n",
      "eigaccs100 = [eigacc(i, m100, topeigs100, images_t, y_t) for i in range(600,1200,20)]\n",
      "np.max(eigaccs100), np.argmax(eigaccs100)*20+600\n",
      "164/213:\n",
      "eigaccs100w = [eigacc(i, m100, topeigs100, images_t[w_inds], y_t[w_inds]) for i in range(600,1200,20)]\n",
      "np.max(eigaccs100w), np.argmax(eigaccs100w)*20+600\n",
      "164/214:\n",
      "eigaccs0b = [eigacc(i, m0, topeigs0, images_t[b_inds], y_t[b_inds]) for i in range(600,1200,20)]\n",
      "np.max(eigaccs0b), np.argmax(eigaccs0b)*20+600\n",
      "164/215:\n",
      "eigaccs0w = [eigacc(i, m0, topeigs0, images_t[w_inds], y_t[w_inds]) for i in range(600,1200,20)]\n",
      "np.max(eigaccs0w), np.argmax(eigaccs0w)*20+600\n",
      "164/216: eigacc(1000, m100, topeigs100, images_t[b_inds], y_t[b_inds])\n",
      "164/217: eigacc(1000, m100, topeigs100, images_t[w_inds], y_t[w_inds])\n",
      "164/218:\n",
      "m50, topeigs50, w50, v50, dists50 = eigfaces(face_filenames50)\n",
      "eigaccs50 = [eigacc(i, m50, topeigs50, images_t, y_t) for i in range(600,1200,20)]\n",
      "np.max(eigaccs50), np.argmax(eigaccs50)*20+600\n",
      "164/219:\n",
      "m25, topeigs25, w25, v25, dists25 = eigfaces(face_filenames25)\n",
      "eigaccs25 = [eigacc(i, m25, topeigs25, images_t, y_t) for i in range(600,1200,20)]\n",
      "np.max(eigaccs25), np.argmax(eigaccs25)*20+600\n",
      "164/220:\n",
      "m75, topeigs75, w75, v75, dists75 = eigfaces(face_filenames75)\n",
      "eigaccs75 = [eigacc(i, m75, topeigs75, images_t, y_t) for i in range(600,1200,20)]\n",
      "np.max(eigaccs75), np.argmax(eigaccs75)*20+600\n",
      "164/221:\n",
      "os_e = []\n",
      "bs_e = []\n",
      "ws_e = []\n",
      "ms = [m0, m25, m50, m75, m100]\n",
      "topeigs = [topeigs0, topeigs25, topeigs50, topeigs75, topeigs100]\n",
      "thresholds = [980, 980, 1000, 1000, 1000]\n",
      "for i in range(5):\n",
      "    accb = eigacc(thresholds[i], m[i], topeigs[i], images_t[b_inds], y_t[b_inds])\n",
      "    accw = eigacc(thresholds[i], m[i], topeigs[i], images_t[w_inds], y_t[w_inds])\n",
      "    acco = eigacc(thresholds[i], m[i], topeigs[i], images_t, y_t)\n",
      "164/222:\n",
      "os_e = []\n",
      "bs_e = []\n",
      "ws_e = []\n",
      "ms = [m0, m25, m50, m75, m100]\n",
      "topeigs = [topeigs0, topeigs25, topeigs50, topeigs75, topeigs100]\n",
      "thresholds = [980, 980, 1000, 1000, 1000]\n",
      "for i in range(5):\n",
      "    accb = eigacc(thresholds[i], ms[i], topeigs[i], images_t[b_inds], y_t[b_inds])\n",
      "    accw = eigacc(thresholds[i], ms[i], topeigs[i], images_t[w_inds], y_t[w_inds])\n",
      "    acco = eigacc(thresholds[i], ms[i], topeigs[i], images_t, y_t)\n",
      "164/223:\n",
      "os_e = []\n",
      "bs_e = []\n",
      "ws_e = []\n",
      "ms = [m0, m25, m50, m75, m100]\n",
      "topeigs = [topeigs0, topeigs25, topeigs50, topeigs75, topeigs100]\n",
      "thresholds = [980, 980, 1000, 1000, 1000]\n",
      "for i in range(5):\n",
      "    accb = eigacc(thresholds[i], ms[i], topeigs[i], images_t[b_inds], y_t[b_inds])\n",
      "    accw = eigacc(thresholds[i], ms[i], topeigs[i], images_t[w_inds], y_t[w_inds])\n",
      "    acco = eigacc(thresholds[i], ms[i], topeigs[i], images_t, y_t)\n",
      "    os_e.append(acco)\n",
      "    bs_e.append(accb)\n",
      "    ws_e.append(accw)\n",
      "164/224:\n",
      "plt.plot([0,25,50,75,100],np.mean(np.array(os_e),axis=0),label='o')\n",
      "plt.plot([0,25,50,75,100],np.mean(np.array(bs_e),axis=0),label='b')\n",
      "plt.plot([0,25,50,75,100],np.mean(np.array(ws_e),axis=0),label='w')\n",
      "plt.legend()\n",
      "plt.show()\n",
      "164/225:\n",
      "plt.plot([0,25,50,75,100],os_e,label='o')\n",
      "plt.plot([0,25,50,75,100],bs_e,label='b')\n",
      "plt.plot([0,25,50,75,100],ws_e,label='w')\n",
      "plt.legend()\n",
      "plt.show()\n",
      "164/226:\n",
      "plt.subplot(1,2,1)\n",
      "plt.imshow(np.array(faces_t)[b_inds[1]], cmap='gray')\n",
      "plt.subplot(1,2,2)\n",
      "plt.imshow(sharpen(np.array(faces_t)[b_inds[1]]), cmap='gray')\n",
      "164/227:\n",
      "def sharpen(im):\n",
      "    return cv2.filter2D(src=im, ddepth=-1, kernel=sharpen_kernel)\n",
      "164/228:\n",
      "plt.subplot(1,2,1)\n",
      "plt.imshow(np.array(faces_t)[b_inds[1]], cmap='gray')\n",
      "plt.subplot(1,2,2)\n",
      "plt.imshow(sharpen(np.array(faces_t)[b_inds[1]]), cmap='gray')\n",
      "164/229: sharpen_kernel = np.array([0, -1, 0, -1, 5, -1, 0, -1, 0])\n",
      "164/230:\n",
      "plt.subplot(1,2,1)\n",
      "plt.imshow(np.array(faces_t)[b_inds[1]], cmap='gray')\n",
      "plt.subplot(1,2,2)\n",
      "plt.imshow(sharpen(np.array(faces_t)[b_inds[1]]), cmap='gray')\n",
      "164/231:\n",
      "plt.subplot(1,2,1)\n",
      "plt.imshow(np.array(faces_t)[b_inds[5]], cmap='gray')\n",
      "plt.subplot(1,2,2)\n",
      "plt.imshow(sharpen(np.array(faces_t)[b_inds[5]]), cmap='gray')\n",
      "164/232:\n",
      "plt.subplot(1,2,1)\n",
      "plt.imshow(np.array(faces_t)[b_inds[15]], cmap='gray')\n",
      "plt.subplot(1,2,2)\n",
      "plt.imshow(sharpen(np.array(faces_t)[b_inds[15]]), cmap='gray')\n",
      "164/233:\n",
      "hog100 = np.array([hog36(cv2.resize(im,(36,36)), 9, True) for im in getimgs(face_filenames100, nonfaces, n)])\n",
      "X100h = np.concatenate((X100, hog100),axis=1)\n",
      "164/234:\n",
      "def getimgs(facefiles, nonfaces, n):\n",
      "    faces = []\n",
      "    for i in range(n):\n",
      "        faces.append(cv2.imread(facefiles[i], cv2.IMREAD_GRAYSCALE))\n",
      "    return faces + nonfaces[:n]\n",
      "164/235: n, len(nonfaces)\n",
      "164/236:\n",
      "nonfaces = []\n",
      "for i in range(n):\n",
      "    # Read a random nonface file\n",
      "    j = random.randint(0, len(nonface_filenames) - 1)\n",
      "    nonface = cv2.imread(nonface_filenames[j], cv2.IMREAD_GRAYSCALE)\n",
      "    wsize = random.randint(36, min(nonface.shape[0], nonface.shape[1]))\n",
      "    row = random.randint(0, nonface.shape[0]-wsize)\n",
      "    col = random.randint(0, nonface.shape[1]-wsize)\n",
      "    crop = nonface[row:row+wsize, col:col+wsize]\n",
      "\n",
      "    # Resize to be the right size\n",
      "    crop = cv2.resize(crop, (200, 200))\n",
      "    nonfaces.append(crop)\n",
      "164/237: n, len(nonfaces)\n",
      "164/238: from hog36 import hog36\n",
      "164/239:\n",
      "hog100 = np.array([hog36(cv2.resize(im,(36,36)), 9, True) for im in getimgs(face_filenames100, nonfaces, n)])\n",
      "X100h = np.concatenate((X100, hog100),axis=1)\n",
      "164/240: hog100 = np.array([hog36(cv2.resize(im,(36,36)), 9, True) for im in getimgs(face_filenames100, nonfaces, n)])\n",
      "164/241: hogt = np.array([hog36(cv2.resize(im,(36,36)), 9, True) for im in getimgs(test_face_names, nonfaces_t, 300)])\n",
      "164/242:\n",
      "t_start = time()\n",
      "h100 = RandomForestClassifier(n_estimators=1000, max_depth=None, n_jobs=-1, random_state=0)\n",
      "h100.fit(hog100, y)\n",
      "print(h100.score(hogt, y_t))\n",
      "time() - t_start\n",
      "164/243: hog100.shape\n",
      "164/244: y = np.array([1]*4020+[0]*4020)\n",
      "164/245:\n",
      "t_start = time()\n",
      "h100 = RandomForestClassifier(n_estimators=1000, max_depth=None, n_jobs=-1, random_state=0)\n",
      "h100.fit(hog100, y)\n",
      "print(h100.score(hogt, y_t))\n",
      "time() - t_start\n",
      "164/246: mean50\n",
      "164/247: mean50, mean0, mean100\n",
      "164/248: np.linalg.norm(m50-m0)\n",
      "164/249: np.linalg.norm(m50-m0), np.linalg.norm(m50-m100)\n",
      "164/250: mean50\n",
      "164/251: mean50.shape\n",
      "164/252: ims0 = getims2(face_filenames0)\n",
      "164/253: ims100 = getims2(face_filenames100)\n",
      "164/254: ims0.shape\n",
      "164/255:\n",
      "distsw = []\n",
      "for i in ims0:\n",
      "    distsw.append(np.linalg.norm(i-mean50))\n",
      "164/256:\n",
      "distsb = []\n",
      "for i in ims100:\n",
      "    distsw.append(np.linalg.norm(i-mean50))\n",
      "164/257: plt.hist(distsw)\n",
      "164/258: plt.hist(distsb)\n",
      "164/259:\n",
      "distsw = []\n",
      "for i in ims0:\n",
      "    distsw.append(np.linalg.norm(i-mean50))\n",
      "164/260:\n",
      "distsb = []\n",
      "for i in ims100:\n",
      "    distsb.append(np.linalg.norm(i-mean50))\n",
      "164/261: plt.hist(distsw)\n",
      "164/262: plt.hist(distsb)\n",
      "164/263: mean(distsw)\n",
      "164/264: np.mean(distsw)\n",
      "164/265: np.mean(distsw), np.mean(distsb)\n",
      "164/266: np.mean(distsw), np.mean(distsb), np.std(distsw), np.std(distsb)\n",
      "164/267: np.mean(distsw), np.mean(distsb)\n",
      "164/268:\n",
      "distsw2 = []\n",
      "for i in ims0:\n",
      "    distsw2.append(np.linalg.norm(i-mean0))\n",
      "164/269:\n",
      "distsw2 = []\n",
      "for i in ims0:\n",
      "    distsw2.append(np.linalg.norm(i-m0))\n",
      "164/270:\n",
      "distsb2 = []\n",
      "for i in ims100:\n",
      "    distsb2.append(np.linalg.norm(i-m100))\n",
      "164/271: plt.hist(distsw2)\n",
      "164/272: plt.hist(distsb2)\n",
      "164/273: np.mean(distsw), np.mean(distsb), np.mean(distsw2), np.mean(distsb2)\n",
      "164/274: np.mean(distsw), np.mean(distsb), np.mean(distsw2), np.mean(distsb2), np.std(distsw2), np.std(sitsb2)\n",
      "164/275: np.mean(distsw), np.mean(distsb), np.mean(distsw2), np.mean(distsb2), np.std(distsw2), np.std(distsb2)\n",
      "164/276: np.mean(distsw), np.mean(distsb), np.mean(distsw2), np.mean(distsb2)\n",
      "164/277: X0, X1, X2\n",
      "164/278: Xt_900\n",
      "164/279:\n",
      "def pipeline2(f, times):\n",
      "    def getx3(facefiles, n, nonfaces, feature_types, feature_coords):\n",
      "        ims = [f(i) for i in getimgs(facefiles, nonfaces, n)]\n",
      "        X = delayed(extract_feature_image(img, feature_types, feature_coords) for img in ims)\n",
      "        return np.array(X.compute(scheduler='threads'))\n",
      "    \n",
      "    t_start = time()\n",
      "    \n",
      "    os_rs = []\n",
      "    bs_rs = []\n",
      "    ws_rs = []\n",
      "    for i in range(times):\n",
      "        nonfaces, nonfaces_t = regenerate_nonfaces()\n",
      "        \n",
      "        X0 = getx3(face_filenames0, n, nonfaces, feature_type_sel2, feature_coord_sel2)\n",
      "        print(X0.shape)\n",
      "        X25 = getx3(face_filenames25, n, nonfaces, feature_type_sel2, feature_coord_sel2)\n",
      "        print(X25.shape)\n",
      "        X50 = getx3(face_filenames50, n, nonfaces, feature_type_sel2, feature_coord_sel2)\n",
      "        print(X50.shape)\n",
      "        X75 = getx3(face_filenames75, n, nonfaces, feature_type_sel2, feature_coord_sel2)\n",
      "        print(X75.shape)\n",
      "        X100 = getx3(face_filenames100, n, nonfaces, feature_type_sel2, feature_coord_sel2)\n",
      "        print(X100.shape)\n",
      "        Xt = getx3(test_face_names, 300, nonfaces_t, feature_type_sel2, feature_coord_sel2)\n",
      "        print(Xt.shape)\n",
      "        \n",
      "        \n",
      "        params0 = logistic_fit(X0, y, 0.005)\n",
      "        predicted = logistic_prob(Xt, params0)\n",
      "        print(getacc(predicted, y_t))\n",
      "        params25 = logistic_fit(X25, y, 0.005)\n",
      "        predicted = logistic_prob(Xt, params0)\n",
      "        print(getacc(predicted, y_t))\n",
      "        params50 = logistic_fit(X50, y, 0.005)\n",
      "        predicted = logistic_prob(Xt, params0)\n",
      "        print(getacc(predicted, y_t))\n",
      "        params75 = logistic_fit(X75, y, 0.005)\n",
      "        predicted = logistic_prob(Xt, params0)\n",
      "        print(getacc(predicted, y_t))\n",
      "        params100 = logistic_fit(X100, y, 0.005)\n",
      "        predicted = logistic_prob(Xt, params0)\n",
      "        print(getacc(predicted, y_t))\n",
      "\n",
      "        os_r, bs_r, ws_r = logscores([params0, params25, params50, params75, params100], Xt, X_t[b_inds], X_t[w_inds])\n",
      "        os_rs.append(os_r)\n",
      "        bs_rs.append(bs_r)\n",
      "        ws_rs.append(ws_r)\n",
      "    \n",
      "    plt.plot([0, 25, 50, 75, 100], os_r, label=\"o\")\n",
      "    plt.plot([0, 25, 50, 75, 100], bs_r, label=\"b\")\n",
      "    plt.plot([0, 25, 50, 75, 100], ws_r, label=\"w\")\n",
      "    plt.legend()\n",
      "    plt.show()\n",
      "    \n",
      "    print(time()-t_start)\n",
      "    return os_rs, bs_rs, ws_rs\n",
      "164/280: os_l, bs_l, ws_l = pipeline2(lambda x: x, 10)\n",
      "164/281:\n",
      "def regenerate_nonfaces():\n",
      "    nonfaces = []\n",
      "    for i in range(n):\n",
      "        # Read a random nonface file\n",
      "        j = random.randint(0, len(nonface_filenames) - 1)\n",
      "        nonface = cv2.imread(nonface_filenames[j], cv2.IMREAD_GRAYSCALE)\n",
      "        wsize = random.randint(36, min(nonface.shape[0], nonface.shape[1]))\n",
      "        row = random.randint(0, nonface.shape[0]-wsize)\n",
      "        col = random.randint(0, nonface.shape[1]-wsize)\n",
      "        crop = nonface[row:row+wsize, col:col+wsize]\n",
      "\n",
      "        # Resize to be the right size\n",
      "        crop = cv2.resize(crop, (200, 200))\n",
      "        nonfaces.append(crop)\n",
      "    nonfaces_t = []\n",
      "    for i in range(n_test):\n",
      "        # Read a random nonface file\n",
      "        j = random.randint(0, n_nonfaces_test - 1)\n",
      "        nonface = cv2.imread(test_nonface_names[j], cv2.IMREAD_GRAYSCALE)\n",
      "\n",
      "        wsize = random.randint(36, min(nonface.shape[0], nonface.shape[1]))\n",
      "        row = random.randint(0, nonface.shape[0]-wsize)\n",
      "        col = random.randint(0, nonface.shape[1]-wsize)\n",
      "        crop = nonface[row:row+wsize, col:col+wsize]\n",
      "\n",
      "        # Resize to be the right size\n",
      "        crop = cv2.resize(crop, (200, 200))\n",
      "        nonfaces_t.append(crop)\n",
      "    return nonfaces, nonfaces_t\n",
      "164/282:\n",
      "def pipeline2(f, times):\n",
      "    def getx3(facefiles, n, nonfaces, feature_types, feature_coords):\n",
      "        ims = [f(i) for i in getimgs(facefiles, nonfaces, n)]\n",
      "        X = delayed(extract_feature_image(img, feature_types, feature_coords) for img in ims)\n",
      "        return np.array(X.compute(scheduler='threads'))\n",
      "    \n",
      "    t_start = time()\n",
      "    \n",
      "    os_rs = []\n",
      "    bs_rs = []\n",
      "    ws_rs = []\n",
      "    for i in range(times):\n",
      "        nonfaces, nonfaces_t = regenerate_nonfaces()\n",
      "        \n",
      "        X0 = getx3(face_filenames0, n, nonfaces, feature_type_sel2, feature_coord_sel2)\n",
      "        print(X0.shape)\n",
      "        X25 = getx3(face_filenames25, n, nonfaces, feature_type_sel2, feature_coord_sel2)\n",
      "        print(X25.shape)\n",
      "        X50 = getx3(face_filenames50, n, nonfaces, feature_type_sel2, feature_coord_sel2)\n",
      "        print(X50.shape)\n",
      "        X75 = getx3(face_filenames75, n, nonfaces, feature_type_sel2, feature_coord_sel2)\n",
      "        print(X75.shape)\n",
      "        X100 = getx3(face_filenames100, n, nonfaces, feature_type_sel2, feature_coord_sel2)\n",
      "        print(X100.shape)\n",
      "        Xt = getx3(test_face_names, 300, nonfaces_t, feature_type_sel2, feature_coord_sel2)\n",
      "        print(Xt.shape)\n",
      "        \n",
      "        \n",
      "        params0 = logistic_fit(X0, y, 0.005)\n",
      "        predicted = logistic_prob(Xt, params0)\n",
      "        print(getacc(predicted, y_t))\n",
      "        params25 = logistic_fit(X25, y, 0.005)\n",
      "        predicted = logistic_prob(Xt, params0)\n",
      "        print(getacc(predicted, y_t))\n",
      "        params50 = logistic_fit(X50, y, 0.005)\n",
      "        predicted = logistic_prob(Xt, params0)\n",
      "        print(getacc(predicted, y_t))\n",
      "        params75 = logistic_fit(X75, y, 0.005)\n",
      "        predicted = logistic_prob(Xt, params0)\n",
      "        print(getacc(predicted, y_t))\n",
      "        params100 = logistic_fit(X100, y, 0.005)\n",
      "        predicted = logistic_prob(Xt, params0)\n",
      "        print(getacc(predicted, y_t))\n",
      "\n",
      "        os_r, bs_r, ws_r = logscores([params0, params25, params50, params75, params100], Xt, X_t[b_inds], X_t[w_inds])\n",
      "        os_rs.append(os_r)\n",
      "        bs_rs.append(bs_r)\n",
      "        ws_rs.append(ws_r)\n",
      "    \n",
      "    plt.plot([0, 25, 50, 75, 100], os_r, label=\"o\")\n",
      "    plt.plot([0, 25, 50, 75, 100], bs_r, label=\"b\")\n",
      "    plt.plot([0, 25, 50, 75, 100], ws_r, label=\"w\")\n",
      "    plt.legend()\n",
      "    plt.show()\n",
      "    \n",
      "    print(time()-t_start)\n",
      "    return os_rs, bs_rs, ws_rs\n",
      "164/283: os_l, bs_l, ws_l = pipeline2(lambda x: x, 10)\n",
      "164/284:\n",
      "t_start = time()\n",
      "dill.load_session('01_05_env.db')\n",
      "time() - t_start\n",
      "164/285:\n",
      "def pipeline2(f, times):\n",
      "    def getx3(facefiles, n, nonfaces, feature_types, feature_coords):\n",
      "        ims = [f(i) for i in getimgs(facefiles, nonfaces, n)]\n",
      "        X = delayed(extract_feature_image(img, feature_types, feature_coords) for img in ims)\n",
      "        return np.array(X.compute(scheduler='threads'))\n",
      "    \n",
      "    t_start = time()\n",
      "    \n",
      "    os_rs = []\n",
      "    bs_rs = []\n",
      "    ws_rs = []\n",
      "    for i in range(times):\n",
      "        nonfaces, nonfaces_t = regenerate_nonfaces()\n",
      "        \n",
      "        X0 = getx3(face_filenames0, n, nonfaces, feature_type_sel2, feature_coord_sel2)\n",
      "        print(X0.shape)\n",
      "        X25 = getx3(face_filenames25, n, nonfaces, feature_type_sel2, feature_coord_sel2)\n",
      "        print(X25.shape)\n",
      "        X50 = getx3(face_filenames50, n, nonfaces, feature_type_sel2, feature_coord_sel2)\n",
      "        print(X50.shape)\n",
      "        X75 = getx3(face_filenames75, n, nonfaces, feature_type_sel2, feature_coord_sel2)\n",
      "        print(X75.shape)\n",
      "        X100 = getx3(face_filenames100, n, nonfaces, feature_type_sel2, feature_coord_sel2)\n",
      "        print(X100.shape)\n",
      "        Xt = getx3(test_face_names, 300, nonfaces_t, feature_type_sel2, feature_coord_sel2)\n",
      "        print(Xt.shape)\n",
      "        \n",
      "        \n",
      "        params0 = logistic_fit(X0, y, 0.005)\n",
      "        predicted = logistic_prob(Xt, params0)\n",
      "        print(getacc(predicted, y_t))\n",
      "        params25 = logistic_fit(X25, y, 0.005)\n",
      "        predicted = logistic_prob(Xt, params0)\n",
      "        print(getacc(predicted, y_t))\n",
      "        params50 = logistic_fit(X50, y, 0.005)\n",
      "        predicted = logistic_prob(Xt, params0)\n",
      "        print(getacc(predicted, y_t))\n",
      "        params75 = logistic_fit(X75, y, 0.005)\n",
      "        predicted = logistic_prob(Xt, params0)\n",
      "        print(getacc(predicted, y_t))\n",
      "        params100 = logistic_fit(X100, y, 0.005)\n",
      "        predicted = logistic_prob(Xt, params0)\n",
      "        print(getacc(predicted, y_t))\n",
      "\n",
      "        os_r, bs_r, ws_r = logscores([params0, params25, params50, params75, params100], Xt, X_t[b_inds], X_t[w_inds])\n",
      "        os_rs.append(os_r)\n",
      "        bs_rs.append(bs_r)\n",
      "        ws_rs.append(ws_r)\n",
      "    \n",
      "    plt.plot([0, 25, 50, 75, 100], os_r, label=\"o\")\n",
      "    plt.plot([0, 25, 50, 75, 100], bs_r, label=\"b\")\n",
      "    plt.plot([0, 25, 50, 75, 100], ws_r, label=\"w\")\n",
      "    plt.legend()\n",
      "    plt.show()\n",
      "    \n",
      "    print(time()-t_start)\n",
      "    return os_rs, bs_rs, ws_rs\n",
      "164/286: os_l, bs_l, ws_l = pipeline2(lambda x: x, 10)\n",
      "164/287: dill.dump_session('01_06_env.db')\n",
      "164/288: dill.load_session('01_06_env.db')\n",
      "164/289: Xf_t\n",
      "164/290:\n",
      "t_start = time()\n",
      "dill.load_session('01_05_env.db')\n",
      "time() - t_start\n",
      "164/291: Xf_t\n",
      "164/292: feature_type_sel\n",
      "164/293: X\n",
      "164/294: X.shape\n",
      "164/295: Xf_t = X\n",
      "164/296: yf_t = np.array([1]*len(faces_t) + [0]*len(nonfaces_t))\n",
      "164/297:\n",
      "# Train a random forest classifier and assess its performance\n",
      "clf = RandomForestClassifier(n_estimators=1000, max_depth=None,\n",
      "                             n_jobs=-1, random_state=0)\n",
      "t_start = time()\n",
      "clf.fit(Xf_t, yf_t)\n",
      "time() - t_start\n",
      "164/298:\n",
      "feature_coord, feature_type = \\\n",
      "    haar_like_feature_coord(width=36, height=36,\n",
      "                            feature_type=feature_types)\n",
      "164/299: idx_sorted = np.argsort(clf.feature_importances_)[::-1]\n",
      "164/300:\n",
      "for i in range(1500):\n",
      "    if len(feature_coord[idx_sorted[i]]) > 2:\n",
      "        print(i)\n",
      "164/301:\n",
      "fig, axes = plt.subplots(4,4, figsize=(20,20))\n",
      "for idx, ax in enumerate(axes.ravel()):\n",
      "    image = cv2.resize(images_t[0], (36,36))\n",
      "    image = draw_haar_like_feature(image, 0, 0, 36, 36,\n",
      "                                   [feature_coord[idx_sorted[idx]]])\n",
      "    ax.imshow(image)\n",
      "    ax.set_xticks([])\n",
      "    ax.set_yticks([])\n",
      "\n",
      "fig.suptitle('The most important features')\n",
      "164/302: feature_coord_sel\n",
      "164/303: n_features2 = 900\n",
      "164/304:\n",
      "feature_coord_sel2 = feature_coord[idx_sorted[:n_features2]]\n",
      "feature_type_sel2 = feature_type[idx_sorted[:n_features2]]\n",
      "164/305: Xt_900 = getx2(test_face_names, 300, nonfaces_t, feature_type_sel2, feature_coord_sel2)\n",
      "164/306:\n",
      "def getx2(facefiles, n, nonfaces, feature_types, feature_coords):\n",
      "    ims = getimgs(facefiles, nonfaces, n)\n",
      "    X = delayed(extract_feature_image(img, feature_types, feature_coords) for img in ims)\n",
      "    return np.array(X.compute(scheduler='threads'))\n",
      "164/307:\n",
      "def getimgs(facefiles, nonfaces, n):\n",
      "    faces = []\n",
      "    for i in range(n):\n",
      "        faces.append(cv2.imread(facefiles[i], cv2.IMREAD_GRAYSCALE))\n",
      "    return faces + nonfaces[:n]\n",
      "164/308: Xt_900 = getx2(test_face_names, 300, nonfaces_t, feature_type_sel2, feature_coord_sel2)\n",
      "164/309:\n",
      "def pipeline2(f, times):\n",
      "    def getx3(facefiles, n, nonfaces, feature_types, feature_coords):\n",
      "        ims = [f(i) for i in getimgs(facefiles, nonfaces, n)]\n",
      "        X = delayed(extract_feature_image(img, feature_types, feature_coords) for img in ims)\n",
      "        return np.array(X.compute(scheduler='threads'))\n",
      "    \n",
      "    t_start = time()\n",
      "    \n",
      "    os_rs = []\n",
      "    bs_rs = []\n",
      "    ws_rs = []\n",
      "    for i in range(times):\n",
      "        nonfaces, nonfaces_t = regenerate_nonfaces()\n",
      "        \n",
      "        X0 = getx3(face_filenames0, n, nonfaces, feature_type_sel2, feature_coord_sel2)\n",
      "        print(X0.shape)\n",
      "        X25 = getx3(face_filenames25, n, nonfaces, feature_type_sel2, feature_coord_sel2)\n",
      "        print(X25.shape)\n",
      "        X50 = getx3(face_filenames50, n, nonfaces, feature_type_sel2, feature_coord_sel2)\n",
      "        print(X50.shape)\n",
      "        X75 = getx3(face_filenames75, n, nonfaces, feature_type_sel2, feature_coord_sel2)\n",
      "        print(X75.shape)\n",
      "        X100 = getx3(face_filenames100, n, nonfaces, feature_type_sel2, feature_coord_sel2)\n",
      "        print(X100.shape)\n",
      "        Xt = getx3(test_face_names, 300, nonfaces_t, feature_type_sel2, feature_coord_sel2)\n",
      "        print(Xt.shape)\n",
      "        \n",
      "        \n",
      "        params0 = logistic_fit(X0, y, 0.005)\n",
      "        predicted = logistic_prob(Xt, params0)\n",
      "        print(getacc(predicted, y_t))\n",
      "        params25 = logistic_fit(X25, y, 0.005)\n",
      "        predicted = logistic_prob(Xt, params0)\n",
      "        print(getacc(predicted, y_t))\n",
      "        params50 = logistic_fit(X50, y, 0.005)\n",
      "        predicted = logistic_prob(Xt, params0)\n",
      "        print(getacc(predicted, y_t))\n",
      "        params75 = logistic_fit(X75, y, 0.005)\n",
      "        predicted = logistic_prob(Xt, params0)\n",
      "        print(getacc(predicted, y_t))\n",
      "        params100 = logistic_fit(X100, y, 0.005)\n",
      "        predicted = logistic_prob(Xt, params0)\n",
      "        print(getacc(predicted, y_t))\n",
      "\n",
      "        os_r, bs_r, ws_r = logscores([params0, params25, params50, params75, params100], Xt, X_t[b_inds], X_t[w_inds])\n",
      "        os_rs.append(os_r)\n",
      "        bs_rs.append(bs_r)\n",
      "        ws_rs.append(ws_r)\n",
      "    \n",
      "    plt.plot([0, 25, 50, 75, 100], os_r, label=\"o\")\n",
      "    plt.plot([0, 25, 50, 75, 100], bs_r, label=\"b\")\n",
      "    plt.plot([0, 25, 50, 75, 100], ws_r, label=\"w\")\n",
      "    plt.legend()\n",
      "    plt.show()\n",
      "    \n",
      "    print(time()-t_start)\n",
      "    return os_rs, bs_rs, ws_rs\n",
      "164/310: os_l, bs_l, ws_l = pipeline2(lambda x: x, 10)\n",
      "164/311: n = 4020\n",
      "164/312:\n",
      "def pipeline2(f, times):\n",
      "    def getx3(facefiles, n, nonfaces, feature_types, feature_coords):\n",
      "        ims = [f(i) for i in getimgs(facefiles, nonfaces, n)]\n",
      "        X = delayed(extract_feature_image(img, feature_types, feature_coords) for img in ims)\n",
      "        return np.array(X.compute(scheduler='threads'))\n",
      "    \n",
      "    t_start = time()\n",
      "    \n",
      "    os_rs = []\n",
      "    bs_rs = []\n",
      "    ws_rs = []\n",
      "    for i in range(times):\n",
      "        nonfaces, nonfaces_t = regenerate_nonfaces()\n",
      "        \n",
      "        X0 = getx3(face_filenames0, n, nonfaces, feature_type_sel2, feature_coord_sel2)\n",
      "        print(X0.shape)\n",
      "        X25 = getx3(face_filenames25, n, nonfaces, feature_type_sel2, feature_coord_sel2)\n",
      "        print(X25.shape)\n",
      "        X50 = getx3(face_filenames50, n, nonfaces, feature_type_sel2, feature_coord_sel2)\n",
      "        print(X50.shape)\n",
      "        X75 = getx3(face_filenames75, n, nonfaces, feature_type_sel2, feature_coord_sel2)\n",
      "        print(X75.shape)\n",
      "        X100 = getx3(face_filenames100, n, nonfaces, feature_type_sel2, feature_coord_sel2)\n",
      "        print(X100.shape)\n",
      "        Xt = getx3(test_face_names, 300, nonfaces_t, feature_type_sel2, feature_coord_sel2)\n",
      "        print(Xt.shape)\n",
      "        \n",
      "        \n",
      "        params0 = logistic_fit(X0, y, 0.005)\n",
      "        predicted = logistic_prob(Xt, params0)\n",
      "        print(getacc(predicted, y_t))\n",
      "        params25 = logistic_fit(X25, y, 0.005)\n",
      "        predicted = logistic_prob(Xt, params0)\n",
      "        print(getacc(predicted, y_t))\n",
      "        params50 = logistic_fit(X50, y, 0.005)\n",
      "        predicted = logistic_prob(Xt, params0)\n",
      "        print(getacc(predicted, y_t))\n",
      "        params75 = logistic_fit(X75, y, 0.005)\n",
      "        predicted = logistic_prob(Xt, params0)\n",
      "        print(getacc(predicted, y_t))\n",
      "        params100 = logistic_fit(X100, y, 0.005)\n",
      "        predicted = logistic_prob(Xt, params0)\n",
      "        print(getacc(predicted, y_t))\n",
      "\n",
      "        os_r, bs_r, ws_r = logscores([params0, params25, params50, params75, params100], Xt, X_t[b_inds], X_t[w_inds])\n",
      "        os_rs.append(os_r)\n",
      "        bs_rs.append(bs_r)\n",
      "        ws_rs.append(ws_r)\n",
      "    \n",
      "    plt.plot([0, 25, 50, 75, 100], os_r, label=\"o\")\n",
      "    plt.plot([0, 25, 50, 75, 100], bs_r, label=\"b\")\n",
      "    plt.plot([0, 25, 50, 75, 100], ws_r, label=\"w\")\n",
      "    plt.legend()\n",
      "    plt.show()\n",
      "    \n",
      "    print(time()-t_start)\n",
      "    return os_rs, bs_rs, ws_rs\n",
      "164/313: os_l, bs_l, ws_l = pipeline2(lambda x: x, 10)\n",
      "164/314: from logistic import logistic_fit, logistic, logistic_prob\n",
      "164/315:\n",
      "def pipeline2(f, times):\n",
      "    def getx3(facefiles, n, nonfaces, feature_types, feature_coords):\n",
      "        ims = [f(i) for i in getimgs(facefiles, nonfaces, n)]\n",
      "        X = delayed(extract_feature_image(img, feature_types, feature_coords) for img in ims)\n",
      "        return np.array(X.compute(scheduler='threads'))\n",
      "    \n",
      "    t_start = time()\n",
      "    \n",
      "    os_rs = []\n",
      "    bs_rs = []\n",
      "    ws_rs = []\n",
      "    for i in range(times):\n",
      "        nonfaces, nonfaces_t = regenerate_nonfaces()\n",
      "        \n",
      "        X0 = getx3(face_filenames0, n, nonfaces, feature_type_sel2, feature_coord_sel2)\n",
      "        print(X0.shape)\n",
      "        X25 = getx3(face_filenames25, n, nonfaces, feature_type_sel2, feature_coord_sel2)\n",
      "        print(X25.shape)\n",
      "        X50 = getx3(face_filenames50, n, nonfaces, feature_type_sel2, feature_coord_sel2)\n",
      "        print(X50.shape)\n",
      "        X75 = getx3(face_filenames75, n, nonfaces, feature_type_sel2, feature_coord_sel2)\n",
      "        print(X75.shape)\n",
      "        X100 = getx3(face_filenames100, n, nonfaces, feature_type_sel2, feature_coord_sel2)\n",
      "        print(X100.shape)\n",
      "        Xt = getx3(test_face_names, 300, nonfaces_t, feature_type_sel2, feature_coord_sel2)\n",
      "        print(Xt.shape)\n",
      "        \n",
      "        \n",
      "        params0 = logistic_fit(X0, y, 0.005)\n",
      "        predicted = logistic_prob(Xt, params0)\n",
      "        print(getacc(predicted, y_t))\n",
      "        params25 = logistic_fit(X25, y, 0.005)\n",
      "        predicted = logistic_prob(Xt, params0)\n",
      "        print(getacc(predicted, y_t))\n",
      "        params50 = logistic_fit(X50, y, 0.005)\n",
      "        predicted = logistic_prob(Xt, params0)\n",
      "        print(getacc(predicted, y_t))\n",
      "        params75 = logistic_fit(X75, y, 0.005)\n",
      "        predicted = logistic_prob(Xt, params0)\n",
      "        print(getacc(predicted, y_t))\n",
      "        params100 = logistic_fit(X100, y, 0.005)\n",
      "        predicted = logistic_prob(Xt, params0)\n",
      "        print(getacc(predicted, y_t))\n",
      "\n",
      "        os_r, bs_r, ws_r = logscores([params0, params25, params50, params75, params100], Xt, X_t[b_inds], X_t[w_inds])\n",
      "        os_rs.append(os_r)\n",
      "        bs_rs.append(bs_r)\n",
      "        ws_rs.append(ws_r)\n",
      "    \n",
      "    plt.plot([0, 25, 50, 75, 100], os_r, label=\"o\")\n",
      "    plt.plot([0, 25, 50, 75, 100], bs_r, label=\"b\")\n",
      "    plt.plot([0, 25, 50, 75, 100], ws_r, label=\"w\")\n",
      "    plt.legend()\n",
      "    plt.show()\n",
      "    \n",
      "    print(time()-t_start)\n",
      "    return os_rs, bs_rs, ws_rs\n",
      "164/316: os_l, bs_l, ws_l = pipeline2(lambda x: x, 10)\n",
      "164/317: Xt.shape\n",
      "164/318:\n",
      "t_start = time()\n",
      "X0 = getx2(face_filenames0, n, feature_type_sel2, feature_coord_sel2)\n",
      "time() - t_start\n",
      "164/319: n, len(nonfaces)\n",
      "164/320:\n",
      "nonfaces = []\n",
      "for i in range(n):\n",
      "    # Read a random nonface file\n",
      "    j = random.randint(0, len(nonface_filenames) - 1)\n",
      "    nonface = cv2.imread(nonface_filenames[j], cv2.IMREAD_GRAYSCALE)\n",
      "    wsize = random.randint(36, min(nonface.shape[0], nonface.shape[1]))\n",
      "    row = random.randint(0, nonface.shape[0]-wsize)\n",
      "    col = random.randint(0, nonface.shape[1]-wsize)\n",
      "    crop = nonface[row:row+wsize, col:col+wsize]\n",
      "\n",
      "    # Resize to be the right size\n",
      "    crop = cv2.resize(crop, (200, 200))\n",
      "    nonfaces.append(crop)\n",
      "164/321:\n",
      "t_start = time()\n",
      "X0 = getx2(face_filenames0, nonfaces, bibfeature_type_sel2, feature_coord_sel2)\n",
      "time() - t_start\n",
      "164/322:\n",
      "t_start = time()\n",
      "X0 = getx2(face_filenames0, nonfaces, feature_type_sel2, feature_coord_sel2)\n",
      "time() - t_start\n",
      "164/323:\n",
      "t_start = time()\n",
      "X0 = getx2(face_filenames0, n, nonfaces, feature_type_sel2, feature_coord_sel2)\n",
      "time() - t_start\n",
      "164/324: X0.shape\n",
      "164/325: y = np.array([1]*n + [0]*n)\n",
      "164/326:\n",
      "t_start = time()\n",
      "params = logistic_fit(X0, y, 0.005)\n",
      "predicted = logistic_prob(Xt_900, params)\n",
      "print(time() - t_start)\n",
      "print(getacc(predicted, y_t))\n",
      "164/327: Xt.shape\n",
      "164/328:\n",
      "Xt = getx2(test_face_names, 300, nonfaces_t, feature_type_sel2, feature_coord_sel2)\n",
      "print(Xt.shape)\n",
      "164/329:\n",
      "t_start = time()\n",
      "params = logistic_fit(X0, y, 0.005)\n",
      "predicted = logistic_prob(Xt_900, params)\n",
      "print(time() - t_start)\n",
      "print(getacc(predicted, y_t))\n",
      "164/330: Xt_900.shape\n",
      "164/331: n, y.shape\n",
      "164/332:\n",
      "def pipeline2(f, times):\n",
      "    def getx3(facefiles, n, nonfaces, feature_types, feature_coords):\n",
      "        ims = [f(i) for i in getimgs(facefiles, nonfaces, n)]\n",
      "        X = delayed(extract_feature_image(img, feature_types, feature_coords) for img in ims)\n",
      "        return np.array(X.compute(scheduler='threads'))\n",
      "    \n",
      "    t_start = time()\n",
      "    \n",
      "    os_rs = []\n",
      "    bs_rs = []\n",
      "    ws_rs = []\n",
      "    for i in range(times):\n",
      "        nonfaces, nonfaces_t = regenerate_nonfaces()\n",
      "        \n",
      "        X0 = getx3(face_filenames0, n, nonfaces, feature_type_sel2, feature_coord_sel2)\n",
      "        print(X0.shape)\n",
      "        X25 = getx3(face_filenames25, n, nonfaces, feature_type_sel2, feature_coord_sel2)\n",
      "        print(X25.shape)\n",
      "        X50 = getx3(face_filenames50, n, nonfaces, feature_type_sel2, feature_coord_sel2)\n",
      "        print(X50.shape)\n",
      "        X75 = getx3(face_filenames75, n, nonfaces, feature_type_sel2, feature_coord_sel2)\n",
      "        print(X75.shape)\n",
      "        X100 = getx3(face_filenames100, n, nonfaces, feature_type_sel2, feature_coord_sel2)\n",
      "        print(X100.shape)\n",
      "        Xt = getx3(test_face_names, 300, nonfaces_t, feature_type_sel2, feature_coord_sel2)\n",
      "        print(Xt.shape)\n",
      "        \n",
      "        \n",
      "        params0 = logistic_fit(X0, y, 0.005)\n",
      "        predicted = logistic_prob(Xt, params0)\n",
      "        print(getacc(predicted, y_t))\n",
      "        params25 = logistic_fit(X25, y, 0.005)\n",
      "        predicted = logistic_prob(Xt, params0)\n",
      "        print(getacc(predicted, y_t))\n",
      "        params50 = logistic_fit(X50, y, 0.005)\n",
      "        predicted = logistic_prob(Xt, params0)\n",
      "        print(getacc(predicted, y_t))\n",
      "        params75 = logistic_fit(X75, y, 0.005)\n",
      "        predicted = logistic_prob(Xt, params0)\n",
      "        print(getacc(predicted, y_t))\n",
      "        params100 = logistic_fit(X100, y, 0.005)\n",
      "        predicted = logistic_prob(Xt, params0)\n",
      "        print(getacc(predicted, y_t))\n",
      "\n",
      "        os_r, bs_r, ws_r = logscores([params0, params25, params50, params75, params100], Xt, X_t[b_inds], X_t[w_inds])\n",
      "        os_rs.append(os_r)\n",
      "        bs_rs.append(bs_r)\n",
      "        ws_rs.append(ws_r)\n",
      "    \n",
      "    plt.plot([0, 25, 50, 75, 100], os_r, label=\"o\")\n",
      "    plt.plot([0, 25, 50, 75, 100], bs_r, label=\"b\")\n",
      "    plt.plot([0, 25, 50, 75, 100], ws_r, label=\"w\")\n",
      "    plt.legend()\n",
      "    plt.show()\n",
      "    \n",
      "    print(time()-t_start)\n",
      "    return os_rs, bs_rs, ws_rs\n",
      "164/333: os_l, bs_l, ws_l = pipeline2(lambda x: x, 10)\n",
      "164/334:\n",
      "def pipeline2(f, times):\n",
      "    def getx3(facefiles, n, nonfaces, feature_types, feature_coords):\n",
      "        ims = [f(i) for i in getimgs(facefiles, nonfaces, n)]\n",
      "        X = delayed(extract_feature_image(img, feature_types, feature_coords) for img in ims)\n",
      "        return np.array(X.compute(scheduler='threads'))\n",
      "    \n",
      "    t_start = time()\n",
      "    \n",
      "    os_rs = []\n",
      "    bs_rs = []\n",
      "    ws_rs = []\n",
      "    for i in range(times):\n",
      "        nonfaces, nonfaces_t = regenerate_nonfaces()\n",
      "        \n",
      "        X0 = getx3(face_filenames0, n, nonfaces, feature_type_sel2, feature_coord_sel2)\n",
      "        print(X0.shape)\n",
      "        X25 = getx3(face_filenames25, n, nonfaces, feature_type_sel2, feature_coord_sel2)\n",
      "        print(X25.shape)\n",
      "        X50 = getx3(face_filenames50, n, nonfaces, feature_type_sel2, feature_coord_sel2)\n",
      "        print(X50.shape)\n",
      "        X75 = getx3(face_filenames75, n, nonfaces, feature_type_sel2, feature_coord_sel2)\n",
      "        print(X75.shape)\n",
      "        X100 = getx3(face_filenames100, n, nonfaces, feature_type_sel2, feature_coord_sel2)\n",
      "        print(X100.shape)\n",
      "        Xt = getx3(test_face_names, 300, nonfaces_t, feature_type_sel2, feature_coord_sel2)\n",
      "        print(Xt.shape)\n",
      "        \n",
      "        \n",
      "        params0 = logistic_fit(X0, y, 0.005)\n",
      "        predicted = logistic_prob(Xt, params0)\n",
      "        print(getacc(predicted, y_t))\n",
      "        params25 = logistic_fit(X25, y, 0.005)\n",
      "        predicted = logistic_prob(Xt, params25)\n",
      "        print(getacc(predicted, y_t))\n",
      "        params50 = logistic_fit(X50, y, 0.005)\n",
      "        predicted = logistic_prob(Xt, params50)\n",
      "        print(getacc(predicted, y_t))\n",
      "        params75 = logistic_fit(X75, y, 0.005)\n",
      "        predicted = logistic_prob(Xt, params75)\n",
      "        print(getacc(predicted, y_t))\n",
      "        params100 = logistic_fit(X100, y, 0.005)\n",
      "        predicted = logistic_prob(Xt, params100)\n",
      "        print(getacc(predicted, y_t))\n",
      "\n",
      "        os_r, bs_r, ws_r = logscores([params0, params25, params50, params75, params100], Xt, X_t[b_inds], X_t[w_inds])\n",
      "        os_rs.append(os_r)\n",
      "        bs_rs.append(bs_r)\n",
      "        ws_rs.append(ws_r)\n",
      "    \n",
      "    plt.plot([0, 25, 50, 75, 100], os_r, label=\"o\")\n",
      "    plt.plot([0, 25, 50, 75, 100], bs_r, label=\"b\")\n",
      "    plt.plot([0, 25, 50, 75, 100], ws_r, label=\"w\")\n",
      "    plt.legend()\n",
      "    plt.show()\n",
      "    \n",
      "    print(time()-t_start)\n",
      "    return os_rs, bs_rs, ws_rs\n",
      "164/335: os_l, bs_l, ws_l = pipeline2(lambda x: x, 10)\n",
      "164/336:\n",
      "def logscores(logs, Xt, Xbt, Xwt):\n",
      "    os = []\n",
      "    bs = []\n",
      "    ws = []\n",
      "    for i in range(len(logs)):\n",
      "        print(i*25)\n",
      "        o = getscore(Xt, y_t, logs[i])\n",
      "        b = getscore(Xbt, yb_t, logs[i])\n",
      "        w = getscore(Xwt, yw_t, logs[i])\n",
      "        print(\"overall:\", o)\n",
      "        print(\"black:\", b)\n",
      "        print(\"white:\", w)\n",
      "        os.append(o)\n",
      "        bs.append(b)\n",
      "        ws.append(w)\n",
      "    return os, bs, ws\n",
      "164/337:\n",
      "def pipeline2(f, times):\n",
      "    def getx3(facefiles, n, nonfaces, feature_types, feature_coords):\n",
      "        ims = [f(i) for i in getimgs(facefiles, nonfaces, n)]\n",
      "        X = delayed(extract_feature_image(img, feature_types, feature_coords) for img in ims)\n",
      "        return np.array(X.compute(scheduler='threads'))\n",
      "    \n",
      "    t_start = time()\n",
      "    \n",
      "    os_rs = []\n",
      "    bs_rs = []\n",
      "    ws_rs = []\n",
      "    for i in range(times):\n",
      "        nonfaces, nonfaces_t = regenerate_nonfaces()\n",
      "        \n",
      "        X0 = getx3(face_filenames0, n, nonfaces, feature_type_sel2, feature_coord_sel2)\n",
      "        print(X0.shape)\n",
      "        X25 = getx3(face_filenames25, n, nonfaces, feature_type_sel2, feature_coord_sel2)\n",
      "        print(X25.shape)\n",
      "        X50 = getx3(face_filenames50, n, nonfaces, feature_type_sel2, feature_coord_sel2)\n",
      "        print(X50.shape)\n",
      "        X75 = getx3(face_filenames75, n, nonfaces, feature_type_sel2, feature_coord_sel2)\n",
      "        print(X75.shape)\n",
      "        X100 = getx3(face_filenames100, n, nonfaces, feature_type_sel2, feature_coord_sel2)\n",
      "        print(X100.shape)\n",
      "        Xt = getx3(test_face_names, 300, nonfaces_t, feature_type_sel2, feature_coord_sel2)\n",
      "        print(Xt.shape)\n",
      "        \n",
      "        \n",
      "        params0 = logistic_fit(X0, y, 0.005)\n",
      "        predicted = logistic_prob(Xt, params0)\n",
      "        print(getacc(predicted, y_t))\n",
      "        params25 = logistic_fit(X25, y, 0.005)\n",
      "        predicted = logistic_prob(Xt, params25)\n",
      "        print(getacc(predicted, y_t))\n",
      "        params50 = logistic_fit(X50, y, 0.005)\n",
      "        predicted = logistic_prob(Xt, params50)\n",
      "        print(getacc(predicted, y_t))\n",
      "        params75 = logistic_fit(X75, y, 0.005)\n",
      "        predicted = logistic_prob(Xt, params75)\n",
      "        print(getacc(predicted, y_t))\n",
      "        params100 = logistic_fit(X100, y, 0.005)\n",
      "        predicted = logistic_prob(Xt, params100)\n",
      "        print(getacc(predicted, y_t))\n",
      "\n",
      "        os_r, bs_r, ws_r = logscores([params0, params25, params50, params75, params100], Xt, X_t[b_inds], X_t[w_inds])\n",
      "        os_rs.append(os_r)\n",
      "        bs_rs.append(bs_r)\n",
      "        ws_rs.append(ws_r)\n",
      "    \n",
      "    plt.plot([0, 25, 50, 75, 100], os_r, label=\"o\")\n",
      "    plt.plot([0, 25, 50, 75, 100], bs_r, label=\"b\")\n",
      "    plt.plot([0, 25, 50, 75, 100], ws_r, label=\"w\")\n",
      "    plt.legend()\n",
      "    plt.show()\n",
      "    \n",
      "    print(time()-t_start)\n",
      "    return os_rs, bs_rs, ws_rs\n",
      "164/338: os_l, bs_l, ws_l = pipeline2(lambda x: x, 10)\n",
      "164/339: os_l, bs_l, ws_l = pipeline2(lambda x: x, 10)\n",
      "164/340:\n",
      "def getscore(X, y, params):\n",
      "    predicted = logistic_prob(X, params)\n",
      "    return getacc(predicted, y)\n",
      "164/341:\n",
      "def getacc(predicted, actual):\n",
      "    preds = []\n",
      "    for p in predicted:\n",
      "        if p >= 0.5:\n",
      "            preds.append(1)\n",
      "        else:\n",
      "            preds.append(0)\n",
      "    return 1 - sum(abs(preds-actual))/len(preds)\n",
      "164/342:\n",
      "def pipeline2(f, times):\n",
      "    def getx3(facefiles, n, nonfaces, feature_types, feature_coords):\n",
      "        ims = [f(i) for i in getimgs(facefiles, nonfaces, n)]\n",
      "        X = delayed(extract_feature_image(img, feature_types, feature_coords) for img in ims)\n",
      "        return np.array(X.compute(scheduler='threads'))\n",
      "    \n",
      "    t_start = time()\n",
      "    \n",
      "    os_rs = []\n",
      "    bs_rs = []\n",
      "    ws_rs = []\n",
      "    for i in range(times):\n",
      "        nonfaces, nonfaces_t = regenerate_nonfaces()\n",
      "        \n",
      "        X0 = getx3(face_filenames0, n, nonfaces, feature_type_sel2, feature_coord_sel2)\n",
      "        print(X0.shape)\n",
      "        X25 = getx3(face_filenames25, n, nonfaces, feature_type_sel2, feature_coord_sel2)\n",
      "        print(X25.shape)\n",
      "        X50 = getx3(face_filenames50, n, nonfaces, feature_type_sel2, feature_coord_sel2)\n",
      "        print(X50.shape)\n",
      "        X75 = getx3(face_filenames75, n, nonfaces, feature_type_sel2, feature_coord_sel2)\n",
      "        print(X75.shape)\n",
      "        X100 = getx3(face_filenames100, n, nonfaces, feature_type_sel2, feature_coord_sel2)\n",
      "        print(X100.shape)\n",
      "        Xt = getx3(test_face_names, 300, nonfaces_t, feature_type_sel2, feature_coord_sel2)\n",
      "        print(Xt.shape)\n",
      "        \n",
      "        \n",
      "        params0 = logistic_fit(X0, y, 0.005)\n",
      "        predicted = logistic_prob(Xt, params0)\n",
      "        print(getacc(predicted, y_t))\n",
      "        params25 = logistic_fit(X25, y, 0.005)\n",
      "        predicted = logistic_prob(Xt, params25)\n",
      "        print(getacc(predicted, y_t))\n",
      "        params50 = logistic_fit(X50, y, 0.005)\n",
      "        predicted = logistic_prob(Xt, params50)\n",
      "        print(getacc(predicted, y_t))\n",
      "        params75 = logistic_fit(X75, y, 0.005)\n",
      "        predicted = logistic_prob(Xt, params75)\n",
      "        print(getacc(predicted, y_t))\n",
      "        params100 = logistic_fit(X100, y, 0.005)\n",
      "        predicted = logistic_prob(Xt, params100)\n",
      "        print(getacc(predicted, y_t))\n",
      "\n",
      "        os_r, bs_r, ws_r = logscores([params0, params25, params50, params75, params100], Xt, X_t[b_inds], X_t[w_inds])\n",
      "        os_rs.append(os_r)\n",
      "        bs_rs.append(bs_r)\n",
      "        ws_rs.append(ws_r)\n",
      "    \n",
      "    plt.plot([0, 25, 50, 75, 100], os_r, label=\"o\")\n",
      "    plt.plot([0, 25, 50, 75, 100], bs_r, label=\"b\")\n",
      "    plt.plot([0, 25, 50, 75, 100], ws_r, label=\"w\")\n",
      "    plt.legend()\n",
      "    plt.show()\n",
      "    \n",
      "    print(time()-t_start)\n",
      "    return os_rs, bs_rs, ws_rs\n",
      "164/343: os_l, bs_l, ws_l = pipeline2(lambda x: x, 10)\n",
      "164/344:\n",
      "def plots(os_rs, bs_rs, ws_rs):\n",
      "    plt.plot([0,25,50,75,100],np.mean(np.array(os_rs),axis=0),label='o')\n",
      "    plt.plot([0,25,50,75,100],np.mean(np.array(bs_rs),axis=0),label='b')\n",
      "    plt.plot([0,25,50,75,100],np.mean(np.array(ws_rs),axis=0),label='w')\n",
      "    plt.plot([0,25,50,75,100], np.array(np.mean(np.array(os_rs),axis=0)+np.std(np.array(os_rs),axis=0)),':',label='o+')\n",
      "    plt.plot([0,25,50,75,100], np.array(np.mean(np.array(os_rs),axis=0)-np.std(np.array(os_rs),axis=0)),':',label='o-')\n",
      "    plt.plot([0,25,50,75,100], np.array(np.mean(np.array(bs_rs),axis=0)+np.std(np.array(bs_rs),axis=0)),':',label='b+')\n",
      "    plt.plot([0,25,50,75,100], np.array(np.mean(np.array(bs_rs),axis=0)-np.std(np.array(bs_rs),axis=0)),':',label='b-')\n",
      "    plt.plot([0,25,50,75,100], np.array(np.mean(np.array(ws_rs),axis=0)+np.std(np.array(ws_rs),axis=0)),':',label='w+')\n",
      "    plt.plot([0,25,50,75,100], np.array(np.mean(np.array(ws_rs),axis=0)-np.std(np.array(ws_rs),axis=0)),':',label='w-')\n",
      "    plt.legend()\n",
      "    plt.show()\n",
      "164/345:\n",
      "def pipeline2(f, times):\n",
      "    def getx3(facefiles, n, nonfaces, feature_types, feature_coords):\n",
      "        ims = [f(i) for i in getimgs(facefiles, nonfaces, n)]\n",
      "        X = delayed(extract_feature_image(img, feature_types, feature_coords) for img in ims)\n",
      "        return np.array(X.compute(scheduler='threads'))\n",
      "    \n",
      "    t_start = time()\n",
      "    \n",
      "    os_rs = []\n",
      "    bs_rs = []\n",
      "    ws_rs = []\n",
      "    for i in range(times):\n",
      "        nonfaces, nonfaces_t = regenerate_nonfaces()\n",
      "        \n",
      "        X0 = getx3(face_filenames0, n, nonfaces, feature_type_sel2, feature_coord_sel2)\n",
      "        print(X0.shape)\n",
      "        X25 = getx3(face_filenames25, n, nonfaces, feature_type_sel2, feature_coord_sel2)\n",
      "        print(X25.shape)\n",
      "        X50 = getx3(face_filenames50, n, nonfaces, feature_type_sel2, feature_coord_sel2)\n",
      "        print(X50.shape)\n",
      "        X75 = getx3(face_filenames75, n, nonfaces, feature_type_sel2, feature_coord_sel2)\n",
      "        print(X75.shape)\n",
      "        X100 = getx3(face_filenames100, n, nonfaces, feature_type_sel2, feature_coord_sel2)\n",
      "        print(X100.shape)\n",
      "        Xt = getx3(test_face_names, 300, nonfaces_t, feature_type_sel2, feature_coord_sel2)\n",
      "        print(Xt.shape)\n",
      "        \n",
      "        \n",
      "        params0 = logistic_fit(X0, y, 0.005)\n",
      "        predicted = logistic_prob(Xt, params0)\n",
      "        print(getacc(predicted, y_t))\n",
      "        params25 = logistic_fit(X25, y, 0.005)\n",
      "        predicted = logistic_prob(Xt, params25)\n",
      "        print(getacc(predicted, y_t))\n",
      "        params50 = logistic_fit(X50, y, 0.005)\n",
      "        predicted = logistic_prob(Xt, params50)\n",
      "        print(getacc(predicted, y_t))\n",
      "        params75 = logistic_fit(X75, y, 0.005)\n",
      "        predicted = logistic_prob(Xt, params75)\n",
      "        print(getacc(predicted, y_t))\n",
      "        params100 = logistic_fit(X100, y, 0.005)\n",
      "        predicted = logistic_prob(Xt, params100)\n",
      "        print(getacc(predicted, y_t))\n",
      "\n",
      "        os_r, bs_r, ws_r = logscores([params0, params25, params50, params75, params100], Xt, Xt[b_inds], Xt[w_inds])\n",
      "        os_rs.append(os_r)\n",
      "        bs_rs.append(bs_r)\n",
      "        ws_rs.append(ws_r)\n",
      "    \n",
      "    plt.plot([0, 25, 50, 75, 100], os_r, label=\"o\")\n",
      "    plt.plot([0, 25, 50, 75, 100], bs_r, label=\"b\")\n",
      "    plt.plot([0, 25, 50, 75, 100], ws_r, label=\"w\")\n",
      "    plt.legend()\n",
      "    plt.show()\n",
      "    \n",
      "    print(time()-t_start)\n",
      "    return os_rs, bs_rs, ws_rs\n",
      "164/346: os_l, bs_l, ws_l = pipeline2(lambda x: x, 10)\n",
      "164/347: plots(os_l, bs_l, ws_l)\n",
      "164/348:\n",
      "predictions = logistic_prob(Xt, params0)\n",
      "plt.subplots(2,3,figsize=(8,6))\n",
      "count = 0\n",
      "for i in range(300):\n",
      "    if predictions[i] != yt[i]:\n",
      "        count += 1\n",
      "        plt.subplot(2,3,count)\n",
      "        plt.imshow(cv2.imread(test_face_names[i], cv2.IMREAD_GRAYSCALE), cmap='gray')\n",
      "164/349: params\n",
      "164/350:\n",
      "predictions = logistic_prob(Xt, params)\n",
      "plt.subplots(2,3,figsize=(8,6))\n",
      "count = 0\n",
      "for i in range(300):\n",
      "    if predictions[i] != yt[i]:\n",
      "        count += 1\n",
      "        plt.subplot(2,3,count)\n",
      "        plt.imshow(cv2.imread(test_face_names[i], cv2.IMREAD_GRAYSCALE), cmap='gray')\n",
      "164/351: yt = [1]*300 + [0]*300\n",
      "164/352:\n",
      "predictions = logistic_prob(Xt, params)\n",
      "plt.subplots(2,3,figsize=(8,6))\n",
      "count = 0\n",
      "for i in range(300):\n",
      "    if predictions[i] != yt[i]:\n",
      "        count += 1\n",
      "        plt.subplot(2,3,count)\n",
      "        plt.imshow(cv2.imread(test_face_names[i], cv2.IMREAD_GRAYSCALE), cmap='gray')\n",
      "164/353:\n",
      "predictions = logistic_prob(Xt, params)\n",
      "plt.subplots(2,4,figsize=(8,6))\n",
      "count = 0\n",
      "for i in range(300):\n",
      "    if predictions[i] != yt[i]:\n",
      "        count += 1\n",
      "        plt.subplot(2,4,count)\n",
      "        plt.imshow(cv2.imread(test_face_names[i], cv2.IMREAD_GRAYSCALE), cmap='gray')\n",
      "164/354:\n",
      "predictions = logistic_prob(Xt, params)\n",
      "plt.subplots(2,3,figsize=(8,6))\n",
      "count = 0\n",
      "for i in range(300):\n",
      "    if predictions[i] != yt[i]:\n",
      "        count += 1\n",
      "        if count < 6:\n",
      "            plt.subplot(2,3,count)\n",
      "            plt.imshow(cv2.imread(test_face_names[i], cv2.IMREAD_GRAYSCALE), cmap='gray')\n",
      "164/355:\n",
      "predictions = logistic_prob(Xt, params)\n",
      "plt.subplots(2,3,figsize=(8,6))\n",
      "count = 0\n",
      "for i in range(300):\n",
      "    if predictions[i] != yt[i]:\n",
      "        count += 1\n",
      "        if count < 7:\n",
      "            plt.subplot(2,3,count)\n",
      "            plt.imshow(cv2.imread(test_face_names[i], cv2.IMREAD_GRAYSCALE), cmap='gray')\n",
      "164/356:\n",
      "predictions = logistic_prob(Xt, params)\n",
      "plt.subplots(2,3,figsize=(8,6))\n",
      "count = 0\n",
      "for i in range(300, 600):\n",
      "    if predictions[i] != yt[i]:\n",
      "        count += 1\n",
      "        if count < 7:\n",
      "            plt.subplot(2,3,count)\n",
      "            plt.imshow(cv2.imread(test_face_names[i], cv2.IMREAD_GRAYSCALE), cmap='gray')\n",
      "164/357:\n",
      "predictions = logistic_prob(Xt, params)\n",
      "plt.subplots(2,3,figsize=(8,6))\n",
      "count = 0\n",
      "for i in range(300, 600):\n",
      "    if predictions[i] != yt[i]:\n",
      "        count += 1\n",
      "        if count < 7:\n",
      "            plt.subplot(2,3,count)\n",
      "            plt.imshow(cv2.imread(nonfaces_t[i], cv2.IMREAD_GRAYSCALE), cmap='gray')\n",
      "164/358:\n",
      "predictions = logistic_prob(Xt, params)\n",
      "plt.subplots(2,3,figsize=(8,6))\n",
      "count = 0\n",
      "for i in range(300, 600):\n",
      "    if predictions[i] != yt[i]:\n",
      "        count += 1\n",
      "        if count < 7:\n",
      "            plt.subplot(2,3,count)\n",
      "            plt.imshow(cv2.imread(nonfaces_t[i-300], cv2.IMREAD_GRAYSCALE), cmap='gray')\n",
      "164/359:\n",
      "predictions = logistic_prob(Xt, params)\n",
      "plt.subplots(2,3,figsize=(8,6))\n",
      "count = 0\n",
      "for i in range(300, 600):\n",
      "    if predictions[i] != yt[i]:\n",
      "        count += 1\n",
      "        if count < 7:\n",
      "            plt.subplot(2,3,count)\n",
      "            plt.imshow(cv2.imread(nonfaces_t[i], cv2.IMREAD_GRAYSCALE), cmap='gray')\n",
      "164/360: len(nonfaces_t)\n",
      "164/361:\n",
      "predictions = logistic_prob(Xt, params)\n",
      "plt.subplots(2,3,figsize=(8,6))\n",
      "count = 0\n",
      "for i in range(300, 600):\n",
      "    if predictions[i] != yt[i]:\n",
      "        count += 1\n",
      "        if count < 7:\n",
      "            plt.subplot(2,3,count)\n",
      "            plt.imshow(cv2.imread(nonfaces_t[i-300], cv2.IMREAD_GRAYSCALE), cmap='gray')\n",
      "164/362:\n",
      "predictions = logistic_prob(Xt, params)\n",
      "plt.subplots(2,3,figsize=(8,6))\n",
      "count = 0\n",
      "for i in range(300, 600):\n",
      "    if predictions[i] != yt[i]:\n",
      "        count += 1\n",
      "        if count < 7:\n",
      "            plt.subplot(2,3,count)\n",
      "            plt.imshow(nonfaces_t[i-300], cmap='gray')\n",
      "164/363: plots(os_l+.1, bs_l, ws_l)\n",
      "164/364: plots(np.array(os_l)+.1, bs_l, ws_l)\n",
      "164/365: plots(np.array(os_l)+.15, bs_l, ws_l)\n",
      "164/366:\n",
      "plt.hist(distsw)\n",
      "plt.title(\"Histogram\")\n",
      "164/367:\n",
      "plt.hist(distsw)\n",
      "plt.title(\"Histogram of distances of white faces from mean face\")\n",
      "164/368:\n",
      "plt.hist(distsw2)\n",
      "plt.title(\"Histogram of distances of white faces from mean white face\")\n",
      "164/369:\n",
      "plt.hist(distsw)\n",
      "plt.title(\"Histogram of distances of white faces from mean overall face\")\n",
      "164/370:\n",
      "plt.hist(distsb2)\n",
      "plt.title(\"Histogram of distances of black faces from mean black face\")\n",
      "164/371:\n",
      "def gettop(fnames, rf):\n",
      "    im = cv2.resize(cv2.imread(fnames[5], cv2.IMREAD_GRAYSCALE),(36,36))\n",
      "    fig, axes = plt.subplots(3,4, figsize=(16,8))\n",
      "    # Sort features in order of importance and plot the six most significant\n",
      "    idx_sorted = np.argsort(rf.feature_importances_)[::-1]\n",
      "    for idx, ax in enumerate(axes.ravel()):\n",
      "        image = draw_haar_like_feature(im, 0, 0, 36, 36,\n",
      "                                       [feature_coord_sel[idx_sorted[idx]]])\n",
      "        ax.imshow(image)\n",
      "        ax.set_xticks([])\n",
      "        ax.set_yticks([])\n",
      "\n",
      "    fig.suptitle('The most important features')\n",
      "164/372: X100.shape\n",
      "164/373: feature_coord_sel2.shape\n",
      "164/374:\n",
      "def gettop(fnames, rf):\n",
      "    im = cv2.resize(cv2.imread(fnames[5], cv2.IMREAD_GRAYSCALE),(36,36))\n",
      "    fig, axes = plt.subplots(3,4, figsize=(16,8))\n",
      "    # Sort features in order of importance and plot the six most significant\n",
      "    idx_sorted = np.argsort(rf.feature_importances_)[::-1]\n",
      "    for idx, ax in enumerate(axes.ravel()):\n",
      "        image = draw_haar_like_feature(im, 0, 0, 36, 36,\n",
      "                                       [feature_coord_sel[idx_sorted[idx]]])\n",
      "        ax.imshow(image)\n",
      "        ax.set_xticks([])\n",
      "        ax.set_yticks([])\n",
      "\n",
      "    fig.suptitle('The most important features')\n",
      "164/375: Xt_900.shape\n",
      "164/376: X100 = getx2(face_filenames100, n, nonfaces, feature_type_sel2, feature_coord_sel2)\n",
      "164/377: X100.shape\n",
      "164/378: gettop(face_filenames100, rf100_)\n",
      "164/379:\n",
      "t_start = time()\n",
      "rf100_ = RandomForestClassifier(n_estimators=1000, max_depth=None, n_jobs=-1, random_state=0)\n",
      "rf100_.fit(X100, y)\n",
      "print(time() - t_start)\n",
      "print(rf100_.score(Xt_900, y_t))\n",
      "164/380:\n",
      "def gettop(fnames, rf, k):\n",
      "    im = cv2.resize(cv2.imread(fnames[k], cv2.IMREAD_GRAYSCALE),(36,36))\n",
      "    fig, axes = plt.subplots(3,4, figsize=(16,8))\n",
      "    # Sort features in order of importance and plot the six most significant\n",
      "    idx_sorted = np.argsort(rf.feature_importances_)[::-1]\n",
      "    for idx, ax in enumerate(axes.ravel()):\n",
      "        image = draw_haar_like_feature(im, 0, 0, 36, 36,\n",
      "                                       [feature_coord_sel[idx_sorted[idx]]])\n",
      "        ax.imshow(image)\n",
      "        ax.set_xticks([])\n",
      "        ax.set_yticks([])\n",
      "\n",
      "    fig.suptitle('The most important features')\n",
      "164/381: gettop(face_filenames100, rf100_, 3)\n",
      "164/382:\n",
      "def gettop(fnames, rf, k):\n",
      "    im = cv2.resize(cv2.imread(fnames[k], cv2.IMREAD_GRAYSCALE),(36,36))\n",
      "    fig, axes = plt.subplots(3,4, figsize=(16,8))\n",
      "    # Sort features in order of importance and plot the six most significant\n",
      "    idx_sorted = np.argsort(rf.feature_importances_)[::-1]\n",
      "    for idx, ax in enumerate(axes.ravel()):\n",
      "        image = draw_haar_like_feature(im, 0, 0, 36, 36,\n",
      "                                       [feature_coord_sel2[idx_sorted[idx]]])\n",
      "        ax.imshow(image)\n",
      "        ax.set_xticks([])\n",
      "        ax.set_yticks([])\n",
      "\n",
      "    fig.suptitle('The most important features')\n",
      "164/383: gettop(face_filenames100, rf100_, 3)\n",
      "164/384:\n",
      "nonfaces, nonfaces_t = regenerate_nonfaces()\n",
      "X100 = getx2(face_filenames100, n, nonfaces, feature_type_sel2, feature_coord_sel2)\n",
      "Xt = getx2(face_filenames0, 300, nonfaces_t, feature_type_sel2, feature_coord_sel2)\n",
      "164/385: gettop(face_filenames100, rf100_, 3)\n",
      "164/386: gettop(face_filenames100, rf100_, 6)\n",
      "164/387: gettop(face_filenames100, rf100_, 7)\n",
      "164/388: gettop(face_filenames100, rf100_, 9)\n",
      "164/389:\n",
      "t_start = time()\n",
      "rf100_ = RandomForestClassifier(n_estimators=1000, max_depth=None, n_jobs=-1, random_state=0)\n",
      "rf100_.fit(X100, y)\n",
      "print(time() - t_start)\n",
      "print(rf100_.score(Xt_900, y_t))\n",
      "164/390: gettop(face_filenames0, rf0_)\n",
      "164/391: gettop(face_filenames100, rf100_, 9)\n",
      "164/392:\n",
      "nonfaces, nonfaces_t = regenerate_nonfaces()\n",
      "X100 = getx2(face_filenames100, n, nonfaces, feature_type_sel2, feature_coord_sel2)\n",
      "rf100_ = RandomForestClassifier(n_estimators=1000, max_depth=None, n_jobs=-1, random_state=0)\n",
      "rf100_.fit(X100, y)\n",
      "164/393: gettop(face_filenames100, rf100_, 9)\n",
      "164/394:\n",
      "nonfaces, nonfaces_t = regenerate_nonfaces()\n",
      "X100 = getx2(face_filenames100, n, nonfaces, feature_type_sel2, feature_coord_sel2)\n",
      "rf100_ = RandomForestClassifier(n_estimators=1000, max_depth=None, n_jobs=-1, random_state=0)\n",
      "rf100_.fit(X100, y)\n",
      "164/395: gettop(face_filenames100, rf100_, 9)\n",
      "164/396:\n",
      "nonfaces, nonfaces_t = regenerate_nonfaces()\n",
      "X100 = getx2(face_filenames100, n, nonfaces, feature_type_sel2, feature_coord_sel2)\n",
      "rf100_ = RandomForestClassifier(n_estimators=1000, max_depth=None, n_jobs=-1, random_state=0)\n",
      "rf100_.fit(X100, y)\n",
      "164/397: gettop(face_filenames100, rf100_, 9)\n",
      "165/1:\n",
      "import os\n",
      "import cv2\n",
      "from time import time\n",
      "import numpy as np\n",
      "import matplotlib.pyplot as plt\n",
      "from glob import glob\n",
      "import random\n",
      "from dask import delayed\n",
      "165/2:\n",
      "from sklearn.ensemble import RandomForestClassifier\n",
      "from sklearn.model_selection import train_test_split\n",
      "from sklearn.metrics import roc_auc_score\n",
      "from sklearn.linear_model import LogisticRegression\n",
      "165/3:\n",
      "from skimage.transform import integral_image\n",
      "from skimage.feature import haar_like_feature\n",
      "from skimage.feature import haar_like_feature_coord\n",
      "from skimage.feature import draw_haar_like_feature\n",
      "165/4: from logistic import logistic_fit, logistic, logistic_prob\n",
      "165/5: import dill\n",
      "165/6:\n",
      "t_start = time()\n",
      "dill.load_session('01_05_env.db')\n",
      "time() - t_start\n",
      "165/7:\n",
      "t_start = time()\n",
      "dill.load_session('01_05_env.db')\n",
      "time() - t_start\n",
      "165/8: from hog36 import hog36\n",
      "165/9: hog0 = np.array([hog36(cv2.resize(im,(36,36)), 9, True) for im in getimgs(face_filenames0, nonfaces, n)])\n",
      "165/10:\n",
      "def getimgs(facefiles, nonfaces, n):\n",
      "    faces = []\n",
      "    for i in range(n):\n",
      "        faces.append(cv2.imread(facefiles[i], cv2.IMREAD_GRAYSCALE))\n",
      "    return faces + nonfaces[:n]\n",
      "165/11:\n",
      "def getx(facefiles, n):\n",
      "    ims = getimgs(facefiles, n)\n",
      "    X = delayed(extract_feature_image(img, feature_types) for img in ims)\n",
      "    return np.array(X.compute(scheduler='threads'))\n",
      "165/12:\n",
      "def getx2(facefiles, n, nonfaces, feature_types, feature_coords):\n",
      "    ims = getimgs(facefiles, nonfaces, n)\n",
      "    X = delayed(extract_feature_image(img, feature_types, feature_coords) for img in ims)\n",
      "    return np.array(X.compute(scheduler='threads'))\n",
      "165/13: hog0 = np.array([hog36(cv2.resize(im,(36,36)), 9, True) for im in getimgs(face_filenames0, nonfaces, n)])\n",
      "165/14: hog0.shape\n",
      "165/15: X0h = np.concatenate((X0, hog0),axis=1)\n",
      "165/16: nonfaces.shape\n",
      "165/17: len(nonfaces), n\n",
      "165/18:\n",
      "n = 4020\n",
      "nonfaces, nonfaces_t = regenerate_nonfaces()\n",
      "165/19:\n",
      "def regenerate_nonfaces():\n",
      "    nonfaces = []\n",
      "    for i in range(n):\n",
      "        # Read a random nonface file\n",
      "        j = random.randint(0, len(nonface_filenames) - 1)\n",
      "        nonface = cv2.imread(nonface_filenames[j], cv2.IMREAD_GRAYSCALE)\n",
      "        wsize = random.randint(36, min(nonface.shape[0], nonface.shape[1]))\n",
      "        row = random.randint(0, nonface.shape[0]-wsize)\n",
      "        col = random.randint(0, nonface.shape[1]-wsize)\n",
      "        crop = nonface[row:row+wsize, col:col+wsize]\n",
      "\n",
      "        # Resize to be the right size\n",
      "        crop = cv2.resize(crop, (200, 200))\n",
      "        nonfaces.append(crop)\n",
      "    nonfaces_t = []\n",
      "    for i in range(n_test):\n",
      "        # Read a random nonface file\n",
      "        j = random.randint(0, n_nonfaces_test - 1)\n",
      "        nonface = cv2.imread(test_nonface_names[j], cv2.IMREAD_GRAYSCALE)\n",
      "\n",
      "        wsize = random.randint(36, min(nonface.shape[0], nonface.shape[1]))\n",
      "        row = random.randint(0, nonface.shape[0]-wsize)\n",
      "        col = random.randint(0, nonface.shape[1]-wsize)\n",
      "        crop = nonface[row:row+wsize, col:col+wsize]\n",
      "\n",
      "        # Resize to be the right size\n",
      "        crop = cv2.resize(crop, (200, 200))\n",
      "        nonfaces_t.append(crop)\n",
      "    return nonfaces, nonfaces_t\n",
      "165/20:\n",
      "n = 4020\n",
      "nonfaces, nonfaces_t = regenerate_nonfaces()\n",
      "165/21: hog0 = np.array([hog36(cv2.resize(im,(36,36)), 9, True) for im in getimgs(face_filenames0, nonfaces, n)])\n",
      "165/22: hog0.shape\n",
      "165/23: X0.shape\n",
      "165/24: feature_type_sel;2\n",
      "165/25: feature_type_sel2\n",
      "165/26:\n",
      "# Train a random forest classifier and assess its performance\n",
      "clf = RandomForestClassifier(n_estimators=1000, max_depth=None,\n",
      "                             n_jobs=-1, random_state=0)\n",
      "t_start = time()\n",
      "clf.fit(Xf_t, yf_t)\n",
      "time() - t_start\n",
      "165/27: Xf_t = X\n",
      "165/28: yf_t = np.array([1]*len(faces_t) + [0]*len(nonfaces_t))\n",
      "165/29:\n",
      "# Train a random forest classifier and assess its performance\n",
      "clf = RandomForestClassifier(n_estimators=1000, max_depth=None,\n",
      "                             n_jobs=-1, random_state=0)\n",
      "t_start = time()\n",
      "clf.fit(Xf_t, yf_t)\n",
      "time() - t_start\n",
      "165/30:\n",
      "feature_coord, feature_type = \\\n",
      "    haar_like_feature_coord(width=36, height=36,\n",
      "                            feature_type=feature_types)\n",
      "165/31: idx_sorted = np.argsort(clf.feature_importances_)[::-1]\n",
      "165/32:\n",
      "for i in range(1500):\n",
      "    if len(feature_coord[idx_sorted[i]]) > 2:\n",
      "        print(i)\n",
      "165/33:\n",
      "fig, axes = plt.subplots(4,4, figsize=(20,20))\n",
      "for idx, ax in enumerate(axes.ravel()):\n",
      "    image = cv2.resize(images_t[0], (36,36))\n",
      "    image = draw_haar_like_feature(image, 0, 0, 36, 36,\n",
      "                                   [feature_coord[idx_sorted[idx]]])\n",
      "    ax.imshow(image)\n",
      "    ax.set_xticks([])\n",
      "    ax.set_yticks([])\n",
      "\n",
      "fig.suptitle('The most important features')\n",
      "165/34:\n",
      "fig, axes = plt.subplots(1,4, figsize=(12,4))\n",
      "idxs = [1250,1325,1341,1403,1448]\n",
      "for idx, ax in enumerate(axes.ravel()):\n",
      "    image = cv2.resize(images_t[0], (36,36))\n",
      "    image = draw_haar_like_feature(image, 0, 0, 36, 36,\n",
      "                                   [feature_coord[idx_sorted[idxs[idx]]]])\n",
      "    ax.imshow(image)\n",
      "    ax.set_xticks([])\n",
      "    ax.set_yticks([])\n",
      "\n",
      "fig.suptitle('The most important features')\n",
      "165/35:\n",
      "cdf_feature_importances = np.cumsum(clf.feature_importances_[idx_sorted])\n",
      "cdf_feature_importances /= np.max(cdf_feature_importances)\n",
      "sig_feature_count = np.count_nonzero(cdf_feature_importances < 0.7)\n",
      "sig_feature_percent = round(sig_feature_count /\n",
      "                            len(cdf_feature_importances) * 100, 1)\n",
      "print(('{} features, or {}%, account for 70% of branch points in the '\n",
      "       'random forest.').format(sig_feature_count, sig_feature_percent))\n",
      "165/36:\n",
      "feature_coord_sel = feature_coord[idx_sorted[:n_features]]\n",
      "feature_type_sel = feature_type[idx_sorted[:n_features]]\n",
      "165/37:\n",
      "X0 = getx2(face_filenames0, n, nonfaces, feature_type_sel2, feature_coord_sel2)\n",
      "X25 = getx2(face_filenames25, n, nonfaces, feature_type_sel2, feature_coord_sel2)\n",
      "X50 = getx2(face_filenames50, n, nonfaces, feature_type_sel2, feature_coord_sel2)\n",
      "X75 = getx2(face_filenames75, n, nonfaces, feature_type_sel2, feature_coord_sel2)\n",
      "X100 = getx2(face_filenames100, n, nonfaces, feature_type_sel2, feature_coord_sel2)\n",
      "Xt = getx2(face_filenames0, 300, nonfaces_t, feature_type_sel2, feature_coord_sel2)\n",
      "y = np.array([1]*n + [0]*n)\n",
      "yt = np.array([1]*300+[0]*300)\n",
      "165/38: n_features2 = 900\n",
      "165/39:\n",
      "feature_coord_sel2 = feature_coord[idx_sorted[:n_features2]]\n",
      "feature_type_sel2 = feature_type[idx_sorted[:n_features2]]\n",
      "165/40: feature_type_sel2\n",
      "165/41:\n",
      "X0 = getx2(face_filenames0, n, nonfaces, feature_type_sel2, feature_coord_sel2)\n",
      "X25 = getx2(face_filenames25, n, nonfaces, feature_type_sel2, feature_coord_sel2)\n",
      "X50 = getx2(face_filenames50, n, nonfaces, feature_type_sel2, feature_coord_sel2)\n",
      "X75 = getx2(face_filenames75, n, nonfaces, feature_type_sel2, feature_coord_sel2)\n",
      "X100 = getx2(face_filenames100, n, nonfaces, feature_type_sel2, feature_coord_sel2)\n",
      "Xt = getx2(face_filenames0, 300, nonfaces_t, feature_type_sel2, feature_coord_sel2)\n",
      "y = np.array([1]*n + [0]*n)\n",
      "yt = np.array([1]*300+[0]*300)\n",
      "165/42: hog0 = np.array([hog36(cv2.resize(im,(36,36)), 9, True) for im in getimgs(face_filenames0, nonfaces, n)])\n",
      "165/43: hog0.shape\n",
      "165/44: X0.shape\n",
      "165/45: X0h = np.concatenate((X0, hog0),axis=1)\n",
      "165/46: X0h.shape\n",
      "165/47:\n",
      "hog25 = np.array([hog36(cv2.resize(im,(36,36)), 9, True) for im in getimgs(face_filenames25, nonfaces, n)])\n",
      "X25h = np.concatenate((X25, hog25),axis=1)\n",
      "165/48:\n",
      "hog50 = np.array([hog36(cv2.resize(im,(36,36)), 9, True) for im in getimgs(face_filenames50, nonfaces, n)])\n",
      "X50h = np.concatenate((X50, hog50),axis=1)\n",
      "165/49:\n",
      "hog75 = np.array([hog36(cv2.resize(im,(36,36)), 9, True) for im in getimgs(face_filenames75, nonfaces, n)])\n",
      "X75h = np.concatenate((X75, hog75),axis=1)\n",
      "165/50:\n",
      "hog100 = np.array([hog36(cv2.resize(im,(36,36)), 9, True) for im in getimgs(face_filenames100, nonfaces, n)])\n",
      "X100h = np.concatenate((X100, hog100),axis=1)\n",
      "165/51: hog100 = np.array([hog36(cv2.resize(im,(36,36)), 9, True) for im in getimgs(face_filenames100, nonfaces, n)])\n",
      "165/52: hogt = np.array([hog36(cv2.resize(im,(36,36)), 9, True) for im in getimgs(test_face_names, nonfaces_t, 300)])\n",
      "165/53:\n",
      "hogt = np.array([hog36(cv2.resize(im,(36,36)), 9, True) for im in getimgs(test_face_names, nonfaces_t, 300)])\n",
      "Xt_900h = np.concatenate((Xt_900, hogt),axis=1)\n",
      "165/54: y = np.array([1]*4020+[0]*4020)\n",
      "165/55:\n",
      "t_start = time()\n",
      "h100 = RandomForestClassifier(n_estimators=1000, max_depth=None, n_jobs=-1, random_state=0)\n",
      "h100.fit(hog100, y)\n",
      "print(h100.score(hogt, y_t))\n",
      "time() - t_start\n",
      "165/56: hogt = np.array([hog36(cv2.resize(im,(36,36)), 9, True) for im in getimgs(test_face_names, nonfaces_t, 300)])\n",
      "165/57: getimgs(test_face_names, nonfaces_t, 300)\n",
      "165/58: test_face_names\n",
      "165/59: Xt_900 = getx2(test_face_names, 300, nonfaces_t, feature_type_sel2, feature_coord_sel2)\n",
      "165/60:\n",
      "test_face_names = sorted(glob(os.path.join('data/testing_faces', '*.jpg')))\n",
      "n_test = len(test_face_names)\n",
      "165/61: Xt_900 = getx2(test_face_names, 300, nonfaces_t, feature_type_sel2, feature_coord_sel2)\n",
      "165/62:\n",
      "t_start = time()\n",
      "h100 = RandomForestClassifier(n_estimators=1000, max_depth=None, n_jobs=-1, random_state=0)\n",
      "h100.fit(hog100, y)\n",
      "print(h100.score(hogt, y_t))\n",
      "time() - t_start\n",
      "165/63:\n",
      "hogt = np.array([hog36(cv2.resize(im,(36,36)), 9, True) for im in getimgs(test_face_names, nonfaces_t, 300)])\n",
      "Xt_900h = np.concatenate((Xt_900, hogt),axis=1)\n",
      "165/64:\n",
      "t_start = time()\n",
      "h100 = RandomForestClassifier(n_estimators=1000, max_depth=None, n_jobs=-1, random_state=0)\n",
      "h100.fit(hog100, y)\n",
      "print(h100.score(hogt, y_t))\n",
      "time() - t_start\n",
      "165/65:\n",
      "def pipeline3(times):\n",
      "    def getx3(facefiles, n, nonfaces, feature_types, feature_coords):\n",
      "        ims = getimgs(facefiles, nonfaces, n)\n",
      "        X = delayed(extract_feature_image(img, feature_types, feature_coords) for img in ims)\n",
      "        X = np.array(X.compute(scheduler='threads'))\n",
      "        hog = np.array([hog36(cv2.resize(im,(36,36)), 9, True) for im in ims])\n",
      "        return np.concatenate((X, hog),axis=1)\n",
      "    \n",
      "    t_start = time()\n",
      "    \n",
      "    os_rs = []\n",
      "    bs_rs = []\n",
      "    ws_rs = []\n",
      "    for i in range(times):\n",
      "        nonfaces, nonfaces_t = regenerate_nonfaces()\n",
      "        \n",
      "        X0 = getx3(face_filenames0, n, nonfaces, feature_type_sel2, feature_coord_sel2)\n",
      "        print(X0.shape)\n",
      "        X25 = getx3(face_filenames25, n, nonfaces, feature_type_sel2, feature_coord_sel2)\n",
      "        print(X25.shape)\n",
      "        X50 = getx3(face_filenames50, n, nonfaces, feature_type_sel2, feature_coord_sel2)\n",
      "        print(X50.shape)\n",
      "        X75 = getx3(face_filenames75, n, nonfaces, feature_type_sel2, feature_coord_sel2)\n",
      "        print(X75.shape)\n",
      "        X100 = getx3(face_filenames100, n, nonfaces, feature_type_sel2, feature_coord_sel2)\n",
      "        print(X100.shape)\n",
      "        Xt = getx3(test_face_names, 300, nonfaces_t, feature_type_sel2, feature_coord_sel2)\n",
      "        print(Xt.shape)\n",
      "        \n",
      "        rf0 = RandomForestClassifier(n_estimators=1000, max_depth=None, n_jobs=-1, random_state=0)\n",
      "        rf0.fit(X0, y)\n",
      "        print(rf0.score(Xt_900, y_t))\n",
      "        rf25 = RandomForestClassifier(n_estimators=1000, max_depth=None, n_jobs=-1, random_state=0)\n",
      "        rf25.fit(X25, y)\n",
      "        print(rf25.score(Xt_900, y_t))\n",
      "        rf50 = RandomForestClassifier(n_estimators=1000, max_depth=None, n_jobs=-1, random_state=0)\n",
      "        rf50.fit(X50, y)\n",
      "        print(rf50.score(Xt_900, y_t))\n",
      "        rf75 = RandomForestClassifier(n_estimators=1000, max_depth=None, n_jobs=-1, random_state=0)\n",
      "        rf75.fit(X75, y)\n",
      "        print(rf75.score(Xt_900, y_t))\n",
      "        rf100 = RandomForestClassifier(n_estimators=1000, max_depth=None, n_jobs=-1, random_state=0)\n",
      "        rf100.fit(X100, y)\n",
      "        print(rf100.score(Xt_900, y_t))\n",
      "\n",
      "        os_r, bs_r, ws_r = rfscores([rf0, rf25, rf50, rf75, rf100], Xt_900, Xb_t, Xw_t)\n",
      "        os_rs.append(os_r)\n",
      "        bs_rs.append(bs_r)\n",
      "        ws_rs.append(ws_r)\n",
      "    \n",
      "    plt.plot([0, 25, 50, 75, 100], os_r, label=\"o\")\n",
      "    plt.plot([0, 25, 50, 75, 100], bs_r, label=\"b\")\n",
      "    plt.plot([0, 25, 50, 75, 100], ws_r, label=\"w\")\n",
      "    plt.legend()\n",
      "    plt.show()\n",
      "    \n",
      "    print(time()-t_start)\n",
      "    return os_rs, bs_rs, ws_rs\n",
      "165/66: pipeline3(2)\n",
      "165/67: len(yt)\n",
      "165/68: len(y)\n",
      "165/69:\n",
      "def pipeline3(times):\n",
      "    def getx3(facefiles, n, nonfaces, feature_types, feature_coords):\n",
      "        ims = getimgs(facefiles, nonfaces, n)\n",
      "        X = delayed(extract_feature_image(img, feature_types, feature_coords) for img in ims)\n",
      "        X = np.array(X.compute(scheduler='threads'))\n",
      "        hog = np.array([hog36(cv2.resize(im,(36,36)), 9, True) for im in ims])\n",
      "        return np.concatenate((X, hog),axis=1)\n",
      "    \n",
      "    t_start = time()\n",
      "    \n",
      "    os_rs = []\n",
      "    bs_rs = []\n",
      "    ws_rs = []\n",
      "    for i in range(times):\n",
      "        nonfaces, nonfaces_t = regenerate_nonfaces()\n",
      "        \n",
      "        X0 = getx3(face_filenames0, n, nonfaces, feature_type_sel2, feature_coord_sel2)\n",
      "        print(X0.shape)\n",
      "        X25 = getx3(face_filenames25, n, nonfaces, feature_type_sel2, feature_coord_sel2)\n",
      "        print(X25.shape)\n",
      "        X50 = getx3(face_filenames50, n, nonfaces, feature_type_sel2, feature_coord_sel2)\n",
      "        print(X50.shape)\n",
      "        X75 = getx3(face_filenames75, n, nonfaces, feature_type_sel2, feature_coord_sel2)\n",
      "        print(X75.shape)\n",
      "        X100 = getx3(face_filenames100, n, nonfaces, feature_type_sel2, feature_coord_sel2)\n",
      "        print(X100.shape)\n",
      "        Xt = getx3(test_face_names, 300, nonfaces_t, feature_type_sel2, feature_coord_sel2)\n",
      "        print(Xt.shape)\n",
      "        \n",
      "        rf0 = RandomForestClassifier(n_estimators=1000, max_depth=None, n_jobs=-1, random_state=0)\n",
      "        rf0.fit(X0, y)\n",
      "        print(rf0.score(Xt, yt))\n",
      "        rf25 = RandomForestClassifier(n_estimators=1000, max_depth=None, n_jobs=-1, random_state=0)\n",
      "        rf25.fit(X25, y)\n",
      "        print(rf25.score(Xt, yt))\n",
      "        rf50 = RandomForestClassifier(n_estimators=1000, max_depth=None, n_jobs=-1, random_state=0)\n",
      "        rf50.fit(X50, y)\n",
      "        print(rf50.score(Xt, yt))\n",
      "        rf75 = RandomForestClassifier(n_estimators=1000, max_depth=None, n_jobs=-1, random_state=0)\n",
      "        rf75.fit(X75, y)\n",
      "        print(rf75.score(Xt, yt))\n",
      "        rf100 = RandomForestClassifier(n_estimators=1000, max_depth=None, n_jobs=-1, random_state=0)\n",
      "        rf100.fit(X100, y)\n",
      "        print(rf100.score(Xt, yt))\n",
      "\n",
      "        os_r, bs_r, ws_r = rfscores([rf0, rf25, rf50, rf75, rf100], Xt, Xt[b_inds], Xt[w_inds])\n",
      "        os_rs.append(os_r)\n",
      "        bs_rs.append(bs_r)\n",
      "        ws_rs.append(ws_r)\n",
      "    \n",
      "    plt.plot([0, 25, 50, 75, 100], os_r, label=\"o\")\n",
      "    plt.plot([0, 25, 50, 75, 100], bs_r, label=\"b\")\n",
      "    plt.plot([0, 25, 50, 75, 100], ws_r, label=\"w\")\n",
      "    plt.legend()\n",
      "    plt.show()\n",
      "    \n",
      "    print(time()-t_start)\n",
      "    return os_rs, bs_rs, ws_rs\n",
      "165/70: pipeline3(2)\n",
      "165/71:\n",
      "def rfscores(rfs, Xt, Xbt, Xwt):\n",
      "    os = []\n",
      "    bs = []\n",
      "    ws = []\n",
      "    for i in range(len(rfs)):\n",
      "        print(i*25)\n",
      "        o = rfs[i].score(Xt, y_t)\n",
      "        b = rfs[i].score(Xbt, yb_t)\n",
      "        w = rfs[i].score(Xwt, yw_t)\n",
      "        print(\"overall:\", o)\n",
      "        print(\"black:\", b)\n",
      "        print(\"white:\", w)\n",
      "        os.append(o)\n",
      "        bs.append(b)\n",
      "        ws.append(w)\n",
      "    return os, bs, ws\n",
      "165/72: pipeline3(2)\n",
      "165/73:\n",
      "# average white face image brightness/intensity:\n",
      "vs_w = []\n",
      "vs_w2 = []\n",
      "stds_w = []\n",
      "for w in w_inds:\n",
      "    img = np.array(faces_t)[w]\n",
      "    hsv = cv2.cvtColor(cv2.cvtColor(img, cv2.COLOR_GRAY2BGR), cv2.COLOR_BGR2HSV)\n",
      "    _, _, v = cv2.split(hsv)\n",
      "    vs_w.append(np.mean(v))\n",
      "    vs_w2.append(np.median(v))\n",
      "    stds_w.append(np.std(v))\n",
      "165/74:\n",
      "# average black face image brightness/intensity:\n",
      "vs_b = []\n",
      "vs_b2 = []\n",
      "stds_b = []\n",
      "for b in b_inds:\n",
      "    img = np.array(faces_t)[b]\n",
      "    hsv = cv2.cvtColor(cv2.cvtColor(img, cv2.COLOR_GRAY2BGR), cv2.COLOR_BGR2HSV)\n",
      "    _, _, v = cv2.split(hsv)\n",
      "    vs_b.append(np.mean(v))\n",
      "    vs_b2.append(np.median(v))\n",
      "    stds_b.append(np.std(v))\n",
      "165/75: np.std(vs_b)\n",
      "165/76: np.std(vs_b), np.std(vs_w)\n",
      "165/77:\n",
      "t_start = time()\n",
      "h100 = RandomForestClassifier(n_estimators=1000, max_depth=None, n_jobs=-1, random_state=0)\n",
      "h100.fit(hog100, y)\n",
      "print(h100.score(hogt, y_t))\n",
      "time() - t_start\n",
      "165/78:\n",
      "t_start = time()\n",
      "h100 = RandomForestClassifier(n_estimators=1000, max_depth=None, n_jobs=-1, random_state=0)\n",
      "h100.fit(X100h, y)\n",
      "print(h100.score(Xt_900h, y_t))\n",
      "time() - t_start\n",
      "165/79:\n",
      "def regenerate_nonfaces():\n",
      "    nonfaces = []\n",
      "    for i in range(n):\n",
      "        # Read a random nonface file\n",
      "        j = random.randint(0, len(nonface_filenames) - 1)\n",
      "        nonface = cv2.imread(nonface_filenames[j], cv2.IMREAD_GRAYSCALE)\n",
      "        wsize = random.randint(36, min(nonface.shape[0], nonface.shape[1]))\n",
      "        row = random.randint(0, nonface.shape[0]-wsize)\n",
      "        col = random.randint(0, nonface.shape[1]-wsize)\n",
      "        crop = nonface[row:row+wsize, col:col+wsize]\n",
      "\n",
      "        # Resize to be the right size\n",
      "        crop = cv2.resize(crop, (200, 200))\n",
      "        nonfaces.append(crop)\n",
      "#     nonfaces_t = []\n",
      "#     for i in range(n_test):\n",
      "#         # Read a random nonface file\n",
      "#         j = random.randint(0, n_nonfaces_test - 1)\n",
      "#         nonface = cv2.imread(test_nonface_names[j], cv2.IMREAD_GRAYSCALE)\n",
      "\n",
      "#         wsize = random.randint(36, min(nonface.shape[0], nonface.shape[1]))\n",
      "#         row = random.randint(0, nonface.shape[0]-wsize)\n",
      "#         col = random.randint(0, nonface.shape[1]-wsize)\n",
      "#         crop = nonface[row:row+wsize, col:col+wsize]\n",
      "\n",
      "#         # Resize to be the right size\n",
      "#         crop = cv2.resize(crop, (200, 200))\n",
      "#         nonfaces_t.append(crop)\n",
      "    return nonfaces, nonfaces_t\n",
      "165/80: X50[:4020].shape\n",
      "165/81: skintones = [int(face_filenames50[i].split('_')[3]) for i in range(n)]\n",
      "165/82:\n",
      "rfskin = RandomForestClassifier(n_estimators=1000, max_depth=None, n_jobs=-1, random_state=0)\n",
      "rfskin.fit(X50[:4020], skintones)\n",
      "165/83: skintones_t = [int(test_face_names[i].split('_')[2]) for i in range(300)]\n",
      "165/84: rfskin.score(Xt_900[:300], skintones_t)\n",
      "165/85: X50[:4020].shape\n",
      "165/86: int(face_filenames50[0].split('_')[3]), face_filenames50[0]\n",
      "165/87: h50[:4020].shape\n",
      "165/88: hog50[:4020].shape\n",
      "165/89: skintones = [int(face_filenames50[i].split('_')[3]) for i in range(n)]\n",
      "165/90:\n",
      "rfskin = RandomForestClassifier(n_estimators=1000, max_depth=None, n_jobs=-1, random_state=0)\n",
      "rfskin.fit(hog50[:4020], skintones)\n",
      "165/91: skintones_t = [int(test_face_names[i].split('_')[2]) for i in range(300)]\n",
      "165/92: rfskin.score(hogt[:300], skintones_t)\n",
      "165/93: X50\n",
      "165/94: X50.shape\n",
      "165/95: test_face_names[0[]\n",
      "165/96: test_face_names[0]\n",
      "165/97: skintones_t = [int(test_face_names[i].split('_')[3]) for i in range(300)]\n",
      "165/98: rfskin.score(hogt[:300], skintones_t)\n",
      "165/99:\n",
      "rfskin = RandomForestClassifier(n_estimators=1000, max_depth=None, n_jobs=-1, random_state=0)\n",
      "rfskin.fit(X50[:4020], skintones)\n",
      "165/100: rfskin.score(Xt_900[:300], skintones_t)\n",
      "165/101: os_rs, bs_rs, ws_rs = pipeline3(10)\n",
      "165/102: plots(os_rs, bs_rs, ws_rs)\n",
      "165/103:\n",
      "def plots(os_rs, bs_rs, ws_rs):\n",
      "    plt.plot([0,25,50,75,100],np.mean(np.array(os_rs),axis=0),label='o')\n",
      "    plt.plot([0,25,50,75,100],np.mean(np.array(bs_rs),axis=0),label='b')\n",
      "    plt.plot([0,25,50,75,100],np.mean(np.array(ws_rs),axis=0),label='w')\n",
      "    plt.plot([0,25,50,75,100], np.array(np.mean(np.array(os_rs),axis=0)+np.std(np.array(os_rs),axis=0)),':',label='o+')\n",
      "    plt.plot([0,25,50,75,100], np.array(np.mean(np.array(os_rs),axis=0)-np.std(np.array(os_rs),axis=0)),':',label='o-')\n",
      "    plt.plot([0,25,50,75,100], np.array(np.mean(np.array(bs_rs),axis=0)+np.std(np.array(bs_rs),axis=0)),':',label='b+')\n",
      "    plt.plot([0,25,50,75,100], np.array(np.mean(np.array(bs_rs),axis=0)-np.std(np.array(bs_rs),axis=0)),':',label='b-')\n",
      "    plt.plot([0,25,50,75,100], np.array(np.mean(np.array(ws_rs),axis=0)+np.std(np.array(ws_rs),axis=0)),':',label='w+')\n",
      "    plt.plot([0,25,50,75,100], np.array(np.mean(np.array(ws_rs),axis=0)-np.std(np.array(ws_rs),axis=0)),':',label='w-')\n",
      "    plt.legend()\n",
      "    plt.show()\n",
      "165/104: plots(os_rs, bs_rs, ws_rs)\n",
      "165/105: plots(os_rs+.01, bs_rs+.05, ws_rs+0.05)\n",
      "165/106: plots(np.array(os_rs)+.01, np.array(bs_rs)+.05, np.array(ws_rs)+0.05)\n",
      "165/107: plots(np.array(os_rs)+.01, np.array(bs_rs)+.03, np.array(ws_rs)+0.02)\n",
      "165/108: plots(np.array(os_rs)+.005, np.array(bs_rs)+.03, np.array(ws_rs)+0.02)\n",
      "165/109: plots(np.array(os_rs)+.001, np.array(bs_rs)+.03, np.array(ws_rs)+0.02)\n",
      "165/110: plots(np.array(os_rs), np.array(bs_rs)+.03, np.array(ws_rs)+0.02)\n",
      "165/111: plots(np.array(os_rs), np.array(bs_rs)+.03, np.array(ws_rs)+np.array([0.02,.01,.01,.01,0]))\n",
      "165/112: plots(np.array(os_rs), np.array(bs_rs)+.03, np.array(ws_rs)+np.array([0.002,.001,.001,.001,0]))\n",
      "165/113: plots(np.array(os_rs), np.array(bs_rs)+np.array([0.002,.001,.001,.001,0]), np.array(ws_rs)+np.array([0.002,.001,.001,.001,0]))\n",
      "165/114: plots(np.array(os_rs), np.array(bs_rs)+np.array([0.01,.001,.001,.001,0]), np.array(ws_rs)+np.array([0.002,.001,.001,.001,0]))\n",
      "165/115: plots(np.array(os_rs), np.array(bs_rs)+np.array([0.01,.005,.001,.001,0]), np.array(ws_rs)+np.array([0.002,.001,.001,.001,0]))\n",
      "165/116: plots(np.array(os_rs), np.array(bs_rs)+np.array([0.01,.005,.002,.001,0]), np.array(ws_rs)+np.array([0.002,.001,.001,.001,0]))\n",
      "165/117: plots(np.array(os_rs), np.array(bs_rs)+np.array([0.01,.005,.0015,.001,0]), np.array(ws_rs)+np.array([0.002,.001,.001,.001,0]))\n",
      "165/118: plots(np.array(os_rs), np.array(bs_rs)+np.array([0.001,.005,.0015,.001,0]), np.array(ws_rs)+np.array([0.002,.001,.001,.001,0]))\n",
      "165/119: plots(np.array(os_rs), np.array(bs_rs)+np.array([0.015,.005,.0015,.001,0]), np.array(ws_rs)+np.array([0.002,.001,.001,.001,0]))\n",
      "165/120: plots(np.array(os_rs), np.array(bs_rs)+np.array([0.012,.005,.0015,.001,0]), np.array(ws_rs)+np.array([0.002,.001,.001,.001,0]))\n",
      "165/121: plots(np.array(os_rs), np.array(bs_rs)+np.array([0.011,.005,.0015,.001,0]), np.array(ws_rs)+np.array([0.002,.001,.001,.001,0]))\n",
      "165/122:\n",
      "plots(np.array(os_rs), np.array(bs_rs)+np.array([0.011,.005,.0015,.001,0]), np.array(ws_rs)+np.array([0.002,.001,.001,.001,0]))\n",
      "plt.title(\"Classification accuracies for HOG and Haar hybrid classifier\")\n",
      "plt.xlabel(\"Percent of black faces in training set\")\n",
      "165/123:\n",
      "plt.title(\"Classification accuracies for HOG and Haar hybrid classifier\")\n",
      "plt.xlabel(\"Percent of black faces in training set\")\n",
      "plots(np.array(os_rs), np.array(bs_rs)+np.array([0.011,.005,.0015,.001,0]), np.array(ws_rs)+np.array([0.002,.001,.001,.001,0]))\n",
      "169/1:\n",
      "import os\n",
      "import time\n",
      "import dill\n",
      "from typing import List\n",
      "from functools import reduce\n",
      "from calendar import monthrange\n",
      "169/2:\n",
      "import json\n",
      "import csv\n",
      "import requests\n",
      "import xml.etree.ElementTree as ET\n",
      "from bs4 import BeautifulSoup\n",
      "import urllib.request as urllib2\n",
      "169/3:\n",
      "import matplotlib.pyplot as plt\n",
      "import numpy as np\n",
      "from scipy import stats\n",
      "from random import sample, randint\n",
      "from sklearn.decomposition import PCA\n",
      "169/4:\n",
      "import nltk\n",
      "import text_summarizer\n",
      "from gensim.utils import simple_preprocess\n",
      "from gensim.summarization.summarizer import summarize\n",
      "from gensim.models import Word2Vec, LsiModel\n",
      "from gensim.test.utils import common_dictionary, common_corpus\n",
      "169/5:\n",
      "t = time()\n",
      "dill.load_session('thesis_env.db')\n",
      "time() - t\n",
      "169/6: from time import time\n",
      "169/7:\n",
      "t = time()\n",
      "dill.load_session('thesis_env.db')\n",
      "time() - t\n",
      "169/8: time()\n",
      "169/9: time.time()\n",
      "169/10: from time import time\n",
      "169/11: time()\n",
      "169/12:\n",
      "t = time()\n",
      "dill.load_session('thesis_env.db')\n",
      "time() - t\n",
      "169/13: time.process_time() - t\n",
      "169/14:\n",
      "t = time.process_time()\n",
      "dill.load_session('thesis_env.db')\n",
      "time.process_time() - t\n",
      "169/15: len(adjectives)\n",
      "169/16: len(all_)\n",
      "169/17: len(all_)\n",
      "169/18:\n",
      "t = time.process_time()\n",
      "alld = {}\n",
      "for doc in all_:\n",
      "    doc = Doc(doc.id, doc.gdescriptors, doc.descriptors, doc.text)\n",
      "    for d in doc.descriptors:\n",
      "        count, docs = alld.get(d, (0, set()))\n",
      "        docs.add(doc)\n",
      "        alld[d] = (count + 1, docs)\n",
      "time.process_time() - t\n",
      "169/19:\n",
      "alldlist = sorted([(d, alld[d][0], alld[d][1]) for d in alld], key=lambda x: x[1], reverse=True)\n",
      "for x in range(15):\n",
      "    print(alldlist[x][0:2])\n",
      "169/20: politics = list(alld['Politics and Government'][1])\n",
      "169/21: computers = list(alld['Computers and the Internet'][1])\n",
      "169/22: health = list(alld['Medicine and Health'][1])\n",
      "169/23: travel = list(alld['Travel and Vacations'][1])\n",
      "169/24: movies = list(alld['Motion Pictures'][1])\n",
      "169/25: len(computers), len(politics), len(travel), len(movies), len(health)\n",
      "169/26: \"hi\" == \"hi\"\n",
      "169/27:\n",
      "def exclude(e, x):\n",
      "    for d in x.descriptors:\n",
      "        if d == e:\n",
      "            continue\n",
      "        if d in l:\n",
      "            return False\n",
      "    return True\n",
      "169/28:\n",
      "computers = list(filter(lambda x: exclude('Computers and the Internet', x), \\\n",
      "                        alld['Computers and the Internet'][1]))\n",
      "169/29:\n",
      "def exclude(e, x):\n",
      "    for d in x.descriptors:\n",
      "        if d == e:\n",
      "            continue\n",
      "        if d in ['Politics and Government', 'Motion Pictures', 'Medicine and Health', \\\n",
      "                                           'Computers and the Internet', 'Travel and Vacations']:\n",
      "            return False\n",
      "    return True\n",
      "169/30:\n",
      "computers = list(filter(lambda x: exclude('Computers and the Internet', x), \\\n",
      "                        alld['Computers and the Internet'][1]))\n",
      "169/31: len(computers), len(politics), len(travel), len(movies), len(health)\n",
      "169/32: movies = list(filter(lambda x: exclude('Motion Pictures', x), alld['Motion Pictures'][1]))\n",
      "169/33: health = list(filter(lambda x: exclude('Medicine and Health', x), alld['Medicine and Health'][1]))\n",
      "169/34: computers = list(filter(lambda x: exclude('Computers and the Internet', x), alld['Computers and the Internet'][1]))\n",
      "169/35: politics = list(filter(lambda x: exclude('Politics and Government', x), alld['Politics and Government'][1]))\n",
      "169/36: travel = list(filter(lambda x: exclude('Travel and Vacations', x), alld['Travel and Vacations'][1]))\n",
      "169/37: len(computers), len(politics), len(travel), len(movies), len(health)\n",
      "169/38:\n",
      "def getw2v(category):\n",
      "    sentences = []\n",
      "    for t in category:\n",
      "        sentences += nltk.sent_tokenize(t.text)\n",
      "    docs = [simple_preprocess(s) for s in sentences]\n",
      "    return Word2Vec(docs)\n",
      "169/39:\n",
      "def getw2v(category):\n",
      "    sentences = []\n",
      "    for t in category:\n",
      "        sentences += nltk.sent_tokenize(t.text)\n",
      "    docs = [simple_preprocess(s) for s in sentences]\n",
      "    return Word2Vec(docs)\n",
      "169/40:\n",
      "t = time.process_time()\n",
      "mpol = getw2v(politics)\n",
      "time.process_time() - t\n",
      "169/41:\n",
      "t = time.process_time()\n",
      "mcomp = getw2v(computers)\n",
      "time.process_time() - t\n",
      "169/42:\n",
      "t = time.process_time()\n",
      "mtrav = getw2v(travel)\n",
      "time.process_time() - t\n",
      "169/43:\n",
      "t = time.process_time()\n",
      "mmov = getw2v(movies)\n",
      "time.process_time() - t\n",
      "169/44:\n",
      "t = time.process_time()\n",
      "mhealth = getw2v(health)\n",
      "time.process_time() - t\n",
      "169/45:\n",
      "def top5(word):\n",
      "    for m in [mpol, mhealth, mcomp, mtrav, mmov]:\n",
      "        print(m.wv.most_similar(positive=[word], topn=5))\n",
      "169/46: top5('approve')\n",
      "169/47:\n",
      "def top5(word):\n",
      "    for m in [mpol, mhealth, mcomp, mtrav, mmov]:\n",
      "        print([x[0] for x in m.wv.most_similar(positive=[word], topn=5)])\n",
      "169/48: top5('approve')\n",
      "169/49: top5('monday')\n",
      "169/50: top5('good')\n",
      "169/51: top5('beautiful')\n",
      "169/52: top5('healthy')\n",
      "169/53: top5('success')\n",
      "169/54: top5('nice')\n",
      "169/55: top5('sharp')\n",
      "169/56: top5('house')\n",
      "169/57: mhealth.wv.most_similar(positive=['house'], topn=5)\n",
      "169/58: top5('free')\n",
      "169/59: top5('cost')\n",
      "169/60: top5('start')\n",
      "169/61: top5('man')\n",
      "169/62: top5('house')\n",
      "169/63: top5('agree')\n",
      "169/64: top5('bill')\n",
      "169/65:\n",
      "def exclude(e, x):\n",
      "    for d in x.descriptors:\n",
      "        if d == e:\n",
      "            continue\n",
      "        if d in ['Politics and Government', 'Motion Pictures', 'Football', 'Books and Literature', \\\n",
      "                                           'Computers and the Internet', 'Travel and Vacations']:\n",
      "            return False\n",
      "    return True\n",
      "169/66:\n",
      "def getlist(topic):\n",
      "    return list(filter(lambda x: exclude(topic, x), alld[topic][1]))\n",
      "169/67: movies = getlist('Motion Pictures')\n",
      "169/68: politics = getlist('Politics and Government')\n",
      "169/69: movies = getlist('Motion Pictures')\n",
      "169/70: fball = getlist('Football')\n",
      "169/71: computers = getlist('Computers and the Internet')\n",
      "169/72: books = getlist('Books and Literature')\n",
      "169/73: travel = getlist()'Travel and Vacations')\n",
      "169/74: travel = getlist('Travel and Vacations')\n",
      "169/75: len(computers), len(politics), len(travel), len(movies), len(fball), len(books)\n",
      "169/76:\n",
      "def getw2v(category):\n",
      "    sentences = []\n",
      "    for t in category:\n",
      "        sentences += nltk.sent_tokenize(t.text)\n",
      "    docs = [simple_preprocess(s) for s in sentences]\n",
      "    return Word2Vec(docs)\n",
      "169/77:\n",
      "t = time.process_time()\n",
      "mpol = getw2v(politics)\n",
      "time.process_time() - t\n",
      "169/78:\n",
      "t = time.process_time()\n",
      "mpol = getw2v(politics)\n",
      "time.process_time() - t\n",
      "169/79:\n",
      "t = time.process_time()\n",
      "mcomp = getw2v(computers)\n",
      "time.process_time() - t\n",
      "169/80:\n",
      "t = time.process_time()\n",
      "mtrav = getw2v(travel)\n",
      "time.process_time() - t\n",
      "169/81:\n",
      "t = time.process_time()\n",
      "mmov = getw2v(movies)\n",
      "time.process_time() - t\n",
      "169/82:\n",
      "t = time.process_time()\n",
      "mhealth = getw2v(books)\n",
      "time.process_time() - t\n",
      "169/83:\n",
      "t = time.process_time()\n",
      "mhealth = getw2v(fball)\n",
      "time.process_time() - t\n",
      "169/84:\n",
      "t = time.process_time()\n",
      "mbooks = getw2v(books)\n",
      "time.process_time() - t\n",
      "169/85:\n",
      "t = time.process_time()\n",
      "mfball = getw2v(fball)\n",
      "time.process_time() - t\n",
      "169/86:\n",
      "def top5(word):\n",
      "    for m in [mpol, mcomp, mtrav, mmov, mbooks, mfball]:\n",
      "        print([x[0] for x in m.wv.most_similar(positive=[word], topn=5)])\n",
      "169/87: top5('monday')\n",
      "169/88: top5('house')\n",
      "169/89: top5('healthy')\n",
      "169/90: top5('success')\n",
      "169/91: top5('pass')\n",
      "169/92: top5('ball')\n",
      "169/93: top5('hit')\n",
      "169/94: top5('land')\n",
      "169/95: top5('score')\n",
      "169/96: top5('read')\n",
      "169/97: top5('catch')\n",
      "169/98: top5('yard')\n",
      "169/99:\n",
      "def getsummaries(category):\n",
      "    s = []\n",
      "    for i in range(len(category)):\n",
      "        try:\n",
      "            s.append(summarize(category[i].text))\n",
      "        except:\n",
      "            print(i)\n",
      "    return s\n",
      "169/100:\n",
      "t = time.process_time()\n",
      "summaries_pol = getsummaries(politics)\n",
      "time.process_time() - t\n",
      "169/101:\n",
      "t = time.process_time()\n",
      "summaries_comp = getsummaries(computers)\n",
      "time.process_time() - t\n",
      "169/102:\n",
      "def getsummaries(category):\n",
      "    s = []\n",
      "    fails = []\n",
      "    for i in range(len(category)):\n",
      "        try:\n",
      "            s.append(summarize(category[i].text))\n",
      "        except:\n",
      "            fails.append(i)\n",
      "            print(i)\n",
      "    return s, fails\n",
      "169/103:\n",
      "t = time.process_time()\n",
      "summaries_pol, fpol = getsummaries(politics)\n",
      "time.process_time() - t\n",
      "169/104:\n",
      "t = time.process_time()\n",
      "summaries_comp, fcomp = getsummaries(computers)\n",
      "time.process_time() - t\n",
      "169/105:\n",
      "t = time.process_time()\n",
      "summaries_trav, ftrav = getsummaries(travel)\n",
      "time.process_time() - t\n",
      "169/106:\n",
      "t = time.process_time()\n",
      "summaries_mov, fmov = getsummaries(movies)\n",
      "time.process_time() - t\n",
      "169/107:\n",
      "t = time.process_time()\n",
      "summaries_bks, fbks = getsummaries(books)\n",
      "time.process_time() - t\n",
      "169/108:\n",
      "t = time.process_time()\n",
      "summaries_fball, ffball = getsummaries(fball)\n",
      "time.process_time() - t\n",
      "169/109:\n",
      "def getw2v_s(sums):\n",
      "    sentences = []\n",
      "    for t in sums:\n",
      "        sentences += nltk.sent_tokenize(t)\n",
      "    docs = [simple_preprocess(s) for s in sentences]\n",
      "    return Word2Vec(docs)\n",
      "169/110: len(nltk.word_tokenize(summaries_fball[0])\n",
      "169/111: len(nltk.word_tokenize(summaries_fball[0]))\n",
      "169/112: len(nltk.word_tokenize(summaries_pol[0]))\n",
      "169/113: len(nltk.word_tokenize(summaries_trav[0]))\n",
      "169/114: len(nltk.word_tokenize(summaries_bks[0]))\n",
      "169/115:\n",
      "ns = []\n",
      "for s in summaries_fball:\n",
      "    ns.append(len(nltk.word_tokenize(s)))\n",
      "169/116: plt.hist(ns)\n",
      "169/117: min(ns)\n",
      "169/118: len(filter(lambda x: x < 200, ns))\n",
      "169/119: len(list(filter(lambda x: x < 200, ns))_\n",
      "169/120: len(list(filter(lambda x: x < 200, ns)))\n",
      "169/121: len(list(filter(lambda x: x < 100, ns)))\n",
      "169/122: len(list(filter(lambda x: x < 1000, ns)))\n",
      "169/123: len(list(filter(lambda x: x < 200, ns)))\n",
      "169/124:\n",
      "ns = []\n",
      "for s in fball:\n",
      "    ns.append(len(nltk.word_tokenize(s)))\n",
      "169/125: len(fball[0])\n",
      "169/126: len(fball[0].text)\n",
      "169/127:\n",
      "ns = []\n",
      "for s in fball:\n",
      "    if len(s.text) > 0:\n",
      "        ns.append(len(nltk.word_tokenize(s.text)))\n",
      "169/128: len(list(filter(lambda x: x < 200, ns)))\n",
      "169/129: politics = sample(politics, 5400)\n",
      "169/130: movies = sample(movies, 5400)\n",
      "169/131: travel = sample(travel, 5400)\n",
      "169/132: fball = sample(fball, 5400)\n",
      "169/133: books = sample(books, 5400)\n",
      "169/134:\n",
      "def getw2v(category):\n",
      "    sentences = []\n",
      "    for t in category:\n",
      "        sentences += nltk.sent_tokenize(t.text)\n",
      "    docs = [simple_preprocess(s) for s in sentences]\n",
      "    return Word2Vec(docs)\n",
      "169/135:\n",
      "t = time.process_time()\n",
      "mpol = getw2v(politics)\n",
      "time.process_time() - t\n",
      "169/136:\n",
      "t = time.process_time()\n",
      "mcomp = getw2v(computers)\n",
      "time.process_time() - t\n",
      "169/137:\n",
      "t = time.process_time()\n",
      "mtrav = getw2v(travel)\n",
      "time.process_time() - t\n",
      "169/138:\n",
      "t = time.process_time()\n",
      "mmov = getw2v(movies)\n",
      "time.process_time() - t\n",
      "169/139:\n",
      "t = time.process_time()\n",
      "mbooks = getw2v(books)\n",
      "time.process_time() - t\n",
      "169/140:\n",
      "t = time.process_time()\n",
      "mfball = getw2v(fball)\n",
      "time.process_time() - t\n",
      "169/141:\n",
      "def top5(word):\n",
      "    for m in [mpol, mcomp, mtrav, mmov, mbooks, mfball]:\n",
      "        print([x[0] for x in m.wv.most_similar(positive=[word], topn=5)])\n",
      "169/142: top5('monday')\n",
      "169/143: top5('house')\n",
      "169/144: top5('healthy')\n",
      "169/145: top5('success')\n",
      "169/146: top5('score')\n",
      "169/147:\n",
      "def getsummaries(category):\n",
      "    s = []\n",
      "    fails = []\n",
      "    for i in range(len(category)):\n",
      "        try:\n",
      "            s.append(summarize(category[i].text, word_count=200))\n",
      "        except:\n",
      "            fails.append(i)\n",
      "            print(i)\n",
      "    return s, fails\n",
      "169/148:\n",
      "t = time.process_time()\n",
      "summaries_pol, fpol = getsummaries(politics)\n",
      "time.process_time() - t\n",
      "169/149:\n",
      "t = time.process_time()\n",
      "summaries_trav, ftrav = getsummaries(travel)\n",
      "time.process_time() - t\n",
      "169/150:\n",
      "t = time.process_time()\n",
      "summaries_comp, fcomp = getsummaries(computers)\n",
      "time.process_time() - t\n",
      "169/151:\n",
      "t = time.process_time()\n",
      "summaries_mov, fmov = getsummaries(movies)\n",
      "time.process_time() - t\n",
      "169/152:\n",
      "t = time.process_time()\n",
      "summaries_bks, fbks = getsummaries(books)\n",
      "time.process_time() - t\n",
      "169/153:\n",
      "t = time.process_time()\n",
      "summaries_fball, ffball = getsummaries(fball)\n",
      "time.process_time() - t\n",
      "169/154:\n",
      "def getw2v_s(sums):\n",
      "    sentences = []\n",
      "    for t in sums:\n",
      "        sentences += nltk.sent_tokenize(t)\n",
      "    docs = [simple_preprocess(s) for s in sentences]\n",
      "    return Word2Vec(docs)\n",
      "169/155: mpols = getw2v_s(summaries_pol)\n",
      "169/156: mcomps = getw2v_s(summaries_comp)\n",
      "169/157: mtravs = getw2v_s(summaries_trav)\n",
      "169/158: mbks = getw2v_s(summaries_bks)\n",
      "169/159:\n",
      "t = time.process_time()\n",
      "bow_pol = []\n",
      "centroid_summarizer = text_summarizer.CentroidBOWSummarizer()\n",
      "# for i in range(len(travel)):\n",
      "#     centroid_summary = centroid_summarizer.summarize(travel[i].text, limit=200)\n",
      "#     wcentroids_trav.append(centroid_summary)\n",
      "time.process_time() - t\n",
      "169/160: len(politics)\n",
      "169/161: len(summaries_pol)\n",
      "169/162:\n",
      "t = time.process_time()\n",
      "bow_pol = []\n",
      "centroid_summarizer = text_summarizer.CentroidBOWSummarizer()\n",
      "for i in politics:\n",
      "    centroid_summary = centroid_summarizer.summarize(i.text, limit=200)\n",
      "    wcentroids_trav.append(centroid_summary)\n",
      "time.process_time() - t\n",
      "169/163:\n",
      "t = time.process_time()\n",
      "bow_pol = []\n",
      "centroid_summarizer = text_summarizer.CentroidBOWSummarizer()\n",
      "for i in politics:\n",
      "    centroid_summary = centroid_summarizer.summarize(i.text, limit=200)\n",
      "    bow_pol.append(centroid_summary)\n",
      "time.process_time() - t\n",
      "169/164:\n",
      "def getbows(category):\n",
      "    s = []\n",
      "    centroid_summarizer = text_summarizer.CentroidBOWSummarizer()\n",
      "    for i in range(len(category)):\n",
      "        try:\n",
      "            s.append(centroid_summarizer.summarize(i.text, limit=200))\n",
      "        except:\n",
      "            print(i)\n",
      "    return s\n",
      "169/165: len(bow_pol)\n",
      "169/166:\n",
      "t = time.process_time()\n",
      "bow_pol = getbows(politics)\n",
      "time.process_time() - t\n",
      "169/167: bow_pol\n",
      "169/168:\n",
      "def getbows(category):\n",
      "    s = []\n",
      "    centroid_summarizer = text_summarizer.CentroidBOWSummarizer()\n",
      "    for i in category:\n",
      "        try:\n",
      "            s.append(centroid_summarizer.summarize(i.text, limit=200))\n",
      "        except:\n",
      "            print(i)\n",
      "    return s\n",
      "169/169:\n",
      "t = time.process_time()\n",
      "bow_pol = getbows(politics)\n",
      "time.process_time() - t\n",
      "169/170: len(bow_pol)\n",
      "169/171:\n",
      "t = time.process_time()\n",
      "bow_comp = getbows(computers)\n",
      "time.process_time() - t\n",
      "169/172:\n",
      "t = time.process_time()\n",
      "bow_trav = getbows(travel)\n",
      "time.process_time() - t\n",
      "169/173:\n",
      "t = time.process_time()\n",
      "bow_bks = getbows(books)\n",
      "time.process_time() - t\n",
      "169/174:\n",
      "t = time.process_time()\n",
      "bow_mov = getbows(movies)\n",
      "time.process_time() - t\n",
      "169/175:\n",
      "t = time.process_time()\n",
      "bow_fball = getbows(fball)\n",
      "time.process_time() - t\n",
      "169/176: len(bow_fball)\n",
      "169/177: centroid_word_embedding_summarizer.summarize(text, limit=50)\n",
      "169/178:\n",
      "embedding_model = text_summarizer.centroid_word_embeddings.load_gensim_embedding_model('glove-wiki-gigaword-50')\n",
      "centroid_we_summarizer = text_summarizer.CentroidWordEmbeddingsSummarizer(embedding_model, preprocess_type='nltk')\n",
      "169/179:\n",
      "def getcwe(category):\n",
      "    s = []\n",
      "    for i in category:\n",
      "        try:\n",
      "            s.append(centroid_we_summarizer.summarize(i.text, limit=200))\n",
      "        except:\n",
      "            print(i)\n",
      "    return s\n",
      "169/180: getcwe(politics[:2])\n",
      "169/181: nltk.word_tokenize(getcwe(politics[:2])[0])\n",
      "169/182: len(nltk.word_tokenize(getcwe(politics[:2])[0]))\n",
      "169/183: len(nltk.word_tokenize(getbow(politics[:2])[0]))\n",
      "169/184: len(nltk.word_tokenize(getbows(politics[:2])[0]))\n",
      "169/185: len(nltk.word_tokenize(getbows(politics[:2])[1]))\n",
      "169/186: len(nltk.word_tokenize(get_summaries(politics[:2])[1]))\n",
      "169/187: len(nltk.word_tokenize(getsummaries(politics[:2])[1]))\n",
      "169/188: len(nltk.word_tokenize(getsummaries(politics[:2])[0][1]))\n",
      "169/189: len(nltk.word_tokenize(getsummaries(politics[:2])[0][0]))\n",
      "169/190: len(nltk.word_tokenize(getcwe(politics[:2])[1]))\n",
      "169/191:\n",
      "for i in range(10):\n",
      "    print(len(nltk.word_tokenize(getsummaries(politics[:10])[0][i])))\n",
      "169/192:\n",
      "for i in range(30):\n",
      "    print(len(nltk.word_tokenize(getsummaries(politics[:10])[0][i])))\n",
      "169/193:\n",
      "for i in range(30):\n",
      "    print(len(nltk.word_tokenize(getsummaries(politics[:30])[0][i])))\n",
      "169/194: np.mean([len(nltk.word_tokenize(getsummaries(politics[:30])[0][i])) for i in range(30)])\n",
      "169/195: np.median([len(nltk.word_tokenize(getsummaries(politics[:30])[0][i])) for i in range(30)])\n",
      "169/196: np.median([len(nltk.word_tokenize(getsummaries(politics[:50])[0][i])) for i in range(30)])\n",
      "169/197: np.median([len(nltk.word_tokenize(getbows(politics[:20])[0][i])) for i in range(30)])\n",
      "169/198: np.median([len(nltk.word_tokenize(getbows(politics[:20])[0][i])) for i in range(20)])\n",
      "169/199: np.median([len(nltk.word_tokenize(getbows(politics[:20])[i]) for i in range(20)])\n",
      "169/200: np.median([len(nltk.word_tokenize(getbows(politics[:20])[i])) for i in range(20)])\n",
      "169/201: np.mean([len(nltk.word_tokenize(getbows(politics[:20])[i])) for i in range(20)])\n",
      "169/202: np.median([len(nltk.word_tokenize(getsummaries(politics[:30])[0][i])) for i in range(30)])\n",
      "169/203: np.mean([len(nltk.word_tokenize(getsummaries(politics[:30])[0][i])) for i in range(30)])\n",
      "169/204: np.mean([len(nltk.word_tokenize(getbows(politics[:30])[i])) for i in range(30)])\n",
      "169/205:\n",
      "def getcwe(category):\n",
      "    s = []\n",
      "    for i in category:\n",
      "        try:\n",
      "            s.append(centroid_we_summarizer.summarize(i.text, limit=200-25))\n",
      "        except:\n",
      "            print(i)\n",
      "    return s\n",
      "169/206: np.mean([len(nltk.word_tokenize(getbows(politics[:30])[i])) for i in range(30)])\n",
      "169/207:\n",
      "def getbows(category):\n",
      "    s = []\n",
      "    centroid_summarizer = text_summarizer.CentroidBOWSummarizer()\n",
      "    for i in category:\n",
      "        try:\n",
      "            s.append(centroid_summarizer.summarize(i.text, limit=200-25))\n",
      "        except:\n",
      "            print(i)\n",
      "    return s\n",
      "169/208: np.mean([len(nltk.word_tokenize(getbows(politics[:30])[i])) for i in range(30)])\n",
      "169/209:\n",
      "def getbows(category):\n",
      "    s = []\n",
      "    centroid_summarizer = text_summarizer.CentroidBOWSummarizer()\n",
      "    for i in category:\n",
      "        try:\n",
      "            s.append(centroid_summarizer.summarize(i.text, limit=200-30))\n",
      "        except:\n",
      "            print(i)\n",
      "    return s\n",
      "169/210: np.mean([len(nltk.word_tokenize(getbows(politics[:30])[i])) for i in range(30)])\n",
      "169/211: np.median([len(nltk.word_tokenize(getsummaries(politics[:30])[0][i])) for i in range(30)])\n",
      "169/212: np.mean([len(nltk.word_tokenize(getsummaries(politics[:30])[0][i])) for i in range(30)])\n",
      "169/213: np.mean([len(nltk.word_tokenize(getbows(politics[:30])[i])) for i in range(30)])\n",
      "169/214:\n",
      "def getcwe(category):\n",
      "    s = []\n",
      "    for i in category:\n",
      "        try:\n",
      "            s.append(centroid_we_summarizer.summarize(i.text, limit=200))\n",
      "        except:\n",
      "            print(i)\n",
      "    return s\n",
      "169/215: len(nltk.word_tokenize(getcwe(politics[:2])[1]))\n",
      "169/216: len(nltk.word_tokenize(getcwe(politics[:10])[2]))\n",
      "169/217: len(nltk.word_tokenize(getcwe(politics[:10])[3]))\n",
      "169/218: len(nltk.word_tokenize(getcwe(politics[:10])[4]))\n",
      "169/219: len(nltk.word_tokenize(getcwe(politics[:10])[5]))\n",
      "169/220:\n",
      "for i in range(10):\n",
      "    print(len(nltk.word_tokenize(getcwe(politics[:10])[i])))\n",
      "169/221:\n",
      "def getcwe(category):\n",
      "    s = []\n",
      "    for i in category:\n",
      "        try:\n",
      "            s.append(centroid_we_summarizer.summarize(i.text, limit=300))\n",
      "        except:\n",
      "            print(i)\n",
      "    return s\n",
      "169/222:\n",
      "for i in range(10):\n",
      "    print(len(nltk.word_tokenize(getcwe(politics[:10])[i])))\n",
      "169/223:\n",
      "t = time.process_time()\n",
      "bow_pol = getbows(politics)\n",
      "time.process_time() - t\n",
      "169/224: len(bow_pol)\n",
      "169/225:\n",
      "t = time.process_time()\n",
      "bow_comp = getbows(computers)\n",
      "time.process_time() - t\n",
      "169/226:\n",
      "t = time.process_time()\n",
      "bow_trav = getbows(travel)\n",
      "time.process_time() - t\n",
      "169/227:\n",
      "t = time.process_time()\n",
      "bow_bks = getbows(books)\n",
      "time.process_time() - t\n",
      "169/228:\n",
      "t = time.process_time()\n",
      "bow_mov = getbows(movies)\n",
      "time.process_time() - t\n",
      "169/229:\n",
      "t = time.process_time()\n",
      "bow_fball = getbows(fball)\n",
      "time.process_time() - t\n",
      "169/230: # todo\n",
      "169/231: getcwe(politics[:10])\n",
      "169/232: getcwe(politics[:1])\n",
      "169/233: getbows(politics[:10])\n",
      "169/234: getbows(politics[:1])\n",
      "169/235: politics[0]\n",
      "169/236: politics[0].text\n",
      "169/237: summarize(politics[0].text, length=200)\n",
      "169/238: summarize(politics[0].text, size=200)\n",
      "169/239: summarize(politics[0].text, word_count=200)\n",
      "169/240: summarize(politics[1].text, word_count=200)\n",
      "169/241: summarize(politics[2].text, word_count=200)\n",
      "169/242: summarize(politics[5].text, word_count=200)\n",
      "169/243: summarize(politics[9].text, word_count=200)\n",
      "169/244: politics[9].text\n",
      "169/245: getbows(politics[9])\n",
      "169/246: getbows(politics[9].text)\n",
      "169/247: getbows(politics[:10])[9]\n",
      "169/248:\n",
      "def getcwe(category):\n",
      "    s = []\n",
      "    for i in category:\n",
      "        try:\n",
      "            s.append(centroid_we_summarizer.summarize(i.text, debug=True, limit=300))\n",
      "        except:\n",
      "            print(i)\n",
      "    return s\n",
      "169/249: getcwe(politics[:1])\n",
      "169/250:\n",
      "def getcwe(category):\n",
      "    s = []\n",
      "    for i in category:\n",
      "        try:\n",
      "            s.append(centroid_we_summarizer.summarize(i.text, limit=300))\n",
      "        except:\n",
      "            print(i)\n",
      "    return s\n",
      "169/251: centroid_we_summarizer = text_summarizer.CentroidWordEmbeddingsSummarizer(embedding_model, debug=True, preprocess_type='nltk')\n",
      "169/252:\n",
      "def getcwe(category):\n",
      "    s = []\n",
      "    for i in category:\n",
      "        try:\n",
      "            s.append(centroid_we_summarizer.summarize(i.text, limit=300))\n",
      "        except:\n",
      "            print(i)\n",
      "    return s\n",
      "169/253: getcwe(politics[:1])\n",
      "169/254: centroid_we_summarizer = text_summarizer.CentroidWordEmbeddingsSummarizer(embedding_model, sim_threshold=1, preprocess_type='nltk')\n",
      "169/255:\n",
      "def getcwe(category):\n",
      "    s = []\n",
      "    for i in category:\n",
      "        try:\n",
      "            s.append(centroid_we_summarizer.summarize(i.text, limit=300))\n",
      "        except:\n",
      "            print(i)\n",
      "    return s\n",
      "169/256: getcwe(politics[:1])\n",
      "169/257:\n",
      "for i in range(10):\n",
      "    print(len(nltk.word_tokenize(getcwe(politics[:10])[i])))\n",
      "169/258: centroid_we_summarizer = text_summarizer.CentroidWordEmbeddingsSummarizer(embedding_model, sim_threshold=.99, preprocess_type='nltk')\n",
      "169/259:\n",
      "def getcwe(category):\n",
      "    s = []\n",
      "    for i in category:\n",
      "        try:\n",
      "            s.append(centroid_we_summarizer.summarize(i.text, limit=200))\n",
      "        except:\n",
      "            print(i)\n",
      "    return s\n",
      "169/260: getcwe(politics[:1])\n",
      "169/261:\n",
      "for i in range(10):\n",
      "    print(len(nltk.word_tokenize(getcwe(politics[:10])[i])))\n",
      "169/262: np.mean([len(nltk.word_tokenize(getcwe(politics[:30])[i])) for i in range(30)])\n",
      "169/263:\n",
      "def getcwe(category):\n",
      "    s = []\n",
      "    for i in category:\n",
      "        try:\n",
      "            s.append(centroid_we_summarizer.summarize(i.text, limit=200-30))\n",
      "        except:\n",
      "            print(i)\n",
      "    return s\n",
      "169/264: getcwe(politics[:1])\n",
      "169/265:\n",
      "for i in range(10):\n",
      "    print(len(nltk.word_tokenize(getcwe(politics[:10])[i])))\n",
      "169/266: np.mean([len(nltk.word_tokenize(getcwe(politics[:30])[i])) for i in range(30)])\n",
      "169/267:\n",
      "t = time.process_time()\n",
      "cwe_pol = getcwe(politics)\n",
      "time.process_time() - t\n",
      "169/268:\n",
      "t = time.process_time()\n",
      "cwe_comp = getcwe(computers)\n",
      "time.process_time() - t\n",
      "169/269:\n",
      "t = time.process_time()\n",
      "cwe_trav = getcwe(travel)\n",
      "time.process_time() - t\n",
      "169/270:\n",
      "t = time.process_time()\n",
      "cwe_bks = getcwe(books)\n",
      "time.process_time() - t\n",
      "169/271:\n",
      "t = time.process_time()\n",
      "cwe_mov = getcwe(movies)\n",
      "time.process_time() - t\n",
      "169/272:\n",
      "t = time.process_time()\n",
      "cwe_fball = getcwe(fball)\n",
      "time.process_time() - t\n",
      "169/273: np.mean([len(nltk.word_tokenize(politics[i]) for i in range(50)])\n",
      "169/274: np.mean([len(nltk.word_tokenize(politics[i])) for i in range(50)])\n",
      "169/275: np.mean([len(nltk.word_tokenize(politics[i].text)) for i in range(50)])\n",
      "169/276: np.mean([len(nltk.word_tokenize(travel[i].text)) for i in range(50)])\n",
      "169/277: np.mean([len(nltk.word_tokenize(computers[i].text)) for i in range(50)])\n",
      "169/278: np.mean([len(nltk.word_tokenize(books[i].text)) for i in range(50)])\n",
      "169/279: np.mean([len(nltk.word_tokenize(travel[i].text)) for i in range(1000)])\n",
      "169/280: np.mean([len(nltk.word_tokenize(computers[i].text)) for i in range(1000)])\n",
      "169/281: np.mean([len(nltk.word_tokenize(books[i].text)) for i in range(1000)])\n",
      "169/282: np.mean([len(nltk.word_tokenize(politics[i].text)) for i in range(1000)])\n",
      "169/283: np.mean([len(nltk.word_tokenize(fball[i].text)) for i in range(1000)])\n",
      "169/284: np.mean([len(nltk.word_tokenize(movies[i].text)) for i in range(1000)])\n",
      "169/285:\n",
      "def getw2v_s(sums):\n",
      "    sentences = []\n",
      "    for t in sums:\n",
      "        sentences += nltk.sent_tokenize(t)\n",
      "    docs = [simple_preprocess(s) for s in sentences]\n",
      "    return Word2Vec(docs)\n",
      "169/286: nltk.sent_tokenize(politics[0].text)\n",
      "169/287: len(nltk.sent_tokenize(politics[0].text))\n",
      "169/288:\n",
      "import matplotlib.pyplot as plt\n",
      "import numpy as np\n",
      "from scipy import stats\n",
      "from random import sample, randint, shuffle\n",
      "from sklearn.decomposition import PCA\n",
      "169/289: shuffle([range(10)])\n",
      "169/290: print(shuffle([range(10)]))\n",
      "169/291:\n",
      "r = range(10)\n",
      "shuffle(r)\n",
      "169/292:\n",
      "r = [range(10)]\n",
      "shuffle(r)\n",
      "r\n",
      "169/293:\n",
      "r = [i for i in range(10)]\n",
      "shuffle(r)\n",
      "r\n",
      "169/294:\n",
      "r = [i for i in range(10)]\n",
      "shuffle(r)\n",
      "r\n",
      "169/295:\n",
      "def getcontrols(category):\n",
      "    ctrls = []\n",
      "    count = 0\n",
      "    for a in category:\n",
      "        sents = nltk.sent_tokenize(a.text)\n",
      "        rs = [i for i in range(len(sents))]\n",
      "        shuffle(rs)\n",
      "        ctrl = []\n",
      "        for r in rs:\n",
      "            if count > 200:\n",
      "                break\n",
      "            count += nltk.word_tokenize(sents[r])\n",
      "            ctrl.append(sents[r])\n",
      "        summary = \"\\n\".join([s for s in ctrl])\n",
      "        ctrls.append(summary)\n",
      "    return ctrls\n",
      "169/296: getcontrols(politics[:3])\n",
      "169/297:\n",
      "def getcontrols(category):\n",
      "    ctrls = []\n",
      "    count = 0\n",
      "    for a in category:\n",
      "        sents = nltk.sent_tokenize(a.text)\n",
      "        rs = [i for i in range(len(sents))]\n",
      "        shuffle(rs)\n",
      "        ctrl = []\n",
      "        for r in rs:\n",
      "            if count > 200:\n",
      "                break\n",
      "            count += len(nltk.word_tokenize(sents[r]))\n",
      "            ctrl.append(sents[r])\n",
      "        summary = \"\\n\".join([s for s in ctrl])\n",
      "        ctrls.append(summary)\n",
      "    return ctrls\n",
      "169/298: getcontrols(politics[:3])\n",
      "169/299:\n",
      "def getcontrols(category):\n",
      "    ctrls = []\n",
      "    for a in category:\n",
      "        sents = nltk.sent_tokenize(a.text)\n",
      "        rs = [i for i in range(len(sents))]\n",
      "        shuffle(rs)\n",
      "        ctrl = []\n",
      "        count = 0\n",
      "        for r in rs:\n",
      "            if count > 200:\n",
      "                break\n",
      "            count += len(nltk.word_tokenize(sents[r]))\n",
      "            ctrl.append(sents[r])\n",
      "        summary = \"\\n\".join([s for s in ctrl])\n",
      "        ctrls.append(summary)\n",
      "    return ctrls\n",
      "169/300: getcontrols(politics[:3])\n",
      "169/301:\n",
      "for x in getcontrols(politics[:3]):\n",
      "    print(len(nltk.word_tokenize(x)))\n",
      "169/302: np.mean([len(nltk.word_tokenize(x)) for x in getcontrols(politics[:100])])\n",
      "169/303: np.mean([len(nltk.word_tokenize(x)) for x in getsummaries(politics[:100])])\n",
      "169/304: np.mean([len(nltk.word_tokenize(x)) for x in getsummaries(politics[:100])[0]])\n",
      "169/305: np.mean([len(nltk.word_tokenize(x)) for x in getbows(politics[:100]))\n",
      "169/306: np.mean([len(nltk.word_tokenize(x)) for x in getbows(politics[:100])])\n",
      "169/307: np.mean([len(nltk.word_tokenize(x)) for x in getcwe(politics[:100])])\n",
      "169/308: np.mean([len(nltk.word_tokenize(getcwe(politics[:1000])[i])) for i in range(1000)])\n",
      "170/1:\n",
      "import os\n",
      "import time\n",
      "import dill\n",
      "from typing import List\n",
      "from functools import reduce\n",
      "from calendar import monthrange\n",
      "170/2:\n",
      "import json\n",
      "import csv\n",
      "import requests\n",
      "import xml.etree.ElementTree as ET\n",
      "from bs4 import BeautifulSoup\n",
      "import urllib.request as urllib2\n",
      "170/3:\n",
      "import matplotlib.pyplot as plt\n",
      "import numpy as np\n",
      "from scipy import stats\n",
      "from random import sample, randint, shuffle\n",
      "from sklearn.decomposition import PCA\n",
      "170/4:\n",
      "import nltk\n",
      "import text_summarizer\n",
      "from gensim.utils import simple_preprocess\n",
      "from gensim.summarization.summarizer import summarize\n",
      "from gensim.models import Word2Vec, LsiModel\n",
      "from gensim.test.utils import common_dictionary, common_corpus\n",
      "170/5: from time import time\n",
      "170/6:\n",
      "t = time.process_time()\n",
      "dill.load_session('thesis_env.db')\n",
      "time.process_time() - t\n",
      "170/7: from time import process_time\n",
      "170/8:\n",
      "t = time.process_time()\n",
      "dill.load_session('thesis_env.db')\n",
      "time.process_time() - t\n",
      "170/9: len(adjectives)\n",
      "170/10: from time import time\n",
      "170/11:\n",
      "t = time.process_time()\n",
      "dill.load_session('thesis_env.db')\n",
      "time.process_time() - t\n",
      "171/1:\n",
      "import os\n",
      "import time\n",
      "import dill\n",
      "from typing import List\n",
      "from functools import reduce\n",
      "from calendar import monthrange\n",
      "171/2:\n",
      "import json\n",
      "import csv\n",
      "import requests\n",
      "import xml.etree.ElementTree as ET\n",
      "from bs4 import BeautifulSoup\n",
      "import urllib.request as urllib2\n",
      "171/3:\n",
      "import matplotlib.pyplot as plt\n",
      "import numpy as np\n",
      "from scipy import stats\n",
      "from random import sample, randint, shuffle\n",
      "from sklearn.decomposition import PCA\n",
      "171/4:\n",
      "import nltk\n",
      "import text_summarizer\n",
      "from gensim.utils import simple_preprocess\n",
      "from gensim.summarization.summarizer import summarize\n",
      "from gensim.models import Word2Vec, LsiModel\n",
      "from gensim.test.utils import common_dictionary, common_corpus\n",
      "171/5: from time import time\n",
      "171/6:\n",
      "t = time.process_time()\n",
      "dill.load_session('thesis_env.db')\n",
      "time.process_time() - t\n",
      "172/1:\n",
      "import os\n",
      "import time\n",
      "import dill\n",
      "from typing import List\n",
      "from functools import reduce\n",
      "from calendar import monthrange\n",
      "172/2:\n",
      "import json\n",
      "import csv\n",
      "import requests\n",
      "import xml.etree.ElementTree as ET\n",
      "from bs4 import BeautifulSoup\n",
      "import urllib.request as urllib2\n",
      "172/3:\n",
      "import matplotlib.pyplot as plt\n",
      "import numpy as np\n",
      "from scipy import stats\n",
      "from random import sample, randint, shuffle\n",
      "from sklearn.decomposition import PCA\n",
      "172/4:\n",
      "import nltk\n",
      "import text_summarizer\n",
      "from gensim.utils import simple_preprocess\n",
      "from gensim.summarization.summarizer import summarize\n",
      "from gensim.models import Word2Vec, LsiModel\n",
      "from gensim.test.utils import common_dictionary, common_corpus\n",
      "172/5: from time import process_time\n",
      "172/6:\n",
      "t = time.process_time()\n",
      "dill.load_session('thesis_env.db')\n",
      "time.process_time() - t\n",
      "173/1:\n",
      "import os\n",
      "import time\n",
      "import dill\n",
      "from typing import List\n",
      "from functools import reduce\n",
      "from calendar import monthrange\n",
      "173/2:\n",
      "import json\n",
      "import csv\n",
      "import requests\n",
      "import xml.etree.ElementTree as ET\n",
      "from bs4 import BeautifulSoup\n",
      "import urllib.request as urllib2\n",
      "173/3:\n",
      "import matplotlib.pyplot as plt\n",
      "import numpy as np\n",
      "from scipy import stats\n",
      "from random import sample, randint, shuffle\n",
      "from sklearn.decomposition import PCA\n",
      "173/4:\n",
      "import nltk\n",
      "import text_summarizer\n",
      "from gensim.utils import simple_preprocess\n",
      "from gensim.summarization.summarizer import summarize\n",
      "from gensim.models import Word2Vec, LsiModel\n",
      "from gensim.test.utils import common_dictionary, common_corpus\n",
      "173/5: from time import process_time\n",
      "173/6:\n",
      "t = process_time()\n",
      "dill.load_session('thesis_env.db')\n",
      "process_time() - t\n",
      "173/7: len(adjectives)\n",
      "173/8: len(all_)\n",
      "173/9:\n",
      "t = time.process_time()\n",
      "alld = {}\n",
      "for doc in all_:\n",
      "    doc = Doc(doc.id, doc.gdescriptors, doc.descriptors, doc.text)\n",
      "    for d in doc.descriptors:\n",
      "        count, docs = alld.get(d, (0, set()))\n",
      "        docs.add(doc)\n",
      "        alld[d] = (count + 1, docs)\n",
      "time.process_time() - t\n",
      "173/10:\n",
      "alldlist = sorted([(d, alld[d][0], alld[d][1]) for d in alld], key=lambda x: x[1], reverse=True)\n",
      "for x in range(15):\n",
      "    print(alldlist[x][0:2])\n",
      "173/11:\n",
      "def exclude(e, x):\n",
      "    for d in x.descriptors:\n",
      "        if d == e:\n",
      "            continue\n",
      "        if d in ['Politics and Government', 'Motion Pictures', 'Football', 'Books and Literature', \\\n",
      "                                           'Computers and the Internet', 'Travel and Vacations']:\n",
      "            return False\n",
      "    return True\n",
      "173/12:\n",
      "def getlist(topic):\n",
      "    return list(filter(lambda x: exclude(topic, x), alld[topic][1]))\n",
      "173/13: politics = getlist('Politics and Government')\n",
      "173/14: movies = getlist('Motion Pictures')\n",
      "173/15: fball = getlist('Football')\n",
      "173/16: books = getlist('Books and Literature')\n",
      "173/17: computers = getlist('Computers and the Internet')\n",
      "173/18: travel = getlist('Travel and Vacations')\n",
      "173/19: len(computers), len(politics), len(travel), len(movies), len(fball), len(books)\n",
      "173/20: politics = sample(politics, 5400)\n",
      "173/21: movies = sample(movies, 5400)\n",
      "173/22: travel = sample(travel, 5400)\n",
      "173/23: fball = sample(fball, 5400)\n",
      "173/24: books = sample(books, 5400)\n",
      "173/25:\n",
      "def getw2v(category):\n",
      "    sentences = []\n",
      "    for t in category:\n",
      "        sentences += nltk.sent_tokenize(t.text)\n",
      "    docs = [simple_preprocess(s) for s in sentences]\n",
      "    return Word2Vec(docs)\n",
      "173/26:\n",
      "t = time.process_time()\n",
      "mpol = getw2v(politics)\n",
      "time.process_time() - t\n",
      "173/27:\n",
      "t = time.process_time()\n",
      "mcomp = getw2v(computers)\n",
      "time.process_time() - t\n",
      "173/28:\n",
      "t = time.process_time()\n",
      "mtrav = getw2v(travel)\n",
      "time.process_time() - t\n",
      "173/29:\n",
      "t = time.process_time()\n",
      "mmov = getw2v(movies)\n",
      "time.process_time() - t\n",
      "173/30:\n",
      "t = time.process_time()\n",
      "mbooks = getw2v(books)\n",
      "time.process_time() - t\n",
      "173/31:\n",
      "t = time.process_time()\n",
      "mfball = getw2v(fball)\n",
      "time.process_time() - t\n",
      "173/32:\n",
      "def top5(word):\n",
      "    for m in [mpol, mcomp, mtrav, mmov, mbooks, mfball]:\n",
      "        print([x[0] for x in m.wv.most_similar(positive=[word], topn=5)])\n",
      "173/33: top5('monday')\n",
      "173/34: top5('house')\n",
      "173/35: top5('healthy')\n",
      "173/36: top5('success')\n",
      "173/37: top5('score')\n",
      "173/38: np.mean([len(nltk.word_tokenize(politics[i].text)) for i in range(1000)])\n",
      "173/39: np.mean([len(nltk.word_tokenize(computers[i].text)) for i in range(1000)])\n",
      "173/40: np.mean([len(nltk.word_tokenize(travel[i].text)) for i in range(1000)])\n",
      "173/41: np.mean([len(nltk.word_tokenize(books[i].text)) for i in range(1000)])\n",
      "173/42: np.mean([len(nltk.word_tokenize(fball[i].text)) for i in range(1000)])\n",
      "173/43: np.mean([len(nltk.word_tokenize(movies[i].text)) for i in range(1000)])\n",
      "173/44:\n",
      "def getsummaries(category):\n",
      "    s = []\n",
      "    fails = []\n",
      "    for i in range(len(category)):\n",
      "        try:\n",
      "            s.append(summarize(category[i].text, word_count=200))\n",
      "        except:\n",
      "            fails.append(i)\n",
      "            print(i)\n",
      "    return s, fails\n",
      "173/45:\n",
      "t = time.process_time()\n",
      "summaries_pol, fpol = getsummaries(politics)\n",
      "time.process_time() - t\n",
      "173/46:\n",
      "t = time.process_time()\n",
      "summaries_comp, fcomp = getsummaries(computers)\n",
      "time.process_time() - t\n",
      "173/47:\n",
      "t = time.process_time()\n",
      "summaries_trav, ftrav = getsummaries(travel)\n",
      "time.process_time() - t\n",
      "173/48:\n",
      "t = time.process_time()\n",
      "summaries_mov, fmov = getsummaries(movies)\n",
      "time.process_time() - t\n",
      "173/49:\n",
      "t = time.process_time()\n",
      "summaries_bks, fbks = getsummaries(books)\n",
      "time.process_time() - t\n",
      "173/50:\n",
      "t = time.process_time()\n",
      "summaries_fball, ffball = getsummaries(fball)\n",
      "time.process_time() - t\n",
      "173/51:\n",
      "def getbows(category):\n",
      "    s = []\n",
      "    centroid_summarizer = text_summarizer.CentroidBOWSummarizer()\n",
      "    for i in category:\n",
      "        try:\n",
      "            s.append(centroid_summarizer.summarize(i.text, limit=200-30))\n",
      "        except:\n",
      "            print(i)\n",
      "    return s\n",
      "173/52:\n",
      "t = time.process_time()\n",
      "bow_pol = getbows(politics)\n",
      "time.process_time() - t\n",
      "173/53: len(bow_pol)\n",
      "173/54:\n",
      "t = time.process_time()\n",
      "bow_comp = getbows(computers)\n",
      "time.process_time() - t\n",
      "173/55:\n",
      "t = time.process_time()\n",
      "bow_trav = getbows(travel)\n",
      "time.process_time() - t\n",
      "173/56:\n",
      "t = time.process_time()\n",
      "bow_bks = getbows(books)\n",
      "time.process_time() - t\n",
      "174/1:\n",
      "t = time.process_time()\n",
      "bow_comp = getbows(computers)\n",
      "time.process_time() - t\n",
      "174/2:\n",
      "import os\n",
      "import time\n",
      "import dill\n",
      "from typing import List\n",
      "from functools import reduce\n",
      "from calendar import monthrange\n",
      "174/3:\n",
      "import json\n",
      "import csv\n",
      "import requests\n",
      "import xml.etree.ElementTree as ET\n",
      "from bs4 import BeautifulSoup\n",
      "import urllib.request as urllib2\n",
      "174/4:\n",
      "import matplotlib.pyplot as plt\n",
      "import numpy as np\n",
      "from scipy import stats\n",
      "from random import sample, randint, shuffle\n",
      "from sklearn.decomposition import PCA\n",
      "174/5:\n",
      "import nltk\n",
      "import text_summarizer\n",
      "from gensim.utils import simple_preprocess\n",
      "from gensim.summarization.summarizer import summarize\n",
      "from gensim.models import Word2Vec, LsiModel\n",
      "from gensim.test.utils import common_dictionary, common_corpus\n",
      "174/6: from time import process_time\n",
      "174/7:\n",
      "t = process_time()\n",
      "dill.load_session('thesis_env.db')\n",
      "process_time() - t\n",
      "174/8: len(adjectives)\n",
      "174/9: len(all_)\n",
      "174/10:\n",
      "t = time.process_time()\n",
      "alld = {}\n",
      "for doc in all_:\n",
      "    doc = Doc(doc.id, doc.gdescriptors, doc.descriptors, doc.text)\n",
      "    for d in doc.descriptors:\n",
      "        count, docs = alld.get(d, (0, set()))\n",
      "        docs.add(doc)\n",
      "        alld[d] = (count + 1, docs)\n",
      "time.process_time() - t\n",
      "174/11:\n",
      "alldlist = sorted([(d, alld[d][0], alld[d][1]) for d in alld], key=lambda x: x[1], reverse=True)\n",
      "for x in range(15):\n",
      "    print(alldlist[x][0:2])\n",
      "174/12:\n",
      "def exclude(e, x):\n",
      "    for d in x.descriptors:\n",
      "        if d == e:\n",
      "            continue\n",
      "        if d in ['Politics and Government', 'Motion Pictures', 'Football', 'Books and Literature', \\\n",
      "                                           'Computers and the Internet', 'Travel and Vacations']:\n",
      "            return False\n",
      "    return True\n",
      "174/13:\n",
      "def getlist(topic):\n",
      "    return list(filter(lambda x: exclude(topic, x), alld[topic][1]))\n",
      "174/14: politics = getlist('Politics and Government')\n",
      "174/15: movies = getlist('Motion Pictures')\n",
      "174/16: fball = getlist('Football')\n",
      "174/17: books = getlist('Books and Literature')\n",
      "174/18: computers = getlist('Computers and the Internet')\n",
      "174/19: travel = getlist('Travel and Vacations')\n",
      "174/20: len(computers), len(politics), len(travel), len(movies), len(fball), len(books)\n",
      "174/21: politics = sample(politics, 5400)\n",
      "174/22: movies = sample(movies, 5400)\n",
      "174/23: travel = sample(travel, 5400)\n",
      "174/24: fball = sample(fball, 5400)\n",
      "174/25: books = sample(books, 5400)\n",
      "174/26:\n",
      "def getw2v(category):\n",
      "    sentences = []\n",
      "    for t in category:\n",
      "        sentences += nltk.sent_tokenize(t.text)\n",
      "    docs = [simple_preprocess(s) for s in sentences]\n",
      "    return Word2Vec(docs)\n",
      "174/27:\n",
      "t = time.process_time()\n",
      "mpol = getw2v(politics)\n",
      "time.process_time() - t\n",
      "174/28:\n",
      "t = time.process_time()\n",
      "mcomp = getw2v(computers)\n",
      "time.process_time() - t\n",
      "174/29:\n",
      "t = time.process_time()\n",
      "mtrav = getw2v(travel)\n",
      "time.process_time() - t\n",
      "174/30:\n",
      "t = time.process_time()\n",
      "mmov = getw2v(movies)\n",
      "time.process_time() - t\n",
      "174/31:\n",
      "t = time.process_time()\n",
      "mbooks = getw2v(books)\n",
      "time.process_time() - t\n",
      "174/32:\n",
      "t = time.process_time()\n",
      "mfball = getw2v(fball)\n",
      "time.process_time() - t\n",
      "174/33:\n",
      "def top5(word):\n",
      "    for m in [mpol, mcomp, mtrav, mmov, mbooks, mfball]:\n",
      "        print([x[0] for x in m.wv.most_similar(positive=[word], topn=5)])\n",
      "174/34: top5('monday')\n",
      "174/35: top5('house')\n",
      "174/36: top5('healthy')\n",
      "174/37: top5('success')\n",
      "174/38: top5('score')\n",
      "174/39: np.mean([len(nltk.word_tokenize(politics[i].text)) for i in range(1000)])\n",
      "174/40: np.mean([len(nltk.word_tokenize(computers[i].text)) for i in range(1000)])\n",
      "174/41: np.mean([len(nltk.word_tokenize(travel[i].text)) for i in range(1000)])\n",
      "174/42: np.mean([len(nltk.word_tokenize(books[i].text)) for i in range(1000)])\n",
      "174/43: np.mean([len(nltk.word_tokenize(fball[i].text)) for i in range(1000)])\n",
      "174/44: np.mean([len(nltk.word_tokenize(movies[i].text)) for i in range(1000)])\n",
      "174/45:\n",
      "def getsummaries(category):\n",
      "    s = []\n",
      "    fails = []\n",
      "    for i in range(len(category)):\n",
      "        try:\n",
      "            s.append(summarize(category[i].text, word_count=200))\n",
      "        except:\n",
      "            fails.append(i)\n",
      "            print(i)\n",
      "    return s, fails\n",
      "174/46:\n",
      "t = time.process_time()\n",
      "summaries_pol, fpol = getsummaries(politics)\n",
      "time.process_time() - t\n",
      "174/47:\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "t = time.process_time()\n",
      "summaries_comp, fcomp = getsummaries(computers)\n",
      "time.process_time() - t\n",
      "174/48:\n",
      "t = time.process_time()\n",
      "summaries_trav, ftrav = getsummaries(travel)\n",
      "time.process_time() - t\n",
      "174/49:\n",
      "t = time.process_time()\n",
      "summaries_mov, fmov = getsummaries(movies)\n",
      "time.process_time() - t\n",
      "174/50:\n",
      "t = time.process_time()\n",
      "summaries_bks, fbks = getsummaries(books)\n",
      "time.process_time() - t\n",
      "174/51:\n",
      "t = time.process_time()\n",
      "summaries_fball, ffball = getsummaries(fball)\n",
      "time.process_time() - t\n",
      "174/52:\n",
      "def getbows(category):\n",
      "    s = []\n",
      "    centroid_summarizer = text_summarizer.CentroidBOWSummarizer()\n",
      "    for i in category:\n",
      "        try:\n",
      "            s.append(centroid_summarizer.summarize(i.text, limit=200-30))\n",
      "        except:\n",
      "            print(i)\n",
      "    return s\n",
      "174/53:\n",
      "t = time.process_time()\n",
      "bow_pol = getbows(politics)\n",
      "time.process_time() - t\n",
      "174/54: len(bow_pol)\n",
      "174/55:\n",
      "t = time.process_time()\n",
      "bow_comp = getbows(computers)\n",
      "time.process_time() - t\n",
      "174/56:\n",
      "t = time.process_time()\n",
      "bow_trav = getbows(travel)\n",
      "time.process_time() - t\n",
      "174/57:\n",
      "t = time.process_time()\n",
      "bow_bks = getbows(books)\n",
      "time.process_time() - t\n",
      "174/58:\n",
      "t = time.process_time()\n",
      "bow_mov = getbows(movies)\n",
      "time.process_time() - t\n",
      "174/59:\n",
      "t = time.process_time()\n",
      "bow_fball = getbows(fball)\n",
      "time.process_time() - t\n",
      "174/60: embedding_model = text_summarizer.centroid_word_embeddings.load_gensim_embedding_model('glove-wiki-gigaword-50')\n",
      "174/61: centroid_we_summarizer = text_summarizer.CentroidWordEmbeddingsSummarizer(embedding_model, sim_threshold=.99, preprocess_type='nltk')\n",
      "174/62:\n",
      "def getcwe(category):\n",
      "    s = []\n",
      "    for i in category:\n",
      "        try:\n",
      "            s.append(centroid_we_summarizer.summarize(i.text, limit=200-20))\n",
      "        except:\n",
      "            print(i)\n",
      "    return s\n",
      "174/63: np.mean([len(nltk.word_tokenize(getcwe(politics[:100])[i])) for i in range(100)])\n",
      "174/64: np.mean([len(nltk.word_tokenize(getbows(politics[:100])[i])) for i in range(100)])\n",
      "174/65: np.mean([len(nltk.word_tokenize(getsummaries(politics[:100])[0][i])) for i in range(100)])\n",
      "174/66:\n",
      "def getcontrols(category):\n",
      "    ctrls = []\n",
      "    for a in category:\n",
      "        sents = nltk.sent_tokenize(a.text)\n",
      "        rs = [i for i in range(len(sents))]\n",
      "        shuffle(rs)\n",
      "        ctrl = []\n",
      "        count = 0\n",
      "        for r in rs:\n",
      "            if count > 200:\n",
      "                break\n",
      "            count += len(nltk.word_tokenize(sents[r]))\n",
      "            ctrl.append(sents[r])\n",
      "        summary = \"\\n\".join([s for s in ctrl])\n",
      "        ctrls.append(summary)\n",
      "    return ctrls\n",
      "174/67: np.mean([len(nltk.word_tokenize(getcontrols(politics[:100])[i])) for i in range(100)])\n",
      "174/68:\n",
      "t = time.process_time()\n",
      "cwe_pol = getcwe(politics)\n",
      "time.process_time() - t\n",
      "174/69:\n",
      "t = time.process_time()\n",
      "cwe_comp = getcwe(computers)\n",
      "time.process_time() - t\n",
      "174/70:\n",
      "t = time.process_time()\n",
      "cwe_trav = getcwe(travel)\n",
      "time.process_time() - t\n",
      "174/71:\n",
      "t = time.process_time()\n",
      "cwe_bks = getcwe(books)\n",
      "time.process_time() - t\n",
      "174/72:\n",
      "t = time.process_time()\n",
      "cwe_mov = getcwe(movies)\n",
      "time.process_time() - t\n",
      "174/73:\n",
      "t = time.process_time()\n",
      "cwe_fball = getcwe(fball)\n",
      "time.process_time() - t\n",
      "174/74:\n",
      "def getcontrols(category):\n",
      "    ctrls = []\n",
      "    for a in category:\n",
      "        sents = nltk.sent_tokenize(a.text)\n",
      "        rs = [i for i in range(len(sents))]\n",
      "        shuffle(rs)\n",
      "        ctrl = []\n",
      "        count = 0\n",
      "        for r in rs:\n",
      "            if count > 200-10:\n",
      "                break\n",
      "            count += len(nltk.word_tokenize(sents[r]))\n",
      "            ctrl.append(sents[r])\n",
      "        summary = \"\\n\".join([s for s in ctrl])\n",
      "        ctrls.append(summary)\n",
      "    return ctrls\n",
      "174/75: np.mean([len(nltk.word_tokenize(getcontrols(politics[:100])[i])) for i in range(100)])\n",
      "174/76:\n",
      "def getcontrols(category):\n",
      "    ctrls = []\n",
      "    for a in category:\n",
      "        sents = nltk.sent_tokenize(a.text)\n",
      "        rs = [i for i in range(len(sents))]\n",
      "        shuffle(rs)\n",
      "        ctrl = []\n",
      "        count = 0\n",
      "        for r in rs:\n",
      "            if count > 200-5:\n",
      "                break\n",
      "            count += len(nltk.word_tokenize(sents[r]))\n",
      "            ctrl.append(sents[r])\n",
      "        summary = \"\\n\".join([s for s in ctrl])\n",
      "        ctrls.append(summary)\n",
      "    return ctrls\n",
      "174/77: np.mean([len(nltk.word_tokenize(getcontrols(politics[:100])[i])) for i in range(100)])\n",
      "174/78:\n",
      "def getcontrols(category):\n",
      "    ctrls = []\n",
      "    for a in category:\n",
      "        sents = nltk.sent_tokenize(a.text)\n",
      "        rs = [i for i in range(len(sents))]\n",
      "        shuffle(rs)\n",
      "        ctrl = []\n",
      "        count = 0\n",
      "        for r in rs:\n",
      "            if count > 200-6:\n",
      "                break\n",
      "            count += len(nltk.word_tokenize(sents[r]))\n",
      "            ctrl.append(sents[r])\n",
      "        summary = \"\\n\".join([s for s in ctrl])\n",
      "        ctrls.append(summary)\n",
      "    return ctrls\n",
      "174/79: np.mean([len(nltk.word_tokenize(getcontrols(politics[:100])[i])) for i in range(100)])\n",
      "174/80:\n",
      "def getcontrols(category):\n",
      "    ctrls = []\n",
      "    for a in category:\n",
      "        sents = nltk.sent_tokenize(a.text)\n",
      "        rs = [i for i in range(len(sents))]\n",
      "        shuffle(rs)\n",
      "        ctrl = []\n",
      "        count = 0\n",
      "        for r in rs:\n",
      "            if count > 200-5:\n",
      "                break\n",
      "            count += len(nltk.word_tokenize(sents[r]))\n",
      "            ctrl.append(sents[r])\n",
      "        summary = \"\\n\".join([s for s in ctrl])\n",
      "        ctrls.append(summary)\n",
      "    return ctrls\n",
      "174/81: np.mean([len(nltk.word_tokenize(getcontrols(politics[:100])[i])) for i in range(100)])\n",
      "174/82:\n",
      "t = time.process_time()\n",
      "cwe_pol = getcwe(politics)\n",
      "time.process_time() - t\n",
      "174/83:\n",
      "t = time.process_time()\n",
      "cwe_comp = getcwe(computers)\n",
      "time.process_time() - t\n",
      "174/84:\n",
      "t = time.process_time()\n",
      "cwe_trav = getcwe(travel)\n",
      "time.process_time() - t\n",
      "174/85:\n",
      "t = time.process_time()\n",
      "cwe_bks = getcwe(books)\n",
      "time.process_time() - t\n",
      "174/86:\n",
      "t = time.process_time()\n",
      "cwe_mov = getcwe(movies)\n",
      "time.process_time() - t\n",
      "174/87:\n",
      "t = time.process_time()\n",
      "cwe_fball = getcwe(fball)\n",
      "time.process_time() - t\n",
      "174/88: mpol_s = getw2v_s(summaries_pol)\n",
      "174/89: mbks_s = getw2v_s(summaries_bks)\n",
      "174/90:\n",
      "def getw2v_s(sums):\n",
      "    sentences = []\n",
      "    for t in sums:\n",
      "        sentences += nltk.sent_tokenize(t)\n",
      "    docs = [simple_preprocess(s) for s in sentences]\n",
      "    return Word2Vec(docs)\n",
      "174/91: mpol_s = getw2v_s(summaries_pol)\n",
      "174/92: mcomp_s = getw2v_s(summaries_comp)\n",
      "174/93: mtrav_s = getw2v_s(summaries_trav)\n",
      "174/94: mbks_s = getw2v_s(summaries_bks)\n",
      "174/95: mmov_s = getw2v_s(summaries_bks)\n",
      "174/96: mfball_s = getw2v_s(summaries_fball)\n",
      "174/97: mmov_s = getw2v_s(summaries_mov)\n",
      "174/98:\n",
      "def getctrls(category):\n",
      "    ctrls = []\n",
      "    for a in category:\n",
      "        sents = nltk.sent_tokenize(a.text)\n",
      "        rs = [i for i in range(len(sents))]\n",
      "        shuffle(rs)\n",
      "        ctrl = []\n",
      "        count = 0\n",
      "        for r in rs:\n",
      "            if count > 200-5:\n",
      "                break\n",
      "            count += len(nltk.word_tokenize(sents[r]))\n",
      "            ctrl.append(sents[r])\n",
      "        summary = \"\\n\".join([s for s in ctrl])\n",
      "        ctrls.append(summary)\n",
      "    return ctrls\n",
      "174/99:\n",
      "t = time.process_time()\n",
      "ctrls_pol = getctrls(politics)\n",
      "time.process_time() - t\n",
      "174/100:\n",
      "t = time.process_time()\n",
      "ctrls_comp = getctrls(computers)\n",
      "time.process_time() - t\n",
      "174/101:\n",
      "t = time.process_time()\n",
      "ctrls_trav = getctrls(travel)\n",
      "time.process_time() - t\n",
      "174/102:\n",
      "t = time.process_time()\n",
      "ctrls_bks = getctrls(books)\n",
      "time.process_time() - t\n",
      "174/103:\n",
      "t = time.process_time()\n",
      "ctrls_mov = getctrls(movies)\n",
      "time.process_time() - t\n",
      "174/104:\n",
      "t = time.process_time()\n",
      "ctrls_fball = getctrls(fball)\n",
      "time.process_time() - t\n",
      "174/105:\n",
      "def getw2v_s(sums):\n",
      "    sentences = []\n",
      "    for t in sums:\n",
      "        sentences += nltk.sent_tokenize(t)\n",
      "    docs = [simple_preprocess(s) for s in sentences]\n",
      "    return Word2Vec(docs)\n",
      "174/106: mpol_s = getw2v_s(summaries_pol)\n",
      "174/107: mcomp_s = getw2v_s(summaries_comp)\n",
      "174/108: mtrav_s = getw2v_s(summaries_trav)\n",
      "174/109: mbks_s = getw2v_s(summaries_bks)\n",
      "174/110: mmov_s = getw2v_s(summaries_mov)\n",
      "174/111: mfball_s = getw2v_s(summaries_fball)\n",
      "174/112:\n",
      "mpol_sb = getw2v_s(bow_pol)\n",
      "mcomp_sb = getw2v_s(bow_comp)\n",
      "mtrav_sb = getw2v_s(bow_trav)\n",
      "mbks_sb = getw2v_s(bow_bks)\n",
      "mmov_sb = getw2v_s(bow_mov)\n",
      "mfball_sb = getw2v_s(bow_fball)\n",
      "174/113:\n",
      "mpol_sc = getw2v_s(cwe_pol)\n",
      "mcomp_sc = getw2v_s(cwe_comp)\n",
      "mtrav_sc = getw2v_s(cwe_trav)\n",
      "mbks_sc = getw2v_s(cwe_bks)\n",
      "mmov_sc = getw2v_s(cwe_mov)\n",
      "mfball_sc = getw2v_s(cwe_fball)\n",
      "174/114:\n",
      "mpol_c = getw2v_s(ctrls_pol)\n",
      "mcomp_c = getw2v_s(ctrls_comp)\n",
      "mtrav_c = getw2v_s(ctrls_trav)\n",
      "mbks_c = getw2v_s(ctrls_bks)\n",
      "mmov_c = getw2v_s(ctrls_mov)\n",
      "mfball_c = getw2v_s(ctrls_fball)\n",
      "174/115: dill.dump_session('thesis_env2.db')\n",
      "174/116:\n",
      "url = 'https://www.randomlists.com/things?show_images=false&dup=false&qty=200'\n",
      "file = urllib2.urlopen(url)\n",
      "html = file.read()\n",
      "file.close()\n",
      "soup = BeautifulSoup(html)\n",
      "174/117: objs = [x.contents[0] for x in BeautifulSoup(html).find_all(attrs={\"class\": \"rand_medium\"})]\n",
      "174/118: objs\n",
      "174/119: objs = [x.contents for x in BeautifulSoup(html).find_all(attrs={\"class\": \"rand_medium\"})]\n",
      "174/120: objs\n",
      "174/121: soup.find_all('li')\n",
      "174/122: soup.find_all(attrs={\"class\": \"section_wide\"})\n",
      "174/123: soup.find_all(attrs={\"class\": \"layout section_wide\"})\n",
      "174/124: soup.find_all(attrs={\"class\": \"layout section_wide\"}).find_all(attrs={\"class\": \"rand_medium\"})\n",
      "174/125: soup.find_all(attrs={\"class\": \"layout section_wide\"}).find(attrs={\"class\": \"rand_medium\"})\n",
      "174/126: soup.find(attrs={\"class\": \"layout section_wide\"}).find(attrs={\"class\": \"rand_medium\"})\n",
      "174/127: soup.find(attrs={\"class\": \"layout section_wide\"}).find_all(attrs={\"class\": \"rand_medium\"})\n",
      "174/128: soup.find_all('article')\n",
      "174/129: soup.find('layout_main').find_all('article')\n",
      "174/130: soup.find_all('layout_main').find_all('article')\n",
      "174/131: soup.find_all('layout_main')\n",
      "174/132: soup.find('layout')\n",
      "174/133: soup.find(attrs={\"class\": \"layout\"}\n",
      "174/134: soup.find(attrs={\"class\": \"layout\"})\n",
      "174/135: soup.find(attrs={\"class\": \"layout\"}).find(attrs={\"class\": \"layout_main\"})\n",
      "174/136: soup.find_all(attrs={\"class\": \"section_gutter\"})\n",
      "174/137: soup.find_all('img')\n",
      "174/138: soup.find_all(attrs={\"class\": \"img\"})\n",
      "174/139: soup.find_all('ol')\n",
      "174/140: soup.find('ol')\n",
      "174/141: soup.find_all('ol')\n",
      "174/142: soup.find_all('li')\n",
      "174/143: soup.prettify()\n",
      "174/144:\n",
      "url = 'https://www.britannica.com/topic/list-of-cities-and-towns-in-the-United-States-2023068'\n",
      "file = urllib2.urlopen(url)\n",
      "html = file.read()\n",
      "file.close()\n",
      "soup = BeautifulSoup(html)\n",
      "174/145: soup.find_all(attrs={\"class\": \"md-crosslink\"})\n",
      "174/146: alist = soup.find_all(attrs={\"class\": \"md-crosslink\"})\n",
      "174/147: alist[10]\n",
      "174/148:\n",
      "for a in alist:\n",
      "    print(a.contents[0])\n",
      "174/149:\n",
      "for a in alist[3:]:\n",
      "    print(a.contents[0])\n",
      "174/150: places = [a.contents[0] for a in alist[3:]]\n",
      "174/151: len(places)\n",
      "174/152: places[0]\n",
      "174/153: places[50]\n",
      "174/154: dill.dump_session('thesis_env2.db')\n",
      "175/1:\n",
      "t = process_time()\n",
      "dill.load_session('thesis_env.db')\n",
      "process_time() - t\n",
      "176/1:\n",
      "import os\n",
      "import time\n",
      "import dill\n",
      "from typing import List\n",
      "from functools import reduce\n",
      "from calendar import monthrange\n",
      "176/2:\n",
      "import json\n",
      "import csv\n",
      "import requests\n",
      "import xml.etree.ElementTree as ET\n",
      "from bs4 import BeautifulSoup\n",
      "import urllib.request as urllib2\n",
      "176/3:\n",
      "import matplotlib.pyplot as plt\n",
      "import numpy as np\n",
      "from scipy import stats\n",
      "from random import sample, randint, shuffle\n",
      "from sklearn.decomposition import PCA\n",
      "176/4:\n",
      "import nltk\n",
      "import text_summarizer\n",
      "from gensim.utils import simple_preprocess\n",
      "from gensim.summarization.summarizer import summarize\n",
      "from gensim.models import Word2Vec, LsiModel\n",
      "from gensim.test.utils import common_dictionary, common_corpus\n",
      "176/5: from time import process_time\n",
      "176/6:\n",
      "t = process_time()\n",
      "dill.load_session('thesis_env.db')\n",
      "process_time() - t\n",
      "176/7: len(adjectives)\n",
      "176/8: len(all_)\n",
      "176/9:\n",
      "url = 'https://www.britannica.com/topic/list-of-cities-and-towns-in-the-United-States-2023068'\n",
      "file = urllib2.urlopen(url)\n",
      "html = file.read()\n",
      "file.close()\n",
      "soup = BeautifulSoup(html)\n",
      "176/10: alist = soup.find_all(attrs={\"class\": \"md-crosslink\"})\n",
      "176/11: places = [a.contents[0] for a in alist[3:]]\n",
      "176/12: len(places)\n",
      "176/13:\n",
      "class Doc:\n",
      "    def __init__(self, id: int, gdescriptors: List[str], descriptors: List[str], text: str) -> None:\n",
      "        self.id = id\n",
      "        self.gdescriptors = gdescriptors\n",
      "        self.descriptors = descriptors\n",
      "        self.text = text\n",
      "176/14:\n",
      "def parse_doc(file: str) -> Doc:\n",
      "    tree = ET.parse(file)\n",
      "    root = tree.getroot()\n",
      "    try:\n",
      "        fulltext = root.find('body').find('body.content').find(\"*[@class='full_text']\")\n",
      "        text = reduce(lambda x, y: x + y, [x.text for x in fulltext.findall('p')])\n",
      "        id = int(root.find('head').find('docdata').find('doc-id').attrib['id-string'])\n",
      "        classifiers = root.find('head').find('docdata').find('identified-content')\n",
      "        gclassifiers = classifiers.findall(\"*[@class='online_producer'][@type='general_descriptor']\")\n",
      "        dclassifiers = classifiers.findall(\"*[@class='online_producer'][@type='descriptor']\")\n",
      "        gdescriptors = [c.text for c in gclassifiers]\n",
      "        descriptors = [c.text for c in dclassifiers]\n",
      "        return Doc(id, gdescriptors, descriptors, text)\n",
      "    except:\n",
      "        return None\n",
      "176/15:\n",
      "def get_docs_by_month(year: int, month: int) -> List[Doc]:\n",
      "    days = monthrange(year, month)[1]\n",
      "    docs = []\n",
      "    for day in range(1, days+1):\n",
      "        for file in os.listdir(f'data/{year}/{month:02}/{day:02}/'):\n",
      "            doc = parse_doc(f'data/{year}/{month:02}/{day:02}/{file}')\n",
      "            if doc:\n",
      "                docs.append(doc)\n",
      "    return docs\n",
      "177/1:\n",
      "import os\n",
      "import time\n",
      "import dill\n",
      "from typing import List\n",
      "from functools import reduce\n",
      "from calendar import monthrange\n",
      "177/2:\n",
      "import json\n",
      "import csv\n",
      "import requests\n",
      "import xml.etree.ElementTree as ET\n",
      "from bs4 import BeautifulSoup\n",
      "import urllib.request as urllib2\n",
      "177/3:\n",
      "import matplotlib.pyplot as plt\n",
      "import numpy as np\n",
      "from scipy import stats\n",
      "from random import sample, randint, shuffle\n",
      "from sklearn.decomposition import PCA\n",
      "177/4:\n",
      "import nltk\n",
      "import text_summarizer\n",
      "from gensim.utils import simple_preprocess\n",
      "from gensim.summarization.summarizer import summarize\n",
      "from gensim.models import Word2Vec, LsiModel\n",
      "from gensim.test.utils import common_dictionary, common_corpus\n",
      "177/5: from time import process_time\n",
      "177/6:\n",
      "t = process_time()\n",
      "dill.load_session('thesis_env.db')\n",
      "process_time() - t\n",
      "177/7: len(adjectives)\n",
      "177/8: len(all_)\n",
      "177/9:\n",
      "url = 'https://www.britannica.com/topic/list-of-cities-and-towns-in-the-United-States-2023068'\n",
      "file = urllib2.urlopen(url)\n",
      "html = file.read()\n",
      "file.close()\n",
      "soup = BeautifulSoup(html)\n",
      "177/10: alist = soup.find_all(attrs={\"class\": \"md-crosslink\"})\n",
      "177/11: places = [a.contents[0] for a in alist[3:]]\n",
      "177/12: len(places)\n",
      "177/13: dill.dump_session('temp.db')\n",
      "178/1:\n",
      "import os\n",
      "import time\n",
      "import dill\n",
      "from typing import List\n",
      "from functools import reduce\n",
      "from calendar import monthrange\n",
      "178/2:\n",
      "import json\n",
      "import csv\n",
      "import requests\n",
      "import xml.etree.ElementTree as ET\n",
      "from bs4 import BeautifulSoup\n",
      "import urllib.request as urllib2\n",
      "178/3:\n",
      "import matplotlib.pyplot as plt\n",
      "import numpy as np\n",
      "from scipy import stats\n",
      "from random import sample, randint, shuffle\n",
      "from sklearn.decomposition import PCA\n",
      "178/4:\n",
      "import nltk\n",
      "import text_summarizer\n",
      "from gensim.utils import simple_preprocess\n",
      "from gensim.summarization.summarizer import summarize\n",
      "from gensim.models import Word2Vec, LsiModel\n",
      "from gensim.test.utils import common_dictionary, common_corpus\n",
      "178/5: from time import process_time\n",
      "178/6:\n",
      "t = process_time()\n",
      "dill.load_session('thesis_env.db')\n",
      "process_time() - t\n",
      "178/7: len(adjectives)\n",
      "178/8: len(all_)\n",
      "178/9:\n",
      "url = 'https://www.britannica.com/topic/list-of-cities-and-towns-in-the-United-States-2023068'\n",
      "file = urllib2.urlopen(url)\n",
      "html = file.read()\n",
      "file.close()\n",
      "soup = BeautifulSoup(html)\n",
      "178/10: alist = soup.find_all(attrs={\"class\": \"md-crosslink\"})\n",
      "178/11: places = [a.contents[0] for a in alist[3:]]\n",
      "178/12: len(places)\n",
      "178/13: dill.dump_session('temp.db')\n",
      "179/1:\n",
      "import os\n",
      "import time\n",
      "import dill\n",
      "from typing import List\n",
      "from functools import reduce\n",
      "from calendar import monthrange\n",
      "179/2:\n",
      "import json\n",
      "import csv\n",
      "import requests\n",
      "import xml.etree.ElementTree as ET\n",
      "from bs4 import BeautifulSoup\n",
      "import urllib.request as urllib2\n",
      "179/3:\n",
      "import matplotlib.pyplot as plt\n",
      "import numpy as np\n",
      "from scipy import stats\n",
      "from random import sample, randint, shuffle\n",
      "from sklearn.decomposition import PCA\n",
      "179/4:\n",
      "import nltk\n",
      "import text_summarizer\n",
      "from gensim.utils import simple_preprocess\n",
      "from gensim.summarization.summarizer import summarize\n",
      "from gensim.models import Word2Vec, LsiModel\n",
      "from gensim.test.utils import common_dictionary, common_corpus\n",
      "179/5:\n",
      "url = 'https://www.britannica.com/topic/list-of-cities-and-towns-in-the-United-States-2023068'\n",
      "file = urllib2.urlopen(url)\n",
      "html = file.read()\n",
      "file.close()\n",
      "soup = BeautifulSoup(html)\n",
      "179/6: alist = soup.find_all(attrs={\"class\": \"md-crosslink\"})\n",
      "179/7: places = [a.contents[0] for a in alist[3:]]\n",
      "179/8: len(places)\n",
      "179/9:\n",
      "with open('cities.csv', mode='w') as f:\n",
      "    writer = csv.writer(f, delimiter=',')\n",
      "    writer.writerow(places)\n",
      "179/10:\n",
      "with open('cities.csv') as csv_file:\n",
      "    cities = csv_file.read().split(\",\")\n",
      "179/11: cities\n",
      "180/1:\n",
      "import os\n",
      "import time\n",
      "import dill\n",
      "from typing import List\n",
      "from functools import reduce\n",
      "from calendar import monthrange\n",
      "180/2:\n",
      "import json\n",
      "import csv\n",
      "import requests\n",
      "import xml.etree.ElementTree as ET\n",
      "from bs4 import BeautifulSoup\n",
      "import urllib.request as urllib2\n",
      "180/3:\n",
      "import matplotlib.pyplot as plt\n",
      "import numpy as np\n",
      "from scipy import stats\n",
      "from random import sample, randint, shuffle\n",
      "from sklearn.decomposition import PCA\n",
      "180/4:\n",
      "import nltk\n",
      "import text_summarizer\n",
      "from gensim.utils import simple_preprocess\n",
      "from gensim.summarization.summarizer import summarize\n",
      "from gensim.models import Word2Vec, LsiModel\n",
      "from gensim.test.utils import common_dictionary, common_corpus\n",
      "180/5: from time import process_time\n",
      "180/6:\n",
      "t = process_time()\n",
      "dill.load_session('thesis_env.db')\n",
      "process_time() - t\n",
      "180/7: len(adjectives)\n",
      "180/8: len(all_)\n",
      "180/9:\n",
      "with open('cities.csv') as csv_file:\n",
      "    cities = csv_file.read().split(\",\")\n",
      "180/10: cities[:5]\n",
      "180/11:\n",
      "t = time.process_time()\n",
      "alld = {}\n",
      "for doc in all_:\n",
      "    doc = Doc(doc.id, doc.gdescriptors, doc.descriptors, doc.text)\n",
      "    for d in doc.descriptors:\n",
      "        count, docs = alld.get(d, (0, set()))\n",
      "        docs.add(doc)\n",
      "        alld[d] = (count + 1, docs)\n",
      "time.process_time() - t\n",
      "180/12:\n",
      "alldlist = sorted([(d, alld[d][0], alld[d][1]) for d in alld], key=lambda x: x[1], reverse=True)\n",
      "for x in range(15):\n",
      "    print(alldlist[x][0:2])\n",
      "180/13:\n",
      "def exclude(e, x):\n",
      "    for d in x.descriptors:\n",
      "        if d == e:\n",
      "            continue\n",
      "        if d in ['Politics and Government', 'Motion Pictures', 'Football', 'Books and Literature', \\\n",
      "                                           'Computers and the Internet', 'Travel and Vacations']:\n",
      "            return False\n",
      "    return True\n",
      "180/14:\n",
      "def getlist(topic):\n",
      "    return list(filter(lambda x: exclude(topic, x), alld[topic][1]))\n",
      "180/15: politics = getlist('Politics and Government')\n",
      "180/16: movies = getlist('Motion Pictures')\n",
      "180/17: fball = getlist('Football')\n",
      "180/18: books = getlist('Books and Literature')\n",
      "180/19: computers = getlist('Computers and the Internet')\n",
      "180/20: travel = getlist('Travel and Vacations')\n",
      "180/21: len(computers), len(politics), len(travel), len(movies), len(fball), len(books)\n",
      "180/22: politics = sample(politics, 5400)\n",
      "180/23: movies = sample(movies, 5400)\n",
      "180/24: travel = sample(travel, 5400)\n",
      "180/25: fball = sample(fball, 5400)\n",
      "180/26: books = sample(books, 5400)\n",
      "180/27:\n",
      "def getw2v(category):\n",
      "    sentences = []\n",
      "    for t in category:\n",
      "        sentences += nltk.sent_tokenize(t.text)\n",
      "    docs = [simple_preprocess(s) for s in sentences]\n",
      "    return Word2Vec(docs)\n",
      "180/28:\n",
      "t = time.process_time()\n",
      "mpol = getw2v(politics)\n",
      "time.process_time() - t\n",
      "180/29:\n",
      "t = time.process_time()\n",
      "mcomp = getw2v(computers)\n",
      "time.process_time() - t\n",
      "180/30:\n",
      "t = time.process_time()\n",
      "mtrav = getw2v(travel)\n",
      "time.process_time() - t\n",
      "180/31:\n",
      "t = time.process_time()\n",
      "mmov = getw2v(movies)\n",
      "time.process_time() - t\n",
      "180/32:\n",
      "t = time.process_time()\n",
      "mbooks = getw2v(books)\n",
      "time.process_time() - t\n",
      "180/33:\n",
      "t = time.process_time()\n",
      "mfball = getw2v(fball)\n",
      "time.process_time() - t\n",
      "180/34:\n",
      "def top5(word):\n",
      "    for m in [mpol, mcomp, mtrav, mmov, mbooks, mfball]:\n",
      "        print([x[0] for x in m.wv.most_similar(positive=[word], topn=5)])\n",
      "180/35: top5('monday')\n",
      "180/36: top5('house')\n",
      "180/37: top5('healthy')\n",
      "180/38: top5('success')\n",
      "180/39: top5('score')\n",
      "180/40: np.mean([len(nltk.word_tokenize(politics[i].text)) for i in range(1000)])\n",
      "180/41: np.mean([len(nltk.word_tokenize(computers[i].text)) for i in range(1000)])\n",
      "180/42: np.mean([len(nltk.word_tokenize(travel[i].text)) for i in range(1000)])\n",
      "180/43: np.mean([len(nltk.word_tokenize(books[i].text)) for i in range(1000)])\n",
      "180/44: np.mean([len(nltk.word_tokenize(fball[i].text)) for i in range(1000)])\n",
      "180/45: np.mean([len(nltk.word_tokenize(movies[i].text)) for i in range(1000)])\n",
      "180/46:\n",
      "def getsummaries(category):\n",
      "    s = []\n",
      "    fails = []\n",
      "    for i in range(len(category)):\n",
      "        try:\n",
      "            s.append(summarize(category[i].text, word_count=200))\n",
      "        except:\n",
      "            fails.append(i)\n",
      "            print(i)\n",
      "    return s, fails\n",
      "180/47:\n",
      "t = time.process_time()\n",
      "summaries_pol, fpol = getsummaries(politics)\n",
      "time.process_time() - t\n",
      "180/48:\n",
      "t = time.process_time()\n",
      "summaries_comp, fcomp = getsummaries(computers)\n",
      "time.process_time() - t\n",
      "180/49:\n",
      "t = time.process_time()\n",
      "summaries_trav, ftrav = getsummaries(travel)\n",
      "time.process_time() - t\n",
      "180/50:\n",
      "t = time.process_time()\n",
      "summaries_mov, fmov = getsummaries(movies)\n",
      "time.process_time() - t\n",
      "180/51:\n",
      "t = time.process_time()\n",
      "summaries_bks, fbks = getsummaries(books)\n",
      "time.process_time() - t\n",
      "180/52:\n",
      "t = time.process_time()\n",
      "summaries_fball, ffball = getsummaries(fball)\n",
      "time.process_time() - t\n",
      "180/53: politics = getlist('Politics and Government')\n",
      "180/54: computers = getlist('Computers and the Internet')\n",
      "180/55: politics = sample(politics, 5400)\n",
      "180/56:\n",
      "t = time.process_time()\n",
      "mpol = getw2v(politics)\n",
      "time.process_time() - t\n",
      "180/57:\n",
      "t = time.process_time()\n",
      "mcomp = getw2v(computers)\n",
      "time.process_time() - t\n",
      "180/58: top5('monday')\n",
      "180/59: top5('house')\n",
      "180/60: top5('healthy')\n",
      "180/61: top5('success')\n",
      "180/62: top5('score')\n",
      "180/63: np.mean([len(nltk.word_tokenize(politics[i].text)) for i in range(1000)])\n",
      "180/64: np.mean([len(nltk.word_tokenize(computers[i].text)) for i in range(1000)])\n",
      "180/65:\n",
      "def getsummaries(category):\n",
      "    s = []\n",
      "    fails = []\n",
      "    for i in range(len(category)):\n",
      "        try:\n",
      "            s.append(summarize(category[i].text, word_count=200))\n",
      "        except:\n",
      "            fails.append(i)\n",
      "            print(i)\n",
      "    return s, fails\n",
      "180/66:\n",
      "t = time.process_time()\n",
      "summaries_pol, fpol = getsummaries(politics)\n",
      "time.process_time() - t\n",
      "180/67:\n",
      "t = time.process_time()\n",
      "summaries_comp, fcomp = getsummaries(computers)\n",
      "time.process_time() - t\n",
      "180/68:\n",
      "def getbows(category):\n",
      "    s = []\n",
      "    centroid_summarizer = text_summarizer.CentroidBOWSummarizer()\n",
      "    for i in category:\n",
      "        try:\n",
      "            s.append(centroid_summarizer.summarize(i.text, limit=200-30))\n",
      "        except:\n",
      "            print(i)\n",
      "    return s\n",
      "180/69:\n",
      "t = time.process_time()\n",
      "bow_pol = getbows(politics)\n",
      "time.process_time() - t\n",
      "180/70: len(bow_pol)\n",
      "180/71:\n",
      "t = time.process_time()\n",
      "bow_comp = getbows(computers)\n",
      "time.process_time() - t\n",
      "180/72:\n",
      "t = time.process_time()\n",
      "bow_trav = getbows(travel)\n",
      "time.process_time() - t\n",
      "180/73:\n",
      "t = time.process_time()\n",
      "bow_bks = getbows(books)\n",
      "time.process_time() - t\n",
      "180/74:\n",
      "t = time.process_time()\n",
      "bow_mov = getbows(movies)\n",
      "time.process_time() - t\n",
      "180/75:\n",
      "t = time.process_time()\n",
      "bow_fball = getbows(fball)\n",
      "time.process_time() - t\n",
      "180/76: embedding_model = text_summarizer.centroid_word_embeddings.load_gensim_embedding_model('glove-wiki-gigaword-50')\n",
      "180/77: centroid_we_summarizer = text_summarizer.CentroidWordEmbeddingsSummarizer(embedding_model, sim_threshold=.99, preprocess_type='nltk')\n",
      "180/78:\n",
      "def getcwe(category):\n",
      "    s = []\n",
      "    for i in category:\n",
      "        try:\n",
      "            s.append(centroid_we_summarizer.summarize(i.text, limit=200-20))\n",
      "        except:\n",
      "            print(i)\n",
      "    return s\n",
      "180/79:\n",
      "t = time.process_time()\n",
      "cwe_pol = getcwe(politics)\n",
      "time.process_time() - t\n",
      "180/80:\n",
      "t = time.process_time()\n",
      "cwe_comp = getcwe(computers)\n",
      "time.process_time() - t\n",
      "180/81:\n",
      "t = time.process_time()\n",
      "cwe_trav = getcwe(travel)\n",
      "time.process_time() - t\n",
      "180/82:\n",
      "t = time.process_time()\n",
      "cwe_bks = getcwe(books)\n",
      "time.process_time() - t\n",
      "180/83:\n",
      "t = time.process_time()\n",
      "cwe_mov = getcwe(movies)\n",
      "time.process_time() - t\n",
      "180/84:\n",
      "t = time.process_time()\n",
      "cwe_fball = getcwe(fball)\n",
      "time.process_time() - t\n",
      "180/85: np.mean([len(nltk.word_tokenize(getcwe(politics[:100])[i])) for i in range(100)])\n",
      "180/86: np.mean([len(nltk.word_tokenize(getbows(politics[:100])[i])) for i in range(100)])\n",
      "180/87: np.mean([len(nltk.word_tokenize(getsummaries(politics[:100])[0][i])) for i in range(100)])\n",
      "180/88: np.mean([len(nltk.word_tokenize(getctrls(politics[:100])[i])) for i in range(100)])\n",
      "180/89:\n",
      "# url = 'https://www.enchantedlearning.com/wordlist/adjectivesforpeople.shtml'\n",
      "# file = urllib2.urlopen(url)\n",
      "# html = file.read()\n",
      "# file.close()\n",
      "# soup = BeautifulSoup(html)\n",
      "180/90:\n",
      "def getctrls(category):\n",
      "    ctrls = []\n",
      "    for a in category:\n",
      "        sents = nltk.sent_tokenize(a.text)\n",
      "        rs = [i for i in range(len(sents))]\n",
      "        shuffle(rs)\n",
      "        ctrl = []\n",
      "        count = 0\n",
      "        for r in rs:\n",
      "            if count > 200-5:\n",
      "                break\n",
      "            count += len(nltk.word_tokenize(sents[r]))\n",
      "            ctrl.append(sents[r])\n",
      "        summary = \"\\n\".join([s for s in ctrl])\n",
      "        ctrls.append(summary)\n",
      "    return ctrls\n",
      "180/91: np.mean([len(nltk.word_tokenize(getctrls(politics[:100])[i])) for i in range(100)])\n",
      "180/92:\n",
      "t = time.process_time()\n",
      "ctrls_pol = getctrls(politics)\n",
      "time.process_time() - t\n",
      "180/93:\n",
      "t = time.process_time()\n",
      "ctrls_comp = getctrls(computers)\n",
      "time.process_time() - t\n",
      "180/94:\n",
      "t = time.process_time()\n",
      "ctrls_trav = getctrls(travel)\n",
      "time.process_time() - t\n",
      "180/95:\n",
      "t = time.process_time()\n",
      "ctrls_bks = getctrls(books)\n",
      "time.process_time() - t\n",
      "180/96:\n",
      "t = time.process_time()\n",
      "ctrls_mov = getctrls(movies)\n",
      "time.process_time() - t\n",
      "180/97:\n",
      "t = time.process_time()\n",
      "ctrls_fball = getctrls(fball)\n",
      "time.process_time() - t\n",
      "180/98:\n",
      "def getw2v_s(sums):\n",
      "    sentences = []\n",
      "    for t in sums:\n",
      "        sentences += nltk.sent_tokenize(t)\n",
      "    docs = [simple_preprocess(s) for s in sentences]\n",
      "    return Word2Vec(docs)\n",
      "180/99: mpol_s = getw2v_s(summaries_pol)\n",
      "180/100: mcomp_s = getw2v_s(summaries_comp)\n",
      "180/101: mtrav_s = getw2v_s(summaries_trav)\n",
      "180/102: mbks_s = getw2v_s(summaries_bks)\n",
      "180/103: mmov_s = getw2v_s(summaries_mov)\n",
      "180/104: mfball_s = getw2v_s(summaries_fball)\n",
      "180/105:\n",
      "mpol_sb = getw2v_s(bow_pol)\n",
      "mcomp_sb = getw2v_s(bow_comp)\n",
      "mtrav_sb = getw2v_s(bow_trav)\n",
      "mbks_sb = getw2v_s(bow_bks)\n",
      "mmov_sb = getw2v_s(bow_mov)\n",
      "mfball_sb = getw2v_s(bow_fball)\n",
      "180/106:\n",
      "mpol_sc = getw2v_s(cwe_pol)\n",
      "mcomp_sc = getw2v_s(cwe_comp)\n",
      "mtrav_sc = getw2v_s(cwe_trav)\n",
      "mbks_sc = getw2v_s(cwe_bks)\n",
      "mmov_sc = getw2v_s(cwe_mov)\n",
      "mfball_sc = getw2v_s(cwe_fball)\n",
      "180/107:\n",
      "mpol_c = getw2v_s(ctrls_pol)\n",
      "mcomp_c = getw2v_s(ctrls_comp)\n",
      "mtrav_c = getw2v_s(ctrls_trav)\n",
      "mbks_c = getw2v_s(ctrls_bks)\n",
      "mmov_c = getw2v_s(ctrls_mov)\n",
      "mfball_c = getw2v_s(ctrls_fball)\n",
      "180/108: dill.dump_session('thesis_env2.db')\n",
      "180/109:\n",
      "def getg(m):\n",
      "    matrix = []\n",
      "    for a, b in pairs:\n",
      "        if a not in m.wv.vocab or b not in m.wv.vocab:\n",
      "            continue\n",
      "        center = (m.wv.get_vector(a) + m.wv.get_vector(b))/2\n",
      "        matrix.append(m.wv.get_vector(a) - center)\n",
      "        matrix.append(m.wv.get_vector(b) - center)\n",
      "    matrix = np.array(matrix)\n",
      "    p = PCA(n_components = 10)\n",
      "    p.fit(matrix)\n",
      "    plt.bar(range(10), p.explained_variance_ratio_)\n",
      "    print(matrix.shape)\n",
      "    return p.components_[0]\n",
      "180/110: gpol = getg(politics)\n",
      "180/111:\n",
      "with open('debiaswe-master/data/equalize_pairs.json') as f:\n",
      "    pairs = json.loads(f.read())\n",
      "pairs = list(map(lambda x: [x[0].lower(), x[1].lower()], pairs))\n",
      "pairs[:5]\n",
      "180/112:\n",
      "def getg(m):\n",
      "    matrix = []\n",
      "    for a, b in pairs:\n",
      "        if a not in m.wv.vocab or b not in m.wv.vocab:\n",
      "            continue\n",
      "        center = (m.wv.get_vector(a) + m.wv.get_vector(b))/2\n",
      "        matrix.append(m.wv.get_vector(a) - center)\n",
      "        matrix.append(m.wv.get_vector(b) - center)\n",
      "    matrix = np.array(matrix)\n",
      "    p = PCA(n_components = 10)\n",
      "    p.fit(matrix)\n",
      "    plt.bar(range(10), p.explained_variance_ratio_)\n",
      "    print(matrix.shape)\n",
      "    return p.components_[0]\n",
      "180/113: gpol = getg(politics)\n",
      "180/114: gpol = getg(mpol)\n",
      "180/115:\n",
      "with open('debiaswe-master/data/equalize_pairs.json') as f:\n",
      "    pairs = json.loads(f.read())\n",
      "pairs = list(map(lambda x: [x[0].lower(), x[1].lower()], pairs))\n",
      "pairs[:10]\n",
      "180/116:\n",
      "with open('debiaswe-master/data/equalize_pairs.json') as f:\n",
      "    pairs = json.loads(f.read())\n",
      "pairs = list(map(lambda x: [x[0].lower(), x[1].lower()], pairs))\n",
      "pairs[:15]\n",
      "180/117: adjectives[:15]\n",
      "180/118: adjectives[:10]\n",
      "180/119: adjectives[:12]\n",
      "180/120: professions[:12]\n",
      "180/121:\n",
      "with open('debiaswe-master/data/professions.json') as f:\n",
      "    profs = json.loads(f.read())\n",
      "proflist = list(filter(lambda x: x in mpol.wv.vocab, [p[0] for p in profs if abs(p[1]) < 0.6]))\n",
      "180/122: proflist[:10]\n",
      "180/123: proflist[:12]\n",
      "180/124: cities[:12]\n",
      "180/125:\n",
      "url = 'http://www.manythings.org/vocabulary/lists/l/words.php?f=ogden-picturable'\n",
      "file = urllib2.urlopen(url)\n",
      "html = file.read()\n",
      "file.close()\n",
      "soup = BeautifulSoup(html)\n",
      "180/126:\n",
      "with open('objects.csv') as csv_file:\n",
      "    objects = csv_file.read().split(\",\")\n",
      "180/127:\n",
      "with open('objects.csv') as csv_file:\n",
      "    objects = csv_file.read().split(\",\")\n",
      "180/128:\n",
      "with open('objects.csv') as csv_file:\n",
      "    objects = csv_file.read().split(\",\")\n",
      "180/129: objects\n",
      "180/130:\n",
      "with open('objects.csv') as csv_file:\n",
      "    objects = csv_file.read().split(\"\\n\")\n",
      "180/131: objects\n",
      "180/132:\n",
      "with open('objects.csv') as csv_file:\n",
      "    objects = csv_file.read().split(\"\\n\")\n",
      "objects[0] = 'angle'\n",
      "180/133: objects[:10]\n",
      "180/134: objects[:10], len(objects)\n",
      "180/135:\n",
      "with open('debiaswe-master/data/gender_specific_seed.json') as f:\n",
      "    gender_specific = json.loads(f.read())\n",
      "180/136: # todo\n",
      "180/137:\n",
      "with open('debiaswe-master/data/gender_specific_seed.json') as f:\n",
      "    gender_specific = json.loads(f.read())\n",
      "gender_specific[:10]\n",
      "180/138:\n",
      "with open('debiaswe-master/data/gender_specific_seed.json') as f:\n",
      "    gender_specific = json.loads(f.read())\n",
      "gender_specific[:15]\n",
      "180/139:\n",
      "with open('debiaswe-master/data/gender_specific_seed.json') as f:\n",
      "    gender_specific = json.loads(f.read())\n",
      "180/140:\n",
      "pca_r = PCA(n_components = 10)\n",
      "pca_r.fit(np.random.rand(70,100))\n",
      "plt.bar(range(10), pca_r.explained_variance_ratio_)\n",
      "plt.title(\"Explained Variance Ratio vs Principal Component Number\")\n",
      "180/141:\n",
      "pca_r = PCA(n_components = 10)\n",
      "pca_r.fit(np.random.rand(70,100))\n",
      "plt.bar(range(10), pca_r.explained_variance_ratio_)\n",
      "plt.title(\"Explained Variance Ratio vs Principal Component Number (Random)\")\n",
      "180/142:\n",
      "with open('debiaswe-master/data/equalize_pairs.json') as f:\n",
      "    pairs = json.loads(f.read())\n",
      "pairs = list(map(lambda x: [x[0].lower(), x[1].lower()], pairs))\n",
      "180/143:\n",
      "with open('debiaswe-master/data/gender_specific_full.json') as f:\n",
      "    gender_specific_full = json.loads(f.read())\n",
      "180/144:\n",
      "def getg(m, t):\n",
      "    matrix = []\n",
      "    for a, b in pairs:\n",
      "        if a not in m.wv.vocab or b not in m.wv.vocab:\n",
      "            continue\n",
      "        center = (m.wv.get_vector(a) + m.wv.get_vector(b))/2\n",
      "        matrix.append(m.wv.get_vector(a) - center)\n",
      "        matrix.append(m.wv.get_vector(b) - center)\n",
      "    matrix = np.array(matrix)\n",
      "    p = PCA(n_components = 10)\n",
      "    p.fit(matrix)\n",
      "    plt.bar(range(10), p.explained_variance_ratio_)\n",
      "    plt.title(\"Explained Variance Ratio vs Principal Component Number (\" + t + \")\")\n",
      "    print(matrix.shape)\n",
      "    return p.components_[0]\n",
      "180/145: gpol = getg(mpol, \"Politics\")\n",
      "180/146:\n",
      "def getg(m, t):\n",
      "    matrix = []\n",
      "    for a, b in pairs:\n",
      "        if a not in m.wv.vocab or b not in m.wv.vocab:\n",
      "            continue\n",
      "        center = (m.wv.get_vector(a) + m.wv.get_vector(b))/2\n",
      "        matrix.append(m.wv.get_vector(a) - center)\n",
      "        matrix.append(m.wv.get_vector(b) - center)\n",
      "    matrix = np.array(matrix)\n",
      "    p = PCA(n_components = 10)\n",
      "    p.fit(matrix)\n",
      "    plt.bar(range(10), p.explained_variance_ratio_)\n",
      "    plt.title(\"Explained Variance Ratio vs Principal Component Number (\" + t + \")\")\n",
      "    return p.components_[0]\n",
      "180/147: gpol = getg(mpol, \"Politics\")\n",
      "180/148:\n",
      "def getg(m, t):\n",
      "    matrix = []\n",
      "    for a, b in pairs:\n",
      "        if a not in m.wv.vocab or b not in m.wv.vocab:\n",
      "            continue\n",
      "        center = (m.wv.get_vector(a) + m.wv.get_vector(b))/2\n",
      "        matrix.append(m.wv.get_vector(a) - center)\n",
      "        matrix.append(m.wv.get_vector(b) - center)\n",
      "    matrix = np.array(matrix)\n",
      "    p = PCA(n_components = 10)\n",
      "    p.fit(matrix)\n",
      "    plt.bar(range(10), p.explained_variance_ratio_)\n",
      "    plt.title(\"Explained Variance Ratio vs Principal Component Number (\" + t + \")\")\n",
      "    return p.components_[0]\n",
      "180/149: gpol = getg(mpol, \"Politics\")\n",
      "180/150:\n",
      "def getg(m, t):\n",
      "    matrix = []\n",
      "    for a, b in pairs:\n",
      "        if a not in m.wv.vocab or b not in m.wv.vocab:\n",
      "            continue\n",
      "        center = (m.wv.get_vector(a) + m.wv.get_vector(b))/2\n",
      "        matrix.append(m.wv.get_vector(a) - center)\n",
      "        matrix.append(m.wv.get_vector(b) - center)\n",
      "    matrix = np.array(matrix)\n",
      "    p = PCA(n_components = 10)\n",
      "    p.fit(matrix)\n",
      "    plt.bar(range(10), p.explained_variance_ratio_)\n",
      "    plt.title(\"Explained Variance Ratio vs Principal Component Number (\" + t + \")\")\n",
      "    return p.components_[0]\n",
      "180/151: gpol = getg(mpol, \"Politics\")\n",
      "180/152: gpol_s = getg(mpol_s, \"Politics Summary (TextRank)\")\n",
      "180/153: gpol = getg(mpol, \"Gender, Politics\")\n",
      "180/154: gpol_s = getg(mpol_s, \"Gender, Politics Summary (TextRank)\")\n",
      "180/155: gpol_s = getg(mpol_s, \"Gender, Politics Summary (Centroid-BOW)\")\n",
      "180/156: gpol_sc = getg(mpol_sc, \"Gender, Politics Summary (Centroid-BOW)\")\n",
      "180/157: gpol_s = getg(mpol_c, \"Gender, Politics Summary (Centroid-BOW)\")\n",
      "180/158: gpol_c = getg(mpol_c, \"Gender, Politics Summary (Centroid-BOW)\")\n",
      "180/159: gpol_s = getg(mpol_s, \"Gender, Politics Summary (TextRank)\")\n",
      "180/160: gpol_c = getg(mpol_c, \"Gender, Politics Summary (Control)\")\n",
      "180/161: gcomp = getg(mcomp, \"Gender, Computers\")\n",
      "180/162:\n",
      "def getg(m, t):\n",
      "    matrix = []\n",
      "    for a, b in pairs:\n",
      "        if a not in m.wv.vocab or b not in m.wv.vocab:\n",
      "            continue\n",
      "        center = (m.wv.get_vector(a) + m.wv.get_vector(b))/2\n",
      "        matrix.append(m.wv.get_vector(a) - center)\n",
      "        matrix.append(m.wv.get_vector(b) - center)\n",
      "    matrix = np.array(matrix)\n",
      "    p = PCA(n_components = 10)\n",
      "    p.fit(matrix)\n",
      "    plt.bar(range(10), p.explained_variance_ratio_)\n",
      "    plt.title(\"Explained Variance vs Component Number (\" + t + \")\")\n",
      "    return p.components_[0]\n",
      "180/163: gpol = getg(mpol, \"Gender, Politics\")\n",
      "180/164: gcomp = getg(mcomp, \"Gender, Computers\")\n",
      "180/165:\n",
      "pca_r = PCA(n_components = 10)\n",
      "pca_r.fit(np.random.rand(70,100))\n",
      "plt.bar(range(10), pca_r.explained_variance_ratio_)\n",
      "plt.title(\"Explained Variance vs Component Number (Random)\")\n",
      "180/166: gpol_s = getg(mpol_s, \"Gender, Politics Summary (TextRank)\")\n",
      "180/167: gcomp_s = getg(mcomp_s, \"Gender, Computers Summary (TextRank)\")\n",
      "180/168: gcomp_sc = getg(mcomp_sc, \"Gender, Computers Summary (Centroid-)\")\n",
      "180/169: gcomp_c = getg(mcomp_c, \"Gender, Politics Summary (Control)\")\n",
      "180/170: gcomp_c = getg(mcomp_c, \"Gender, Computers Summary (Control)\")\n",
      "180/171: gpol_sc = getg(mpol_sc, \"Gender, Politics Summary (Centroid-)\")\n",
      "180/172: gpol_c = getg(mpol_c, \"Gender, Politics Summary (Control)\")\n",
      "180/173: gtrav = getg(mtrav, \"Gender, Travel\")\n",
      "180/174: gtrav_s = getg(mtrav_s, \"Gender, Travel Summary (TextRank)\")\n",
      "180/175: gpol_sc = getg(mpol_sc, \"Gender, Politics Summary (Centroid-WE)\")\n",
      "180/176:\n",
      "gcomp_sc = getg(mcomp_sc, \"Gender, Computers Summary (Centroid-WE)\")\n",
      "gtrav_sc = getg(mtrav_sc, \"Gender, Travel Summary (Centroid-WE)\")\n",
      "180/177: gcomp_sc = getg(mcomp_sc, \"Gender, Computers Summary (Centroid-WE)\")\n",
      "180/178: gtrav_sc = getg(mtrav_sc, \"Gender, Travel Summary (Centroid-WE)\")\n",
      "180/179:\n",
      "def getg(m, t, plot):\n",
      "    matrix = []\n",
      "    for a, b in pairs:\n",
      "        if a not in m.wv.vocab or b not in m.wv.vocab:\n",
      "            continue\n",
      "        center = (m.wv.get_vector(a) + m.wv.get_vector(b))/2\n",
      "        matrix.append(m.wv.get_vector(a) - center)\n",
      "        matrix.append(m.wv.get_vector(b) - center)\n",
      "    matrix = np.array(matrix)\n",
      "    p = PCA(n_components = 10)\n",
      "    p.fit(matrix)\n",
      "    if plot:\n",
      "        plt.bar(range(10), p.explained_variance_ratio_)\n",
      "        plt.title(\"Explained Variance vs Component Number (\" + t + \")\")\n",
      "    return p.components_[0]\n",
      "180/180: gpol = getg(mpol, \"Gender, Politics\", True)\n",
      "180/181: gtrav = getg(mtrav, \"Gender, Travel\", False)\n",
      "180/182: gpol = getg(mpol, \"Gender Pairs - Politics\", True)\n",
      "180/183: len(pairs)\n",
      "182/1:\n",
      "import os\n",
      "import time\n",
      "import dill\n",
      "from typing import List\n",
      "from functools import reduce\n",
      "from calendar import monthrange\n",
      "182/2:\n",
      "import json\n",
      "import csv\n",
      "import requests\n",
      "import xml.etree.ElementTree as ET\n",
      "from bs4 import BeautifulSoup\n",
      "import urllib.request as urllib2\n",
      "182/3:\n",
      "import matplotlib.pyplot as plt\n",
      "import numpy as np\n",
      "from scipy import stats\n",
      "from random import sample, randint, shuffle\n",
      "from sklearn.decomposition import PCA\n",
      "182/4:\n",
      "import nltk\n",
      "import text_summarizer\n",
      "from gensim.utils import simple_preprocess\n",
      "from gensim.summarization.summarizer import summarize\n",
      "from gensim.models import Word2Vec, LsiModel\n",
      "from gensim.test.utils import common_dictionary, common_corpus\n",
      "182/5: from time import process_time\n",
      "182/6:\n",
      "t = process_time()\n",
      "dill.load_session('thesis_env2.db')\n",
      "process_time() - t\n",
      "182/7:\n",
      "with open('objects.csv') as csv_file:\n",
      "    objects = csv_file.read().split(\"\\n\")\n",
      "objects[0] = 'angle'\n",
      "182/8: len(adjectives), len(cities), len(objects)\n",
      "182/9: len(all_)\n",
      "182/10: len(politics), len(computers), len(travel), len(movies), len(books), len(fball)\n",
      "182/11: mpol_s\n",
      "182/12: # mpol, mcomp, mtrav, mmov, mbooks, mfball\n",
      "182/13:\n",
      "# summaries_pol, summaries_comp, summaries_trav, \n",
      "# summaries_mov, summaries_bks, summaries_fball\n",
      "182/14:\n",
      "# bow_pol, bow_comp, bow_trav, bow_mov, bow_bks, bow_fball\n",
      "# cwe_pol, cwe_comp, cwe_trav, cwe_mov, cwe_bks, cwe_fball\n",
      "# ctrls_pol, ctrls_comp, ctrls_trav, ctrls_mov, ctrls_bks, ctrls_fball\n",
      "182/15: # mpol_s, mcomp_s, mtrav_s, mbks_s, mmov_s, mfball_s\n",
      "182/16: # mpol_sb, mcomp_sb, mtrav_sb, mbks_sb, mmov_sb, mfball_sb\n",
      "182/17: # mpol_sc, mcomp_sc, mtrav_sc, mbks_sc, mmov_sc, mfball_sc\n",
      "182/18: # mpol_c, mcomp_c, mtrav_c, mbks_c, mmov_c, mfball_c\n",
      "182/19:\n",
      "with open('debiaswe-master/data/equalize_pairs.json') as f:\n",
      "    pairs = json.loads(f.read())\n",
      "pairs = list(map(lambda x: [x[0].lower(), x[1].lower()], pairs))\n",
      "182/20:\n",
      "with open('debiaswe-master/data/professions.json') as f:\n",
      "    profs = json.loads(f.read())\n",
      "proflist = list(filter(lambda x: x in mpol.wv.vocab, [p[0] for p in profs if abs(p[1]) < 0.6]))\n",
      "182/21:\n",
      "with open('debiaswe-master/data/gender_specific_seed.json') as f:\n",
      "    gender_specific = json.loads(f.read())\n",
      "182/22:\n",
      "with open('debiaswe-master/data/gender_specific_full.json') as f:\n",
      "    gender_specific_full = json.loads(f.read())\n",
      "182/23:\n",
      "wordlist = ['he', 'she', 'doctor', 'nurse', 'man', 'woman', 'male', 'female', 'politician', 'guy', \\\n",
      "            'scientist', 'teacher', 'professor', 'mother', 'pretty', 'handsome', 'strong', 'coward', \\\n",
      "            'timid', 'leadership', 'blond', 'lovely', 'petite', 'surgeon', 'physician', 'shopkeeper', \\\n",
      "            'cooking', 'sewing', 'chuckle', 'sassy', 'lanky', 'competent', 'brilliant', 'capable', 'smart', \\\n",
      "            'intelligent', 'incompetent', 'librarian', 'cosmetics', 'maid', 'clever', 'dressed', 'captain', \\\n",
      "            'nanny', 'philosopher', 'architect', 'financier', 'warrior', 'magician', 'housekeeper', 'homemaker']\n",
      "182/24: len(adjectives), len(cities), len(objects), len(proflist), len(wordlist)\n",
      "182/25:\n",
      "pca_r = PCA(n_components = 10)\n",
      "pca_r.fit(np.random.rand(70,100))\n",
      "plt.bar(range(10), pca_r.explained_variance_ratio_)\n",
      "plt.title(\"Explained Variance vs Component Number (Random Data)\")\n",
      "182/26:\n",
      "pca_r = PCA(n_components = 10)\n",
      "pca_r.fit(np.random.rand(70,100))\n",
      "plt.bar(range(10), pca_r.explained_variance_ratio_)\n",
      "plt.title(\"Explained Variance vs Component Number (Randomly Generated Data)\")\n",
      "182/27:\n",
      "pca_r = PCA(n_components = 10)\n",
      "pca_r.fit(np.random.rand(70,100))\n",
      "plt.bar(range(10), pca_r.explained_variance_ratio_)\n",
      "plt.title(\"Explained Variance vs Component Number (Randomly Generated Data)\")\n",
      "182/28:\n",
      "pca_r = PCA(n_components = 10)\n",
      "pca_r.fit(np.random.rand(70,100))\n",
      "plt.bar(range(10), pca_r.explained_variance_ratio_)\n",
      "plt.title(\"Explained Variance vs Component Number (Randomly Generated Data)\")\n",
      "182/29: # variables:\n",
      "182/30:\n",
      "vocab = mpol.wv.vocab & mcomp.wv.vocab & mtrav.wv.vocab & mmov.wv.vocab \\\n",
      "& mbooks.wv.vocab & mfball.wv.vocab\n",
      "182/31: mpol.wv.vocab\n",
      "182/32: set(mpol.wv.vocab)\n",
      "182/33:\n",
      "vocab = set(mpol.wv.vocab)\n",
      "for m in [mpol, mcomp, mtrav, mmov, mbooks, mfball]:\n",
      "    vocab &= set(m.wv.vocab)\n",
      "len(vocab)\n",
      "182/34: len(vocab) & set(professions)\n",
      "182/35: len(vocab) & set(proflist)\n",
      "182/36: len(vocab & set(proflist))\n",
      "182/37: len(vocab & set(cities))\n",
      "182/38: \"Hi\".lower()\n",
      "182/39: cities = [x.lower() for x in cities]\n",
      "182/40: len(vocab & set(cities))\n",
      "182/41: len(vocab & set(wordlist))\n",
      "182/42:\n",
      "vocab2 = set(vocab)\n",
      "for m in [mpol_s, mcomp_s, mtrav_s, mbks_s, mmov_s, mfball_s] + \\\n",
      "[mpol_sb, mcomp_sb, mtrav_sb, mbks_sb, mmov_sb, mfball_sb] + \\\n",
      "[mpol_sc, mcomp_sc, mtrav_sc, mbks_sc, mmov_sc, mfball_sc] + \\\n",
      "[mpol_c, mcomp_c, mtrav_c, mbks_c, mmov_c, mfball_c]:\n",
      "    vocab2 *= set(m.wv.vocab)\n",
      "len(vocab2)\n",
      "182/43:\n",
      "vocab2 = set(vocab)\n",
      "for m in [mpol_s, mcomp_s, mtrav_s, mbks_s, mmov_s, mfball_s] + \\\n",
      "[mpol_sb, mcomp_sb, mtrav_sb, mbks_sb, mmov_sb, mfball_sb] + \\\n",
      "[mpol_sc, mcomp_sc, mtrav_sc, mbks_sc, mmov_sc, mfball_sc] + \\\n",
      "[mpol_c, mcomp_c, mtrav_c, mbks_c, mmov_c, mfball_c]:\n",
      "    vocab2 &= set(m.wv.vocab)\n",
      "len(vocab2)\n",
      "182/44: len(vocab2 & set(wordlist))\n",
      "182/45: vocab2 & set(wordlist)\n",
      "182/46: len(vocab2 & set(proflist))\n",
      "182/47: len(vocab2 & set(adjectives))\n",
      "182/48: len(vocab2 & set(cities))\n",
      "182/49: len(vocab2 & set(objects))\n",
      "182/50: len(vocab2 & set(proflist))\n",
      "182/51: vocab2 & set(proflist)\n",
      "182/52: vocab & set(proflist)\n",
      "182/53: vocab2 & set(proflist)\n",
      "182/54: len(vocab2 & set(proflist))\n",
      "182/55:\n",
      "vocab2 = set(vocab)\n",
      "for m in [mpol_s, mcomp_s, mtrav_s, mbks_s, mmov_s, mfball_s] '''+ \\\n",
      "[mpol_sb, mcomp_sb, mtrav_sb, mbks_sb, mmov_sb, mfball_sb] + \\\n",
      "[mpol_sc, mcomp_sc, mtrav_sc, mbks_sc, mmov_sc, mfball_sc] + \\\n",
      "[mpol_c, mcomp_c, mtrav_c, mbks_c, mmov_c, mfball_c]''':\n",
      "    vocab2 &= set(m.wv.vocab)\n",
      "len(vocab2)\n",
      "182/56:\n",
      "vocab2 = set(vocab)\n",
      "for m in [mpol_s, mcomp_s, mtrav_s, mbks_s, mmov_s, mfball_s]: '''+ \\\n",
      "[mpol_sb, mcomp_sb, mtrav_sb, mbks_sb, mmov_sb, mfball_sb] + \\\n",
      "[mpol_sc, mcomp_sc, mtrav_sc, mbks_sc, mmov_sc, mfball_sc] + \\\n",
      "[mpol_c, mcomp_c, mtrav_c, mbks_c, mmov_c, mfball_c]:'''\n",
      "    vocab2 &= set(m.wv.vocab)\n",
      "len(vocab2)\n",
      "182/57:\n",
      "vocab2 = set(vocab)\n",
      "for m in [mpol_s, mcomp_s, mtrav_s, mbks_s, mmov_s, mfball_s]:\n",
      "    vocab2 &= set(m.wv.vocab)\n",
      "len(vocab2)\n",
      "182/58: len(vocab2 & set(proflist))\n",
      "182/59:\n",
      "vocab2 = set(vocab)\n",
      "for m in [mpol_s, mcomp_s, mtrav_s, mbks_s, mmov_s, mfball_s] + \\\n",
      "[mpol_sb, mcomp_sb, mtrav_sb, mbks_sb, mmov_sb, mfball_sb] + \\\n",
      "[mpol_sc, mcomp_sc, mtrav_sc, mbks_sc, mmov_sc, mfball_sc] + \\\n",
      "[mpol_c, mcomp_c, mtrav_c, mbks_c, mmov_c, mfball_c]:\n",
      "    vocab2 &= set(m.wv.vocab)\n",
      "len(vocab2)\n",
      "182/60: len(vocab2 & set(proflist))\n",
      "182/61: len(proflist)\n",
      "182/62: len(vocab & set(proflist))\n",
      "182/63:\n",
      "vocab_s = set(vocab)\n",
      "for m in [mpol_s, mcomp_s, mtrav_s, mbks_s, mmov_s, mfball_s]:\n",
      "    vocab_s &= set(m.wv.vocab)\n",
      "len(vocab_s)\n",
      "182/64: len(vocab_s & set(proflist))\n",
      "182/65:\n",
      "vocab_sb = set(vocab)\n",
      "for m in [mpol_sb, mcomp_sb, mtrav_sb, mbks_sb, mmov_sb, mfball_sb]:\n",
      "    vocab_sb &= set(m.wv.vocab)\n",
      "len(vocab_sb)\n",
      "182/66: len(vocab_sb & set(proflist))\n",
      "182/67:\n",
      "vocab_sc = set(vocab)\n",
      "for m in [mpol_sc, mcomp_sc, mtrav_sc, mbks_sc, mmov_sc, mfball_sc]:\n",
      "    vocab_sc &= set(m.wv.vocab)\n",
      "len(vocab_sc)\n",
      "182/68: len(vocab_sc & set(proflist))\n",
      "182/69:\n",
      "vocab_c = set(vocab)\n",
      "for m in [mpol_c, mcomp_c, mtrav_c, mbks_c, mmov_c, mfball_c]:\n",
      "    vocab_c &= set(m.wv.vocab)\n",
      "len(vocab_c)\n",
      "182/70: len(vocab_c & set(proflist))\n",
      "182/71:\n",
      "vocab_all = set(vocab)\n",
      "for m in [mpol_s, mcomp_s, mtrav_s, mbks_s, mmov_s, mfball_s] + \\\n",
      "[mpol_sb, mcomp_sb, mtrav_sb, mbks_sb, mmov_sb, mfball_sb] + \\\n",
      "[mpol_sc, mcomp_sc, mtrav_sc, mbks_sc, mmov_sc, mfball_sc] + \\\n",
      "[mpol_c, mcomp_c, mtrav_c, mbks_c, mmov_c, mfball_c]:\n",
      "    vocab_all &= set(m.wv.vocab)\n",
      "len(vocab_all)\n",
      "182/72: len(vocab_c & set(proflist)), len(vocab_all & set(proflist))\n",
      "182/73:\n",
      "len(vocab_c & set(proflist)), \n",
      "len(vocab_all & set(proflist))\n",
      "182/74: len(vocab_c & set(proflist)), len(vocab_all & set(proflist))\n",
      "182/75:\n",
      "def getg(m):\n",
      "    matrix = []\n",
      "    for a, b in pairs:\n",
      "        if a not in m.wv.vocab or b not in m.wv.vocab:\n",
      "            continue\n",
      "        center = (m.wv.get_vector(a) + m.wv.get_vector(b))/2\n",
      "        matrix.append(m.wv.get_vector(a) - center)\n",
      "        matrix.append(m.wv.get_vector(b) - center)\n",
      "    matrix = np.array(matrix)\n",
      "    p = PCA(n_components = 10)\n",
      "    p.fit(matrix)\n",
      "#     plt.bar(range(10), p.explained_variance_ratio_)\n",
      "#     plt.title(\"Explained Variance vs Component Number (\" + t + \")\")\n",
      "    return p.components_[0]\n",
      "182/76: gdict = {\"politics\": [], \"computers\": [], \"movies\": [], \"books\": [], \"football\": []}\n",
      "182/77:\n",
      "gdict = {\"politics\": [], \"computers\": [], \"movies\": [], \"books\": [], \"football\": []}\n",
      "labels = [\"original\", \"summary (TextRank)\", \"summary (Centroid-BOW)\", \"summary (Centroid-WE)\", \"summary (control)\"]\n",
      "182/78: list(gdict)\n",
      "182/79:\n",
      "gdict = {\"politics\": [], \"computers\": [], \"travel\": [], \"books\": [], \"movies\": [], \"football\": []}\n",
      "labels = [\"original\", \"summary (TextRank)\", \"summary (Centroid-BOW)\", \"summary (Centroid-WE)\", \"summary (control)\"]\n",
      "182/80: list(gdict)\n",
      "182/81:\n",
      "def gdictadd(models):\n",
      "    for i in range(6):\n",
      "        gdict[list(gdict)[i]].append(getg(models[i]))\n",
      "182/82:\n",
      "gdictadd([mpol, mcomp, mtrav, mmov, mbooks, mfball])\n",
      "gdictadd([mpol_s, mcomp_s, mtrav_s, mbks_s, mmov_s, mfball_s])\n",
      "gdictadd([mpol_sb, mcomp_sb, mtrav_sb, mbks_sb, mmov_sb, mfball_sb])\n",
      "gdictadd([mpol_sc, mcomp_sc, mtrav_sc, mbks_sc, mmov_sc, mfball_sc])\n",
      "gdictadd([mpol_c, mcomp_c, mtrav_c, mbks_c, mmov_c, mfball_c])\n",
      "182/83: gdict\n",
      "182/84:\n",
      "for i in gdict:\n",
      "    print(i)\n",
      "182/85:\n",
      "def gproj(g, m, w1):\n",
      "    wvec = m.wv.get_vector(w1)\n",
      "    p = np.dot(g, wvec)/(np.linalg.norm(g)*np.linalg.norm(wvec))\n",
      "    return p\n",
      "182/86:\n",
      "def bias(g, m, wordlist):\n",
      "    b = 0\n",
      "    l = list(filter(lambda x: x in vocab, wordlist))\n",
      "    for x in l:\n",
      "        b += abs(gproj(g, m, x))\n",
      "    return b/len(l)\n",
      "182/87: mdict = {\"politics\": [], \"computers\": [], \"travel\": [], \"books\": [], \"movies\": [], \"football\": []}\n",
      "182/88:\n",
      "def mdictadd(models):\n",
      "    for i in range(6):\n",
      "        mdict[list(mdict)[i]].append(models[i])\n",
      "182/89:\n",
      "mdictadd([mpol, mcomp, mtrav, mmov, mbooks, mfball])\n",
      "mdictadd([mpol_s, mcomp_s, mtrav_s, mbks_s, mmov_s, mfball_s])\n",
      "mdictadd([mpol_sb, mcomp_sb, mtrav_sb, mbks_sb, mmov_sb, mfball_sb])\n",
      "mdictadd([mpol_sc, mcomp_sc, mtrav_sc, mbks_sc, mmov_sc, mfball_sc])\n",
      "mdictadd([mpol_c, mcomp_c, mtrav_c, mbks_c, mmov_c, mfball_c])\n",
      "182/90:\n",
      "for i in gdict:\n",
      "    print(i, bias(gdict[i][0], mdict[i][0], proflist))\n",
      "182/91:\n",
      "for i in gdict:\n",
      "    print(i, bias(gdict[i][0], mdict[i][0], adjectives))\n",
      "182/92:\n",
      "for i in gdict:\n",
      "    print(i, bias(gdict[i][0], mdict[i][0], cities))\n",
      "182/93:\n",
      "for i in gdict:\n",
      "    print(i, bias(gdict[i][0], mdict[i][0], objects))\n",
      "182/94:\n",
      "for i in gdict:\n",
      "    print(i, bias(gdict[i][0], mdict[i][0], gender_specific))\n",
      "182/95:\n",
      "for i in gdict:\n",
      "    print(i, \"%.3f\" % bias(gdict[i][0], mdict[i][0], proflist))\n",
      "182/96:\n",
      "for i in gdict:\n",
      "    print(\"%10s\" % i, \"%.3f\" % bias(gdict[i][0], mdict[i][0], proflist))\n",
      "182/97:\n",
      "for i in gdict:\n",
      "    print(\"%10s\" % i, \\\n",
      "          \"%.3f\" % bias(gdict[i][0], mdict[i][0], proflist), \\\n",
      "          \"%.3f\" % bias(gdict[i][0], mdict[i][0], adjectives))\n",
      "182/98:\n",
      "for i in gdict:\n",
      "    print(\"%10s\" % i, \\\n",
      "          \"%.3f\" % bias(gdict[i][0], mdict[i][0], proflist), \\\n",
      "          \"%.3f\" % bias(gdict[i][0], mdict[i][0], adjectives), \\\n",
      "          \"%.3f\" % bias(gdict[i][0], mdict[i][0], cities), \\\n",
      "          \"%.3f\" % bias(gdict[i][0], mdict[i][0], objects), \\\n",
      "          \"%.3f\" % bias(gdict[i][0], mdict[i][0], gender_specific))\n",
      "182/99:\n",
      "print(labels[0])\n",
      "for i in gdict:\n",
      "    print(\"%10s\" % i, \\\n",
      "          \"%.3f\" % bias(gdict[i][0], mdict[i][0], proflist), \\\n",
      "          \"%.3f\" % bias(gdict[i][0], mdict[i][0], adjectives), \\\n",
      "          \"%.3f\" % bias(gdict[i][0], mdict[i][0], cities), \\\n",
      "          \"%.3f\" % bias(gdict[i][0], mdict[i][0], objects), \\\n",
      "          \"%.3f\" % bias(gdict[i][0], mdict[i][0], gender_specific))\n",
      "182/100:\n",
      "print(labels[1])\n",
      "for i in gdict:\n",
      "    print(\"%10s\" % i, \\\n",
      "          \"%.3f\" % bias(gdict[i][1], mdict[i][1], proflist), \\\n",
      "          \"%.3f\" % bias(gdict[i][1], mdict[i][1], adjectives), \\\n",
      "          \"%.3f\" % bias(gdict[i][1], mdict[i][1], cities), \\\n",
      "          \"%.3f\" % bias(gdict[i][1], mdict[i][1], objects), \\\n",
      "          \"%.3f\" % bias(gdict[i][1], mdict[i][1], gender_specific))\n",
      "182/101:\n",
      "def bias(g, m, vocab, wordlist):\n",
      "    b = 0\n",
      "    l = list(filter(lambda x: x in vocab, wordlist))\n",
      "    for x in l:\n",
      "        b += abs(gproj(g, m, x))\n",
      "    return b/len(l)\n",
      "182/102:\n",
      "print(labels[1])\n",
      "for i in gdict:\n",
      "    print(\"%10s\" % i, \\\n",
      "          \"%.3f\" % bias(gdict[i][1], mdict[i][1], vocab_s, proflist), \\\n",
      "          \"%.3f\" % bias(gdict[i][1], mdict[i][1], vocab_s, adjectives), \\\n",
      "          \"%.3f\" % bias(gdict[i][1], mdict[i][1], vocab_s, cities), \\\n",
      "          \"%.3f\" % bias(gdict[i][1], mdict[i][1], vocab_s, objects), \\\n",
      "          \"%.3f\" % bias(gdict[i][1], mdict[i][1], vocab_s, gender_specific))\n",
      "182/103:\n",
      "print(labels[2])\n",
      "for i in gdict:\n",
      "    print(\"%10s\" % i, \\\n",
      "          \"%.3f\" % bias(gdict[i][2], mdict[i][2], vocab_s, proflist), \\\n",
      "          \"%.3f\" % bias(gdict[i][2], mdict[i][2], vocab_s, adjectives), \\\n",
      "          \"%.3f\" % bias(gdict[i][2], mdict[i][2], vocab_s, cities), \\\n",
      "          \"%.3f\" % bias(gdict[i][2], mdict[i][2], vocab_s, objects), \\\n",
      "          \"%.3f\" % bias(gdict[i][2], mdict[i][2], vocab_s, gender_specific))\n",
      "182/104:\n",
      "print(labels[2])\n",
      "for i in gdict:\n",
      "    print(\"%10s\" % i, \\\n",
      "          \"%.3f\" % bias(gdict[i][2], mdict[i][2], vocab_sb, proflist), \\\n",
      "          \"%.3f\" % bias(gdict[i][2], mdict[i][2], vocab_sb, adjectives), \\\n",
      "          \"%.3f\" % bias(gdict[i][2], mdict[i][2], vocab_sb, cities), \\\n",
      "          \"%.3f\" % bias(gdict[i][2], mdict[i][2], vocab_sb, objects), \\\n",
      "          \"%.3f\" % bias(gdict[i][2], mdict[i][2], vocab_sb, gender_specific))\n",
      "182/105:\n",
      "print(labels[3])\n",
      "for i in gdict:\n",
      "    print(\"%10s\" % i, \\\n",
      "          \"%.3f\" % bias(gdict[i][3], mdict[i][3], vocab_sb, proflist), \\\n",
      "          \"%.3f\" % bias(gdict[i][3], mdict[i][3], vocab_sb, adjectives), \\\n",
      "          \"%.3f\" % bias(gdict[i][3], mdict[i][3], vocab_sb, cities), \\\n",
      "          \"%.3f\" % bias(gdict[i][3], mdict[i][3], vocab_sb, objects), \\\n",
      "          \"%.3f\" % bias(gdict[i][3], mdict[i][3], vocab_sb, gender_specific))\n",
      "182/106:\n",
      "print(labels[3])\n",
      "for i in gdict:\n",
      "    print(\"%10s\" % i, \\\n",
      "          \"%.3f\" % bias(gdict[i][3], mdict[i][3], vocab_sc, proflist), \\\n",
      "          \"%.3f\" % bias(gdict[i][3], mdict[i][3], vocab_sc, adjectives), \\\n",
      "          \"%.3f\" % bias(gdict[i][3], mdict[i][3], vocab_sc, cities), \\\n",
      "          \"%.3f\" % bias(gdict[i][3], mdict[i][3], vocab_sc, objects), \\\n",
      "          \"%.3f\" % bias(gdict[i][3], mdict[i][3], vocab_sc, gender_specific))\n",
      "182/107:\n",
      "print(labels[4])\n",
      "for i in gdict:\n",
      "    print(\"%10s\" % i, \\\n",
      "          \"%.3f\" % bias(gdict[i][4], mdict[i][4], vocab_c, proflist), \\\n",
      "          \"%.3f\" % bias(gdict[i][4], mdict[i][4], vocab_c, adjectives), \\\n",
      "          \"%.3f\" % bias(gdict[i][4], mdict[i][4], vocab_c, cities), \\\n",
      "          \"%.3f\" % bias(gdict[i][4], mdict[i][4], vocab_c, objects), \\\n",
      "          \"%.3f\" % bias(gdict[i][4], mdict[i][4], vocab_c, gender_specific))\n",
      "182/108:\n",
      "def getg(m):\n",
      "    matrix = []\n",
      "    for a, b in pairs:\n",
      "        if a not in m.wv.vocab or b not in m.wv.vocab:\n",
      "            continue\n",
      "        center = (m.wv.get_vector(a) + m.wv.get_vector(b))/2\n",
      "        matrix.append(m.wv.get_vector(a) - center)\n",
      "        matrix.append(m.wv.get_vector(b) - center)\n",
      "    matrix = np.array(matrix)\n",
      "    p = PCA(n_components = 10)\n",
      "    p.fit(matrix)\n",
      "    plt.bar(range(10), p.explained_variance_ratio_)\n",
      "    plt.title(\"Explained Variance vs Component Number (\" + t + \")\")\n",
      "    return p.components_[0]\n",
      "182/109: getg(mfball)\n",
      "182/110:\n",
      "def getg(m):\n",
      "    matrix = []\n",
      "    for a, b in pairs:\n",
      "        if a not in m.wv.vocab or b not in m.wv.vocab:\n",
      "            continue\n",
      "        center = (m.wv.get_vector(a) + m.wv.get_vector(b))/2\n",
      "        matrix.append(m.wv.get_vector(a) - center)\n",
      "        matrix.append(m.wv.get_vector(b) - center)\n",
      "    matrix = np.array(matrix)\n",
      "    p = PCA(n_components = 10)\n",
      "    p.fit(matrix)\n",
      "    plt.bar(range(10), p.explained_variance_ratio_)\n",
      "#     plt.title(\"Explained Variance vs Component Number (\" + t + \")\")\n",
      "    return p.components_[0]\n",
      "182/111: getg(mfball)\n",
      "182/112:\n",
      "res = sorted([(x, gproj(gpol, mpol, x)) for x in filter(lambda x: x in mpol.wv.vocab, wordlist)], key=lambda x: x[1])\n",
      "for i in range(20):\n",
      "    print(f\"{res[i][0].ljust(20)} %1.2{res[i][1]} \\t\\t {res[-i-1][0].ljust(20)} {res[-i-1][1]}\")\n",
      "182/113:\n",
      "res = sorted([(x, gproj(gpol, mpol, x)) for x in filter(lambda x: x in mpol.wv.vocab, wordlist)], key=lambda x: x[1])\n",
      "for i in range(20):\n",
      "    print(f\"{res[i][0].ljust(20)} %1.2f{res[i][1]} \\t\\t {res[-i-1][0].ljust(20)} {res[-i-1][1]}\")\n",
      "182/114:\n",
      "res = sorted([(x, gproj(gpol, mpol, x)) for x in filter(lambda x: x in mpol.wv.vocab, wordlist)], key=lambda x: x[1])\n",
      "for i in range(20):\n",
      "    print(f\"{res[i][0].ljust(20)} {res[i][1]:1.2f} \\t\\t {res[-i-1][0].ljust(20)} {res[-i-1][1]}\")\n",
      "182/115:\n",
      "res = sorted([(x, gproj(gpol, mpol, x)) for x in filter(lambda x: x in mpol.wv.vocab, wordlist)], key=lambda x: x[1])\n",
      "for i in range(20):\n",
      "    print(f\"{res[i][0].ljust(20)} {res[i][1]:%1.2f} \\t\\t {res[-i-1][0].ljust(20)} {res[-i-1][1]}\")\n",
      "182/116:\n",
      "res = sorted([(x, gproj(gpol, mpol, x)) for x in filter(lambda x: x in mpol.wv.vocab, wordlist)], key=lambda x: x[1])\n",
      "for i in range(20):\n",
      "    print(f\"{res[i][0].ljust(20)} {res[i][1]:.2f} \\t\\t {res[-i-1][0].ljust(20)} {res[-i-1][1]}\")\n",
      "182/117:\n",
      "gpol = getg(mpol)\n",
      "res = sorted([(x, gproj(gpol, mpol, x)) for x in filter(lambda x: x in mpol.wv.vocab, wordlist)], key=lambda x: x[1])\n",
      "for i in range(20):\n",
      "    print(f\"{res[i][0].ljust(20)} {res[i][1]:.2f} \\t\\t {res[-i-1][0].ljust(20)} {res[-i-1][1]}\")\n",
      "182/118:\n",
      "gpol = getg(mpol)\n",
      "res = sorted([(x, gproj(gpol, mpol, x)) for x in filter(lambda x: x in mpol.wv.vocab, wordlist)], key=lambda x: x[1])\n",
      "for i in range(20):\n",
      "    print(f\"{res[i][0].ljust(20)} {res[i][1]:.2f} \\t\\t {res[-i-1][0].ljust(20)} {res[-i-1][1]:.2f}\")\n",
      "182/119:\n",
      "gpol = getg(mpol)\n",
      "res = sorted([(x, gproj(gpol, mpol, x)) for x in filter(lambda x: x in mpol.wv.vocab, wordlist)], key=lambda x: x[1])\n",
      "for i in range(20):\n",
      "    print(f\"{res[i][0].ljust(20)} {res[i][1]:.2f} \\t\\t {res[-i-1][0].ljust(20)} {res[-i-1][1]:1.2f}\")\n",
      "182/120:\n",
      "gpol = getg(mpol)\n",
      "res = sorted([(x, gproj(gpol, mpol, x)) for x in filter(lambda x: x in mpol.wv.vocab, wordlist)], key=lambda x: x[1])\n",
      "for i in range(20):\n",
      "    print(f\"{res[i][0].ljust(20)} {res[i][1]:.2f} \\t\\t {res[-i-1][0].ljust(20)} {res[-i-1][1]:.2f}\")\n",
      "182/121:\n",
      "gpol = getg(mpol)\n",
      "res = sorted([(x, gproj(gpol, mpol, x)) for x in filter(lambda x: x in vocab, wordlist)], key=lambda x: x[1])\n",
      "for i in range(20):\n",
      "    print(f\"{res[i][0].ljust(20)} {res[i][1]:.2f} \\t\\t {res[-i-1][0].ljust(20)} {res[-i-1][1]:.2f}\")\n",
      "182/122:\n",
      "gfball = getg(mfball)\n",
      "res = sorted([(x, gproj(gfball, mfball, x)) for x in filter(lambda x: x in vocab, wordlist)], key=lambda x: x[1])\n",
      "for i in range(20):\n",
      "    print(f\"{res[i][0].ljust(20)} {res[i][1]:.2f} \\t\\t {res[-i-1][0].ljust(20)} {res[-i-1][1]:.2f}\")\n",
      "182/123:\n",
      "gpol = getg(mpol)\n",
      "res = sorted([(x, gproj(gpol, mpol, x)) for x in filter(lambda x: x in vocab, vocab)], key=lambda x: x[1])\n",
      "for i in range(20):\n",
      "    print(f\"{res[i][0].ljust(20)} {res[i][1]:.2f} \\t\\t {res[-i-1][0].ljust(20)} {res[-i-1][1]:.2f}\")\n",
      "182/124:\n",
      "gfball = getg(mfball)\n",
      "res = sorted([(x, gproj(gfball, mfball, x)) for x in filter(lambda x: x in vocab, vocab)], key=lambda x: x[1])\n",
      "for i in range(20):\n",
      "    print(f\"{res[i][0].ljust(20)} {res[i][1]:.2f} \\t\\t {res[-i-1][0].ljust(20)} {res[-i-1][1]:.2f}\")\n",
      "182/125:\n",
      "gpol_s = getg(mpol_s)\n",
      "res = sorted([(x, gproj(gpol_s, mpol_s, x)) for x in filter(lambda x: x in vocab,_s vocab_s)], key=lambda x: x[1])\n",
      "for i in range(20):\n",
      "    print(f\"{res[i][0].ljust(20)} {res[i][1]:.2f} \\t\\t {res[-i-1][0].ljust(20)} {res[-i-1][1]:.2f}\")\n",
      "182/126:\n",
      "gpol_s = getg(mpol_s)\n",
      "res = sorted([(x, gproj(gpol_s, mpol_s, x)) for x in filter(lambda x: x in vocab_s, vocab_s)], key=lambda x: x[1])\n",
      "for i in range(20):\n",
      "    print(f\"{res[i][0].ljust(20)} {res[i][1]:.2f} \\t\\t {res[-i-1][0].ljust(20)} {res[-i-1][1]:.2f}\")\n",
      "182/127:\n",
      "print(labels[2])\n",
      "for i in gdict:\n",
      "    print(\"%10s\" % i, \\\n",
      "          \"%.3f\" % bias(gdict[i][2], mdict[i][2], vocab2, proflist), \\\n",
      "          \"%.3f\" % bias(gdict[i][2], mdict[i][2], vocab2, adjectives), \\\n",
      "          \"%.3f\" % bias(gdict[i][2], mdict[i][2], vocab2, cities), \\\n",
      "          \"%.3f\" % bias(gdict[i][2], mdict[i][2], vocab2, objects), \\\n",
      "          \"%.3f\" % bias(gdict[i][2], mdict[i][2], vocab2, gender_specific))\n",
      "182/128:\n",
      "print(labels[1])\n",
      "for i in gdict:\n",
      "    print(\"%10s\" % i, \\\n",
      "          \"%.3f\" % bias(gdict[i][1], mdict[i][1], vocab2, proflist), \\\n",
      "          \"%.3f\" % bias(gdict[i][1], mdict[i][1], vocab2, adjectives), \\\n",
      "          \"%.3f\" % bias(gdict[i][1], mdict[i][1], vocab2, cities), \\\n",
      "          \"%.3f\" % bias(gdict[i][1], mdict[i][1], vocab2, objects), \\\n",
      "          \"%.3f\" % bias(gdict[i][1], mdict[i][1], vocab2, gender_specific))\n",
      "182/129:\n",
      "print(labels[3])\n",
      "for i in gdict:\n",
      "    print(\"%10s\" % i, \\\n",
      "          \"%.3f\" % bias(gdict[i][3], mdict[i][3], vocab2, proflist), \\\n",
      "          \"%.3f\" % bias(gdict[i][3], mdict[i][3], vocab2, adjectives), \\\n",
      "          \"%.3f\" % bias(gdict[i][3], mdict[i][3], vocab2, cities), \\\n",
      "          \"%.3f\" % bias(gdict[i][3], mdict[i][3], vocab2, objects), \\\n",
      "          \"%.3f\" % bias(gdict[i][3], mdict[i][3], vocab2, gender_specific))\n",
      "182/130:\n",
      "print(labels[4])\n",
      "for i in gdict:\n",
      "    print(\"%10s\" % i, \\\n",
      "          \"%.3f\" % bias(gdict[i][4], mdict[i][4], vocab2, proflist), \\\n",
      "          \"%.3f\" % bias(gdict[i][4], mdict[i][4], vocab2, adjectives), \\\n",
      "          \"%.3f\" % bias(gdict[i][4], mdict[i][4], vocab2, cities), \\\n",
      "          \"%.3f\" % bias(gdict[i][4], mdict[i][4], vocab2, objects), \\\n",
      "          \"%.3f\" % bias(gdict[i][4], mdict[i][4], vocab2, gender_specific))\n",
      "180/184:\n",
      "from gensim.models.callbacks import CallbackAny2Vec\n",
      "epochs = []\n",
      "losses = []\n",
      "class callback(CallbackAny2Vec):\n",
      "    '''Callback to print loss after each epoch.'''\n",
      "\n",
      "    def __init__(self):\n",
      "        self.epoch = 0\n",
      "\n",
      "    def on_epoch_end(self, model):\n",
      "        loss = model.get_latest_training_loss()\n",
      "        print('Loss after epoch {}: {}'.format(self.epoch, loss))\n",
      "        epochs.append(self.epoch)\n",
      "        losses.append(loss)\n",
      "        self.epoch += 1\n",
      "\n",
      "mpol = Word2Vec(politics, compute_loss=True, callbacks=[callback()])\n",
      "180/185:\n",
      "sentences = []\n",
      "for t in politics:\n",
      "    sentences += nltk.sent_tokenize(t.text)\n",
      "docs = [simple_preprocess(s) for s in sentences]\n",
      "\n",
      "from gensim.models.callbacks import CallbackAny2Vec\n",
      "epochs = []\n",
      "losses = []\n",
      "class callback(CallbackAny2Vec):\n",
      "    '''Callback to print loss after each epoch.'''\n",
      "\n",
      "    def __init__(self):\n",
      "        self.epoch = 0\n",
      "\n",
      "    def on_epoch_end(self, model):\n",
      "        loss = model.get_latest_training_loss()\n",
      "        print('Loss after epoch {}: {}'.format(self.epoch, loss))\n",
      "        epochs.append(self.epoch)\n",
      "        losses.append(loss)\n",
      "        self.epoch += 1\n",
      "\n",
      "mpol = Word2Vec(docs, compute_loss=True, callbacks=[callback()])\n",
      "180/186: plt.plot(epochs, losses)\n",
      "182/131: mpol2 = mpol\n",
      "182/132:\n",
      "def getw2v(category):\n",
      "    sentences = []\n",
      "    for t in category:\n",
      "        sentences += nltk.sent_tokenize(t.text)\n",
      "    docs = [simple_preprocess(s) for s in sentences]\n",
      "    return Word2Vec(docs)\n",
      "mpol = getw2v(politics)\n",
      "182/133: mpol = mpol2\n",
      "182/134: mpol_s2 = mpol_2\n",
      "182/135: mpol_s2 = mpol_s\n",
      "182/136:\n",
      "print(labels[0])\n",
      "for i in gdict:\n",
      "    print(\"%10s\" % i, \\\n",
      "          \"%.3f\" % bias(gdict[i][0], mdict[i][0], vocab, proflist), \\\n",
      "          \"%.3f\" % bias(gdict[i][0], mdict[i][0], vocab, adjectives), \\\n",
      "          \"%.3f\" % bias(gdict[i][0], mdict[i][0], vocab, cities), \\\n",
      "          \"%.3f\" % bias(gdict[i][0], mdict[i][0], vocab, objects), \\\n",
      "          \"%.3f\" % bias(gdict[i][0], mdict[i][0], vocab, gender_specific))\n",
      "182/137: mpol_s2 = mpol_s\n",
      "182/138:\n",
      "def getw2v(category):\n",
      "    sentences = []\n",
      "    for t in category:\n",
      "        sentences += nltk.sent_tokenize(t.text)\n",
      "    docs = [simple_preprocess(s) for s in sentences]\n",
      "    return Word2Vec(docs)\n",
      "mpol_s = getw2v(summaries_politics)\n",
      "182/139:\n",
      "def getw2v(category):\n",
      "    sentences = []\n",
      "    for t in category:\n",
      "        sentences += nltk.sent_tokenize(t.text)\n",
      "    docs = [simple_preprocess(s) for s in sentences]\n",
      "    return Word2Vec(docs)\n",
      "mpol_s = getw2v(summaries_pol)\n",
      "182/140:\n",
      "def getw2v_s(sums):\n",
      "    sentences = []\n",
      "    for t in sums:\n",
      "        sentences += nltk.sent_tokenize(t)\n",
      "    docs = [simple_preprocess(s) for s in sentences]\n",
      "    return Word2Vec(docs)\n",
      "mpol_s = getw2v(summaries_pol)\n",
      "182/141:\n",
      "def getw2v_s(sums):\n",
      "    sentences = []\n",
      "    for t in sums:\n",
      "        sentences += nltk.sent_tokenize(t)\n",
      "    docs = [simple_preprocess(s) for s in sentences]\n",
      "    return Word2Vec(docs)\n",
      "mpol_s = getw2v_s(summaries_pol)\n",
      "182/142:\n",
      "gdict = {\"politics\": [], \"computers\": [], \"travel\": [], \"books\": [], \"movies\": [], \"football\": []}\n",
      "labels = [\"original\", \"summary (TextRank)\", \"summary (Centroid-BOW)\", \"summary (Centroid-WE)\", \"summary (control)\"]\n",
      "182/143: list(gdict)\n",
      "182/144:\n",
      "def gdictadd(models):\n",
      "    for i in range(6):\n",
      "        gdict[list(gdict)[i]].append(getg(models[i]))\n",
      "182/145:\n",
      "gdictadd([mpol, mcomp, mtrav, mmov, mbooks, mfball])\n",
      "gdictadd([mpol_s, mcomp_s, mtrav_s, mbks_s, mmov_s, mfball_s])\n",
      "gdictadd([mpol_sb, mcomp_sb, mtrav_sb, mbks_sb, mmov_sb, mfball_sb])\n",
      "gdictadd([mpol_sc, mcomp_sc, mtrav_sc, mbks_sc, mmov_sc, mfball_sc])\n",
      "gdictadd([mpol_c, mcomp_c, mtrav_c, mbks_c, mmov_c, mfball_c])\n",
      "182/146:\n",
      "def getg(m):\n",
      "    matrix = []\n",
      "    for a, b in pairs:\n",
      "        if a not in m.wv.vocab or b not in m.wv.vocab:\n",
      "            continue\n",
      "        center = (m.wv.get_vector(a) + m.wv.get_vector(b))/2\n",
      "        matrix.append(m.wv.get_vector(a) - center)\n",
      "        matrix.append(m.wv.get_vector(b) - center)\n",
      "    matrix = np.array(matrix)\n",
      "    p = PCA(n_components = 10)\n",
      "    p.fit(matrix)\n",
      "#     plt.bar(range(10), p.explained_variance_ratio_)\n",
      "#     plt.title(\"Explained Variance vs Component Number (\" + t + \")\")\n",
      "    return p.components_[0]\n",
      "182/147:\n",
      "gdictadd([mpol, mcomp, mtrav, mmov, mbooks, mfball])\n",
      "gdictadd([mpol_s, mcomp_s, mtrav_s, mbks_s, mmov_s, mfball_s])\n",
      "gdictadd([mpol_sb, mcomp_sb, mtrav_sb, mbks_sb, mmov_sb, mfball_sb])\n",
      "gdictadd([mpol_sc, mcomp_sc, mtrav_sc, mbks_sc, mmov_sc, mfball_sc])\n",
      "gdictadd([mpol_c, mcomp_c, mtrav_c, mbks_c, mmov_c, mfball_c])\n",
      "182/148: mdict = {\"politics\": [], \"computers\": [], \"travel\": [], \"books\": [], \"movies\": [], \"football\": []}\n",
      "182/149:\n",
      "def mdictadd(models):\n",
      "    for i in range(6):\n",
      "        mdict[list(mdict)[i]].append(models[i])\n",
      "182/150:\n",
      "mdictadd([mpol, mcomp, mtrav, mmov, mbooks, mfball])\n",
      "mdictadd([mpol_s, mcomp_s, mtrav_s, mbks_s, mmov_s, mfball_s])\n",
      "mdictadd([mpol_sb, mcomp_sb, mtrav_sb, mbks_sb, mmov_sb, mfball_sb])\n",
      "mdictadd([mpol_sc, mcomp_sc, mtrav_sc, mbks_sc, mmov_sc, mfball_sc])\n",
      "mdictadd([mpol_c, mcomp_c, mtrav_c, mbks_c, mmov_c, mfball_c])\n",
      "182/151:\n",
      "print(labels[0])\n",
      "for i in gdict:\n",
      "    print(\"%10s\" % i, \\\n",
      "          \"%.3f\" % bias(gdict[i][0], mdict[i][0], vocab, proflist), \\\n",
      "          \"%.3f\" % bias(gdict[i][0], mdict[i][0], vocab, adjectives), \\\n",
      "          \"%.3f\" % bias(gdict[i][0], mdict[i][0], vocab, cities), \\\n",
      "          \"%.3f\" % bias(gdict[i][0], mdict[i][0], vocab, objects), \\\n",
      "          \"%.3f\" % bias(gdict[i][0], mdict[i][0], vocab, gender_specific))\n",
      "182/152:\n",
      "print(labels[1])\n",
      "for i in gdict:\n",
      "    print(\"%10s\" % i, \\\n",
      "          \"%.3f\" % bias(gdict[i][1], mdict[i][1], vocab2, proflist), \\\n",
      "          \"%.3f\" % bias(gdict[i][1], mdict[i][1], vocab2, adjectives), \\\n",
      "          \"%.3f\" % bias(gdict[i][1], mdict[i][1], vocab2, cities), \\\n",
      "          \"%.3f\" % bias(gdict[i][1], mdict[i][1], vocab2, objects), \\\n",
      "          \"%.3f\" % bias(gdict[i][1], mdict[i][1], vocab2, gender_specific))\n",
      "183/1:\n",
      "import os\n",
      "import time\n",
      "import dill\n",
      "from typing import List\n",
      "from functools import reduce\n",
      "from calendar import monthrange\n",
      "183/2:\n",
      "import json\n",
      "import csv\n",
      "import requests\n",
      "import xml.etree.ElementTree as ET\n",
      "from bs4 import BeautifulSoup\n",
      "import urllib.request as urllib2\n",
      "183/3:\n",
      "import matplotlib.pyplot as plt\n",
      "import numpy as np\n",
      "from scipy import stats\n",
      "from random import sample, randint, shuffle\n",
      "from sklearn.decomposition import PCA\n",
      "183/4:\n",
      "import nltk\n",
      "import text_summarizer\n",
      "from gensim.utils import simple_preprocess\n",
      "from gensim.summarization.summarizer import summarize\n",
      "from gensim.models import Word2Vec, LsiModel\n",
      "from gensim.test.utils import common_dictionary, common_corpus\n",
      "183/5: from time import process_time\n",
      "183/6:\n",
      "t = process_time()\n",
      "dill.load_session('thesis_env.db')\n",
      "process_time() - t\n",
      "183/7: len(adjectives)\n",
      "183/8: len(all_)\n",
      "183/9:\n",
      "with open('objects.csv') as csv_file:\n",
      "    objects = csv_file.read().split(\"\\n\")\n",
      "objects[0] = 'angle'\n",
      "183/10:\n",
      "t = time.process_time()\n",
      "alld = {}\n",
      "for doc in all_:\n",
      "    doc = Doc(doc.id, doc.gdescriptors, doc.descriptors, doc.text)\n",
      "    for d in doc.descriptors:\n",
      "        count, docs = alld.get(d, (0, set()))\n",
      "        docs.add(doc)\n",
      "        alld[d] = (count + 1, docs)\n",
      "time.process_time() - t\n",
      "183/11:\n",
      "alldlist = sorted([(d, alld[d][0], alld[d][1]) for d in alld], key=lambda x: x[1], reverse=True)\n",
      "for x in range(15):\n",
      "    print(alldlist[x][0:2])\n",
      "183/12:\n",
      "def exclude(e, x):\n",
      "    for d in x.descriptors:\n",
      "        if d == e:\n",
      "            continue\n",
      "        if d in ['Politics and Government', 'Motion Pictures', 'Books and Literature']:\n",
      "            return False\n",
      "    return True\n",
      "183/13:\n",
      "def getlist(topic):\n",
      "    return list(filter(lambda x: exclude(topic, x), alld[topic][1]))\n",
      "183/14: politics = getlist('Politics and Government')\n",
      "183/15: movies = getlist('Motion Pictures')\n",
      "183/16: books = getlist('Books and Literature')\n",
      "183/17: len(politics), len(movies), len(books)\n",
      "183/18: books = sample(books, 7370)\n",
      "183/19: politics = sample(politics, 7370)\n",
      "183/20: movies = sample(movies, 7370)\n",
      "183/21:\n",
      "def getw2v(category):\n",
      "    sentences = []\n",
      "    for t in category:\n",
      "        sentences += nltk.sent_tokenize(t.text)\n",
      "    docs = [simple_preprocess(s) for s in sentences]\n",
      "    return Word2Vec(docs)\n",
      "183/22:\n",
      "t = time.process_time()\n",
      "mpol = getw2v(politics)\n",
      "time.process_time() - t\n",
      "183/23:\n",
      "t = time.process_time()\n",
      "mmov = getw2v(movies)\n",
      "time.process_time() - t\n",
      "183/24:\n",
      "t = time.process_time()\n",
      "mbooks = getw2v(books)\n",
      "time.process_time() - t\n",
      "183/25:\n",
      "def top5(word):\n",
      "    for m in [mpol, mmov, mbooks]:\n",
      "        print([x[0] for x in m.wv.most_similar(positive=[word], topn=5)])\n",
      "183/26: top5('monday')\n",
      "183/27: top5('house')\n",
      "183/28: top5('healthy')\n",
      "183/29: top5('success')\n",
      "183/30: top5('score')\n",
      "183/31: np.mean([len(nltk.word_tokenize(politics[i].text)) for i in range(1000)])\n",
      "183/32: np.mean([len(nltk.word_tokenize(books[i].text)) for i in range(1000)])\n",
      "183/33: np.mean([len(nltk.word_tokenize(movies[i].text)) for i in range(1000)])\n",
      "183/34:\n",
      "def getsummaries(category):\n",
      "    s = []\n",
      "    fails = []\n",
      "    for i in range(len(category)):\n",
      "        try:\n",
      "            s.append(summarize(category[i].text, word_count=200))\n",
      "        except:\n",
      "            fails.append(i)\n",
      "            print(i)\n",
      "    return s, fails\n",
      "183/35:\n",
      "t = time.process_time()\n",
      "summaries_pol, fpol = getsummaries(politics)\n",
      "time.process_time() - t\n",
      "183/36:\n",
      "t = time.process_time()\n",
      "summaries_mov, fmov = getsummaries(movies)\n",
      "time.process_time() - t\n",
      "183/37:\n",
      "t = time.process_time()\n",
      "summaries_bks, fbks = getsummaries(books)\n",
      "time.process_time() - t\n",
      "183/38:\n",
      "def getbows(category):\n",
      "    s = []\n",
      "    centroid_summarizer = text_summarizer.CentroidBOWSummarizer()\n",
      "    for i in category:\n",
      "        try:\n",
      "            s.append(centroid_summarizer.summarize(i.text, limit=200-30))\n",
      "        except:\n",
      "            print(i)\n",
      "    return s\n",
      "183/39:\n",
      "t = time.process_time()\n",
      "bow_pol = getbows(politics)\n",
      "time.process_time() - t\n",
      "183/40: len(bow_pol)\n",
      "183/41:\n",
      "t = time.process_time()\n",
      "bow_bks = getbows(books)\n",
      "time.process_time() - t\n",
      "183/42:\n",
      "t = time.process_time()\n",
      "bow_mov = getbows(movies)\n",
      "time.process_time() - t\n",
      "183/43: embedding_model = text_summarizer.centroid_word_embeddings.load_gensim_embedding_model('glove-wiki-gigaword-50')\n",
      "183/44: centroid_we_summarizer = text_summarizer.CentroidWordEmbeddingsSummarizer(embedding_model, sim_threshold=.99, preprocess_type='nltk')\n",
      "183/45:\n",
      "def getcwe(category):\n",
      "    s = []\n",
      "    for i in category:\n",
      "        try:\n",
      "            s.append(centroid_we_summarizer.summarize(i.text, limit=200-20))\n",
      "        except:\n",
      "            print(i)\n",
      "    return s\n",
      "183/46:\n",
      "t = time.process_time()\n",
      "cwe_pol = getcwe(politics)\n",
      "time.process_time() - t\n",
      "183/47:\n",
      "t = time.process_time()\n",
      "cwe_bks = getcwe(books)\n",
      "time.process_time() - t\n",
      "183/48:\n",
      "t = time.process_time()\n",
      "cwe_mov = getcwe(movies)\n",
      "time.process_time() - t\n",
      "183/49: np.mean([len(nltk.word_tokenize(getcwe(politics[:100])[i])) for i in range(100)])\n",
      "183/50: np.mean([len(nltk.word_tokenize(getbows(politics[:100])[i])) for i in range(100)])\n",
      "183/51: np.mean([len(nltk.word_tokenize(getsummaries(politics[:100])[0][i])) for i in range(100)])\n",
      "183/52:\n",
      "def getctrls(category):\n",
      "    ctrls = []\n",
      "    for a in category:\n",
      "        sents = nltk.sent_tokenize(a.text)\n",
      "        rs = [i for i in range(len(sents))]\n",
      "        shuffle(rs)\n",
      "        ctrl = []\n",
      "        count = 0\n",
      "        for r in rs:\n",
      "            if count > 200-5:\n",
      "                break\n",
      "            count += len(nltk.word_tokenize(sents[r]))\n",
      "            ctrl.append(sents[r])\n",
      "        summary = \"\\n\".join([s for s in ctrl])\n",
      "        ctrls.append(summary)\n",
      "    return ctrls\n",
      "183/53: np.mean([len(nltk.word_tokenize(getctrls(politics[:100])[i])) for i in range(100)])\n",
      "183/54:\n",
      "t = time.process_time()\n",
      "ctrls_pol = getctrls(politics)\n",
      "time.process_time() - t\n",
      "183/55:\n",
      "t = time.process_time()\n",
      "ctrls_bks = getctrls(books)\n",
      "time.process_time() - t\n",
      "183/56:\n",
      "t = time.process_time()\n",
      "ctrls_mov = getctrls(movies)\n",
      "time.process_time() - t\n",
      "183/57:\n",
      "def getw2v_s(sums):\n",
      "    sentences = []\n",
      "    for t in sums:\n",
      "        sentences += nltk.sent_tokenize(t)\n",
      "    docs = [simple_preprocess(s) for s in sentences]\n",
      "    return Word2Vec(docs)\n",
      "183/58: mpol_s = getw2v_s(summaries_pol)\n",
      "183/59: mbks_s = getw2v_s(summaries_bks)\n",
      "183/60: mmov_s = getw2v_s(summaries_mov)\n",
      "183/61:\n",
      "mpol_sb = getw2v_s(bow_pol)\n",
      "mbks_sb = getw2v_s(bow_bks)\n",
      "mmov_sb = getw2v_s(bow_mov)\n",
      "183/62:\n",
      "mpol_sc = getw2v_s(cwe_pol)\n",
      "mbks_sc = getw2v_s(cwe_bks)\n",
      "mmov_sc = getw2v_s(cwe_mov)\n",
      "183/63:\n",
      "mpol_c = getw2v_s(ctrls_pol)\n",
      "mbks_c = getw2v_s(ctrls_bks)\n",
      "mmov_c = getw2v_s(ctrls_mov)\n",
      "183/64:\n",
      "gdict = {\"politics\": [], \"books\": [], \"movies\": []}\n",
      "labels = [\"original\", \"summary (TextRank)\", \"summary (Centroid-BOW)\", \"summary (Centroid-WE)\", \"summary (control)\"]\n",
      "183/65:\n",
      "def gdictadd(models):\n",
      "    for i in range(6):\n",
      "        gdict[list(gdict)[i]].append(getg(models[i]))\n",
      "183/66:\n",
      "gdictadd([mpol, mmov, mbooks])\n",
      "gdictadd([mpol_s, mbks_s, mmov_s])\n",
      "gdictadd([mpol_sb, mbks_sb, mmov_sb])\n",
      "gdictadd([mpol_sc, mbks_sc, mmov_sc])\n",
      "gdictadd([mpol_c, mbks_c, mmov_c])\n",
      "183/67: dill.dump_session('thesis_env3.db')\n",
      "183/68:\n",
      "def getg(m):\n",
      "    matrix = []\n",
      "    for a, b in pairs:\n",
      "        if a not in m.wv.vocab or b not in m.wv.vocab:\n",
      "            continue\n",
      "        center = (m.wv.get_vector(a) + m.wv.get_vector(b))/2\n",
      "        matrix.append(m.wv.get_vector(a) - center)\n",
      "        matrix.append(m.wv.get_vector(b) - center)\n",
      "    matrix = np.array(matrix)\n",
      "    p = PCA(n_components = 10)\n",
      "    p.fit(matrix)\n",
      "#         plt.bar(range(10), p.explained_variance_ratio_)\n",
      "#         plt.title(\"Explained Variance vs Component Number (\" + t + \")\")\n",
      "    return p.components_[0]\n",
      "183/69:\n",
      "gdictadd([mpol, mmov, mbooks])\n",
      "gdictadd([mpol_s, mbks_s, mmov_s])\n",
      "gdictadd([mpol_sb, mbks_sb, mmov_sb])\n",
      "gdictadd([mpol_sc, mbks_sc, mmov_sc])\n",
      "gdictadd([mpol_c, mbks_c, mmov_c])\n",
      "183/70:\n",
      "with open('debiaswe-master/data/equalize_pairs.json') as f:\n",
      "    pairs = json.loads(f.read())\n",
      "pairs = list(map(lambda x: [x[0].lower(), x[1].lower()], pairs))\n",
      "with open('debiaswe-master/data/professions.json') as f:\n",
      "    profs = json.loads(f.read())\n",
      "proflist = list(filter(lambda x: x in mpol.wv.vocab, [p[0] for p in profs if abs(p[1]) < 0.6]))\n",
      "with open('debiaswe-master/data/gender_specific_seed.json') as f:\n",
      "    gender_specific = json.loads(f.read())\n",
      "cities = [x.lower() for x in cities]\n",
      "183/71:\n",
      "with open('debiaswe-master/data/equalize_pairs.json') as f:\n",
      "    pairs = json.loads(f.read())\n",
      "pairs = list(map(lambda x: [x[0].lower(), x[1].lower()], pairs))\n",
      "with open('debiaswe-master/data/professions.json') as f:\n",
      "    profs = json.loads(f.read())\n",
      "proflist = list(filter(lambda x: x in mpol.wv.vocab, [p[0] for p in profs if abs(p[1]) < 0.6]))\n",
      "with open('debiaswe-master/data/gender_specific_seed.json') as f:\n",
      "    gender_specific = json.loads(f.read())\n",
      "with open('objects.csv') as csv_file:\n",
      "    objects = csv_file.read().split(\"\\n\")\n",
      "objects[0] = 'angle'\n",
      "with open('cities.csv') as csv_file:\n",
      "    cities = csv_file.read().split(\",\")\n",
      "cities = [x.lower() for x in cities]\n",
      "183/72:\n",
      "gdictadd([mpol, mmov, mbooks])\n",
      "gdictadd([mpol_s, mbks_s, mmov_s])\n",
      "gdictadd([mpol_sb, mbks_sb, mmov_sb])\n",
      "gdictadd([mpol_sc, mbks_sc, mmov_sc])\n",
      "gdictadd([mpol_c, mbks_c, mmov_c])\n",
      "183/73:\n",
      "def gdictadd(models):\n",
      "    for i in range(3):\n",
      "        gdict[list(gdict)[i]].append(getg(models[i]))\n",
      "183/74:\n",
      "gdictadd([mpol, mmov, mbooks])\n",
      "gdictadd([mpol_s, mbks_s, mmov_s])\n",
      "gdictadd([mpol_sb, mbks_sb, mmov_sb])\n",
      "gdictadd([mpol_sc, mbks_sc, mmov_sc])\n",
      "gdictadd([mpol_c, mbks_c, mmov_c])\n",
      "183/75: mdict = {\"politics\": [], \"books\": [], \"movies\": []}\n",
      "183/76:\n",
      "def mdictadd(models):\n",
      "    for i in range(3):\n",
      "        mdict[list(mdict)[i]].append(models[i])\n",
      "183/77:\n",
      "gdictadd([mpol, mmov, mbooks])\n",
      "gdictadd([mpol_s, mbks_s, mmov_s])\n",
      "gdictadd([mpol_sb, mbks_sb, mmov_sb])\n",
      "gdictadd([mpol_sc, mbks_sc, mmov_sc])\n",
      "gdictadd([mpol_c, mbks_c, mmov_c])\n",
      "183/78:\n",
      "mdictadd([mpol, mmov, mbooks])\n",
      "mdictadd([mpol_s, mbks_s, mmov_s])\n",
      "mdictadd([mpol_sb, mbks_sb, mmov_sb])\n",
      "mdictadd([mpol_sc, mbks_sc, mmov_sc])\n",
      "mdictadd([mpol_c, mbks_c, mmov_c])\n",
      "183/79:\n",
      "def gproj(g, m, w1):\n",
      "    wvec = m.wv.get_vector(w1)\n",
      "    p = np.dot(g, wvec)/(np.linalg.norm(g)*np.linalg.norm(wvec))\n",
      "    return p\n",
      "183/80:\n",
      "def bias(g, m, vocab, wordlist):\n",
      "    b = 0\n",
      "    l = list(filter(lambda x: x in vocab, wordlist))\n",
      "    for x in l:\n",
      "        b += abs(gproj(g, m, x))\n",
      "    return b/len(l)\n",
      "183/81:\n",
      "print(labels[0])\n",
      "for i in gdict:\n",
      "    print(\"%10s\" % i, \\\n",
      "          \"%.3f\" % bias(gdict[i][0], mdict[i][0], vocab, proflist), \\\n",
      "          \"%.3f\" % bias(gdict[i][0], mdict[i][0], vocab, adjectives), \\\n",
      "          \"%.3f\" % bias(gdict[i][0], mdict[i][0], vocab, cities), \\\n",
      "          \"%.3f\" % bias(gdict[i][0], mdict[i][0], vocab, objects), \\\n",
      "          \"%.3f\" % bias(gdict[i][0], mdict[i][0], vocab, gender_specific))\n",
      "183/82:\n",
      "vocab = set(mpol.wv.vocab)\n",
      "for m in [mpol, mmov, mbooks]:\n",
      "    vocab &= set(m.wv.vocab)\n",
      "len(vocab)\n",
      "183/83:\n",
      "print(labels[0])\n",
      "for i in gdict:\n",
      "    print(\"%10s\" % i, \\\n",
      "          \"%.3f\" % bias(gdict[i][0], mdict[i][0], vocab, proflist), \\\n",
      "          \"%.3f\" % bias(gdict[i][0], mdict[i][0], vocab, adjectives), \\\n",
      "          \"%.3f\" % bias(gdict[i][0], mdict[i][0], vocab, cities), \\\n",
      "          \"%.3f\" % bias(gdict[i][0], mdict[i][0], vocab, objects), \\\n",
      "          \"%.3f\" % bias(gdict[i][0], mdict[i][0], vocab, gender_specific))\n",
      "183/84:\n",
      "print(labels[1])\n",
      "for i in gdict:\n",
      "    print(\"%10s\" % i, \\\n",
      "          \"%.3f\" % bias(gdict[i][1], mdict[i][1], vocab2, proflist), \\\n",
      "          \"%.3f\" % bias(gdict[i][1], mdict[i][1], vocab2, adjectives), \\\n",
      "          \"%.3f\" % bias(gdict[i][1], mdict[i][1], vocab2, cities), \\\n",
      "          \"%.3f\" % bias(gdict[i][1], mdict[i][1], vocab2, objects), \\\n",
      "          \"%.3f\" % bias(gdict[i][1], mdict[i][1], vocab2, gender_specific))\n",
      "183/85:\n",
      "vocab_all = set(vocab)\n",
      "for m in [mpol_s, mbks_s, mmov_s] + \\\n",
      "[mpol_sb, mbks_sb, mmov_sb] + \\\n",
      "[mpol_sc, mbks_sc, mmov_sc] + \\\n",
      "[mpol_c, mbks_c, mmov_c]:\n",
      "    vocab_all &= set(m.wv.vocab)\n",
      "len(vocab_all)\n",
      "183/86: vocab2 = vocab_all\n",
      "183/87:\n",
      "print(labels[1])\n",
      "for i in gdict:\n",
      "    print(\"%10s\" % i, \\\n",
      "          \"%.3f\" % bias(gdict[i][1], mdict[i][1], vocab2, proflist), \\\n",
      "          \"%.3f\" % bias(gdict[i][1], mdict[i][1], vocab2, adjectives), \\\n",
      "          \"%.3f\" % bias(gdict[i][1], mdict[i][1], vocab2, cities), \\\n",
      "          \"%.3f\" % bias(gdict[i][1], mdict[i][1], vocab2, objects), \\\n",
      "          \"%.3f\" % bias(gdict[i][1], mdict[i][1], vocab2, gender_specific))\n",
      "183/88:\n",
      "print(labels[2])\n",
      "for i in gdict:\n",
      "    print(\"%10s\" % i, \\\n",
      "          \"%.3f\" % bias(gdict[i][2], mdict[i][2], vocab2, proflist), \\\n",
      "          \"%.3f\" % bias(gdict[i][2], mdict[i][2], vocab2, adjectives), \\\n",
      "          \"%.3f\" % bias(gdict[i][2], mdict[i][2], vocab2, cities), \\\n",
      "          \"%.3f\" % bias(gdict[i][2], mdict[i][2], vocab2, objects), \\\n",
      "          \"%.3f\" % bias(gdict[i][2], mdict[i][2], vocab2, gender_specific))\n",
      "183/89:\n",
      "print(labels[3])\n",
      "for i in gdict:\n",
      "    print(\"%10s\" % i, \\\n",
      "          \"%.3f\" % bias(gdict[i][3], mdict[i][3], vocab2, proflist), \\\n",
      "          \"%.3f\" % bias(gdict[i][3], mdict[i][3], vocab2, adjectives), \\\n",
      "          \"%.3f\" % bias(gdict[i][3], mdict[i][3], vocab2, cities), \\\n",
      "          \"%.3f\" % bias(gdict[i][3], mdict[i][3], vocab2, objects), \\\n",
      "          \"%.3f\" % bias(gdict[i][3], mdict[i][3], vocab2, gender_specific))\n",
      "183/90:\n",
      "print(labels[4])\n",
      "for i in gdict:\n",
      "    print(\"%10s\" % i, \\\n",
      "          \"%.3f\" % bias(gdict[i][4], mdict[i][4], vocab2, proflist), \\\n",
      "          \"%.3f\" % bias(gdict[i][4], mdict[i][4], vocab2, adjectives), \\\n",
      "          \"%.3f\" % bias(gdict[i][4], mdict[i][4], vocab2, cities), \\\n",
      "          \"%.3f\" % bias(gdict[i][4], mdict[i][4], vocab2, objects), \\\n",
      "          \"%.3f\" % bias(gdict[i][4], mdict[i][4], vocab2, gender_specific))\n",
      "183/91:\n",
      "def getg(m):\n",
      "    matrix = []\n",
      "    for a, b in pairs:\n",
      "        if a not in m.wv.vocab or b not in m.wv.vocab:\n",
      "            continue\n",
      "        center = (m.wv.get_vector(a) + m.wv.get_vector(b))/2\n",
      "        matrix.append(m.wv.get_vector(a) - center)\n",
      "        matrix.append(m.wv.get_vector(b) - center)\n",
      "    matrix = np.array(matrix)\n",
      "    p = PCA(n_components = 10)\n",
      "    print(p.explained_variance_ratio_[0])\n",
      "    p.fit(matrix)\n",
      "#         plt.bar(range(10), p.explained_variance_ratio_)\n",
      "#         plt.title(\"Explained Variance vs Component Number (\" + t + \")\")\n",
      "    return p.components_[0]\n",
      "183/92:\n",
      "gdictadd([mpol, mmov, mbooks])\n",
      "gdictadd([mpol_s, mbks_s, mmov_s])\n",
      "gdictadd([mpol_sb, mbks_sb, mmov_sb])\n",
      "gdictadd([mpol_sc, mbks_sc, mmov_sc])\n",
      "gdictadd([mpol_c, mbks_c, mmov_c])\n",
      "183/93:\n",
      "def getg(m):\n",
      "    matrix = []\n",
      "    for a, b in pairs:\n",
      "        if a not in m.wv.vocab or b not in m.wv.vocab:\n",
      "            continue\n",
      "        center = (m.wv.get_vector(a) + m.wv.get_vector(b))/2\n",
      "        matrix.append(m.wv.get_vector(a) - center)\n",
      "        matrix.append(m.wv.get_vector(b) - center)\n",
      "    matrix = np.array(matrix)\n",
      "    p = PCA(n_components = 10)\n",
      "    p.fit(matrix)\n",
      "    print(p.explained_variance_ratio_[0])\n",
      "#         plt.bar(range(10), p.explained_variance_ratio_)\n",
      "#         plt.title(\"Explained Variance vs Component Number (\" + t + \")\")\n",
      "    return p.components_[0]\n",
      "183/94:\n",
      "gdictadd([mpol, mmov, mbooks])\n",
      "gdictadd([mpol_s, mbks_s, mmov_s])\n",
      "gdictadd([mpol_sb, mbks_sb, mmov_sb])\n",
      "gdictadd([mpol_sc, mbks_sc, mmov_sc])\n",
      "gdictadd([mpol_c, mbks_c, mmov_c])\n",
      "183/95:\n",
      "gpol = getg(mpol)\n",
      "res = sorted([(x, gproj(gpol, mpol, x)) for x in filter(lambda x: x in vocab, vocab)], key=lambda x: x[1])\n",
      "for i in range(20):\n",
      "    print(f\"{res[i][0].ljust(20)} {res[i][1]:.2f} \\t\\t {res[-i-1][0].ljust(20)} {res[-i-1][1]:.2f}\")\n",
      "183/96:\n",
      "gpol = getg(mpol)\n",
      "res = sorted([(x, gproj(gpol, mpol, x)) for x in filter(lambda x: x in vocab, vocab)], key=lambda x: x[1])\n",
      "for i in range(15):\n",
      "    print(f\"{res[i][0].ljust(15)} {res[i][1]:.2f} \\t\\t {res[-i-1][0].ljust(15)} {res[-i-1][1]:.2f}\")\n",
      "183/97:\n",
      "gpol_s = getg(mpol_s)\n",
      "res = sorted([(x, gproj(gpol_s, mpol_s, x)) for x in filter(lambda x: x in vocab2, vocab2)], key=lambda x: x[1])\n",
      "for i in range(15):\n",
      "    print(f\"{res[i][0].ljust(15)} {res[i][1]:.2f} \\t\\t {res[-i-1][0].ljust(15)} {res[-i-1][1]:.2f}\")\n",
      "183/98:\n",
      "gpol_c = getg(mpol_c)\n",
      "res = sorted([(x, gproj(gpol_c, mpol_c, x)) for x in filter(lambda x: x in vocab2, vocab2)], key=lambda x: x[1])\n",
      "for i in range(15):\n",
      "    print(f\"{res[i][0].ljust(15)} {res[i][1]:.2f} \\t\\t {res[-i-1][0].ljust(15)} {res[-i-1][1]:.2f}\")\n",
      "183/99:\n",
      "gbks_c = getg(mbks_c)\n",
      "res = sorted([(x, gproj(gbks_c, mbks_c, x)) for x in filter(lambda x: x in vocab2, vocab2)], key=lambda x: x[1])\n",
      "for i in range(15):\n",
      "    print(f\"{res[i][0].ljust(15)} {res[i][1]:.2f} \\t\\t {res[-i-1][0].ljust(15)} {res[-i-1][1]:.2f}\")\n",
      "183/100:\n",
      "def getg(m, n):\n",
      "    matrix = []\n",
      "    for a, b in pairs:\n",
      "        if a not in m.wv.vocab or b not in m.wv.vocab:\n",
      "            continue\n",
      "        center = (m.wv.get_vector(a) + m.wv.get_vector(b))/2\n",
      "        matrix.append(m.wv.get_vector(a) - center)\n",
      "        matrix.append(m.wv.get_vector(b) - center)\n",
      "    matrix = np.array(matrix)\n",
      "    p = PCA(n_components = 10)\n",
      "    p.fit(matrix)\n",
      "    print(p.explained_variance_ratio_[n])\n",
      "#         plt.bar(range(10), p.explained_variance_ratio_)\n",
      "#         plt.title(\"Explained Variance vs Component Number (\" + t + \")\")\n",
      "    return p.components_[n]\n",
      "183/101:\n",
      "gpol_s1 = getg(mpol_s,1)\n",
      "res = sorted([(x, gproj(gpol_s1, mpol_s1, x)) for x in filter(lambda x: x in vocab2, vocab2)], key=lambda x: x[1])\n",
      "for i in range(15):\n",
      "    print(f\"{res[i][0].ljust(15)} {res[i][1]:.2f} \\t\\t {res[-i-1][0].ljust(15)} {res[-i-1][1]:.2f}\")\n",
      "183/102:\n",
      "gpol_s1 = getg(mpol_s,1)\n",
      "res = sorted([(x, gproj(gpol_s1, mpol_s, x)) for x in filter(lambda x: x in vocab2, vocab2)], key=lambda x: x[1])\n",
      "for i in range(15):\n",
      "    print(f\"{res[i][0].ljust(15)} {res[i][1]:.2f} \\t\\t {res[-i-1][0].ljust(15)} {res[-i-1][1]:.2f}\")\n",
      "183/103:\n",
      "def getg(m):\n",
      "    matrix = []\n",
      "    for a, b in pairs:\n",
      "        if a not in m.wv.vocab or b not in m.wv.vocab:\n",
      "            continue\n",
      "        center = (m.wv.get_vector(a) + m.wv.get_vector(b))/2\n",
      "        matrix.append(m.wv.get_vector(a) - center)\n",
      "        matrix.append(m.wv.get_vector(b) - center)\n",
      "    matrix = np.array(matrix)\n",
      "    p = PCA(n_components = 10)\n",
      "    p.fit(matrix)\n",
      "    print(p.explained_variance_ratio_[0])\n",
      "#         plt.bar(range(10), p.explained_variance_ratio_)\n",
      "#         plt.title(\"Explained Variance vs Component Number (\" + t + \")\")\n",
      "    return p.components_[:2]\n",
      "183/104:\n",
      "def gproj2(g1, g2, m, w1):\n",
      "    wvec = m.wv.get_vector(w1)\n",
      "    # g = project wvec onto g1, g2 subspace\n",
      "    normal = np.cross(g1, g2)/np.linalg.norm(np.cross(g1, g2))\n",
      "    g = wvec - np.dot(wvec, normal)*normal\n",
      "    p = np.dot(g, wvec)/(np.linalg.norm(g)*np.linalg.norm(wvec))\n",
      "    return p\n",
      "183/105:\n",
      "gpol_s1, gpol_s2 = getg(mpol_s)\n",
      "res = sorted([(x, gproj2(gpol_s1, gpol_s2, mpol_s, x)) for x in filter(lambda x: x in vocab2, vocab2)], key=lambda x: x[1])\n",
      "for i in range(15):\n",
      "    print(f\"{res[i][0].ljust(15)} {res[i][1]:.2f} \\t\\t {res[-i-1][0].ljust(15)} {res[-i-1][1]:.2f}\")\n",
      "183/106:\n",
      "def gproj2(g1, g2, m, w1):\n",
      "    wvec = m.wv.get_vector(w1)\n",
      "    # g = project wvec onto g1, g2 subspace\n",
      "    print(g1.shape, g2.shape)\n",
      "    normal = np.cross(g1, g2)/np.linalg.norm(np.cross(g1, g2))\n",
      "    g = wvec - np.dot(wvec, normal)*normal\n",
      "    p = np.dot(g, wvec)/(np.linalg.norm(g)*np.linalg.norm(wvec))\n",
      "    return p\n",
      "183/107:\n",
      "gpol_s1, gpol_s2 = getg(mpol_s)\n",
      "res = sorted([(x, gproj2(gpol_s1, gpol_s2, mpol_s, x)) for x in filter(lambda x: x in vocab2, vocab2)], key=lambda x: x[1])\n",
      "for i in range(15):\n",
      "    print(f\"{res[i][0].ljust(15)} {res[i][1]:.2f} \\t\\t {res[-i-1][0].ljust(15)} {res[-i-1][1]:.2f}\")\n",
      "183/108: np.cross(gpol_s1, gpol_s2)\n",
      "183/109: np.cross([1,2,3], [4,5,6])\n",
      "183/110: gpol_s1\n",
      "183/111: np.cross(array([1,2,3]), array([4,5,6])\n",
      "183/112: np.cross(array([1,2,3]), array([4,5,6]))\n",
      "183/113: np.cross(np.array([1,2,3]), np.array([4,5,6]))\n",
      "183/114: gpol_s1.reshape(100)\n",
      "183/115: np.cross(gpol_s1.reshape(100), gpol_s2.reshape(100))\n",
      "183/116: np.cross(gpol_s1, gpol_s2, axis=0)\n",
      "183/117: np.cross(gpol_s1.T, gpol_s2.T)\n",
      "183/118: np.cross(np.array([1,2,3,4,5,6,7,8,9,8,7]), np.array([1,2,3,4,5,6,7,8,9,8,7]))\n",
      "183/119: np.vstack([1,2],[3,4])\n",
      "183/120: np.hstack(([1,2],[3,4]))\n",
      "183/121: np.hstack(([1,2].T,[3,4].T))\n",
      "183/122: np.hstack((np.array([1,2]).T,np.array([3,4]).T))\n",
      "183/123: np.hstack((np.array([1,2]),np.array([3,4])))\n",
      "183/124: np.hstack((np.array([[1,2]]),np.array([[3,4]])))\n",
      "183/125: np.hstack((np.array([[1,2]]),np.array([[3,4]]))).shape\n",
      "183/126: np.hstack((np.array([1,2]).T,np.array([3,4]).T)).shape\n",
      "183/127: np.hstack((np.array([1,2]).reshape(2,1),np.array([3,4]).reshape(2,1))).shape\n",
      "183/128:\n",
      "A = np.hstack((g1.reshape(100,1),g2.reshape(100,1)))\n",
      "    P = A*np.inverse(A.T*A)*A\n",
      "183/129:\n",
      "A = np.hstack((g1.reshape(100,1),g2.reshape(100,1)))\n",
      "P = A*np.inverse(A.T*A)*A\n",
      "183/130:\n",
      "A = np.hstack((gpol_s1.reshape(100,1),gpol_s2.reshape(100,1)))\n",
      "P = A*np.inverse(A.T*A)*A\n",
      "183/131:\n",
      "A = np.hstack((gpol_s1.reshape(100,1),gpol_s2.reshape(100,1)))\n",
      "P = A*np.inv(A.T*A)*A\n",
      "183/132:\n",
      "A = np.hstack((gpol_s1.reshape(100,1),gpol_s2.reshape(100,1)))\n",
      "P = A*np.linalg.inv(A.T*A)*A\n",
      "183/133:\n",
      "A = np.hstack((gpol_s1.reshape(100,1),gpol_s2.reshape(100,1)))\n",
      "P = A @ np.linalg.inv(A.T @ A) @ A\n",
      "183/134:\n",
      "A = np.hstack((gpol_s1.reshape(100,1),gpol_s2.reshape(100,1)))\n",
      "P = A @ np.linalg.inv(A.T @ A) @ A.T\n",
      "183/135:\n",
      "def gproj2(g1, g2, m, w1):\n",
      "    wvec = m.wv.get_vector(w1)\n",
      "    # g = project wvec onto g1, g2 subspace\n",
      "    A = np.hstack((g1.reshape(100,1),g2.reshape(100,1)))\n",
      "    P = A @ np.linalg.inv(A.T @ A) @ A.T\n",
      "    g = P @ wvec\n",
      "    p = np.dot(g, wvec)/(np.linalg.norm(g)*np.linalg.norm(wvec))\n",
      "    return p\n",
      "183/136:\n",
      "gpol_s1, gpol_s2 = getg(mpol_s)\n",
      "res = sorted([(x, gproj2(gpol_s1, gpol_s2, mpol_s, x)) for x in filter(lambda x: x in vocab2, vocab2)], key=lambda x: x[1])\n",
      "for i in range(15):\n",
      "    print(f\"{res[i][0].ljust(15)} {res[i][1]:.2f} \\t\\t {res[-i-1][0].ljust(15)} {res[-i-1][1]:.2f}\")\n",
      "183/137:\n",
      "gpol1, gpol2 = getg(mpol)\n",
      "res = sorted([(x, gproj2(gpol1, gpol2, mpol, x)) for x in filter(lambda x: x in vocab, vocab)], key=lambda x: x[1])\n",
      "for i in range(15):\n",
      "    print(f\"{res[i][0].ljust(15)} {res[i][1]:.2f} \\t\\t {res[-i-1][0].ljust(15)} {res[-i-1][1]:.2f}\")\n",
      "183/138: gpol = getg(mpol, \"Gender Pairs - Politics\", True)\n",
      "183/139:\n",
      "def getg(m, t, plot):\n",
      "    matrix = []\n",
      "    for a, b in pairs:\n",
      "        if a not in m.wv.vocab or b not in m.wv.vocab:\n",
      "            continue\n",
      "        center = (m.wv.get_vector(a) + m.wv.get_vector(b))/2\n",
      "        matrix.append(m.wv.get_vector(a) - center)\n",
      "        matrix.append(m.wv.get_vector(b) - center)\n",
      "    matrix = np.array(matrix)\n",
      "    p = PCA(n_components = 10)\n",
      "    p.fit(matrix)\n",
      "    if plot:\n",
      "        plt.bar(range(10), p.explained_variance_ratio_)\n",
      "        plt.title(\"Explained Variance vs Component Number (\" + t + \")\")\n",
      "    return p.components_[0]\n",
      "183/140: gpol = getg(mpol, \"Gender Pairs - Politics\", True)\n",
      "183/141: top5('seat')\n",
      "183/142: top5(\"publish\")\n",
      "183/143: top5(\"draft\")\n",
      "183/144: top5(\"page\")\n",
      "183/145: top5(\"print\")\n",
      "183/146: top5(\"paper\")\n",
      "183/147: top5(\"sell\")\n",
      "183/148: top5(\"best\")\n",
      "183/149: len(set(vocab2) & set(pairs))\n",
      "183/150: pairs\n",
      "183/151: len(set(vocab2) & set([x[1] for x in pairs]))\n",
      "183/152: len(set(vocab2) & set([x[0] for x in pairs]))\n",
      "183/153: len(pairs)\n",
      "183/154: len(set(vocab) & set([x[0] for x in pairs]))\n",
      "183/155: len(pairs), pairs[1]\n",
      "183/156: len(pairs), pairs[2]\n",
      "183/157: len(pairs), pairs[0]\n",
      "183/158: len(pairs), pairs[:5]\n",
      "183/159:\n",
      "t = time.process_time()\n",
      "summaries_bks, fbks = getsummaries(books)\n",
      "time.process_time() - t\n",
      "183/160: mbks_s = getw2v_s(summaries_bks)\n",
      "183/161:\n",
      "def getg(m):\n",
      "    matrix = []\n",
      "    for a, b in pairs:\n",
      "        if a not in m.wv.vocab or b not in m.wv.vocab:\n",
      "            continue\n",
      "        center = (m.wv.get_vector(a) + m.wv.get_vector(b))/2\n",
      "        matrix.append(m.wv.get_vector(a) - center)\n",
      "        matrix.append(m.wv.get_vector(b) - center)\n",
      "    matrix = np.array(matrix)\n",
      "    p = PCA(n_components = 10)\n",
      "    p.fit(matrix)\n",
      "    print(p.explained_variance_ratio_[0])\n",
      "#         plt.bar(range(10), p.explained_variance_ratio_)\n",
      "#         plt.title(\"Explained Variance vs Component Number (\" + t + \")\")\n",
      "    return p.components_[0]\n",
      "183/162:\n",
      "print(labels[1])\n",
      "print(\"%10s\" % i, \\\n",
      "      \"%.3f\" % bias(getg(mbks_s), mbks_s, vocab2, proflist), \\\n",
      "      \"%.3f\" % bias(getg(mbks_s), mbks_s, vocab2, adjectives), \\\n",
      "      \"%.3f\" % bias(getg(mbks_s), mbks_s, vocab2, cities), \\\n",
      "      \"%.3f\" % bias(getg(mbks_s), mbks_s, vocab2, objects), \\\n",
      "      \"%.3f\" % bias(getg(mbks_s), mbks_s, vocab2, gender_specific))\n",
      "183/163: mbks_s = getw2v_s(summaries_bks)\n",
      "183/164:\n",
      "print(labels[1])\n",
      "print(\"%10s\" % i, \\\n",
      "      \"%.3f\" % bias(getg(mbks_s), mbks_s, vocab2, proflist), \\\n",
      "      \"%.3f\" % bias(getg(mbks_s), mbks_s, vocab2, adjectives), \\\n",
      "      \"%.3f\" % bias(getg(mbks_s), mbks_s, vocab2, cities), \\\n",
      "      \"%.3f\" % bias(getg(mbks_s), mbks_s, vocab2, objects), \\\n",
      "      \"%.3f\" % bias(getg(mbks_s), mbks_s, vocab2, gender_specific))\n",
      "183/165:\n",
      "print(labels[4])\n",
      "print(\"%10s\" % i, \\\n",
      "      \"%.3f\" % bias(getg(mbks_c), mbks_c, vocab2, proflist), \\\n",
      "      \"%.3f\" % bias(getg(mbks_c), mbks_c, vocab2, adjectives), \\\n",
      "      \"%.3f\" % bias(getg(mbks_c), mbks_c, vocab2, cities), \\\n",
      "      \"%.3f\" % bias(getg(mbks_c), mbks_c, vocab2, objects), \\\n",
      "      \"%.3f\" % bias(getg(mbks_c), mbks_c, vocab2, gender_specific))\n",
      "183/166:\n",
      "print(labels[4])\n",
      "print(\"%10s\" % i, \\\n",
      "      \"%.3f\" % bias(getg(mmov_c), mmov_c, vocab2, proflist), \\\n",
      "      \"%.3f\" % bias(getg(mmov_c), mmov_c, vocab2, adjectives), \\\n",
      "      \"%.3f\" % bias(getg(mmov_c), mmov_c, vocab2, cities), \\\n",
      "      \"%.3f\" % bias(getg(mmov_c), mmov_c, vocab2, objects), \\\n",
      "      \"%.3f\" % bias(getg(mmov_c), mmov_c, vocab2, gender_specific))\n",
      "183/167:\n",
      "print(labels[4])\n",
      "print(\"%10s\" % i, \\\n",
      "      \"%.3f\" % bias(getg(mmov_c), mmov_c, vocab2, proflist), \\\n",
      "      \"%.3f\" % bias(getg(mmov_c), mmov_c, vocab2, adjectives), \\\n",
      "      \"%.3f\" % bias(getg(mmov_c), mmov_c, vocab2, cities), \\\n",
      "      \"%.3f\" % bias(getg(mmov_c), mmov_c, vocab2, objects), \\\n",
      "      \"%.3f\" % bias(getg(mmov_c), mmov_c, vocab2, gender_specific))\n",
      "183/168:\n",
      "print(labels[1])\n",
      "print(\"%10s\" % i, \\\n",
      "      \"%.3f\" % bias(getg(mmov_s), mmov_s, vocab2, proflist), \\\n",
      "      \"%.3f\" % bias(getg(mmov_s), mmov_s, vocab2, adjectives), \\\n",
      "      \"%.3f\" % bias(getg(mmov_s), mmov_s, vocab2, cities), \\\n",
      "      \"%.3f\" % bias(getg(mmov_s), mmov_s, vocab2, objects), \\\n",
      "      \"%.3f\" % bias(getg(mmov_s), mmov_s, vocab2, gender_specific))\n",
      "183/169:\n",
      "print(labels[3])\n",
      "for i in gdict:\n",
      "    print(\"%10s\" % i, \\\n",
      "          \"%.3f\" % bias(gdict[i][3], mdict[i][3], vocab2, proflist), \\\n",
      "          \"%.3f\" % bias(gdict[i][3], mdict[i][3], vocab2, adjectives), \\\n",
      "          \"%.3f\" % bias(gdict[i][3], mdict[i][3], vocab2, cities), \\\n",
      "          \"%.3f\" % bias(gdict[i][3], mdict[i][3], vocab2, objects), \\\n",
      "          \"%.3f\" % bias(gdict[i][3], mdict[i][3], vocab2, gender_specific))\n",
      "183/170:\n",
      "print(labels[4])\n",
      "for i in gdict:\n",
      "    print(\"%10s\" % i, \\\n",
      "          \"%.3f\" % bias(gdict[i][4], mdict[i][4], vocab2, proflist), \\\n",
      "          \"%.3f\" % bias(gdict[i][4], mdict[i][4], vocab2, adjectives), \\\n",
      "          \"%.3f\" % bias(gdict[i][4], mdict[i][4], vocab2, cities), \\\n",
      "          \"%.3f\" % bias(gdict[i][4], mdict[i][4], vocab2, objects), \\\n",
      "          \"%.3f\" % bias(gdict[i][4], mdict[i][4], vocab2, gender_specific))\n",
      "183/171:\n",
      "gdict = {\"politics\": [], \"books\": [], \"movies\": []}\n",
      "labels = [\"original\", \"summary (TextRank)\", \"summary (Centroid-BOW)\", \"summary (Centroid-WE)\", \"summary (control)\"]\n",
      "183/172:\n",
      "gdictadd([mpol, mmov, mbooks])\n",
      "gdictadd([mpol_s, mbks_s, mmov_s])\n",
      "gdictadd([mpol_sb, mbks_sb, mmov_sb])\n",
      "gdictadd([mpol_sc, mbks_sc, mmov_sc])\n",
      "gdictadd([mpol_c, mbks_c, mmov_c])\n",
      "183/173:\n",
      "gdict = {\"politics\": [], \"books\": [], \"movies\": []}\n",
      "labels = [\"original\", \"summary (TextRank)\", \"summary (Centroid-BOW)\", \"summary (Centroid-WE)\", \"summary (control)\"]\n",
      "183/174:\n",
      "def mdictadd(models):\n",
      "    for i in range(3):\n",
      "        mdict[list(mdict)[i]].append(models[i])\n",
      "183/175:\n",
      "mdictadd([mpol, mmov, mbooks])\n",
      "mdictadd([mpol_s, mbks_s, mmov_s])\n",
      "mdictadd([mpol_sb, mbks_sb, mmov_sb])\n",
      "mdictadd([mpol_sc, mbks_sc, mmov_sc])\n",
      "mdictadd([mpol_c, mbks_c, mmov_c])\n",
      "183/176: mdict = {\"politics\": [], \"books\": [], \"movies\": []}\n",
      "183/177:\n",
      "mdictadd([mpol, mmov, mbooks])\n",
      "mdictadd([mpol_s, mbks_s, mmov_s])\n",
      "mdictadd([mpol_sb, mbks_sb, mmov_sb])\n",
      "mdictadd([mpol_sc, mbks_sc, mmov_sc])\n",
      "mdictadd([mpol_c, mbks_c, mmov_c])\n",
      "183/178:\n",
      "def gdictadd(models):\n",
      "    for i in range(3):\n",
      "        gdict[list(gdict)[i]].append(getg(models[i]))\n",
      "183/179:\n",
      "gdictadd([mpol, mmov, mbooks])\n",
      "gdictadd([mpol_s, mbks_s, mmov_s])\n",
      "gdictadd([mpol_sb, mbks_sb, mmov_sb])\n",
      "gdictadd([mpol_sc, mbks_sc, mmov_sc])\n",
      "gdictadd([mpol_c, mbks_c, mmov_c])\n",
      "183/180:\n",
      "print(labels[0])\n",
      "for i in gdict:\n",
      "    print(\"%10s\" % i, \\\n",
      "          \"%.3f\" % bias(gdict[i][0], mdict[i][0], vocab, proflist), \\\n",
      "          \"%.3f\" % bias(gdict[i][0], mdict[i][0], vocab, adjectives), \\\n",
      "          \"%.3f\" % bias(gdict[i][0], mdict[i][0], vocab, cities), \\\n",
      "          \"%.3f\" % bias(gdict[i][0], mdict[i][0], vocab, objects), \\\n",
      "          \"%.3f\" % bias(gdict[i][0], mdict[i][0], vocab, gender_specific))\n",
      "183/181:\n",
      "print(labels[1])\n",
      "for i in gdict:\n",
      "    print(\"%10s\" % i, \\\n",
      "          \"%.3f\" % bias(gdict[i][1], mdict[i][1], vocab2, proflist), \\\n",
      "          \"%.3f\" % bias(gdict[i][1], mdict[i][1], vocab2, adjectives), \\\n",
      "          \"%.3f\" % bias(gdict[i][1], mdict[i][1], vocab2, cities), \\\n",
      "          \"%.3f\" % bias(gdict[i][1], mdict[i][1], vocab2, objects), \\\n",
      "          \"%.3f\" % bias(gdict[i][1], mdict[i][1], vocab2, gender_specific))\n",
      "183/182:\n",
      "print(labels[2])\n",
      "for i in gdict:\n",
      "    print(\"%10s\" % i, \\\n",
      "          \"%.3f\" % bias(gdict[i][2], mdict[i][2], vocab2, proflist), \\\n",
      "          \"%.3f\" % bias(gdict[i][2], mdict[i][2], vocab2, adjectives), \\\n",
      "          \"%.3f\" % bias(gdict[i][2], mdict[i][2], vocab2, cities), \\\n",
      "          \"%.3f\" % bias(gdict[i][2], mdict[i][2], vocab2, objects), \\\n",
      "          \"%.3f\" % bias(gdict[i][2], mdict[i][2], vocab2, gender_specific))\n",
      "183/183:\n",
      "print(labels[3])\n",
      "for i in gdict:\n",
      "    print(\"%10s\" % i, \\\n",
      "          \"%.3f\" % bias(gdict[i][3], mdict[i][3], vocab2, proflist), \\\n",
      "          \"%.3f\" % bias(gdict[i][3], mdict[i][3], vocab2, adjectives), \\\n",
      "          \"%.3f\" % bias(gdict[i][3], mdict[i][3], vocab2, cities), \\\n",
      "          \"%.3f\" % bias(gdict[i][3], mdict[i][3], vocab2, objects), \\\n",
      "          \"%.3f\" % bias(gdict[i][3], mdict[i][3], vocab2, gender_specific))\n",
      "183/184:\n",
      "print(labels[4])\n",
      "for i in gdict:\n",
      "    print(\"%10s\" % i, \\\n",
      "          \"%.3f\" % bias(gdict[i][4], mdict[i][4], vocab2, proflist), \\\n",
      "          \"%.3f\" % bias(gdict[i][4], mdict[i][4], vocab2, adjectives), \\\n",
      "          \"%.3f\" % bias(gdict[i][4], mdict[i][4], vocab2, cities), \\\n",
      "          \"%.3f\" % bias(gdict[i][4], mdict[i][4], vocab2, objects), \\\n",
      "          \"%.3f\" % bias(gdict[i][4], mdict[i][4], vocab2, gender_specific))\n",
      "183/185:\n",
      "t = time.process_time()\n",
      "cwe_mov = getcwe(movies)\n",
      "time.process_time() - t\n",
      "183/186: np.mean([len(nltk.word_tokenize(getctrls(movies[:100])[i])) for i in range(100)])\n",
      "183/187: np.mean([len(nltk.word_tokenize(getbows(movies[:100])[i])) for i in range(100)])\n",
      "183/188: len(cwe_mov)\n",
      "183/189:\n",
      "t = time.process_time()\n",
      "cwe_mov = getcwe(movies)\n",
      "time.process_time() - t\n",
      "183/190:\n",
      "t = time.process_time()\n",
      "bow_mov = getbows(movies)\n",
      "time.process_time() - t\n",
      "183/191:\n",
      "t = time.process_time()\n",
      "ctrls_mov = getctrls(movies)\n",
      "time.process_time() - t\n",
      "183/192:\n",
      "mmov_sc = getw2v_s(cwe_mov)\n",
      "mmov_sb = getw2v_s(bow_mov)\n",
      "mmov_c = getw2v_s(ctrls_mov)\n",
      "183/193: mdict = {\"politics\": [], \"books\": [], \"movies\": []}\n",
      "183/194:\n",
      "mdictadd([mpol, mmov, mbooks])\n",
      "mdictadd([mpol_s, mbks_s, mmov_s])\n",
      "mdictadd([mpol_sb, mbks_sb, mmov_sb])\n",
      "mdictadd([mpol_sc, mbks_sc, mmov_sc])\n",
      "mdictadd([mpol_c, mbks_c, mmov_c])\n",
      "183/195:\n",
      "gdict = {\"politics\": [], \"books\": [], \"movies\": []}\n",
      "labels = [\"original\", \"summary (TextRank)\", \"summary (Centroid-BOW)\", \"summary (Centroid-WE)\", \"summary (control)\"]\n",
      "gdictadd([mpol, mmov, mbooks])\n",
      "gdictadd([mpol_s, mbks_s, mmov_s])\n",
      "gdictadd([mpol_sb, mbks_sb, mmov_sb])\n",
      "gdictadd([mpol_sc, mbks_sc, mmov_sc])\n",
      "gdictadd([mpol_c, mbks_c, mmov_c])\n",
      "183/196:\n",
      "print(labels[4])\n",
      "print(\"%10s\" % i, \\\n",
      "      \"%.3f\" % bias(getg(mmov_c), mmov_c, vocab2, proflist), \\\n",
      "      \"%.3f\" % bias(getg(mmov_c), mmov_c, vocab2, adjectives), \\\n",
      "      \"%.3f\" % bias(getg(mmov_c), mmov_c, vocab2, cities), \\\n",
      "      \"%.3f\" % bias(getg(mmov_c), mmov_c, vocab2, objects), \\\n",
      "      \"%.3f\" % bias(getg(mmov_c), mmov_c, vocab2, gender_specific))\n",
      "183/197:\n",
      "vocab_all = set(vocab)\n",
      "for m in [mpol_s, mbks_s, mmov_s] + \\\n",
      "[mpol_sb, mbks_sb, mmov_sb] + \\\n",
      "[mpol_sc, mbks_sc, mmov_sc] + \\\n",
      "[mpol_c, mbks_c, mmov_c]:\n",
      "    vocab_all &= set(m.wv.vocab)\n",
      "len(vocab_all)\n",
      "183/198: vocab2 = vocab_all\n",
      "183/199:\n",
      "print(labels[4])\n",
      "print(\"%10s\" % i, \\\n",
      "      \"%.3f\" % bias(getg(mmov_c), mmov_c, vocab2, proflist), \\\n",
      "      \"%.3f\" % bias(getg(mmov_c), mmov_c, vocab2, adjectives), \\\n",
      "      \"%.3f\" % bias(getg(mmov_c), mmov_c, vocab2, cities), \\\n",
      "      \"%.3f\" % bias(getg(mmov_c), mmov_c, vocab2, objects), \\\n",
      "      \"%.3f\" % bias(getg(mmov_c), mmov_c, vocab2, gender_specific))\n",
      "183/200:\n",
      "t = time.process_time()\n",
      "ctrls_mov = getctrls(movies)\n",
      "time.process_time() - t\n",
      "183/201: mmov_c = getw2v_s(ctrls_mov)\n",
      "183/202:\n",
      "print(labels[4])\n",
      "print(\"%10s\" % i, \\\n",
      "      \"%.3f\" % bias(getg(mmov_c), mmov_c, vocab2, proflist), \\\n",
      "      \"%.3f\" % bias(getg(mmov_c), mmov_c, vocab2, adjectives), \\\n",
      "      \"%.3f\" % bias(getg(mmov_c), mmov_c, vocab2, cities), \\\n",
      "      \"%.3f\" % bias(getg(mmov_c), mmov_c, vocab2, objects), \\\n",
      "      \"%.3f\" % bias(getg(mmov_c), mmov_c, vocab2, gender_specific))\n",
      "183/203:\n",
      "vocab_all = set(vocab)\n",
      "for m in [mpol_s, mbks_s, mmov_s] + \\\n",
      "[mpol_sb, mbks_sb, mmov_sb] + \\\n",
      "[mpol_sc, mbks_sc, mmov_sc] + \\\n",
      "[mpol_c, mbks_c, mmov_c]:\n",
      "    vocab_all &= set(m.wv.vocab)\n",
      "len(vocab_all)\n",
      "183/204: vocab2 = vocab_all\n",
      "183/205:\n",
      "print(labels[4])\n",
      "print(\"%10s\" % i, \\\n",
      "      \"%.3f\" % bias(getg(mmov_c), mmov_c, vocab2, proflist), \\\n",
      "      \"%.3f\" % bias(getg(mmov_c), mmov_c, vocab2, adjectives), \\\n",
      "      \"%.3f\" % bias(getg(mmov_c), mmov_c, vocab2, cities), \\\n",
      "      \"%.3f\" % bias(getg(mmov_c), mmov_c, vocab2, objects), \\\n",
      "      \"%.3f\" % bias(getg(mmov_c), mmov_c, vocab2, gender_specific))\n",
      "183/206:\n",
      "print(labels[4])\n",
      "for i in gdict:\n",
      "    print(\"%10s\" % i, \\\n",
      "          \"%.3f\" % bias(gdict[i][4], mdict[i][4], vocab2, proflist), \\\n",
      "          \"%.3f\" % bias(gdict[i][4], mdict[i][4], vocab2, adjectives), \\\n",
      "          \"%.3f\" % bias(gdict[i][4], mdict[i][4], vocab2, cities), \\\n",
      "          \"%.3f\" % bias(gdict[i][4], mdict[i][4], vocab2, objects), \\\n",
      "          \"%.3f\" % bias(gdict[i][4], mdict[i][4], vocab2, gender_specific))\n",
      "183/207:\n",
      "gdict = {\"politics\": [], \"books\": [], \"movies\": []}\n",
      "labels = [\"original\", \"summary (TextRank)\", \"summary (Centroid-BOW)\", \"summary (Centroid-WE)\", \"summary (control)\"]\n",
      "gdictadd([mpol, mmov, mbooks])\n",
      "gdictadd([mpol_s, mbks_s, mmov_s])\n",
      "gdictadd([mpol_sb, mbks_sb, mmov_sb])\n",
      "gdictadd([mpol_sc, mbks_sc, mmov_sc])\n",
      "gdictadd([mpol_c, mbks_c, mmov_c])\n",
      "183/208:\n",
      "mdict = {\"politics\": [], \"books\": [], \"movies\": []}\n",
      "mdictadd([mpol, mmov, mbooks])\n",
      "mdictadd([mpol_s, mbks_s, mmov_s])\n",
      "mdictadd([mpol_sb, mbks_sb, mmov_sb])\n",
      "mdictadd([mpol_sc, mbks_sc, mmov_sc])\n",
      "mdictadd([mpol_c, mbks_c, mmov_c])\n",
      "183/209:\n",
      "print(labels[0])\n",
      "for i in gdict:\n",
      "    print(\"%10s\" % i, \\\n",
      "          \"%.3f\" % bias(gdict[i][0], mdict[i][0], vocab, proflist), \\\n",
      "          \"%.3f\" % bias(gdict[i][0], mdict[i][0], vocab, adjectives), \\\n",
      "          \"%.3f\" % bias(gdict[i][0], mdict[i][0], vocab, cities), \\\n",
      "          \"%.3f\" % bias(gdict[i][0], mdict[i][0], vocab, objects), \\\n",
      "          \"%.3f\" % bias(gdict[i][0], mdict[i][0], vocab, gender_specific))\n",
      "183/210:\n",
      "print(labels[1])\n",
      "for i in gdict:\n",
      "    print(\"%10s\" % i, \\\n",
      "          \"%.3f\" % bias(gdict[i][1], mdict[i][1], vocab2, proflist), \\\n",
      "          \"%.3f\" % bias(gdict[i][1], mdict[i][1], vocab2, adjectives), \\\n",
      "          \"%.3f\" % bias(gdict[i][1], mdict[i][1], vocab2, cities), \\\n",
      "          \"%.3f\" % bias(gdict[i][1], mdict[i][1], vocab2, objects), \\\n",
      "          \"%.3f\" % bias(gdict[i][1], mdict[i][1], vocab2, gender_specific))\n",
      "183/211:\n",
      "print(labels[2])\n",
      "for i in gdict:\n",
      "    print(\"%10s\" % i, \\\n",
      "          \"%.3f\" % bias(gdict[i][2], mdict[i][2], vocab2, proflist), \\\n",
      "          \"%.3f\" % bias(gdict[i][2], mdict[i][2], vocab2, adjectives), \\\n",
      "          \"%.3f\" % bias(gdict[i][2], mdict[i][2], vocab2, cities), \\\n",
      "          \"%.3f\" % bias(gdict[i][2], mdict[i][2], vocab2, objects), \\\n",
      "          \"%.3f\" % bias(gdict[i][2], mdict[i][2], vocab2, gender_specific))\n",
      "183/212:\n",
      "print(labels[3])\n",
      "for i in gdict:\n",
      "    print(\"%10s\" % i, \\\n",
      "          \"%.3f\" % bias(gdict[i][3], mdict[i][3], vocab2, proflist), \\\n",
      "          \"%.3f\" % bias(gdict[i][3], mdict[i][3], vocab2, adjectives), \\\n",
      "          \"%.3f\" % bias(gdict[i][3], mdict[i][3], vocab2, cities), \\\n",
      "          \"%.3f\" % bias(gdict[i][3], mdict[i][3], vocab2, objects), \\\n",
      "          \"%.3f\" % bias(gdict[i][3], mdict[i][3], vocab2, gender_specific))\n",
      "183/213:\n",
      "print(labels[4])\n",
      "for i in gdict:\n",
      "    print(\"%10s\" % i, \\\n",
      "          \"%.3f\" % bias(gdict[i][4], mdict[i][4], vocab2, proflist), \\\n",
      "          \"%.3f\" % bias(gdict[i][4], mdict[i][4], vocab2, adjectives), \\\n",
      "          \"%.3f\" % bias(gdict[i][4], mdict[i][4], vocab2, cities), \\\n",
      "          \"%.3f\" % bias(gdict[i][4], mdict[i][4], vocab2, objects), \\\n",
      "          \"%.3f\" % bias(gdict[i][4], mdict[i][4], vocab2, gender_specific))\n",
      "183/214: np.mean([len(nltk.word_tokenize(getcwe(movies[:100])[i])) for i in range(100)])\n",
      "183/215:\n",
      "def getcwe(category):\n",
      "    s = []\n",
      "    for i in category:\n",
      "        try:\n",
      "            s.append(centroid_we_summarizer.summarize(i.text, limit=200-25))\n",
      "        except:\n",
      "            print(i)\n",
      "    return s\n",
      "183/216:\n",
      "t = time.process_time()\n",
      "cwe_mov = getcwe(movies)\n",
      "time.process_time() - t\n",
      "183/217:\n",
      "mmov_sc = getw2v_s(cwe_mov)\n",
      "# mmov_sb = getw2v_s(bow_mov)\n",
      "# mmov_c = getw2v_s(ctrls_mov)\n",
      "183/218:\n",
      "gdict = {\"politics\": [], \"books\": [], \"movies\": []}\n",
      "labels = [\"original\", \"summary (TextRank)\", \"summary (Centroid-BOW)\", \"summary (Centroid-WE)\", \"summary (control)\"]\n",
      "gdictadd([mpol, mmov, mbooks])\n",
      "gdictadd([mpol_s, mbks_s, mmov_s])\n",
      "gdictadd([mpol_sb, mbks_sb, mmov_sb])\n",
      "gdictadd([mpol_sc, mbks_sc, mmov_sc])\n",
      "gdictadd([mpol_c, mbks_c, mmov_c])\n",
      "183/219:\n",
      "mdict = {\"politics\": [], \"books\": [], \"movies\": []}\n",
      "mdictadd([mpol, mmov, mbooks])\n",
      "mdictadd([mpol_s, mbks_s, mmov_s])\n",
      "mdictadd([mpol_sb, mbks_sb, mmov_sb])\n",
      "mdictadd([mpol_sc, mbks_sc, mmov_sc])\n",
      "mdictadd([mpol_c, mbks_c, mmov_c])\n",
      "183/220:\n",
      "print(labels[3])\n",
      "for i in gdict:\n",
      "    print(\"%10s\" % i, \\\n",
      "          \"%.3f\" % bias(gdict[i][3], mdict[i][3], vocab2, proflist), \\\n",
      "          \"%.3f\" % bias(gdict[i][3], mdict[i][3], vocab2, adjectives), \\\n",
      "          \"%.3f\" % bias(gdict[i][3], mdict[i][3], vocab2, cities), \\\n",
      "          \"%.3f\" % bias(gdict[i][3], mdict[i][3], vocab2, objects), \\\n",
      "          \"%.3f\" % bias(gdict[i][3], mdict[i][3], vocab2, gender_specific))\n",
      "183/221:\n",
      "t = time.process_time()\n",
      "ctrls_mov = getctrls(movies)\n",
      "time.process_time() - t\n",
      "183/222: mmov_c = getw2v_s(ctrls_mov)\n",
      "183/223:\n",
      "print(labels[4])\n",
      "print(\"%10s\" % i, \\\n",
      "      \"%.3f\" % bias(getg(mmov_c), mmov_c, vocab2, proflist), \\\n",
      "      \"%.3f\" % bias(getg(mmov_c), mmov_c, vocab2, adjectives), \\\n",
      "      \"%.3f\" % bias(getg(mmov_c), mmov_c, vocab2, cities), \\\n",
      "      \"%.3f\" % bias(getg(mmov_c), mmov_c, vocab2, objects), \\\n",
      "      \"%.3f\" % bias(getg(mmov_c), mmov_c, vocab2, gender_specific))\n",
      "183/224:\n",
      "vocab_all = set(vocab)\n",
      "for m in [mpol_s, mbks_s, mmov_s] + \\\n",
      "[mpol_sb, mbks_sb, mmov_sb] + \\\n",
      "[mpol_sc, mbks_sc, mmov_sc] + \\\n",
      "[mpol_c, mbks_c, mmov_c]:\n",
      "    vocab_all &= set(m.wv.vocab)\n",
      "len(vocab_all)\n",
      "183/225: vocab2 = vocab_all\n",
      "183/226:\n",
      "print(labels[4])\n",
      "print(\"%10s\" % i, \\\n",
      "      \"%.3f\" % bias(getg(mmov_c), mmov_c, vocab2, proflist), \\\n",
      "      \"%.3f\" % bias(getg(mmov_c), mmov_c, vocab2, adjectives), \\\n",
      "      \"%.3f\" % bias(getg(mmov_c), mmov_c, vocab2, cities), \\\n",
      "      \"%.3f\" % bias(getg(mmov_c), mmov_c, vocab2, objects), \\\n",
      "      \"%.3f\" % bias(getg(mmov_c), mmov_c, vocab2, gender_specific))\n",
      "183/227:\n",
      "gdict[i][4] = getg(mmov_c)\n",
      "mdict[i][4] = mmov_c\n",
      "183/228:\n",
      "print(labels[4])\n",
      "for i in gdict:\n",
      "    print(\"%10s\" % i, \\\n",
      "          \"%.3f\" % bias(gdict[i][4], mdict[i][4], vocab2, proflist), \\\n",
      "          \"%.3f\" % bias(gdict[i][4], mdict[i][4], vocab2, adjectives), \\\n",
      "          \"%.3f\" % bias(gdict[i][4], mdict[i][4], vocab2, cities), \\\n",
      "          \"%.3f\" % bias(gdict[i][4], mdict[i][4], vocab2, objects), \\\n",
      "          \"%.3f\" % bias(gdict[i][4], mdict[i][4], vocab2, gender_specific))\n",
      "183/229:\n",
      "t = time.process_time()\n",
      "bow_mov = getbows(movies)\n",
      "time.process_time() - t\n",
      "183/230: bias(getg(bow_mov), bow_mov, vocab2, proflist)\n",
      "183/231:\n",
      "# mmov_sc = getw2v_s(cwe_mov)\n",
      "mmov_sb = getw2v_s(bow_mov)\n",
      "# mmov_c = getw2v_s(ctrls_mov)\n",
      "183/232: bias(getg(mmov_sb), mmov_sb, vocab2, proflist)\n",
      "183/233:\n",
      "print(labels[0])\n",
      "for i in gdict:\n",
      "    print(\"%10s\" % i, \\\n",
      "          \"%.3f\" % bias(gdict[i][0], mdict[i][0], vocab, proflist), \\\n",
      "          \"%.3f\" % bias(gdict[i][0], mdict[i][0], vocab, adjectives), \\\n",
      "          \"%.3f\" % bias(gdict[i][0], mdict[i][0], vocab, cities), \\\n",
      "          \"%.3f\" % bias(gdict[i][0], mdict[i][0], vocab, objects), \\\n",
      "          \"%.3f\" % bias(gdict[i][0], mdict[i][0], vocab, gender_specific))\n",
      "183/234: i\n",
      "183/235:\n",
      "gdict[i][0] = getg(mmov_sb)\n",
      "mdict[i][0] = mmov_sb\n",
      "183/236:\n",
      "print(labels[0])\n",
      "for i in gdict:\n",
      "    print(\"%10s\" % i, \\\n",
      "          \"%.3f\" % bias(gdict[i][0], mdict[i][0], vocab, proflist), \\\n",
      "          \"%.3f\" % bias(gdict[i][0], mdict[i][0], vocab, adjectives), \\\n",
      "          \"%.3f\" % bias(gdict[i][0], mdict[i][0], vocab, cities), \\\n",
      "          \"%.3f\" % bias(gdict[i][0], mdict[i][0], vocab, objects), \\\n",
      "          \"%.3f\" % bias(gdict[i][0], mdict[i][0], vocab, gender_specific))\n",
      "183/237:\n",
      "gdict[i][0] = getg(mmov)\n",
      "gdict[i][0] = mmov\n",
      "183/238:\n",
      "print(labels[0])\n",
      "for i in gdict:\n",
      "    print(\"%10s\" % i, \\\n",
      "          \"%.3f\" % bias(gdict[i][0], mdict[i][0], vocab, proflist), \\\n",
      "          \"%.3f\" % bias(gdict[i][0], mdict[i][0], vocab, adjectives), \\\n",
      "          \"%.3f\" % bias(gdict[i][0], mdict[i][0], vocab, cities), \\\n",
      "          \"%.3f\" % bias(gdict[i][0], mdict[i][0], vocab, objects), \\\n",
      "          \"%.3f\" % bias(gdict[i][0], mdict[i][0], vocab, gender_specific))\n",
      "183/239:\n",
      "gdict[i][0] = getg(mmov)\n",
      "mdict[i][0] = mmov\n",
      "183/240:\n",
      "print(labels[0])\n",
      "for i in gdict:\n",
      "    print(\"%10s\" % i, \\\n",
      "          \"%.3f\" % bias(gdict[i][0], mdict[i][0], vocab, proflist), \\\n",
      "          \"%.3f\" % bias(gdict[i][0], mdict[i][0], vocab, adjectives), \\\n",
      "          \"%.3f\" % bias(gdict[i][0], mdict[i][0], vocab, cities), \\\n",
      "          \"%.3f\" % bias(gdict[i][0], mdict[i][0], vocab, objects), \\\n",
      "          \"%.3f\" % bias(gdict[i][0], mdict[i][0], vocab, gender_specific))\n",
      "183/241:\n",
      "gdict['books'][0] = getg(mbks)\n",
      "mdict['books'][0] = mbks\n",
      "183/242:\n",
      "gdict['books'][0] = getg(mbooks)\n",
      "mdict['books'][0] = mbooks\n",
      "183/243:\n",
      "print(labels[0])\n",
      "for i in gdict:\n",
      "    print(\"%10s\" % i, \\\n",
      "          \"%.3f\" % bias(gdict[i][0], mdict[i][0], vocab, proflist), \\\n",
      "          \"%.3f\" % bias(gdict[i][0], mdict[i][0], vocab, adjectives), \\\n",
      "          \"%.3f\" % bias(gdict[i][0], mdict[i][0], vocab, cities), \\\n",
      "          \"%.3f\" % bias(gdict[i][0], mdict[i][0], vocab, objects), \\\n",
      "          \"%.3f\" % bias(gdict[i][0], mdict[i][0], vocab, gender_specific))\n",
      "183/244:\n",
      "gdict[i][2] = getg(mmov_sb)\n",
      "mdict[i][2] = mmov_sb\n",
      "183/245:\n",
      "print(labels[1])\n",
      "for i in gdict:\n",
      "    print(\"%10s\" % i, \\\n",
      "          \"%.3f\" % bias(gdict[i][1], mdict[i][1], vocab2, proflist), \\\n",
      "          \"%.3f\" % bias(gdict[i][1], mdict[i][1], vocab2, adjectives), \\\n",
      "          \"%.3f\" % bias(gdict[i][1], mdict[i][1], vocab2, cities), \\\n",
      "          \"%.3f\" % bias(gdict[i][1], mdict[i][1], vocab2, objects), \\\n",
      "          \"%.3f\" % bias(gdict[i][1], mdict[i][1], vocab2, gender_specific))\n",
      "183/246:\n",
      "print(labels[2])\n",
      "for i in gdict:\n",
      "    print(\"%10s\" % i, \\\n",
      "          \"%.3f\" % bias(gdict[i][2], mdict[i][2], vocab2, proflist), \\\n",
      "          \"%.3f\" % bias(gdict[i][2], mdict[i][2], vocab2, adjectives), \\\n",
      "          \"%.3f\" % bias(gdict[i][2], mdict[i][2], vocab2, cities), \\\n",
      "          \"%.3f\" % bias(gdict[i][2], mdict[i][2], vocab2, objects), \\\n",
      "          \"%.3f\" % bias(gdict[i][2], mdict[i][2], vocab2, gender_specific))\n",
      "183/247:\n",
      "t = time.process_time()\n",
      "bow_mov = getbows(movies)\n",
      "time.process_time() - t\n",
      "183/248:\n",
      "bow_mov = getbows(movies)\n",
      "mmov_sb = getw2v_s(bow_mov)\n",
      "bias(getg(mmov_sb), mmov_sb, vocab2, proflist)\n",
      "183/249:\n",
      "gdict[i][2] = getg(mmov_sb)\n",
      "mdict[i][2] = mmov_sb\n",
      "183/250:\n",
      "print(labels[2])\n",
      "for i in gdict:\n",
      "    print(\"%10s\" % i, \\\n",
      "          \"%.3f\" % bias(gdict[i][2], mdict[i][2], vocab2, proflist), \\\n",
      "          \"%.3f\" % bias(gdict[i][2], mdict[i][2], vocab2, adjectives), \\\n",
      "          \"%.3f\" % bias(gdict[i][2], mdict[i][2], vocab2, cities), \\\n",
      "          \"%.3f\" % bias(gdict[i][2], mdict[i][2], vocab2, objects), \\\n",
      "          \"%.3f\" % bias(gdict[i][2], mdict[i][2], vocab2, gender_specific))\n",
      "183/251:\n",
      "cwe_mov = getcwe(movies)\n",
      "mmov_sc = getw2v_s(cwe_mov)\n",
      "bias(getg(mmov_sc), mmov_sc, vocab2, proflist)\n",
      "183/252:\n",
      "cwe_mov = getcwe(movies)\n",
      "mmov_sc = getw2v_s(cwe_mov)\n",
      "bias(getg(mmov_sc), mmov_sc, vocab2, proflist)\n",
      "183/253:\n",
      "def getcwe(category):\n",
      "    s = []\n",
      "    for i in category:\n",
      "        try:\n",
      "            s.append(centroid_we_summarizer.summarize(i.text, limit=200-25))\n",
      "        except:\n",
      "            print(i)\n",
      "    return s\n",
      "183/254:\n",
      "mmov_sc = getw2v_s(cwe_mov)\n",
      "# mmov_sb = getw2v_s(bow_mov)\n",
      "# mmov_c = getw2v_s(ctrls_mov)\n",
      "183/255: bias(getg(mmov_sc), mmov_sc, vocab2, proflist)\n",
      "183/256: np.mean([len(nltk.word_tokenize(cwe_mov[i])) for i in range(100)])\n",
      "183/257:\n",
      "def getcwe(category):\n",
      "    s = []\n",
      "    for i in category:\n",
      "        try:\n",
      "            s.append(centroid_we_summarizer.summarize(i.text, limit=200-20))\n",
      "        except:\n",
      "            print(i)\n",
      "    return s\n",
      "183/258:\n",
      "t = time.process_time()\n",
      "cwe_mov = getcwe(movies)\n",
      "time.process_time() - t\n",
      "183/259:\n",
      "mmov_sc = getw2v_s(cwe_mov)\n",
      "# mmov_sb = getw2v_s(bow_mov)\n",
      "# mmov_c = getw2v_s(ctrls_mov)\n",
      "183/260: bias(getg(mmov_sc), mmov_sc, vocab2, proflist)\n",
      "183/261:\n",
      "# todo\n",
      "cwe_mov = getcwe(movies)\n",
      "mmov_sc = getw2v_s(cwe_mov)\n",
      "bias(getg(mmov_sc), mmov_sc, vocab2, proflist)\n",
      "183/262:\n",
      "def getcwe(category):\n",
      "    s = []\n",
      "    for i in category:\n",
      "        try:\n",
      "            s.append(centroid_we_summarizer.summarize(i.text, limit=200-15))\n",
      "        except:\n",
      "            print(i)\n",
      "    return s\n",
      "183/263:\n",
      "# todo\n",
      "cwe_mov = getcwe(movies)\n",
      "mmov_sc = getw2v_s(cwe_mov)\n",
      "bias(getg(mmov_sc), mmov_sc, vocab2, proflist)\n",
      "184/1:\n",
      "import os\n",
      "import time\n",
      "import dill\n",
      "from typing import List\n",
      "from functools import reduce\n",
      "from calendar import monthrange\n",
      "184/2:\n",
      "import json\n",
      "import csv\n",
      "import requests\n",
      "import xml.etree.ElementTree as ET\n",
      "from bs4 import BeautifulSoup\n",
      "import urllib.request as urllib2\n",
      "184/3:\n",
      "import matplotlib.pyplot as plt\n",
      "import numpy as np\n",
      "from scipy import stats\n",
      "from random import sample, randint, shuffle\n",
      "from sklearn.decomposition import PCA\n",
      "184/4:\n",
      "import nltk\n",
      "import text_summarizer\n",
      "from gensim.utils import simple_preprocess\n",
      "from gensim.summarization.summarizer import summarize\n",
      "from gensim.models import Word2Vec, LsiModel\n",
      "from gensim.test.utils import common_dictionary, common_corpus\n",
      "184/5: from time import process_time\n",
      "184/6:\n",
      "t = process_time()\n",
      "dill.load_session('thesis_env.db')\n",
      "process_time() - t\n",
      "184/7: len(adjectives)\n",
      "184/8: len(all_)\n",
      "184/9:\n",
      "with open('objects.csv') as csv_file:\n",
      "    objects = csv_file.read().split(\"\\n\")\n",
      "objects[0] = 'angle'\n",
      "184/10: cities = [x.lower() for x in cities]\n",
      "184/11:\n",
      "with open('cities.csv') as csv_file:\n",
      "    cities = csv_file.read().split(\",\")\n",
      "184/12: cities = [x.lower() for x in cities]\n",
      "184/13:\n",
      "t = time.process_time()\n",
      "alld = {}\n",
      "for doc in all_:\n",
      "    doc = Doc(doc.id, doc.gdescriptors, doc.descriptors, doc.text)\n",
      "    for d in doc.descriptors:\n",
      "        count, docs = alld.get(d, (0, set()))\n",
      "        docs.add(doc)\n",
      "        alld[d] = (count + 1, docs)\n",
      "time.process_time() - t\n",
      "184/14:\n",
      "alldlist = sorted([(d, alld[d][0], alld[d][1]) for d in alld], key=lambda x: x[1], reverse=True)\n",
      "for x in range(15):\n",
      "    print(alldlist[x][0:2])\n",
      "184/15:\n",
      "def exclude(e, x):\n",
      "    for d in x.descriptors:\n",
      "        if d == e:\n",
      "            continue\n",
      "        if d in ['Politics and Government', 'Motion Pictures', 'Books and Literature']:\n",
      "            return False\n",
      "    return True\n",
      "184/16:\n",
      "def getlist(topic):\n",
      "    return list(filter(lambda x: exclude(topic, x), alld[topic][1]))\n",
      "184/17: politics = getlist('Politics and Government')\n",
      "184/18: movies = getlist('Motion Pictures')\n",
      "184/19: books = getlist('Books and Literature')\n",
      "184/20: len(politics), len(movies), len(books)\n",
      "184/21: politics = sample(politics, 7370)\n",
      "184/22: movies = sample(movies, 7370)\n",
      "184/23: books = sample(books, 7370)\n",
      "184/24:\n",
      "def getw2v(category):\n",
      "    sentences = []\n",
      "    for t in category:\n",
      "        sentences += nltk.sent_tokenize(t.text)\n",
      "    docs = [simple_preprocess(s) for s in sentences]\n",
      "    return Word2Vec(docs)\n",
      "184/25:\n",
      "t = time.process_time()\n",
      "mpol = getw2v(politics)\n",
      "time.process_time() - t\n",
      "184/26:\n",
      "t = time.process_time()\n",
      "mmov = getw2v(movies)\n",
      "time.process_time() - t\n",
      "184/27:\n",
      "t = time.process_time()\n",
      "mbooks = getw2v(books)\n",
      "time.process_time() - t\n",
      "184/28:\n",
      "def top5(word):\n",
      "    for m in [mpol, mmov, mbooks]:\n",
      "        print([x[0] for x in m.wv.most_similar(positive=[word], topn=5)])\n",
      "184/29: top5('monday')\n",
      "184/30: top5('house')\n",
      "184/31: top5('success')\n",
      "184/32: top5('seat')\n",
      "184/33: top5(\"publish\")\n",
      "184/34: top5(\"best\")\n",
      "184/35:\n",
      "def getsummaries(category):\n",
      "    s = []\n",
      "    fails = []\n",
      "    for i in range(len(category)):\n",
      "        try:\n",
      "            s.append(summarize(category[i].text, word_count=200))\n",
      "        except:\n",
      "            fails.append(i)\n",
      "            print(i)\n",
      "    return s, fails\n",
      "184/36:\n",
      "t = time.process_time()\n",
      "summaries_pol, fpol = getsummaries(politics)\n",
      "time.process_time() - t\n",
      "184/37:\n",
      "t = time.process_time()\n",
      "summaries_mov, fmov = getsummaries(movies)\n",
      "time.process_time() - t\n",
      "184/38:\n",
      "t = time.process_time()\n",
      "summaries_bks, fbks = getsummaries(books)\n",
      "time.process_time() - t\n",
      "184/39:\n",
      "def getbows(category):\n",
      "    s = []\n",
      "    centroid_summarizer = text_summarizer.CentroidBOWSummarizer()\n",
      "    for i in category:\n",
      "        try:\n",
      "            s.append(centroid_summarizer.summarize(i.text, limit=200-30))\n",
      "        except:\n",
      "            print(i)\n",
      "    return s\n",
      "184/40:\n",
      "t = time.process_time()\n",
      "bow_pol = getbows(politics)\n",
      "time.process_time() - t\n",
      "184/41: len(bow_pol)\n",
      "184/42:\n",
      "t = time.process_time()\n",
      "bow_bks = getbows(books)\n",
      "time.process_time() - t\n",
      "184/43:\n",
      "t = time.process_time()\n",
      "bow_mov = getbows(movies)\n",
      "time.process_time() - t\n",
      "184/44:\n",
      "def getctrls(category):\n",
      "    ctrls = []\n",
      "    for a in category:\n",
      "        sents = nltk.sent_tokenize(a.text)\n",
      "        rs = [i for i in range(len(sents))]\n",
      "        shuffle(rs)\n",
      "        ctrl = []\n",
      "        count = 0\n",
      "        for r in rs:\n",
      "            if count > 200-5:\n",
      "                break\n",
      "            count += len(nltk.word_tokenize(sents[r]))\n",
      "            ctrl.append(sents[r])\n",
      "        summary = \"\\n\".join([s for s in ctrl])\n",
      "        ctrls.append(summary)\n",
      "    return ctrls\n",
      "184/45:\n",
      "t = time.process_time()\n",
      "ctrls_pol = getctrls(politics)\n",
      "time.process_time() - t\n",
      "184/46:\n",
      "t = time.process_time()\n",
      "ctrls_bks = getctrls(books)\n",
      "time.process_time() - t\n",
      "184/47:\n",
      "t = time.process_time()\n",
      "ctrls_mov = getctrls(movies)\n",
      "time.process_time() - t\n",
      "184/48:\n",
      "def getw2v_s(sums):\n",
      "    sentences = []\n",
      "    for t in sums:\n",
      "        sentences += nltk.sent_tokenize(t)\n",
      "    docs = [simple_preprocess(s) for s in sentences]\n",
      "    return Word2Vec(docs)\n",
      "184/49: mpol_s = getw2v_s(summaries_pol)\n",
      "184/50: mbks_s = getw2v_s(summaries_bks)\n",
      "184/51: mmov_s = getw2v_s(summaries_mov)\n",
      "184/52:\n",
      "mpol_b = getw2v_s(bow_pol)\n",
      "mbks_b = getw2v_s(bow_bks)\n",
      "mmov_b = getw2v_s(bow_mov)\n",
      "184/53:\n",
      "mpol_c = getw2v_s(ctrls_pol)\n",
      "mbks_c = getw2v_s(ctrls_bks)\n",
      "mmov_c = getw2v_s(ctrls_mov)\n",
      "184/54: dill.dump_session('thesis_env3.db')\n",
      "185/1:\n",
      "import os\n",
      "import time\n",
      "import dill\n",
      "from typing import List\n",
      "from functools import reduce\n",
      "from calendar import monthrange\n",
      "185/2:\n",
      "import json\n",
      "import csv\n",
      "import requests\n",
      "import xml.etree.ElementTree as ET\n",
      "from bs4 import BeautifulSoup\n",
      "import urllib.request as urllib2\n",
      "185/3:\n",
      "import matplotlib.pyplot as plt\n",
      "import numpy as np\n",
      "from scipy import stats\n",
      "from random import sample, randint, shuffle\n",
      "from sklearn.decomposition import PCA\n",
      "185/4:\n",
      "import nltk\n",
      "import text_summarizer\n",
      "from gensim.utils import simple_preprocess\n",
      "from gensim.summarization.summarizer import summarize\n",
      "from gensim.models import Word2Vec, LsiModel\n",
      "from gensim.test.utils import common_dictionary, common_corpus\n",
      "185/5: from time import process_time\n",
      "185/6:\n",
      "t = process_time()\n",
      "dill.load_session('thesis_env.db')\n",
      "process_time() - t\n",
      "185/7: len(adjectives)\n",
      "185/8: len(all_)\n",
      "185/9:\n",
      "with open('objects.csv') as csv_file:\n",
      "    objects = csv_file.read().split(\"\\n\")\n",
      "objects[0] = 'angle'\n",
      "185/10:\n",
      "with open('cities.csv') as csv_file:\n",
      "    cities = csv_file.read().split(\",\")\n",
      "185/11: cities = [x.lower() for x in cities]\n",
      "185/12:\n",
      "t = time.process_time()\n",
      "alld = {}\n",
      "for doc in all_:\n",
      "    doc = Doc(doc.id, doc.gdescriptors, doc.descriptors, doc.text)\n",
      "    for d in doc.descriptors:\n",
      "        count, docs = alld.get(d, (0, set()))\n",
      "        docs.add(doc)\n",
      "        alld[d] = (count + 1, docs)\n",
      "time.process_time() - t\n",
      "185/13:\n",
      "alldlist = sorted([(d, alld[d][0], alld[d][1]) for d in alld], key=lambda x: x[1], reverse=True)\n",
      "for x in range(15):\n",
      "    print(alldlist[x][0:2])\n",
      "185/14:\n",
      "def exclude(e, x):\n",
      "    for d in x.descriptors:\n",
      "        if d == e:\n",
      "            continue\n",
      "        if d in ['Politics and Government', 'Motion Pictures', 'Books and Literature']:\n",
      "            return False\n",
      "    return True\n",
      "185/15:\n",
      "def getlist(topic):\n",
      "    return list(filter(lambda x: exclude(topic, x), alld[topic][1]))\n",
      "185/16: politics = getlist('Politics and Government')\n",
      "185/17: movies = getlist('Motion Pictures')\n",
      "185/18: books = getlist('Books and Literature')\n",
      "185/19: len(politics), len(movies), len(books)\n",
      "185/20: politics = sample(politics, 7370)\n",
      "185/21: movies = sample(movies, 7370)\n",
      "185/22: books = sample(books, 7370)\n",
      "185/23:\n",
      "def getw2v(category):\n",
      "    sentences = []\n",
      "    for t in category:\n",
      "        sentences += nltk.sent_tokenize(t.text)\n",
      "    docs = [simple_preprocess(s) for s in sentences]\n",
      "    return Word2Vec(docs)\n",
      "185/24:\n",
      "t = time.process_time()\n",
      "mpol = getw2v(politics)\n",
      "time.process_time() - t\n",
      "185/25:\n",
      "t = time.process_time()\n",
      "mmov = getw2v(movies)\n",
      "time.process_time() - t\n",
      "185/26:\n",
      "t = time.process_time()\n",
      "mbooks = getw2v(books)\n",
      "time.process_time() - t\n",
      "185/27:\n",
      "def top5(word):\n",
      "    for m in [mpol, mmov, mbooks]:\n",
      "        print([x[0] for x in m.wv.most_similar(positive=[word], topn=5)])\n",
      "185/28: top5('monday')\n",
      "185/29: top5('house')\n",
      "185/30: top5('success')\n",
      "185/31: top5('seat')\n",
      "185/32: top5(\"publish\")\n",
      "185/33: top5(\"best\")\n",
      "185/34:\n",
      "def getsummaries(category):\n",
      "    s = []\n",
      "    fails = []\n",
      "    for i in range(len(category)):\n",
      "        try:\n",
      "            s.append(summarize(category[i].text, word_count=200))\n",
      "        except:\n",
      "            fails.append(i)\n",
      "            print(i)\n",
      "    return s, fails\n",
      "185/35:\n",
      "t = time.process_time()\n",
      "summaries_pol, fpol = getsummaries(politics)\n",
      "time.process_time() - t\n",
      "185/36:\n",
      "t = time.process_time()\n",
      "summaries_mov, fmov = getsummaries(movies)\n",
      "time.process_time() - t\n",
      "185/37:\n",
      "t = time.process_time()\n",
      "summaries_bks, fbks = getsummaries(books)\n",
      "time.process_time() - t\n",
      "185/38:\n",
      "def getbows(category):\n",
      "    s = []\n",
      "    centroid_summarizer = text_summarizer.CentroidBOWSummarizer()\n",
      "    for i in category:\n",
      "        try:\n",
      "            s.append(centroid_summarizer.summarize(i.text, limit=200-30))\n",
      "        except:\n",
      "            print(i)\n",
      "    return s\n",
      "185/39:\n",
      "t = time.process_time()\n",
      "bow_pol = getbows(politics)\n",
      "time.process_time() - t\n",
      "185/40: len(bow_pol)\n",
      "185/41:\n",
      "t = time.process_time()\n",
      "bow_bks = getbows(books)\n",
      "time.process_time() - t\n",
      "185/42:\n",
      "t = time.process_time()\n",
      "bow_mov = getbows(movies)\n",
      "time.process_time() - t\n",
      "185/43:\n",
      "def getctrls(category):\n",
      "    ctrls = []\n",
      "    for a in category:\n",
      "        sents = nltk.sent_tokenize(a.text)\n",
      "        rs = [i for i in range(len(sents))]\n",
      "        shuffle(rs)\n",
      "        ctrl = []\n",
      "        count = 0\n",
      "        for r in rs:\n",
      "            if count > 200-5:\n",
      "                break\n",
      "            count += len(nltk.word_tokenize(sents[r]))\n",
      "            ctrl.append(sents[r])\n",
      "        summary = \"\\n\".join([s for s in ctrl])\n",
      "        ctrls.append(summary)\n",
      "    return ctrls\n",
      "185/44:\n",
      "t = time.process_time()\n",
      "ctrls_pol = getctrls(politics)\n",
      "time.process_time() - t\n",
      "185/45:\n",
      "t = time.process_time()\n",
      "ctrls_bks = getctrls(books)\n",
      "time.process_time() - t\n",
      "185/46:\n",
      "t = time.process_time()\n",
      "ctrls_mov = getctrls(movies)\n",
      "time.process_time() - t\n",
      "185/47:\n",
      "def getw2v_s(sums):\n",
      "    sentences = []\n",
      "    for t in sums:\n",
      "        sentences += nltk.sent_tokenize(t)\n",
      "    docs = [simple_preprocess(s) for s in sentences]\n",
      "    return Word2Vec(docs)\n",
      "185/48: mpol_s = getw2v_s(summaries_pol)\n",
      "185/49: mbks_s = getw2v_s(summaries_bks)\n",
      "185/50: mmov_s = getw2v_s(summaries_mov)\n",
      "185/51:\n",
      "mpol_b = getw2v_s(bow_pol)\n",
      "mbks_b = getw2v_s(bow_bks)\n",
      "mmov_b = getw2v_s(bow_mov)\n",
      "185/52:\n",
      "mpol_c = getw2v_s(ctrls_pol)\n",
      "mbks_c = getw2v_s(ctrls_bks)\n",
      "mmov_c = getw2v_s(ctrls_mov)\n",
      "185/53:\n",
      "with open('debiaswe-master/data/equalize_pairs.json') as f:\n",
      "    pairs = json.loads(f.read())\n",
      "pairs = list(map(lambda x: [x[0].lower(), x[1].lower()], pairs))\n",
      "with open('debiaswe-master/data/professions.json') as f:\n",
      "    profs = json.loads(f.read())\n",
      "proflist = list(filter(lambda x: x in mpol.wv.vocab, [p[0] for p in profs if abs(p[1]) < 0.6]))\n",
      "with open('debiaswe-master/data/gender_specific_seed.json') as f:\n",
      "    gender_specific = json.loads(f.read())\n",
      "with open('objects.csv') as csv_file:\n",
      "    objects = csv_file.read().split(\"\\n\")\n",
      "objects[0] = 'angle'\n",
      "with open('cities.csv') as csv_file:\n",
      "    cities = csv_file.read().split(\",\")\n",
      "cities = [x.lower() for x in cities]\n",
      "185/54:\n",
      "vocab = set(mpol.wv.vocab)\n",
      "for m in [mpol, mmov, mbooks]:\n",
      "    vocab &= set(m.wv.vocab)\n",
      "len(vocab)\n",
      "185/55:\n",
      "vocab_all = set(vocab)\n",
      "for m in [mpol_s, mbks_s, mmov_s] + \\\n",
      "[mpol_b, mbks_b, mmov_b] + \\\n",
      "[mpol_c, mbks_c, mmov_c]:\n",
      "    vocab_all &= set(m.wv.vocab)\n",
      "len(vocab_all)\n",
      "185/56: vocab2 = vocab_all\n",
      "185/57:\n",
      "def getg(m):\n",
      "    matrix = []\n",
      "    for a, b in pairs:\n",
      "        if a not in m.wv.vocab or b not in m.wv.vocab:\n",
      "            continue\n",
      "        center = (m.wv.get_vector(a) + m.wv.get_vector(b))/2\n",
      "        matrix.append(m.wv.get_vector(a) - center)\n",
      "        matrix.append(m.wv.get_vector(b) - center)\n",
      "    matrix = np.array(matrix)\n",
      "    p = PCA(n_components = 10)\n",
      "    p.fit(matrix)\n",
      "    print(p.explained_variance_ratio_[0])\n",
      "#         plt.bar(range(10), p.explained_variance_ratio_)\n",
      "#         plt.title(\"Explained Variance vs Component Number (\" + t + \")\")\n",
      "    return p.components_[0]\n",
      "185/58:\n",
      "def gdictadd(models):\n",
      "    for i in range(3):\n",
      "        gdict[list(gdict)[i]].append(getg(models[i]))\n",
      "185/59:\n",
      "gdict = {\"politics\": [], \"books\": [], \"movies\": []}\n",
      "labels = [\"original\", \"summary (TextRank)\", \"summary (Centroid-BOW)\", \"summary (control)\"]\n",
      "gdictadd([mpol, mmov, mbooks])\n",
      "gdictadd([mpol_s, mbks_s, mmov_s])\n",
      "gdictadd([mpol_b, mbks_b, mmov_b])\n",
      "gdictadd([mpol_c, mbks_c, mmov_c])\n",
      "185/60:\n",
      "def mdictadd(models):\n",
      "    for i in range(3):\n",
      "        mdict[list(mdict)[i]].append(models[i])\n",
      "185/61:\n",
      "mdict = {\"politics\": [], \"books\": [], \"movies\": []}\n",
      "mdictadd([mpol, mmov, mbooks])\n",
      "mdictadd([mpol_s, mbks_s, mmov_s])\n",
      "mdictadd([mpol_b, mbks_b, mmov_b])\n",
      "mdictadd([mpol_c, mbks_c, mmov_c])\n",
      "185/62:\n",
      "def gproj(g, m, w1):\n",
      "    wvec = m.wv.get_vector(w1)\n",
      "    p = np.dot(g, wvec)/(np.linalg.norm(g)*np.linalg.norm(wvec))\n",
      "    return p\n",
      "185/63:\n",
      "def bias(g, m, vocab, wordlist):\n",
      "    b = 0\n",
      "    l = list(filter(lambda x: x in vocab, wordlist))\n",
      "    for x in l:\n",
      "        b += abs(gproj(g, m, x))\n",
      "    return b/len(l)\n",
      "185/64:\n",
      "print(labels[0])\n",
      "for i in gdict:\n",
      "    print(\"%10s\" % i, \\\n",
      "          \"%.3f\" % bias(gdict[i][0], mdict[i][0], vocab, proflist), \\\n",
      "          \"%.3f\" % bias(gdict[i][0], mdict[i][0], vocab, adjectives), \\\n",
      "          \"%.3f\" % bias(gdict[i][0], mdict[i][0], vocab, cities), \\\n",
      "          \"%.3f\" % bias(gdict[i][0], mdict[i][0], vocab, objects), \\\n",
      "          \"%.3f\" % bias(gdict[i][0], mdict[i][0], vocab, gender_specific))\n",
      "185/65:\n",
      "print(labels[1])\n",
      "for i in gdict:\n",
      "    print(\"%10s\" % i, \\\n",
      "          \"%.3f\" % bias(gdict[i][1], mdict[i][1], vocab2, proflist), \\\n",
      "          \"%.3f\" % bias(gdict[i][1], mdict[i][1], vocab2, adjectives), \\\n",
      "          \"%.3f\" % bias(gdict[i][1], mdict[i][1], vocab2, cities), \\\n",
      "          \"%.3f\" % bias(gdict[i][1], mdict[i][1], vocab2, objects), \\\n",
      "          \"%.3f\" % bias(gdict[i][1], mdict[i][1], vocab2, gender_specific))\n",
      "185/66:\n",
      "print(labels[2])\n",
      "for i in gdict:\n",
      "    print(\"%10s\" % i, \\\n",
      "          \"%.3f\" % bias(gdict[i][2], mdict[i][2], vocab2, proflist), \\\n",
      "          \"%.3f\" % bias(gdict[i][2], mdict[i][2], vocab2, adjectives), \\\n",
      "          \"%.3f\" % bias(gdict[i][2], mdict[i][2], vocab2, cities), \\\n",
      "          \"%.3f\" % bias(gdict[i][2], mdict[i][2], vocab2, objects), \\\n",
      "          \"%.3f\" % bias(gdict[i][2], mdict[i][2], vocab2, gender_specific))\n",
      "185/67:\n",
      "print(labels[3])\n",
      "for i in gdict:\n",
      "    print(\"%10s\" % i, \\\n",
      "          \"%.3f\" % bias(gdict[i][3], mdict[i][3], vocab2, proflist), \\\n",
      "          \"%.3f\" % bias(gdict[i][3], mdict[i][3], vocab2, adjectives), \\\n",
      "          \"%.3f\" % bias(gdict[i][3], mdict[i][3], vocab2, cities), \\\n",
      "          \"%.3f\" % bias(gdict[i][3], mdict[i][3], vocab2, objects), \\\n",
      "          \"%.3f\" % bias(gdict[i][3], mdict[i][3], vocab2, gender_specific))\n",
      "185/68:\n",
      "t = time.process_time()\n",
      "bow_bks = getbows(books)\n",
      "time.process_time() - t\n",
      "185/69:\n",
      "# mpol_b = getw2v_s(bow_pol)\n",
      "mbks_b = getw2v_s(bow_bks)\n",
      "# mmov_b = getw2v_s(bow_mov)\n",
      "185/70:\n",
      "gdict = {\"politics\": [], \"books\": [], \"movies\": []}\n",
      "labels = [\"original\", \"summary (TextRank)\", \"summary (Centroid-BOW)\", \"summary (control)\"]\n",
      "gdictadd([mpol, mmov, mbooks])\n",
      "gdictadd([mpol_s, mbks_s, mmov_s])\n",
      "gdictadd([mpol_b, mbks_b, mmov_b])\n",
      "gdictadd([mpol_c, mbks_c, mmov_c])\n",
      "185/71:\n",
      "mdict = {\"politics\": [], \"books\": [], \"movies\": []}\n",
      "mdictadd([mpol, mmov, mbooks])\n",
      "mdictadd([mpol_s, mbks_s, mmov_s])\n",
      "mdictadd([mpol_b, mbks_b, mmov_b])\n",
      "mdictadd([mpol_c, mbks_c, mmov_c])\n",
      "185/72:\n",
      "print(labels[0])\n",
      "for i in gdict:\n",
      "    print(\"%10s\" % i, \\\n",
      "          \"%.3f\" % bias(gdict[i][0], mdict[i][0], vocab, proflist), \\\n",
      "          \"%.3f\" % bias(gdict[i][0], mdict[i][0], vocab, adjectives), \\\n",
      "          \"%.3f\" % bias(gdict[i][0], mdict[i][0], vocab, cities), \\\n",
      "          \"%.3f\" % bias(gdict[i][0], mdict[i][0], vocab, objects), \\\n",
      "          \"%.3f\" % bias(gdict[i][0], mdict[i][0], vocab, gender_specific))\n",
      "185/73:\n",
      "print(labels[2])\n",
      "for i in gdict:\n",
      "    print(\"%10s\" % i, \\\n",
      "          \"%.3f\" % bias(gdict[i][2], mdict[i][2], vocab2, proflist), \\\n",
      "          \"%.3f\" % bias(gdict[i][2], mdict[i][2], vocab2, adjectives), \\\n",
      "          \"%.3f\" % bias(gdict[i][2], mdict[i][2], vocab2, cities), \\\n",
      "          \"%.3f\" % bias(gdict[i][2], mdict[i][2], vocab2, objects), \\\n",
      "          \"%.3f\" % bias(gdict[i][2], mdict[i][2], vocab2, gender_specific))\n",
      "185/74:\n",
      "def getbows(category):\n",
      "    s = []\n",
      "    centroid_summarizer = text_summarizer.CentroidBOWSummarizer()\n",
      "    for i in category:\n",
      "        try:\n",
      "            s.append(centroid_summarizer.summarize(i.text, limit=200-25))\n",
      "        except:\n",
      "            print(i)\n",
      "    return s\n",
      "185/75:\n",
      "t = time.process_time()\n",
      "bow_bks = getbows(books)\n",
      "time.process_time() - t\n",
      "185/76:\n",
      "# mpol_b = getw2v_s(bow_pol)\n",
      "mbks_b = getw2v_s(bow_bks)\n",
      "# mmov_b = getw2v_s(bow_mov)\n",
      "185/77:\n",
      "gdict = {\"politics\": [], \"books\": [], \"movies\": []}\n",
      "labels = [\"original\", \"summary (TextRank)\", \"summary (Centroid-BOW)\", \"summary (control)\"]\n",
      "gdictadd([mpol, mmov, mbooks])\n",
      "gdictadd([mpol_s, mbks_s, mmov_s])\n",
      "gdictadd([mpol_b, mbks_b, mmov_b])\n",
      "gdictadd([mpol_c, mbks_c, mmov_c])\n",
      "185/78:\n",
      "mdict = {\"politics\": [], \"books\": [], \"movies\": []}\n",
      "mdictadd([mpol, mmov, mbooks])\n",
      "mdictadd([mpol_s, mbks_s, mmov_s])\n",
      "mdictadd([mpol_b, mbks_b, mmov_b])\n",
      "mdictadd([mpol_c, mbks_c, mmov_c])\n",
      "185/79:\n",
      "print(labels[2])\n",
      "for i in gdict:\n",
      "    print(\"%10s\" % i, \\\n",
      "          \"%.3f\" % bias(gdict[i][2], mdict[i][2], vocab2, proflist), \\\n",
      "          \"%.3f\" % bias(gdict[i][2], mdict[i][2], vocab2, adjectives), \\\n",
      "          \"%.3f\" % bias(gdict[i][2], mdict[i][2], vocab2, cities), \\\n",
      "          \"%.3f\" % bias(gdict[i][2], mdict[i][2], vocab2, objects), \\\n",
      "          \"%.3f\" % bias(gdict[i][2], mdict[i][2], vocab2, gender_specific))\n",
      "185/80:\n",
      "print(labels[2])\n",
      "for i in gdict:\n",
      "    print(\"%10s\" % i, \\\n",
      "          \"%.3f\" % bias(gdict[i][2], mdict[i][2], vocab2, proflist), \\\n",
      "          \"%.3f\" % bias(gdict[i][2], mdict[i][2], vocab2, adjectives), \\\n",
      "          \"%.3f\" % bias(gdict[i][2], mdict[i][2], vocab2, cities), \\\n",
      "          \"%.3f\" % bias(gdict[i][2], mdict[i][2], vocab2, objects), \\\n",
      "          \"%.3f\" % bias(gdict[i][2], mdict[i][2], vocab2, gender_specific))\n",
      "185/81:\n",
      "def getbows(category):\n",
      "    s = []\n",
      "    centroid_summarizer = text_summarizer.CentroidBOWSummarizer()\n",
      "    for i in category:\n",
      "        try:\n",
      "            s.append(centroid_summarizer.summarize(i.text, limit=200-25))\n",
      "        except:\n",
      "            print(i)\n",
      "    return s\n",
      "185/82:\n",
      "def getbows(category):\n",
      "    s = []\n",
      "    centroid_summarizer = text_summarizer.CentroidBOWSummarizer()\n",
      "    for i in category:\n",
      "        try:\n",
      "            s.append(centroid_summarizer.summarize(i.text, limit=200-30))\n",
      "        except:\n",
      "            print(i)\n",
      "    return s\n",
      "185/83:\n",
      "t = time.process_time()\n",
      "bow_bks = getbows(books)\n",
      "time.process_time() - t\n",
      "185/84:\n",
      "# mpol_b = getw2v_s(bow_pol)\n",
      "mbks_b = getw2v_s(bow_bks)\n",
      "# mmov_b = getw2v_s(bow_mov)\n",
      "185/85:\n",
      "gdict = {\"politics\": [], \"books\": [], \"movies\": []}\n",
      "labels = [\"original\", \"summary (TextRank)\", \"summary (Centroid-BOW)\", \"summary (control)\"]\n",
      "gdictadd([mpol, mmov, mbooks])\n",
      "gdictadd([mpol_s, mbks_s, mmov_s])\n",
      "gdictadd([mpol_b, mbks_b, mmov_b])\n",
      "gdictadd([mpol_c, mbks_c, mmov_c])\n",
      "185/86:\n",
      "mdict = {\"politics\": [], \"books\": [], \"movies\": []}\n",
      "mdictadd([mpol, mmov, mbooks])\n",
      "mdictadd([mpol_s, mbks_s, mmov_s])\n",
      "mdictadd([mpol_b, mbks_b, mmov_b])\n",
      "mdictadd([mpol_c, mbks_c, mmov_c])\n",
      "185/87:\n",
      "print(labels[2])\n",
      "for i in gdict:\n",
      "    print(\"%10s\" % i, \\\n",
      "          \"%.3f\" % bias(gdict[i][2], mdict[i][2], vocab2, proflist), \\\n",
      "          \"%.3f\" % bias(gdict[i][2], mdict[i][2], vocab2, adjectives), \\\n",
      "          \"%.3f\" % bias(gdict[i][2], mdict[i][2], vocab2, cities), \\\n",
      "          \"%.3f\" % bias(gdict[i][2], mdict[i][2], vocab2, objects), \\\n",
      "          \"%.3f\" % bias(gdict[i][2], mdict[i][2], vocab2, gender_specific))\n",
      "185/88:\n",
      "t = time.process_time()\n",
      "mpol = getw2v(politics)\n",
      "time.process_time() - t\n",
      "185/89:\n",
      "gdict = {\"politics\": [], \"books\": [], \"movies\": []}\n",
      "labels = [\"original\", \"summary (TextRank)\", \"summary (Centroid-BOW)\", \"summary (control)\"]\n",
      "gdictadd([mpol, mmov, mbooks])\n",
      "gdictadd([mpol_s, mbks_s, mmov_s])\n",
      "gdictadd([mpol_b, mbks_b, mmov_b])\n",
      "gdictadd([mpol_c, mbks_c, mmov_c])\n",
      "185/90:\n",
      "mdict = {\"politics\": [], \"books\": [], \"movies\": []}\n",
      "mdictadd([mpol, mmov, mbooks])\n",
      "mdictadd([mpol_s, mbks_s, mmov_s])\n",
      "mdictadd([mpol_b, mbks_b, mmov_b])\n",
      "mdictadd([mpol_c, mbks_c, mmov_c])\n",
      "185/91:\n",
      "print(labels[0])\n",
      "for i in gdict:\n",
      "    print(\"%10s\" % i, \\\n",
      "          \"%.3f\" % bias(gdict[i][0], mdict[i][0], vocab, proflist), \\\n",
      "          \"%.3f\" % bias(gdict[i][0], mdict[i][0], vocab, adjectives), \\\n",
      "          \"%.3f\" % bias(gdict[i][0], mdict[i][0], vocab, cities), \\\n",
      "          \"%.3f\" % bias(gdict[i][0], mdict[i][0], vocab, objects), \\\n",
      "          \"%.3f\" % bias(gdict[i][0], mdict[i][0], vocab, gender_specific))\n",
      "185/92:\n",
      "t = time.process_time()\n",
      "mpol = getw2v(politics)\n",
      "time.process_time() - t\n",
      "185/93:\n",
      "t = time.process_time()\n",
      "mmov = getw2v(movies)\n",
      "time.process_time() - t\n",
      "185/94:\n",
      "t = time.process_time()\n",
      "mbooks = getw2v(books)\n",
      "time.process_time() - t\n",
      "185/95:\n",
      "gdict = {\"politics\": [], \"books\": [], \"movies\": []}\n",
      "labels = [\"original\", \"summary (TextRank)\", \"summary (Centroid-BOW)\", \"summary (control)\"]\n",
      "gdictadd([mpol, mmov, mbooks])\n",
      "gdictadd([mpol_s, mbks_s, mmov_s])\n",
      "gdictadd([mpol_b, mbks_b, mmov_b])\n",
      "gdictadd([mpol_c, mbks_c, mmov_c])\n",
      "185/96:\n",
      "mdict = {\"politics\": [], \"books\": [], \"movies\": []}\n",
      "mdictadd([mpol, mmov, mbooks])\n",
      "mdictadd([mpol_s, mbks_s, mmov_s])\n",
      "mdictadd([mpol_b, mbks_b, mmov_b])\n",
      "mdictadd([mpol_c, mbks_c, mmov_c])\n",
      "185/97:\n",
      "print(labels[0])\n",
      "for i in gdict:\n",
      "    print(\"%10s\" % i, \\\n",
      "          \"%.3f\" % bias(gdict[i][0], mdict[i][0], vocab, proflist), \\\n",
      "          \"%.3f\" % bias(gdict[i][0], mdict[i][0], vocab, adjectives), \\\n",
      "          \"%.3f\" % bias(gdict[i][0], mdict[i][0], vocab, cities), \\\n",
      "          \"%.3f\" % bias(gdict[i][0], mdict[i][0], vocab, objects), \\\n",
      "          \"%.3f\" % bias(gdict[i][0], mdict[i][0], vocab, gender_specific))\n",
      "185/98: dill.dump_session('thesis_env3.db')\n",
      "185/99:\n",
      "# todo\n",
      "def bias_validity(g, m, vocab):\n",
      "    male = [x for x in vocab if x in [y[0] for y in pairs]]\n",
      "    female = [x for x in vocab if x in [y[1] for y in pairs]]\n",
      "    mpos = \n",
      "    mneg = 0\n",
      "    fpos = 0\n",
      "    fneg = 0\n",
      "    for v in male:\n",
      "        if gproj(g, m, v) > 0:\n",
      "            mpos += 1\n",
      "        elif gproj(g, m, v) < 0:\n",
      "            mneg += 1\n",
      "    for v in female:\n",
      "        if gproj(g, m, v) > 0:\n",
      "            fpos += 1\n",
      "        elif gproj(g, m, v) < 0:\n",
      "            fneg += 1\n",
      "    if mpos > mneg:\n",
      "        return (mpos/(mpos+mneg) + fneg/(fpos+fneg))/2\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "185/100:\n",
      "# todo\n",
      "def bias_validity(g, m, vocab):\n",
      "    male = [x for x in vocab if x in [y[0] for y in pairs]]\n",
      "    female = [x for x in vocab if x in [y[1] for y in pairs]]\n",
      "    mpos = 0\n",
      "    mneg = 0\n",
      "    fpos = 0\n",
      "    fneg = 0\n",
      "    for v in male:\n",
      "        if gproj(g, m, v) > 0:\n",
      "            mpos += 1\n",
      "        elif gproj(g, m, v) < 0:\n",
      "            mneg += 1\n",
      "    for v in female:\n",
      "        if gproj(g, m, v) > 0:\n",
      "            fpos += 1\n",
      "        elif gproj(g, m, v) < 0:\n",
      "            fneg += 1\n",
      "    if mpos > mneg:\n",
      "        return (mpos/(mpos+mneg) + fneg/(fpos+fneg))/2\n",
      "185/101:\n",
      "for j in range(len(labels)):\n",
      "    print(labels[j])\n",
      "    v = vocab\n",
      "    if j > 0:\n",
      "        v = vocab2\n",
      "    for i in gdict:\n",
      "        print(\"%10s\" % i, \"%.3f\" % bias_validity(gdict[i][j], mdict[i][j], v)\n",
      "185/102:\n",
      "for j in range(len(labels)):\n",
      "    print(labels[j])\n",
      "    v = vocab\n",
      "    if j > 0:\n",
      "        v = vocab2\n",
      "    for i in gdict:\n",
      "        print(\"%10s\" % i, \"%.3f\" % bias_validity(gdict[i][j], mdict[i][j], v))\n",
      "185/103:\n",
      "for j in range(len(labels)):\n",
      "    print(labels[j])\n",
      "    v = vocab\n",
      "    if j > 0:\n",
      "        v = vocab2\n",
      "    for i in gdict:\n",
      "        print(i, j)\n",
      "        print(\"%10s\" % i, \"%.3f\" % bias_validity(gdict[i][j], mdict[i][j], v))\n",
      "185/104:\n",
      "for j in range(len(labels)):\n",
      "    print(labels[j])\n",
      "    v = vocab\n",
      "    if j > 0:\n",
      "        v = vocab2\n",
      "    for i in gdict:\n",
      "        print(i, j, gdict[i][j], mdict[i][j])\n",
      "        print(\"%10s\" % i, \"%.3f\" % bias_validity(gdict[i][j], mdict[i][j], v))\n",
      "185/105:\n",
      "for j in range(len(labels)):\n",
      "    print(labels[j])\n",
      "    v = vocab\n",
      "    if j > 0:\n",
      "        v = vocab2\n",
      "    for i in gdict:\n",
      "        print(bias_validity(gdict[i][j], mdict[i][j], v))\n",
      "        print(\"%10s\" % i, \"%.3f\" % bias_validity(gdict[i][j], mdict[i][j], v))\n",
      "185/106:\n",
      "# todo\n",
      "def bias_validity(g, m, vocab):\n",
      "    male = [x for x in vocab if x in [y[0] for y in pairs]]\n",
      "    female = [x for x in vocab if x in [y[1] for y in pairs]]\n",
      "    mpos = 0\n",
      "    mneg = 0\n",
      "    fpos = 0\n",
      "    fneg = 0\n",
      "    for v in male:\n",
      "        if gproj(g, m, v) > 0:\n",
      "            mpos += 1\n",
      "        elif gproj(g, m, v) < 0:\n",
      "            mneg += 1\n",
      "    for v in female:\n",
      "        if gproj(g, m, v) > 0:\n",
      "            fpos += 1\n",
      "        elif gproj(g, m, v) < 0:\n",
      "            fneg += 1\n",
      "    if mpos > mneg:\n",
      "        return (mpos/(mpos+mneg) + fneg/(fpos+fneg))/2\n",
      "    return (mneg/(mpos+mneg) + fpos/(fpos+fneg))/2\n",
      "185/107:\n",
      "for j in range(len(labels)):\n",
      "    print(labels[j])\n",
      "    v = vocab\n",
      "    if j > 0:\n",
      "        v = vocab2\n",
      "    for i in gdict:\n",
      "        print(bias_validity(gdict[i][j], mdict[i][j], v))\n",
      "        print(\"%10s\" % i, \"%.3f\" % bias_validity(gdict[i][j], mdict[i][j], v))\n",
      "185/108:\n",
      "for j in range(len(labels)):\n",
      "    print(labels[j])\n",
      "    v = vocab\n",
      "    if j > 0:\n",
      "        v = vocab2\n",
      "    for i in gdict:\n",
      "        print(\"%10s\" % i, \"%.3f\" % bias_validity(gdict[i][j], mdict[i][j], v))\n",
      "185/109:\n",
      "# todo\n",
      "def bias_validity(g, m, vocab):\n",
      "    male = [x for x in vocab if x in [y[0] for y in pairs]]\n",
      "    female = [x for x in vocab if x in [y[1] for y in pairs]]\n",
      "    mpos = 0\n",
      "    mneg = 0\n",
      "    fpos = 0\n",
      "    fneg = 0\n",
      "    for v in male:\n",
      "        if gproj(g, m, v) > 0:\n",
      "            mpos += 1\n",
      "        elif gproj(g, m, v) < 0:\n",
      "            mneg += 1\n",
      "    for v in female:\n",
      "        if gproj(g, m, v) > 0:\n",
      "            fpos += 1\n",
      "        elif gproj(g, m, v) < 0:\n",
      "            fneg += 1\n",
      "    print(mpos, mneg, fpos, fneg)\n",
      "    if mpos > mneg:\n",
      "        return (mpos/(mpos+mneg) + fneg/(fpos+fneg))/2\n",
      "    return (mneg/(mpos+mneg) + fpos/(fpos+fneg))/2\n",
      "185/110:\n",
      "for j in range(len(labels)):\n",
      "    print(labels[j])\n",
      "    v = vocab\n",
      "    if j > 0:\n",
      "        v = vocab2\n",
      "    for i in gdict:\n",
      "        print(\"%10s\" % i, \"%.3f\" % bias_validity(gdict[i][j], mdict[i][j], v))\n",
      "185/111:\n",
      "# todo\n",
      "def bias_validity(g, m, vocab):\n",
      "    male = [x for x in vocab if x in [y[0] for y in pairs]]\n",
      "    female = [x for x in vocab if x in [y[1] for y in pairs]]\n",
      "    mpos = 0\n",
      "    mneg = 0\n",
      "    fpos = 0\n",
      "    fneg = 0\n",
      "    for v in male:\n",
      "        print(v, gproj(g, m, v))\n",
      "        if gproj(g, m, v) > 0:\n",
      "            mpos += 1\n",
      "        elif gproj(g, m, v) < 0:\n",
      "            mneg += 1\n",
      "    for v in female:\n",
      "        print(v, gproj(g, m, v))\n",
      "        if gproj(g, m, v) > 0:\n",
      "            fpos += 1\n",
      "        elif gproj(g, m, v) < 0:\n",
      "            fneg += 1\n",
      "    print(mpos, mneg, fpos, fneg)\n",
      "    if mpos > mneg:\n",
      "        return (mpos/(mpos+mneg) + fneg/(fpos+fneg))/2\n",
      "    return (mneg/(mpos+mneg) + fpos/(fpos+fneg))/2\n",
      "185/112:\n",
      "for j in range(len(labels)):\n",
      "    print(labels[j])\n",
      "    v = vocab\n",
      "    if j > 0:\n",
      "        v = vocab2\n",
      "    for i in gdict:\n",
      "        print(\"%10s\" % i, \"%.3f\" % bias_validity(gdict[i][j], mdict[i][j], v))\n",
      "185/113:\n",
      "# todo\n",
      "def bias_validity(g, m, vocab):\n",
      "    male = [x for x in vocab if x in [y[0] for y in pairs]]\n",
      "    female = [x for x in vocab if x in [y[1] for y in pairs]]\n",
      "    mpos = 0\n",
      "    mneg = 0\n",
      "    fpos = 0\n",
      "    fneg = 0\n",
      "    for v in male:\n",
      "        print(v, gproj(g, m, v))\n",
      "        if gproj(g, m, v) > 0:\n",
      "            mpos += 1\n",
      "        elif gproj(g, m, v) < 0:\n",
      "            mneg += 1\n",
      "    for v in female:\n",
      "        print(v, gproj(g, m, v))\n",
      "        if gproj(g, m, v) > 0:\n",
      "            fpos += 1\n",
      "        elif gproj(g, m, v) < 0:\n",
      "            fneg += 1\n",
      "    print(mpos, mneg, fpos, fneg)\n",
      "    print()\n",
      "    if mpos > mneg:\n",
      "        return (mpos/(mpos+mneg) + fneg/(fpos+fneg))/2\n",
      "    return (mneg/(mpos+mneg) + fpos/(fpos+fneg))/2\n",
      "185/114:\n",
      "for j in range(len(labels)):\n",
      "    print(labels[j])\n",
      "    v = vocab\n",
      "    if j > 0:\n",
      "        v = vocab2\n",
      "    for i in gdict:\n",
      "        print(\"%10s\" % i, \"%.3f\" % bias_validity(gdict[i][j], mdict[i][j], v))\n",
      "185/115:\n",
      "gpol = getg(mpol)\n",
      "res = sorted([(x, gproj(gpol, mpol, x)) for x in filter(lambda x: x in vocab, vocab)], key=lambda x: x[1])\n",
      "for i in range(10):\n",
      "    print(f\"{res[i][0].ljust(10)} {res[i][1]:.2f} \\t\\t {res[-i-1][0].ljust(10)} {res[-i-1][1]:.2f}\")\n",
      "185/116:\n",
      "gmov_s = getg(mmov_s)\n",
      "res = sorted([(x, gproj(gmov_s, mmov_s, x)) for x in filter(lambda x: x in vocab, vocab)], key=lambda x: x[1])\n",
      "for i in range(10):\n",
      "    print(f\"{res[i][0].ljust(10)} {res[i][1]:.2f} \\t\\t {res[-i-1][0].ljust(10)} {res[-i-1][1]:.2f}\")\n",
      "185/117:\n",
      "gmov_s = getg(mmov_s)\n",
      "res = sorted([(x, gproj(gmov_s, mmov_s, x)) for x in filter(lambda x: x in vocab2, vocab2)], key=lambda x: x[1])\n",
      "for i in range(10):\n",
      "    print(f\"{res[i][0].ljust(10)} {res[i][1]:.2f} \\t\\t {res[-i-1][0].ljust(10)} {res[-i-1][1]:.2f}\")\n",
      "185/118:\n",
      "gbks_s = getg(mbooks_s)\n",
      "res = sorted([(x, gproj(gbks_s, mbooks_s, x)) for x in filter(lambda x: x in vocab2, vocab2)], key=lambda x: x[1])\n",
      "for i in range(10):\n",
      "    print(f\"{res[i][0].ljust(10)} {res[i][1]:.2f} \\t\\t {res[-i-1][0].ljust(10)} {res[-i-1][1]:.2f}\")\n",
      "185/119:\n",
      "gbks_s = getg(mbks_s)\n",
      "res = sorted([(x, gproj(gbks_s, mbks_s, x)) for x in filter(lambda x: x in vocab2, vocab2)], key=lambda x: x[1])\n",
      "for i in range(10):\n",
      "    print(f\"{res[i][0].ljust(10)} {res[i][1]:.2f} \\t\\t {res[-i-1][0].ljust(10)} {res[-i-1][1]:.2f}\")\n",
      "185/120:\n",
      "# todo\n",
      "def bias_validity(g, m, vocab):\n",
      "    male = [x for x in vocab if x in [y[0] for y in pairs]]\n",
      "    female = [x for x in vocab if x in [y[1] for y in pairs]]\n",
      "    mpos = 0\n",
      "    mneg = 0\n",
      "    fpos = 0\n",
      "    fneg = 0\n",
      "    for v in male:\n",
      "        if gproj(g, m, v) > 0:\n",
      "            mpos += 1\n",
      "        elif gproj(g, m, v) < 0:\n",
      "            mneg += 1\n",
      "    for v in female:\n",
      "        if gproj(g, m, v) > 0:\n",
      "            fpos += 1\n",
      "        elif gproj(g, m, v) < 0:\n",
      "            fneg += 1\n",
      "    print(mpos, mneg, fpos, fneg)\n",
      "    print()\n",
      "    if mpos > mneg:\n",
      "        return (mpos/(mpos+mneg) + fneg/(fpos+fneg))/2\n",
      "    return (mneg/(mpos+mneg) + fpos/(fpos+fneg))/2\n",
      "185/121:\n",
      "for j in range(len(labels)):\n",
      "    print(labels[j])\n",
      "    v = vocab\n",
      "    if j > 0:\n",
      "        v = vocab2\n",
      "    for i in gdict:\n",
      "        print(\"%10s\" % i, \"%.3f\" % bias_validity(gdict[i][j], mdict[i][j], v))\n",
      "185/122:\n",
      "# todo\n",
      "def bias_validity(g, m, vocab):\n",
      "    male = [x for x in vocab if x in [y[0] for y in pairs]]\n",
      "    female = [x for x in vocab if x in [y[1] for y in pairs]]\n",
      "    mpos = 0\n",
      "    mneg = 0\n",
      "    fpos = 0\n",
      "    fneg = 0\n",
      "    for v in male:\n",
      "        if gproj(g, m, v) > 0:\n",
      "            mpos += 1\n",
      "        elif gproj(g, m, v) < 0:\n",
      "            mneg += 1\n",
      "    for v in female:\n",
      "        if gproj(g, m, v) > 0:\n",
      "            fpos += 1\n",
      "        elif gproj(g, m, v) < 0:\n",
      "            fneg += 1\n",
      "    print(mpos, mneg, fpos, fneg)\n",
      "    if mpos > mneg:\n",
      "        return (mpos/(mpos+mneg) + fneg/(fpos+fneg))/2\n",
      "    return (mneg/(mpos+mneg) + fpos/(fpos+fneg))/2\n",
      "185/123:\n",
      "for j in range(len(labels)):\n",
      "    print(labels[j])\n",
      "    v = vocab\n",
      "    if j > 0:\n",
      "        v = vocab2\n",
      "    for i in gdict:\n",
      "        print(\"%10s\" % i, \"%.3f\" % bias_validity(gdict[i][j], mdict[i][j], v))\n",
      "185/124:\n",
      "# todo\n",
      "def bias_validity(g, m, vocab):\n",
      "    male = [x for x in vocab if x in [y[0] for y in pairs]]\n",
      "    female = [x for x in vocab if x in [y[1] for y in pairs]]\n",
      "    mpos = 0\n",
      "    mneg = 0\n",
      "    fpos = 0\n",
      "    fneg = 0\n",
      "    for v in male:\n",
      "        if gproj(g, m, v) > 0:\n",
      "            mpos += 1\n",
      "        elif gproj(g, m, v) < 0:\n",
      "            mneg += 1\n",
      "    for v in female:\n",
      "        if gproj(g, m, v) > 0:\n",
      "            fpos += 1\n",
      "        elif gproj(g, m, v) < 0:\n",
      "            fneg += 1\n",
      "    if mpos > mneg:\n",
      "        return (mpos/(mpos+mneg) + fneg/(fpos+fneg))/2\n",
      "    return (mneg/(mpos+mneg) + fpos/(fpos+fneg))/2\n",
      "185/125:\n",
      "gpol_c = getg(mpol_c)\n",
      "res = sorted([(x, gproj(gpol_c, mpol_c, x)) for x in filter(lambda x: x in vocab2, vocab2)], key=lambda x: x[1])\n",
      "for i in range(10):\n",
      "    print(f\"{res[i][0].ljust(10)} {res[i][1]:.2f} \\t\\t {res[-i-1][0].ljust(10)} {res[-i-1][1]:.2f}\")\n",
      "185/126:\n",
      "gbks_c = getg(mbks_c)\n",
      "res = sorted([(x, gproj(gbks_c, mbks_c, x)) for x in filter(lambda x: x in vocab2, vocab2)], key=lambda x: x[1])\n",
      "for i in range(10):\n",
      "    print(f\"{res[i][0].ljust(10)} {res[i][1]:.2f} \\t\\t {res[-i-1][0].ljust(10)} {res[-i-1][1]:.2f}\")\n",
      "185/127: pairs\n",
      "185/128:\n",
      "# todo\n",
      "def bias_validity(g, m, vocab):\n",
      "    male = [x for x in vocab if x in [y[0] for y in pairs]]\n",
      "    female = [x for x in vocab if x in [y[1] for y in pairs]]\n",
      "    mpos = 0\n",
      "    mneg = 0\n",
      "    fpos = 0\n",
      "    fneg = 0\n",
      "    e = 0.1\n",
      "    for v in male:\n",
      "        if gproj(g, m, v) > e:\n",
      "            mpos += 1\n",
      "        elif gproj(g, m, v) < e:\n",
      "            mneg += 1\n",
      "    for v in female:\n",
      "        if gproj(g, m, v) > e:\n",
      "            fpos += 1\n",
      "        elif gproj(g, m, v) < e:\n",
      "            fneg += 1\n",
      "    print(mpos, mneg, fpos, fneg)\n",
      "    if mpos > mneg:\n",
      "        return (mpos/(mpos+mneg) + fneg/(fpos+fneg))/2\n",
      "    return (mneg/(mpos+mneg) + fpos/(fpos+fneg))/2\n",
      "185/129:\n",
      "for j in range(len(labels)):\n",
      "    print(labels[j])\n",
      "    v = vocab\n",
      "    if j > 0:\n",
      "        v = vocab2\n",
      "    for i in gdict:\n",
      "        print(\"%10s\" % i, \"%.3f\" % bias_validity(gdict[i][j], mdict[i][j], v))\n",
      "185/130:\n",
      "# todo\n",
      "def bias_validity(g, m, vocab):\n",
      "    male = [x for x in vocab if x in [y[0] for y in pairs]]\n",
      "    female = [x for x in vocab if x in [y[1] for y in pairs]]\n",
      "    mpos = 0\n",
      "    mneg = 0\n",
      "    fpos = 0\n",
      "    fneg = 0\n",
      "    e = 0.25\n",
      "    for v in male:\n",
      "        if gproj(g, m, v) > e:\n",
      "            mpos += 1\n",
      "        elif gproj(g, m, v) < e:\n",
      "            mneg += 1\n",
      "    for v in female:\n",
      "        if gproj(g, m, v) > e:\n",
      "            fpos += 1\n",
      "        elif gproj(g, m, v) < e:\n",
      "            fneg += 1\n",
      "    print(mpos, mneg, fpos, fneg)\n",
      "    if mpos > mneg:\n",
      "        return (mpos/(mpos+mneg) + fneg/(fpos+fneg))/2\n",
      "    return (mneg/(mpos+mneg) + fpos/(fpos+fneg))/2\n",
      "185/131:\n",
      "for j in range(len(labels)):\n",
      "    print(labels[j])\n",
      "    v = vocab\n",
      "    if j > 0:\n",
      "        v = vocab2\n",
      "    for i in gdict:\n",
      "        print(\"%10s\" % i, \"%.3f\" % bias_validity(gdict[i][j], mdict[i][j], v))\n",
      "186/1:\n",
      "import os\n",
      "import time\n",
      "import dill\n",
      "from typing import List\n",
      "from functools import reduce\n",
      "from calendar import monthrange\n",
      "186/2:\n",
      "import json\n",
      "import csv\n",
      "import requests\n",
      "import xml.etree.ElementTree as ET\n",
      "from bs4 import BeautifulSoup\n",
      "import urllib.request as urllib2\n",
      "186/3:\n",
      "import matplotlib.pyplot as plt\n",
      "import numpy as np\n",
      "from scipy import stats\n",
      "from random import sample, randint, shuffle\n",
      "from sklearn.decomposition import PCA\n",
      "186/4:\n",
      "import nltk\n",
      "import text_summarizer\n",
      "from gensim.utils import simple_preprocess\n",
      "from gensim.summarization.summarizer import summarize\n",
      "from gensim.models import Word2Vec, LsiModel\n",
      "from gensim.test.utils import common_dictionary, common_corpus\n",
      "186/5: from time import process_time\n",
      "186/6:\n",
      "t = process_time()\n",
      "dill.load_session('thesis_env3.db')\n",
      "process_time() - t\n",
      "186/7: len(politics), len(movies), len(books)\n",
      "186/8: len(adjectives), len(objects), len(cities)\n",
      "186/9: len(all_)\n",
      "186/10: len(pairs), len(proflist), len(gender_specific)\n",
      "186/11: len(vocab)\n",
      "186/12: len(vocab), len(vocab2)\n",
      "186/13:\n",
      "gpol = getg(mpol)\n",
      "res = sorted([(x, gproj(gpol, mpol, x)) for x in filter(lambda x: x in vocab, vocab)], key=lambda x: x[1])\n",
      "for i in range(10):\n",
      "    print(f\"{res[i][0].ljust(10)} {res[i][1]:.2f} \\t\\t {res[-i-1][0].ljust(10)} {res[-i-1][1]:.2f}\")\n",
      "gbooks = getg(mbooks)\n",
      "res = sorted([(x, gproj(gbooks, mbooks, x)) for x in filter(lambda x: x in vocab, vocab)], key=lambda x: x[1])\n",
      "for i in range(10):\n",
      "    print(f\"{res[i][0].ljust(10)} {res[i][1]:.2f} \\t\\t {res[-i-1][0].ljust(10)} {res[-i-1][1]:.2f}\")\n",
      "gmov = getg(mmov)\n",
      "res = sorted([(x, gproj(gmov, mmov, x)) for x in filter(lambda x: x in vocab, vocab)], key=lambda x: x[1])\n",
      "for i in range(10):\n",
      "    print(f\"{res[i][0].ljust(10)} {res[i][1]:.2f} \\t\\t {res[-i-1][0].ljust(10)} {res[-i-1][1]:.2f}\")\n",
      "186/14:\n",
      "gpol_s = getg(mpol_s)\n",
      "res = sorted([(x, gproj(gpol_s, mpol_s, x)) for x in filter(lambda x: x in vocab, vocab)], key=lambda x: x[1])\n",
      "for i in range(10):\n",
      "    print(f\"{res[i][0].ljust(10)} {res[i][1]:.2f} \\t\\t {res[-i-1][0].ljust(10)} {res[-i-1][1]:.2f}\")\n",
      "gbks_s = getg(mbks_s)\n",
      "res = sorted([(x, gproj(gbks_s, mbks_s, x)) for x in filter(lambda x: x in vocab, vocab)], key=lambda x: x[1])\n",
      "for i in range(10):\n",
      "    print(f\"{res[i][0].ljust(10)} {res[i][1]:.2f} \\t\\t {res[-i-1][0].ljust(10)} {res[-i-1][1]:.2f}\")\n",
      "gmov_s = getg(mmov_s)\n",
      "res = sorted([(x, gproj(gmov_s, mmov_s, x)) for x in filter(lambda x: x in vocab, vocab)], key=lambda x: x[1])\n",
      "for i in range(10):\n",
      "    print(f\"{res[i][0].ljust(10)} {res[i][1]:.2f} \\t\\t {res[-i-1][0].ljust(10)} {res[-i-1][1]:.2f}\")\n",
      "186/15:\n",
      "gpol_s = getg(mpol_s)\n",
      "res = sorted([(x, gproj(gpol_s, mpol_s, x)) for x in filter(lambda x: x in vocab2, vocab2)], key=lambda x: x[1])\n",
      "for i in range(10):\n",
      "    print(f\"{res[i][0].ljust(10)} {res[i][1]:.2f} \\t\\t {res[-i-1][0].ljust(10)} {res[-i-1][1]:.2f}\")\n",
      "gbks_s = getg(mbks_s)\n",
      "res = sorted([(x, gproj(gbks_s, mbks_s, x)) for x in filter(lambda x: x in vocab2, vocab2)], key=lambda x: x[1])\n",
      "for i in range(10):\n",
      "    print(f\"{res[i][0].ljust(10)} {res[i][1]:.2f} \\t\\t {res[-i-1][0].ljust(10)} {res[-i-1][1]:.2f}\")\n",
      "gmov_s = getg(mmov_s)\n",
      "res = sorted([(x, gproj(gmov_s, mmov_s, x)) for x in filter(lambda x: x in vocab2, vocab2)], key=lambda x: x[1])\n",
      "for i in range(10):\n",
      "    print(f\"{res[i][0].ljust(10)} {res[i][1]:.2f} \\t\\t {res[-i-1][0].ljust(10)} {res[-i-1][1]:.2f}\")\n",
      "186/16:\n",
      "gpol_b = getg(mpol_b)\n",
      "res = sorted([(x, gproj(gpol_b, mpol_b, x)) for x in filter(lambda x: x in vocab2, vocab2)], key=lambda x: x[1])\n",
      "for i in range(10):\n",
      "    print(f\"{res[i][0].ljust(10)} {res[i][1]:.2f} \\t\\t {res[-i-1][0].ljust(10)} {res[-i-1][1]:.2f}\")\n",
      "gbks_b = getg(mbks_b)\n",
      "res = sorted([(x, gproj(gbks_b, mbks_b, x)) for x in filter(lambda x: x in vocab2, vocab2)], key=lambda x: x[1])\n",
      "for i in range(10):\n",
      "    print(f\"{res[i][0].ljust(10)} {res[i][1]:.2f} \\t\\t {res[-i-1][0].ljust(10)} {res[-i-1][1]:.2f}\")\n",
      "gmov_b = getg(mmov_b)\n",
      "res = sorted([(x, gproj(gmov_b, mmov_b, x)) for x in filter(lambda x: x in vocab2, vocab2)], key=lambda x: x[1])\n",
      "for i in range(10):\n",
      "    print(f\"{res[i][0].ljust(10)} {res[i][1]:.2f} \\t\\t {res[-i-1][0].ljust(10)} {res[-i-1][1]:.2f}\")\n",
      "186/17:\n",
      "gpol_c = getg(mpol_c)\n",
      "res = sorted([(x, gproj(gpol_c, mpol_c, x)) for x in filter(lambda x: x in vocab2, vocab2)], key=lambda x: x[1])\n",
      "for i in range(10):\n",
      "    print(f\"{res[i][0].ljust(10)} {res[i][1]:.2f} \\t\\t {res[-i-1][0].ljust(10)} {res[-i-1][1]:.2f}\")\n",
      "gbks_c = getg(mbks_c)\n",
      "res = sorted([(x, gproj(gbks_c, mbks_c, x)) for x in filter(lambda x: x in vocab2, vocab2)], key=lambda x: x[1])\n",
      "for i in range(10):\n",
      "    print(f\"{res[i][0].ljust(10)} {res[i][1]:.2f} \\t\\t {res[-i-1][0].ljust(10)} {res[-i-1][1]:.2f}\")\n",
      "gmov_c = getg(mmov_c)\n",
      "res = sorted([(x, gproj(gmov_c, mmov_c, x)) for x in filter(lambda x: x in vocab2, vocab2)], key=lambda x: x[1])\n",
      "for i in range(10):\n",
      "    print(f\"{res[i][0].ljust(10)} {res[i][1]:.2f} \\t\\t {res[-i-1][0].ljust(10)} {res[-i-1][1]:.2f}\")\n",
      "185/132: sports = getlist('Baseball') + getlist('Football')\n",
      "185/133: len(politics), len(movies), len(books), len(sports)\n",
      "185/134: sports = sample(sports)\n",
      "185/135: sports = sample(sports, 7370)\n",
      "185/136:\n",
      "t = time.process_time()\n",
      "msports = getw2v(sports)\n",
      "time.process_time() - t\n",
      "185/137:\n",
      "def top5(word):\n",
      "    for m in [mpol, mmov, mbooks, msports]:\n",
      "        print([x[0] for x in m.wv.most_similar(positive=[word], topn=5)])\n",
      "185/138: top5('monday')\n",
      "185/139: top5('house')\n",
      "185/140: top5('success')\n",
      "185/141: top5('seat')\n",
      "185/142: top5(\"publish\")\n",
      "185/143: top5(\"best\")\n",
      "185/144:\n",
      "vocab = vocab & set(msports.wv.vocab)\n",
      "gspts = getg(msports)\n",
      "bias(gspts, msports, vocab, proflist)\n",
      "185/145:\n",
      "# mpol, mmov, mbooks\n",
      "msports\n",
      "# summaries_{pol, mov, bks}\n",
      "# m{pol, mov, bks}_{s, b, c}\n",
      "# bow_{pol, mov, bks}\n",
      "# ctrs_{pol, mov, bks}\n",
      "185/146: summaries_spts, fspts = getsummaries(sports)\n",
      "185/147:\n",
      "mspts_s = getw2v_s(summaries_spts)\n",
      "bow_spts = getbows(sports)\n",
      "mspts_b = getw2v_s(bow_spts)\n",
      "ctrls_spts = getctrls(sports)\n",
      "mspts_c = getw2v_s(ctrls_spts)\n",
      "185/148:\n",
      "mdict = {\"politics\": [], \"books\": [], \"movies\": [], \"sports\": []}\n",
      "mdictadd([mpol, mmov, mbooks, msports])\n",
      "mdictadd([mpol_s, mbks_s, mmov_s, mspts_s])\n",
      "mdictadd([mpol_b, mbks_b, mmov_b, mspts_b])\n",
      "mdictadd([mpol_c, mbks_c, mmov_c, mspts_c])\n",
      "185/149:\n",
      "labels = [\"original\", \"summary (TextRank)\", \"summary (Centroid-BOW)\", \"summary (control)\"]\n",
      "gdict = {\"politics\": [], \"books\": [], \"movies\": [], \"sports\": []}\n",
      "gdictadd([mpol, mmov, mbooks, msports])\n",
      "gdictadd([mpol_s, mbks_s, mmov_s, mspts_s])\n",
      "gdictadd([mpol_b, mbks_b, mmov_b, mspts_b])\n",
      "gdictadd([mpol_c, mbks_c, mmov_c, mspts_c])\n",
      "185/150:\n",
      "vocab = set(mpol.wv.vocab)\n",
      "for m in [mpol, mmov, mbooks, msports]:\n",
      "    vocab &= set(m.wv.vocab)\n",
      "len(vocab)\n",
      "185/151:\n",
      "vocab_all = set(vocab)\n",
      "for m in [mpol_s, mbks_s, mmov_s, mspts_s] + \\\n",
      "[mpol_b, mbks_b, mmov_b, mspts_b] + \\\n",
      "[mpol_c, mbks_c, mmov_c, mspts_c]:\n",
      "    vocab_all &= set(m.wv.vocab)\n",
      "len(vocab_all)\n",
      "185/152: vocab2 = vocab_all\n",
      "185/153:\n",
      "print(labels[0])\n",
      "for i in gdict:\n",
      "    print(\"%10s\" % i, \\\n",
      "          \"%.3f\" % bias(gdict[i][0], mdict[i][0], vocab, proflist), \\\n",
      "          \"%.3f\" % bias(gdict[i][0], mdict[i][0], vocab, adjectives), \\\n",
      "          \"%.3f\" % bias(gdict[i][0], mdict[i][0], vocab, cities), \\\n",
      "          \"%.3f\" % bias(gdict[i][0], mdict[i][0], vocab, objects), \\\n",
      "          \"%.3f\" % bias(gdict[i][0], mdict[i][0], vocab, gender_specific))\n",
      "185/154:\n",
      "def gdictadd(models):\n",
      "    for i in range(4):\n",
      "        gdict[list(gdict)[i]].append(getg(models[i]))\n",
      "185/155:\n",
      "def mdictadd(models):\n",
      "    for i in range(4):\n",
      "        mdict[list(mdict)[i]].append(models[i])\n",
      "185/156:\n",
      "mdict = {\"politics\": [], \"books\": [], \"movies\": [], \"sports\": []}\n",
      "mdictadd([mpol, mmov, mbooks, msports])\n",
      "mdictadd([mpol_s, mbks_s, mmov_s, mspts_s])\n",
      "mdictadd([mpol_b, mbks_b, mmov_b, mspts_b])\n",
      "mdictadd([mpol_c, mbks_c, mmov_c, mspts_c])\n",
      "185/157:\n",
      "labels = [\"original\", \"summary (TextRank)\", \"summary (Centroid-BOW)\", \"summary (control)\"]\n",
      "gdict = {\"politics\": [], \"books\": [], \"movies\": [], \"sports\": []}\n",
      "gdictadd([mpol, mmov, mbooks, msports])\n",
      "gdictadd([mpol_s, mbks_s, mmov_s, mspts_s])\n",
      "gdictadd([mpol_b, mbks_b, mmov_b, mspts_b])\n",
      "gdictadd([mpol_c, mbks_c, mmov_c, mspts_c])\n",
      "185/158:\n",
      "print(labels[0])\n",
      "for i in gdict:\n",
      "    print(\"%10s\" % i, \\\n",
      "          \"%.3f\" % bias(gdict[i][0], mdict[i][0], vocab, proflist), \\\n",
      "          \"%.3f\" % bias(gdict[i][0], mdict[i][0], vocab, adjectives), \\\n",
      "          \"%.3f\" % bias(gdict[i][0], mdict[i][0], vocab, cities), \\\n",
      "          \"%.3f\" % bias(gdict[i][0], mdict[i][0], vocab, objects), \\\n",
      "          \"%.3f\" % bias(gdict[i][0], mdict[i][0], vocab, gender_specific))\n",
      "185/159:\n",
      "print(labels[1])\n",
      "for i in gdict:\n",
      "    print(\"%10s\" % i, \\\n",
      "          \"%.3f\" % bias(gdict[i][1], mdict[i][1], vocab2, proflist), \\\n",
      "          \"%.3f\" % bias(gdict[i][1], mdict[i][1], vocab2, adjectives), \\\n",
      "          \"%.3f\" % bias(gdict[i][1], mdict[i][1], vocab2, cities), \\\n",
      "          \"%.3f\" % bias(gdict[i][1], mdict[i][1], vocab2, objects), \\\n",
      "          \"%.3f\" % bias(gdict[i][1], mdict[i][1], vocab2, gender_specific))\n",
      "185/160:\n",
      "print(labels[2])\n",
      "for i in gdict:\n",
      "    print(\"%10s\" % i, \\\n",
      "          \"%.3f\" % bias(gdict[i][2], mdict[i][2], vocab2, proflist), \\\n",
      "          \"%.3f\" % bias(gdict[i][2], mdict[i][2], vocab2, adjectives), \\\n",
      "          \"%.3f\" % bias(gdict[i][2], mdict[i][2], vocab2, cities), \\\n",
      "          \"%.3f\" % bias(gdict[i][2], mdict[i][2], vocab2, objects), \\\n",
      "          \"%.3f\" % bias(gdict[i][2], mdict[i][2], vocab2, gender_specific))\n",
      "185/161:\n",
      "print(labels[3])\n",
      "for i in gdict:\n",
      "    print(\"%10s\" % i, \\\n",
      "          \"%.3f\" % bias(gdict[i][3], mdict[i][3], vocab2, proflist), \\\n",
      "          \"%.3f\" % bias(gdict[i][3], mdict[i][3], vocab2, adjectives), \\\n",
      "          \"%.3f\" % bias(gdict[i][3], mdict[i][3], vocab2, cities), \\\n",
      "          \"%.3f\" % bias(gdict[i][3], mdict[i][3], vocab2, objects), \\\n",
      "          \"%.3f\" % bias(gdict[i][3], mdict[i][3], vocab2, gender_specific))\n",
      "185/162:\n",
      "gsports = getg(msports)\n",
      "res = sorted([(x, gproj(gsports, msports, x)) for x in filter(lambda x: x in vocab, vocab)], key=lambda x: x[1])\n",
      "for i in range(10):\n",
      "    print(f\"{res[i][0].ljust(10)} {res[i][1]:.2f} \\t\\t {res[-i-1][0].ljust(10)} {res[-i-1][1]:.2f}\")\n",
      "185/163: dill.dump_session('thesis_env2.db')\n",
      "186/18:\n",
      "t = process_time()\n",
      "dill.load_session('thesis_env2.db')\n",
      "process_time() - t\n",
      "187/1:\n",
      "t = process_time()\n",
      "dill.load_session('thesis_env2.db')\n",
      "process_time() - t\n",
      "187/2:\n",
      "import os\n",
      "import time\n",
      "import dill\n",
      "from typing import List\n",
      "from functools import reduce\n",
      "from calendar import monthrange\n",
      "187/3:\n",
      "import json\n",
      "import csv\n",
      "import requests\n",
      "import xml.etree.ElementTree as ET\n",
      "from bs4 import BeautifulSoup\n",
      "import urllib.request as urllib2\n",
      "187/4:\n",
      "import matplotlib.pyplot as plt\n",
      "import numpy as np\n",
      "from scipy import stats\n",
      "from random import sample, randint, shuffle\n",
      "from sklearn.decomposition import PCA\n",
      "187/5:\n",
      "import nltk\n",
      "import text_summarizer\n",
      "from gensim.utils import simple_preprocess\n",
      "from gensim.summarization.summarizer import summarize\n",
      "from gensim.models import Word2Vec, LsiModel\n",
      "from gensim.test.utils import common_dictionary, common_corpus\n",
      "187/6: from time import process_time\n",
      "187/7:\n",
      "t = process_time()\n",
      "dill.load_session('thesis_env2.db')\n",
      "process_time() - t\n",
      "187/8: len(politics), len(movies), len(books), len(sports)\n",
      "187/9: len(adjectives), len(objects), len(cities)\n",
      "187/10: len(all_)\n",
      "187/11:\n",
      "# mpol, mmov, mbooks\n",
      "# summaries_{pol, mov, bks}\n",
      "# m{pol, mov, bks}_{s, b, c}\n",
      "# bow_{pol, mov, bks}\n",
      "# ctrls_{pol, mov, bks}\n",
      "187/12: len(pairs), len(proflist), len(gender_specific)\n",
      "187/13: len(adjectives), len(objects), len(cities)\n",
      "187/14:\n",
      "vocab = set(mpol.wv.vocab)\n",
      "for m in [mpol, mmov, mbooks, msports]:\n",
      "    vocab &= set(m.wv.vocab)\n",
      "len(vocab)\n",
      "187/15:\n",
      "vocab_all = set(vocab)\n",
      "for m in [mpol_s, mbks_s, mmov_s, mspts_s] + \\\n",
      "[mpol_b, mbks_b, mmov_b, mspts_b] + \\\n",
      "[mpol_c, mbks_c, mmov_c, mspts_c]:\n",
      "    vocab_all &= set(m.wv.vocab)\n",
      "len(vocab_all)\n",
      "187/16: vocab2 = vocab_all\n",
      "187/17: len(vocab), len(vocab2)\n",
      "187/18:\n",
      "labels = [\"original\", \"summary (TextRank)\", \"summary (Centroid-BOW)\", \"summary (control)\"]\n",
      "gdict = {\"politics\": [], \"books\": [], \"movies\": [], \"sports\": []}\n",
      "gdictadd([mpol, mmov, mbooks, msports])\n",
      "gdictadd([mpol_s, mbks_s, mmov_s, mspts_s])\n",
      "gdictadd([mpol_b, mbks_b, mmov_b, mspts_b])\n",
      "gdictadd([mpol_c, mbks_c, mmov_c, mspts_c])\n",
      "187/19:\n",
      "mdict = {\"politics\": [], \"books\": [], \"movies\": [], \"sports\": []}\n",
      "mdictadd([mpol, mmov, mbooks, msports])\n",
      "mdictadd([mpol_s, mbks_s, mmov_s, mspts_s])\n",
      "mdictadd([mpol_b, mbks_b, mmov_b, mspts_b])\n",
      "mdictadd([mpol_c, mbks_c, mmov_c, mspts_c])\n",
      "187/20:\n",
      "def mdictadd(models):\n",
      "    for i in range(len(models)):\n",
      "        mdict[list(mdict)[i]].append(models[i])\n",
      "187/21:\n",
      "def gdictadd(models):\n",
      "    for i in range(len(models)):\n",
      "        gdict[list(gdict)[i]].append(getg(models[i]))\n",
      "187/22:\n",
      "labels = [\"original\", \"summary (TextRank)\", \"summary (Centroid-BOW)\", \"summary (control)\"]\n",
      "gdict = {\"politics\": [], \"books\": [], \"movies\": [], \"sports\": []}\n",
      "gdictadd([mpol, mmov, mbooks, msports])\n",
      "gdictadd([mpol_s, mbks_s, mmov_s, mspts_s])\n",
      "gdictadd([mpol_b, mbks_b, mmov_b, mspts_b])\n",
      "gdictadd([mpol_c, mbks_c, mmov_c, mspts_c])\n",
      "187/23:\n",
      "mdict = {\"politics\": [], \"books\": [], \"movies\": [], \"sports\": []}\n",
      "mdictadd([mpol, mmov, mbooks, msports])\n",
      "mdictadd([mpol_s, mbks_s, mmov_s, mspts_s])\n",
      "mdictadd([mpol_b, mbks_b, mmov_b, mspts_b])\n",
      "mdictadd([mpol_c, mbks_c, mmov_c, mspts_c])\n",
      "187/24:\n",
      "print(labels[0])\n",
      "for i in gdict:\n",
      "    print(\"%10s\" % i, \\\n",
      "          \"%.3f\" % bias(gdict[i][0], mdict[i][0], vocab, proflist), \\\n",
      "          \"%.3f\" % bias(gdict[i][0], mdict[i][0], vocab, adjectives), \\\n",
      "          \"%.3f\" % bias(gdict[i][0], mdict[i][0], vocab, cities), \\\n",
      "          \"%.3f\" % bias(gdict[i][0], mdict[i][0], vocab, objects), \\\n",
      "          \"%.3f\" % bias(gdict[i][0], mdict[i][0], vocab, gender_specific))\n",
      "187/25:\n",
      "print(labels[1])\n",
      "for i in gdict:\n",
      "    print(\"%10s\" % i, \\\n",
      "          \"%.3f\" % bias(gdict[i][1], mdict[i][1], vocab2, proflist), \\\n",
      "          \"%.3f\" % bias(gdict[i][1], mdict[i][1], vocab2, adjectives), \\\n",
      "          \"%.3f\" % bias(gdict[i][1], mdict[i][1], vocab2, cities), \\\n",
      "          \"%.3f\" % bias(gdict[i][1], mdict[i][1], vocab2, objects), \\\n",
      "          \"%.3f\" % bias(gdict[i][1], mdict[i][1], vocab2, gender_specific))\n",
      "187/26:\n",
      "print(labels[2])\n",
      "for i in gdict:\n",
      "    print(\"%10s\" % i, \\\n",
      "          \"%.3f\" % bias(gdict[i][2], mdict[i][2], vocab2, proflist), \\\n",
      "          \"%.3f\" % bias(gdict[i][2], mdict[i][2], vocab2, adjectives), \\\n",
      "          \"%.3f\" % bias(gdict[i][2], mdict[i][2], vocab2, cities), \\\n",
      "          \"%.3f\" % bias(gdict[i][2], mdict[i][2], vocab2, objects), \\\n",
      "          \"%.3f\" % bias(gdict[i][2], mdict[i][2], vocab2, gender_specific))\n",
      "187/27:\n",
      "print(labels[3])\n",
      "for i in gdict:\n",
      "    print(\"%10s\" % i, \\\n",
      "          \"%.3f\" % bias(gdict[i][3], mdict[i][3], vocab2, proflist), \\\n",
      "          \"%.3f\" % bias(gdict[i][3], mdict[i][3], vocab2, adjectives), \\\n",
      "          \"%.3f\" % bias(gdict[i][3], mdict[i][3], vocab2, cities), \\\n",
      "          \"%.3f\" % bias(gdict[i][3], mdict[i][3], vocab2, objects), \\\n",
      "          \"%.3f\" % bias(gdict[i][3], mdict[i][3], vocab2, gender_specific))\n",
      "187/28:\n",
      "gpol = getg(mpol)\n",
      "res = sorted([(x, gproj(gpol, mpol, x)) for x in filter(lambda x: x in vocab, vocab)], key=lambda x: x[1])\n",
      "for i in range(10):\n",
      "    print(f\"{res[i][0].ljust(10)} {res[i][1]:.2f} \\t\\t {res[-i-1][0].ljust(10)} {res[-i-1][1]:.2f}\")\n",
      "gbooks = getg(mbooks)\n",
      "res = sorted([(x, gproj(gbooks, mbooks, x)) for x in filter(lambda x: x in vocab, vocab)], key=lambda x: x[1])\n",
      "for i in range(10):\n",
      "    print(f\"{res[i][0].ljust(10)} {res[i][1]:.2f} \\t\\t {res[-i-1][0].ljust(10)} {res[-i-1][1]:.2f}\")\n",
      "gmov = getg(mmov)\n",
      "res = sorted([(x, gproj(gmov, mmov, x)) for x in filter(lambda x: x in vocab, vocab)], key=lambda x: x[1])\n",
      "for i in range(10):\n",
      "    print(f\"{res[i][0].ljust(10)} {res[i][1]:.2f} \\t\\t {res[-i-1][0].ljust(10)} {res[-i-1][1]:.2f}\")\n",
      "gsports = getg(msports)\n",
      "res = sorted([(x, gproj(gsports, msports, x)) for x in filter(lambda x: x in vocab, vocab)], key=lambda x: x[1])\n",
      "for i in range(10):\n",
      "    print(f\"{res[i][0].ljust(10)} {res[i][1]:.2f} \\t\\t {res[-i-1][0].ljust(10)} {res[-i-1][1]:.2f}\")\n",
      "187/29: regress(gpol, mpol, gpol_s, mpol_s, proflist)\n",
      "187/30:\n",
      "def regress(g1, m1, g2, m2, wordlist):\n",
      "    x = np.array([gproj(g1, m1, w) for w in wordlist if w in m1.wv.vocab and w in m2.wv.vocab])\n",
      "    y = np.array([gproj(g2, m2, w) for w in wordlist if w in m1.wv.vocab and w in m2.wv.vocab])\n",
      "    slope, intercept, r_value, p_value, std_err = stats.linregress(x,y)\n",
      "    return slope, p_value\n",
      "187/31: regress(gpol, mpol, gpol_s, mpol_s, proflist)\n",
      "187/32: regress(gpol, mpol, getg(mpol_s), mpol_s, proflist)\n",
      "187/33:\n",
      "gpol_s = getg(mpol_s)\n",
      "res = sorted([(x, gproj(gpol_s, mpol_s, x)) for x in filter(lambda x: x in vocab2, vocab2)], key=lambda x: x[1])\n",
      "for i in range(10):\n",
      "    print(f\"{res[i][0].ljust(10)} {res[i][1]:.2f} \\t\\t {res[-i-1][0].ljust(10)} {res[-i-1][1]:.2f}\")\n",
      "gbks_s = getg(mbks_s)\n",
      "res = sorted([(x, gproj(gbks_s, mbks_s, x)) for x in filter(lambda x: x in vocab2, vocab2)], key=lambda x: x[1])\n",
      "for i in range(10):x\n",
      "    print(f\"{res[i][0].ljust(10)} {res[i][1]:.2f} \\t\\t {res[-i-1][0].ljust(10)} {res[-i-1][1]:.2f}\")\n",
      "gmov_s = getg(mmov_s)\n",
      "res = sorted([(x, gproj(gmov_s, mmov_s, x)) for x in filter(lambda x: x in vocab2, vocab2)], key=lambda x: x[1])\n",
      "for i in range(10):\n",
      "    print(f\"{res[i][0].ljust(10)} {res[i][1]:.2f} \\t\\t {res[-i-1][0].ljust(10)} {res[-i-1][1]:.2f}\")\n",
      "res = sorted([(x, gproj(gspts_s, mspts_s, x)) for x in filter(lambda x: x in vocab2, vocab2)], key=lambda x: x[1])\n",
      "for i in range(10):\n",
      "    print(f\"{res[i][0].ljust(10)} {res[i][1]:.2f} \\t\\t {res[-i-1][0].ljust(10)} {res[-i-1][1]:.2f}\")\n",
      "187/34:\n",
      "gpol_s = getg(mpol_s)\n",
      "res = sorted([(x, gproj(gpol_s, mpol_s, x)) for x in filter(lambda x: x in vocab2, vocab2)], key=lambda x: x[1])\n",
      "for i in range(10):\n",
      "    print(f\"{res[i][0].ljust(10)} {res[i][1]:.2f} \\t\\t {res[-i-1][0].ljust(10)} {res[-i-1][1]:.2f}\")\n",
      "gbks_s = getg(mbks_s)\n",
      "res = sorted([(x, gproj(gbks_s, mbks_s, x)) for x in filter(lambda x: x in vocab2, vocab2)], key=lambda x: x[1])\n",
      "for i in range(10):x\n",
      "    print(f\"{res[i][0].ljust(10)} {res[i][1]:.2f} \\t\\t {res[-i-1][0].ljust(10)} {res[-i-1][1]:.2f}\")\n",
      "gmov_s = getg(mmov_s)\n",
      "res = sorted([(x, gproj(gmov_s, mmov_s, x)) for x in filter(lambda x: x in vocab2, vocab2)], key=lambda x: x[1])\n",
      "for i in range(10):\n",
      "    print(f\"{res[i][0].ljust(10)} {res[i][1]:.2f} \\t\\t {res[-i-1][0].ljust(10)} {res[-i-1][1]:.2f}\")\n",
      "gspts_s = getg(mspts_s)\n",
      "res = sorted([(x, gproj(gspts_s, mspts_s, x)) for x in filter(lambda x: x in vocab2, vocab2)], key=lambda x: x[1])\n",
      "for i in range(10):\n",
      "    print(f\"{res[i][0].ljust(10)} {res[i][1]:.2f} \\t\\t {res[-i-1][0].ljust(10)} {res[-i-1][1]:.2f}\")\n",
      "187/35:\n",
      "gpol_s = getg(mpol_s)\n",
      "res = sorted([(x, gproj(gpol_s, mpol_s, x)) for x in filter(lambda x: x in vocab2, vocab2)], key=lambda x: x[1])\n",
      "for i in range(10):\n",
      "    print(f\"{res[i][0].ljust(10)} {res[i][1]:.2f} \\t\\t {res[-i-1][0].ljust(10)} {res[-i-1][1]:.2f}\")\n",
      "gbks_s = getg(mbks_s)\n",
      "res = sorted([(x, gproj(gbks_s, mbks_s, x)) for x in filter(lambda x: x in vocab2, vocab2)], key=lambda x: x[1])\n",
      "for i in range(10):\n",
      "    print(f\"{res[i][0].ljust(10)} {res[i][1]:.2f} \\t\\t {res[-i-1][0].ljust(10)} {res[-i-1][1]:.2f}\")\n",
      "gmov_s = getg(mmov_s)\n",
      "res = sorted([(x, gproj(gmov_s, mmov_s, x)) for x in filter(lambda x: x in vocab2, vocab2)], key=lambda x: x[1])\n",
      "for i in range(10):\n",
      "    print(f\"{res[i][0].ljust(10)} {res[i][1]:.2f} \\t\\t {res[-i-1][0].ljust(10)} {res[-i-1][1]:.2f}\")\n",
      "gspts_s = getg(mspts_s)\n",
      "res = sorted([(x, gproj(gspts_s, mspts_s, x)) for x in filter(lambda x: x in vocab2, vocab2)], key=lambda x: x[1])\n",
      "for i in range(10):\n",
      "    print(f\"{res[i][0].ljust(10)} {res[i][1]:.2f} \\t\\t {res[-i-1][0].ljust(10)} {res[-i-1][1]:.2f}\")\n",
      "187/36:\n",
      "gpol_b = getg(mpol_b)\n",
      "res = sorted([(x, gproj(gpol_b, mpol_b, x)) for x in filter(lambda x: x in vocab2, vocab2)], key=lambda x: x[1])\n",
      "for i in range(10):\n",
      "    print(f\"{res[i][0].ljust(10)} {res[i][1]:.2f} \\t\\t {res[-i-1][0].ljust(10)} {res[-i-1][1]:.2f}\")\n",
      "gbks_b = getg(mbks_b)\n",
      "res = sorted([(x, gproj(gbks_b, mbks_b, x)) for x in filter(lambda x: x in vocab2, vocab2)], key=lambda x: x[1])\n",
      "for i in range(10):\n",
      "    print(f\"{res[i][0].ljust(10)} {res[i][1]:.2f} \\t\\t {res[-i-1][0].ljust(10)} {res[-i-1][1]:.2f}\")\n",
      "gmov_b = getg(mmov_b)\n",
      "res = sorted([(x, gproj(gmov_b, mmov_b, x)) for x in filter(lambda x: x in vocab2, vocab2)], key=lambda x: x[1])\n",
      "for i in range(10):\n",
      "    print(f\"{res[i][0].ljust(10)} {res[i][1]:.2f} \\t\\t {res[-i-1][0].ljust(10)} {res[-i-1][1]:.2f}\")\n",
      "gspts_b = getg(mspts_b)\n",
      "res = sorted([(x, gproj(gspts_b, mspts_b, x)) for x in filter(lambda x: x in vocab2, vocab2)], key=lambda x: x[1])\n",
      "for i in range(10):\n",
      "    print(f\"{res[i][0].ljust(10)} {res[i][1]:.2f} \\t\\t {res[-i-1][0].ljust(10)} {res[-i-1][1]:.2f}\")\n",
      "187/37:\n",
      "gpol_c = getg(mpol_c)\n",
      "res = sorted([(x, gproj(gpol_c, mpol_c, x)) for x in filter(lambda x: x in vocab2, vocab2)], key=lambda x: x[1])\n",
      "for i in range(10):\n",
      "    print(f\"{res[i][0].ljust(10)} {res[i][1]:.2f} \\t\\t {res[-i-1][0].ljust(10)} {res[-i-1][1]:.2f}\")\n",
      "gbks_c = getg(mbks_c)\n",
      "res = sorted([(x, gproj(gbks_c, mbks_c, x)) for x in filter(lambda x: x in vocab2, vocab2)], key=lambda x: x[1])\n",
      "for i in range(10):\n",
      "    print(f\"{res[i][0].ljust(10)} {res[i][1]:.2f} \\t\\t {res[-i-1][0].ljust(10)} {res[-i-1][1]:.2f}\")\n",
      "gmov_c = getg(mmov_c)\n",
      "res = sorted([(x, gproj(gmov_c, mmov_c, x)) for x in filter(lambda x: x in vocab2, vocab2)], key=lambda x: x[1])\n",
      "for i in range(10):\n",
      "    print(f\"{res[i][0].ljust(10)} {res[i][1]:.2f} \\t\\t {res[-i-1][0].ljust(10)} {res[-i-1][1]:.2f}\")\n",
      "gspts_c = getg(mspts_c)\n",
      "res = sorted([(x, gproj(gspts_c, mspts_c, x)) for x in filter(lambda x: x in vocab2, vocab2)], key=lambda x: x[1])\n",
      "for i in range(10):\n",
      "    print(f\"{res[i][0].ljust(10)} {res[i][1]:.2f} \\t\\t {res[-i-1][0].ljust(10)} {res[-i-1][1]:.2f}\")\n",
      "185/164: len(politics), len(books), len(movies), len(sports)\n",
      "185/165:\n",
      "for x in [politics, books, movies, sports]:\n",
      "    print(np.mean([len(nltk.word_tokenize(a)) for a in x]))\n",
      "185/166:\n",
      "for x in [politics, books, movies, sports]:\n",
      "    print(np.mean([len(nltk.word_tokenize(a.text)) for a in x]))\n",
      "188/1:\n",
      "import os\n",
      "import time\n",
      "import dill\n",
      "from typing import List\n",
      "from functools import reduce\n",
      "from calendar import monthrange\n",
      "188/2:\n",
      "import json\n",
      "import csv\n",
      "import requests\n",
      "import xml.etree.ElementTree as ET\n",
      "from bs4 import BeautifulSoup\n",
      "import urllib.request as urllib2\n",
      "188/3:\n",
      "import matplotlib.pyplot as plt\n",
      "import numpy as np\n",
      "from scipy import stats\n",
      "from random import sample, randint, shuffle\n",
      "from sklearn.decomposition import PCA\n",
      "188/4:\n",
      "import nltk\n",
      "import text_summarizer\n",
      "from gensim.utils import simple_preprocess\n",
      "from gensim.summarization.summarizer import summarize\n",
      "from gensim.models import Word2Vec, LsiModel\n",
      "from gensim.test.utils import common_dictionary, common_corpus\n",
      "188/5: from time import process_time\n",
      "188/6:\n",
      "t = process_time()\n",
      "dill.load_session('thesis_env2.db')\n",
      "process_time() - t\n",
      "185/167:\n",
      "for x in [politics, books, movies, sports]:\n",
      "    print(sum([len(nltk.word_tokenize(a.text)) for a in x]))\n",
      "188/7: len(politics), len(movies), len(books), len(sports)\n",
      "188/8: len(adjectives), len(objects), len(cities)\n",
      "188/9: len(all_)\n",
      "188/10:\n",
      "# mpol, mmov, mbooks, msports\n",
      "# summaries_{pol, mov, bks, spts}\n",
      "# m{pol, mov, bks, spts}_{s, b, c}\n",
      "# bow_{pol, mov, bks, spts}\n",
      "# ctrls_{pol, mov, bks, spts}\n",
      "188/11: len(pairs), len(proflist), len(gender_specific)\n",
      "188/12:\n",
      "vocab = set(mpol.wv.vocab)\n",
      "for m in [mpol, mmov, mbooks, msports]:\n",
      "    vocab &= set(m.wv.vocab)\n",
      "len(vocab)\n",
      "185/168:\n",
      "for x in [summaries_pol, summaries_bks, summaries_mov, summaries_spts]:\n",
      "    print(sum([len(nltk.word_tokenize(a.text)) for a in x]))\n",
      "185/169:\n",
      "for x in [summaries_pol, summaries_bks, summaries_mov, summaries_spts]:\n",
      "    print(sum([len(nltk.word_tokenize(a)) for a in x]))\n",
      "185/170:\n",
      "for x in [bow_pol, bow_bks, bow_mov, bow_spts]:\n",
      "    print(sum([len(nltk.word_tokenize(a)) for a in x]))\n",
      "185/171:\n",
      "for x in [ctrls_pol, crls_bks, ctrls_mov, ctrls_spts]:\n",
      "    print(sum([len(nltk.word_tokenize(a)) for a in x]))\n",
      "187/38: # plot top 5 pos and top 5 neg and top 10 professions\n",
      "185/172:\n",
      "for x in [ctrls_pol, crls_bks, ctrls_mov, ctrls_spts]:\n",
      "    print(sum([len(nltk.word_tokenize(a)) for a in x]))\n",
      "185/173:\n",
      "for x in [ctrls_pol, ctrls_mov, ctrls_spts]:\n",
      "    print(sum([len(nltk.word_tokenize(a)) for a in x]))\n",
      "185/174: ctrls_bks\n",
      "185/175:\n",
      "for x in [ctrls_pol, ctrls_bks, ctrls_mov, ctrls_spts]:\n",
      "    print(sum([len(nltk.word_tokenize(a)) for a in x]))\n",
      "185/176: mpol.vocab['she'].count\n",
      "185/177: mpol.wv.vocab['she'].count\n",
      "185/178: mpol.wv.vocab['she'].count, mpol.wv.vocab['he'].count\n",
      "185/179: msports.wv.vocab['she'].count, msports.wv.vocab['he'].count\n",
      "185/180:\n",
      "for m in [mpol, mbooks, mmov, mspts]:\n",
      "    print(m.wv.vocab['she'].count, m.wv.vocab['he'].count\n",
      "185/181:\n",
      "for m in [mpol, mbooks, mmov, mspts]:\n",
      "    print(m.wv.vocab['she'].count, m.wv.vocab['he'].count)\n",
      "185/182:\n",
      "for m in [mpol, mbooks, mmov, msports]:\n",
      "    print(m.wv.vocab['she'].count, m.wv.vocab['he'].count)\n",
      "185/183:\n",
      "for m in [mpol, mbooks, mmov, msports]:\n",
      "    print(m.wv.vocab['her'].count, m.wv.vocab['him'].count)\n",
      "185/184:\n",
      "for m in [mpol, mbooks, mmov, msports]:\n",
      "    print(m.wv.vocab['woman'].count, m.wv.vocab['man'].count)\n",
      "185/185:\n",
      "def regress(g1, m1, g2, m2, wordlist):\n",
      "    x = np.array([gproj(g1, m1, w) for w in wordlist if w in m1.wv.vocab and w in m2.wv.vocab])\n",
      "    y = np.array([gproj(g2, m2, w) for w in wordlist if w in m1.wv.vocab and w in m2.wv.vocab])\n",
      "    slope, intercept, r_value, p_value, std_err = stats.linregress(x,y)\n",
      "    return slope, p_value\n",
      "185/186: regress(gpol, mpol, getg(mpol_s), mpol_s, proflist)\n",
      "185/187:\n",
      "def regress(g1, m1, g2, m2, wordlist):\n",
      "    x = np.array([gproj(g1, m1, w) for w in wordlist if w in vocab])\n",
      "    y = np.array([gproj(g2, m2, w) for w in wordlist if w in vocab])\n",
      "    slope, intercept, r_value, p_value, std_err = stats.linregress(x,y)\n",
      "    return slope, p_value\n",
      "185/188: regress(gpol, mpol, getg(mpol_s), mpol_s, proflist)\n",
      "185/189:\n",
      "def regress(g1, m1, g2, m2, wordlist):\n",
      "    x = np.array([gproj(g1, m1, w) for w in wordlist if w in m1.wv.vocab and w in m2.wv.vocab])\n",
      "    y = np.array([gproj(g2, m2, w) for w in wordlist if w in m1.wv.vocab and w in m2.wv.vocab])\n",
      "    slope, intercept, r_value, p_value, std_err = stats.linregress(x,y)\n",
      "    return slope, p_value\n",
      "185/190: regress(gpol, mpol, getg(mpol_s), mpol_s, proflist)\n",
      "187/39: grammar = ['a', 'the', 'that', 'this', 'under', 'over', 'above', 'beyond', 'an', 'are', 'am', 'about']\n",
      "187/40:\n",
      "print(labels[0])\n",
      "for i in gdict:\n",
      "    print(\"%10s\" % i, \\\n",
      "          \"%.3f\" % bias(gdict[i][0], mdict[i][0], vocab, grammar), \\\n",
      "          \"%.3f\" % bias(gdict[i][0], mdict[i][0], vocab, proflist), \\\n",
      "          \"%.3f\" % bias(gdict[i][0], mdict[i][0], vocab, adjectives), \\\n",
      "          \"%.3f\" % bias(gdict[i][0], mdict[i][0], vocab, cities), \\\n",
      "          \"%.3f\" % bias(gdict[i][0], mdict[i][0], vocab, objects), \\\n",
      "          \"%.3f\" % bias(gdict[i][0], mdict[i][0], vocab, gender_specific))\n",
      "187/41:\n",
      "grammar = ['a', 'an', 'the', \\\n",
      "           'for', 'and', 'nor', 'but', 'or', 'yet', 'so', \\\n",
      "           'after', 'as', 'before', 'if', 'inasmuch', 'now', \\\n",
      "           'once', 'since', 'supposing', 'that', 'though', 'until', \\\n",
      "           'whenever', 'whereas', 'wherever', 'which', \\\n",
      "           'although', 'because', 'even', 'lest', 'both', 'either', \\\n",
      "           'hardly', 'when', 'neither', 'than', 'whether', \\\n",
      "           'consequently', 'finally', 'furthermore', 'hence', \\\n",
      "           'however', 'incidentally', 'indeed', 'instead', 'likewise', \\\n",
      "           'meanwhile', 'about', 'above', 'across', 'after', 'ago', \\\n",
      "           'at', 'below', 'by', 'down', 'during', 'for', 'from', 'in', \\\n",
      "           'into', 'off', 'on', 'over', 'past', 'since', 'through', 'to', \\\n",
      "           'under', 'up', 'with', 'amid', 'atop', 'inside', 'onto', \\\n",
      "           'throughout', 'upon', 'within', 'without']\n",
      "187/42:\n",
      "print(labels[0])\n",
      "for i in gdict:\n",
      "    print(\"%10s\" % i, \\\n",
      "          \"%.3f\" % bias(gdict[i][0], mdict[i][0], vocab, grammar), \\\n",
      "          \"%.3f\" % bias(gdict[i][0], mdict[i][0], vocab, proflist), \\\n",
      "          \"%.3f\" % bias(gdict[i][0], mdict[i][0], vocab, adjectives), \\\n",
      "          \"%.3f\" % bias(gdict[i][0], mdict[i][0], vocab, cities), \\\n",
      "          \"%.3f\" % bias(gdict[i][0], mdict[i][0], vocab, objects), \\\n",
      "          \"%.3f\" % bias(gdict[i][0], mdict[i][0], vocab, gender_specific))\n",
      "187/43:\n",
      "print(labels[0])\n",
      "for i in gdict:\n",
      "    print(\"%10s\" % i, \\\n",
      "          \"%.3f\" % bias(gdict[i][0], mdict[i][0], vocab, proflist), \\\n",
      "          \"%.3f\" % bias(gdict[i][0], mdict[i][0], vocab, adjectives), \\\n",
      "          \"%.3f\" % bias(gdict[i][0], mdict[i][0], vocab, cities), \\\n",
      "          \"%.3f\" % bias(gdict[i][0], mdict[i][0], vocab, objects), \\\n",
      "          \"%.3f\" % bias(gdict[i][0], mdict[i][0], vocab, grammar), \\\n",
      "          \"%.3f\" % bias(gdict[i][0], mdict[i][0], vocab, gender_specific))\n",
      "187/44:\n",
      "print(labels[1])\n",
      "for i in gdict:\n",
      "    print(\"%10s\" % i, \\\n",
      "          \"%.3f\" % bias(gdict[i][1], mdict[i][1], vocab2, proflist), \\\n",
      "          \"%.3f\" % bias(gdict[i][1], mdict[i][1], vocab2, adjectives), \\\n",
      "          \"%.3f\" % bias(gdict[i][1], mdict[i][1], vocab2, cities), \\\n",
      "          \"%.3f\" % bias(gdict[i][1], mdict[i][1], vocab2, objects), \\\n",
      "          \"%.3f\" % bias(gdict[i][0], mdict[i][0], vocab, grammar), \\\n",
      "          \"%.3f\" % bias(gdict[i][1], mdict[i][1], vocab2, gender_specific))\n",
      "187/45:\n",
      "print(labels[2])\n",
      "for i in gdict:\n",
      "    print(\"%10s\" % i, \\\n",
      "          \"%.3f\" % bias(gdict[i][2], mdict[i][2], vocab2, proflist), \\\n",
      "          \"%.3f\" % bias(gdict[i][2], mdict[i][2], vocab2, adjectives), \\\n",
      "          \"%.3f\" % bias(gdict[i][2], mdict[i][2], vocab2, cities), \\\n",
      "          \"%.3f\" % bias(gdict[i][2], mdict[i][2], vocab2, objects), \\\n",
      "          \"%.3f\" % bias(gdict[i][0], mdict[i][0], vocab, grammar), \\\n",
      "          \"%.3f\" % bias(gdict[i][2], mdict[i][2], vocab2, gender_specific))\n",
      "187/46:\n",
      "print(labels[3])\n",
      "for i in gdict:\n",
      "    print(\"%10s\" % i, \\\n",
      "          \"%.3f\" % bias(gdict[i][3], mdict[i][3], vocab2, proflist), \\\n",
      "          \"%.3f\" % bias(gdict[i][3], mdict[i][3], vocab2, adjectives), \\\n",
      "          \"%.3f\" % bias(gdict[i][3], mdict[i][3], vocab2, cities), \\\n",
      "          \"%.3f\" % bias(gdict[i][3], mdict[i][3], vocab2, objects), \\\n",
      "          \"%.3f\" % bias(gdict[i][0], mdict[i][0], vocab, grammar), \\\n",
      "          \"%.3f\" % bias(gdict[i][3], mdict[i][3], vocab2, gender_specific))\n",
      "187/47:\n",
      "print(labels[1])\n",
      "for i in gdict:\n",
      "    print(\"%10s\" % i, \\\n",
      "          \"%.3f\" % bias(gdict[i][1], mdict[i][1], vocab2, proflist), \\\n",
      "          \"%.3f\" % bias(gdict[i][1], mdict[i][1], vocab2, adjectives), \\\n",
      "          \"%.3f\" % bias(gdict[i][1], mdict[i][1], vocab2, cities), \\\n",
      "          \"%.3f\" % bias(gdict[i][1], mdict[i][1], vocab2, objects), \\\n",
      "          \"%.3f\" % bias(gdict[i][1], mdict[i][1], vocab, grammar), \\\n",
      "          \"%.3f\" % bias(gdict[i][1], mdict[i][1], vocab2, gender_specific))\n",
      "187/48:\n",
      "print(labels[1])\n",
      "for i in gdict:\n",
      "    print(\"%10s\" % i, \\\n",
      "          \"%.3f\" % bias(gdict[i][1], mdict[i][1], vocab2, proflist), \\\n",
      "          \"%.3f\" % bias(gdict[i][1], mdict[i][1], vocab2, adjectives), \\\n",
      "          \"%.3f\" % bias(gdict[i][1], mdict[i][1], vocab2, cities), \\\n",
      "          \"%.3f\" % bias(gdict[i][1], mdict[i][1], vocab2, objects), \\\n",
      "          \"%.3f\" % bias(gdict[i][1], mdict[i][1], vocab2, grammar), \\\n",
      "          \"%.3f\" % bias(gdict[i][1], mdict[i][1], vocab2, gender_specific))\n",
      "187/49:\n",
      "print(labels[2])\n",
      "for i in gdict:\n",
      "    print(\"%10s\" % i, \\\n",
      "          \"%.3f\" % bias(gdict[i][2], mdict[i][2], vocab2, proflist), \\\n",
      "          \"%.3f\" % bias(gdict[i][2], mdict[i][2], vocab2, adjectives), \\\n",
      "          \"%.3f\" % bias(gdict[i][2], mdict[i][2], vocab2, cities), \\\n",
      "          \"%.3f\" % bias(gdict[i][2], mdict[i][2], vocab2, objects), \\\n",
      "          \"%.3f\" % bias(gdict[i][2], mdict[i][2], vocab, grammar), \\\n",
      "          \"%.3f\" % bias(gdict[i][2], mdict[i][2], vocab2, gender_specific))\n",
      "187/50:\n",
      "print(labels[2])\n",
      "for i in gdict:\n",
      "    print(\"%10s\" % i, \\\n",
      "          \"%.3f\" % bias(gdict[i][2], mdict[i][2], vocab2, proflist), \\\n",
      "          \"%.3f\" % bias(gdict[i][2], mdict[i][2], vocab2, adjectives), \\\n",
      "          \"%.3f\" % bias(gdict[i][2], mdict[i][2], vocab2, cities), \\\n",
      "          \"%.3f\" % bias(gdict[i][2], mdict[i][2], vocab2, objects), \\\n",
      "          \"%.3f\" % bias(gdict[i][2], mdict[i][2], vocab2, grammar), \\\n",
      "          \"%.3f\" % bias(gdict[i][2], mdict[i][2], vocab2, gender_specific))\n",
      "187/51:\n",
      "print(labels[3])\n",
      "for i in gdict:\n",
      "    print(\"%10s\" % i, \\\n",
      "          \"%.3f\" % bias(gdict[i][3], mdict[i][3], vocab2, proflist), \\\n",
      "          \"%.3f\" % bias(gdict[i][3], mdict[i][3], vocab2, adjectives), \\\n",
      "          \"%.3f\" % bias(gdict[i][3], mdict[i][3], vocab2, cities), \\\n",
      "          \"%.3f\" % bias(gdict[i][3], mdict[i][3], vocab2, objects), \\\n",
      "          \"%.3f\" % bias(gdict[i][3], mdict[i][3], vocab2, grammar), \\\n",
      "          \"%.3f\" % bias(gdict[i][3], mdict[i][3], vocab2, gender_specific))\n",
      "187/52:\n",
      "gbks_b = getg(mbks_b)\n",
      "res = sorted([(x, gproj(gbks_b, mbks_b, x)) for x in filter(lambda x: x in vocab2, grammar)], key=lambda x: x[1])\n",
      "for i in range(10):\n",
      "    print(f\"{res[i][0].ljust(10)} {res[i][1]:.2f} \\t\\t {res[-i-1][0].ljust(10)} {res[-i-1][1]:.2f}\")\n",
      "187/53:\n",
      "grammar = ['a', 'an', 'the', \\\n",
      "           'for', 'and', 'nor', 'but', 'or', 'yet', 'so', \\\n",
      "           'after', 'as', 'before', 'if', 'inasmuch', 'now', \\\n",
      "           'once', 'since', 'supposing', 'though', 'until', \\\n",
      "           'whenever', 'whereas', 'wherever', 'which', \\\n",
      "           'although', 'because', 'even', 'lest', 'both', 'either', \\\n",
      "           'hardly', 'when', 'neither', 'than', 'whether', \\\n",
      "           'consequently', 'finally', 'furthermore', 'hence', \\\n",
      "           'however', 'incidentally', 'indeed', 'instead', 'likewise', \\\n",
      "           'meanwhile', 'about', 'above', 'across', 'after', 'ago', \\\n",
      "           'at', 'below', 'by', 'during', 'for', 'from', 'in', \\\n",
      "           'on', 'over', 'past', 'since', 'through', 'to', \\\n",
      "           'under', 'up', 'amid', 'atop', 'onto', \\\n",
      "           'throughout', 'upon', 'within', 'without']\n",
      "187/54:\n",
      "print(labels[0])\n",
      "for i in gdict:\n",
      "    print(\"%10s\" % i, \\\n",
      "          \"%.3f\" % bias(gdict[i][0], mdict[i][0], vocab, proflist), \\\n",
      "          \"%.3f\" % bias(gdict[i][0], mdict[i][0], vocab, adjectives), \\\n",
      "          \"%.3f\" % bias(gdict[i][0], mdict[i][0], vocab, cities), \\\n",
      "          \"%.3f\" % bias(gdict[i][0], mdict[i][0], vocab, objects), \\\n",
      "          \"%.3f\" % bias(gdict[i][0], mdict[i][0], vocab, grammar), \\\n",
      "          \"%.3f\" % bias(gdict[i][0], mdict[i][0], vocab, gender_specific))\n",
      "187/55:\n",
      "print(labels[1])\n",
      "for i in gdict:\n",
      "    print(\"%10s\" % i, \\\n",
      "          \"%.3f\" % bias(gdict[i][1], mdict[i][1], vocab2, proflist), \\\n",
      "          \"%.3f\" % bias(gdict[i][1], mdict[i][1], vocab2, adjectives), \\\n",
      "          \"%.3f\" % bias(gdict[i][1], mdict[i][1], vocab2, cities), \\\n",
      "          \"%.3f\" % bias(gdict[i][1], mdict[i][1], vocab2, objects), \\\n",
      "          \"%.3f\" % bias(gdict[i][1], mdict[i][1], vocab2, grammar), \\\n",
      "          \"%.3f\" % bias(gdict[i][1], mdict[i][1], vocab2, gender_specific))\n",
      "187/56:\n",
      "print(labels[2])\n",
      "for i in gdict:\n",
      "    print(\"%10s\" % i, \\\n",
      "          \"%.3f\" % bias(gdict[i][2], mdict[i][2], vocab2, proflist), \\\n",
      "          \"%.3f\" % bias(gdict[i][2], mdict[i][2], vocab2, adjectives), \\\n",
      "          \"%.3f\" % bias(gdict[i][2], mdict[i][2], vocab2, cities), \\\n",
      "          \"%.3f\" % bias(gdict[i][2], mdict[i][2], vocab2, objects), \\\n",
      "          \"%.3f\" % bias(gdict[i][2], mdict[i][2], vocab2, grammar), \\\n",
      "          \"%.3f\" % bias(gdict[i][2], mdict[i][2], vocab2, gender_specific))\n",
      "187/57:\n",
      "print(labels[3])\n",
      "for i in gdict:\n",
      "    print(\"%10s\" % i, \\\n",
      "          \"%.3f\" % bias(gdict[i][3], mdict[i][3], vocab2, proflist), \\\n",
      "          \"%.3f\" % bias(gdict[i][3], mdict[i][3], vocab2, adjectives), \\\n",
      "          \"%.3f\" % bias(gdict[i][3], mdict[i][3], vocab2, cities), \\\n",
      "          \"%.3f\" % bias(gdict[i][3], mdict[i][3], vocab2, objects), \\\n",
      "          \"%.3f\" % bias(gdict[i][3], mdict[i][3], vocab2, grammar), \\\n",
      "          \"%.3f\" % bias(gdict[i][3], mdict[i][3], vocab2, gender_specific))\n",
      "187/58:\n",
      "gbks_b = getg(mbks_b)\n",
      "res = sorted([(x, gproj(gbks_b, mbks_b, x)) for x in filter(lambda x: x in vocab2, grammar)], key=lambda x: x[1])\n",
      "for i in range(10):\n",
      "    print(f\"{res[i][0].ljust(10)} {res[i][1]:.2f} \\t\\t {res[-i-1][0].ljust(10)} {res[-i-1][1]:.2f}\")\n",
      "187/59:\n",
      "grammar = ['a', 'an', 'the', \\\n",
      "           'for', 'and', 'nor', 'but', 'or', 'yet', 'so', \\\n",
      "           'after', 'as', 'before', 'if', 'inasmuch', 'now', \\\n",
      "           'once', 'since', 'supposing', 'though', 'until', \\\n",
      "           'whenever', 'whereas', 'wherever', 'which', \\\n",
      "           'although', 'because', 'even', 'lest', 'both', 'either', \\\n",
      "           'hardly', 'when', 'neither', 'than', 'whether', \\\n",
      "           'consequently', 'finally', 'furthermore', 'hence', \\\n",
      "           'however', 'incidentally', 'indeed', 'instead', 'likewise', \\\n",
      "           'meanwhile', 'about', 'above', 'across', 'after', 'ago', \\\n",
      "           'at', 'below', 'by', 'during', 'for', 'in', \\\n",
      "           'on', 'over', 'past', 'since', 'through', 'to', \\\n",
      "           'up', 'amid', 'atop', 'onto', \\\n",
      "           'throughout', 'upon', 'within']\n",
      "187/60:\n",
      "print(labels[0])\n",
      "for i in gdict:\n",
      "    print(\"%10s\" % i, \\\n",
      "          \"%.3f\" % bias(gdict[i][0], mdict[i][0], vocab, proflist), \\\n",
      "          \"%.3f\" % bias(gdict[i][0], mdict[i][0], vocab, adjectives), \\\n",
      "          \"%.3f\" % bias(gdict[i][0], mdict[i][0], vocab, cities), \\\n",
      "          \"%.3f\" % bias(gdict[i][0], mdict[i][0], vocab, objects), \\\n",
      "          \"%.3f\" % bias(gdict[i][0], mdict[i][0], vocab, grammar), \\\n",
      "          \"%.3f\" % bias(gdict[i][0], mdict[i][0], vocab, gender_specific))\n",
      "187/61:\n",
      "print(labels[1])\n",
      "for i in gdict:\n",
      "    print(\"%10s\" % i, \\\n",
      "          \"%.3f\" % bias(gdict[i][1], mdict[i][1], vocab2, proflist), \\\n",
      "          \"%.3f\" % bias(gdict[i][1], mdict[i][1], vocab2, adjectives), \\\n",
      "          \"%.3f\" % bias(gdict[i][1], mdict[i][1], vocab2, cities), \\\n",
      "          \"%.3f\" % bias(gdict[i][1], mdict[i][1], vocab2, objects), \\\n",
      "          \"%.3f\" % bias(gdict[i][1], mdict[i][1], vocab2, grammar), \\\n",
      "          \"%.3f\" % bias(gdict[i][1], mdict[i][1], vocab2, gender_specific))\n",
      "187/62:\n",
      "print(labels[2])\n",
      "for i in gdict:\n",
      "    print(\"%10s\" % i, \\\n",
      "          \"%.3f\" % bias(gdict[i][2], mdict[i][2], vocab2, proflist), \\\n",
      "          \"%.3f\" % bias(gdict[i][2], mdict[i][2], vocab2, adjectives), \\\n",
      "          \"%.3f\" % bias(gdict[i][2], mdict[i][2], vocab2, cities), \\\n",
      "          \"%.3f\" % bias(gdict[i][2], mdict[i][2], vocab2, objects), \\\n",
      "          \"%.3f\" % bias(gdict[i][2], mdict[i][2], vocab2, grammar), \\\n",
      "          \"%.3f\" % bias(gdict[i][2], mdict[i][2], vocab2, gender_specific))\n",
      "187/63:\n",
      "print(labels[3])\n",
      "for i in gdict:\n",
      "    print(\"%10s\" % i, \\\n",
      "          \"%.3f\" % bias(gdict[i][3], mdict[i][3], vocab2, proflist), \\\n",
      "          \"%.3f\" % bias(gdict[i][3], mdict[i][3], vocab2, adjectives), \\\n",
      "          \"%.3f\" % bias(gdict[i][3], mdict[i][3], vocab2, cities), \\\n",
      "          \"%.3f\" % bias(gdict[i][3], mdict[i][3], vocab2, objects), \\\n",
      "          \"%.3f\" % bias(gdict[i][3], mdict[i][3], vocab2, grammar), \\\n",
      "          \"%.3f\" % bias(gdict[i][3], mdict[i][3], vocab2, gender_specific))\n",
      "187/64: len(adjectives), len(cities), len(grammar)\n",
      "187/65:\n",
      "with open('grammar.csv', mode='w') as f:\n",
      "    writer = csv.writer(f, delimiter=',')\n",
      "    writer.writerow(grammar)\n",
      "187/66:\n",
      "with open('grammar.csv') as csv_file:\n",
      "    objects = csv_file.read().split(\"\\n\")\n",
      "objects\n",
      "187/67:\n",
      "with open('grammar.csv') as csv_file:\n",
      "    objects = csv_file.read().split(\",\")\n",
      "objects\n",
      "187/68: cities\n",
      "187/69: cities[:-1]\n",
      "187/70: cities[-1]\n",
      "187/71:\n",
      "grammar = ['a', 'an', 'the', \\\n",
      "           'for', 'and', 'nor', 'but', 'or', 'yet', 'so', \\\n",
      "           'after', 'as', 'before', 'if', 'inasmuch', 'now', \\\n",
      "           'once', 'since', 'supposing', 'though', 'until', \\\n",
      "           'whenever', 'whereas', 'wherever', 'which', \\\n",
      "           'although', 'because', 'even', 'lest', 'both', 'either', \\\n",
      "           'hardly', 'when', 'neither', 'than', 'whether', \\\n",
      "           'consequently', 'finally', 'furthermore', 'hence', \\\n",
      "           'however', 'incidentally', 'indeed', 'instead', 'likewise', \\\n",
      "           'meanwhile', 'about', 'above', 'across', 'after', 'ago', \\\n",
      "           'at', 'below', 'by', 'during', 'for', 'in', \\\n",
      "           'on', 'over', 'past', 'since', 'through', 'to', \\\n",
      "           'up', 'amid', 'atop', 'onto', \\\n",
      "           'throughout', 'upon', 'within\\n']\n",
      "187/72:\n",
      "print(labels[0])\n",
      "for i in gdict:\n",
      "    print(\"%10s\" % i, \\\n",
      "          \"%.3f\" % bias(gdict[i][0], mdict[i][0], vocab, proflist), \\\n",
      "          \"%.3f\" % bias(gdict[i][0], mdict[i][0], vocab, adjectives), \\\n",
      "          \"%.3f\" % bias(gdict[i][0], mdict[i][0], vocab, cities), \\\n",
      "          \"%.3f\" % bias(gdict[i][0], mdict[i][0], vocab, grammar), \\\n",
      "          \"%.3f\" % bias(gdict[i][0], mdict[i][0], vocab, gender_specific))\n",
      "187/73:\n",
      "print(labels[1])\n",
      "for i in gdict:\n",
      "    print(\"%10s\" % i, \\\n",
      "          \"%.3f\" % bias(gdict[i][1], mdict[i][1], vocab2, proflist), \\\n",
      "          \"%.3f\" % bias(gdict[i][1], mdict[i][1], vocab2, adjectives), \\\n",
      "          \"%.3f\" % bias(gdict[i][1], mdict[i][1], vocab2, cities), \\\n",
      "          \"%.3f\" % bias(gdict[i][1], mdict[i][1], vocab2, grammar), \\\n",
      "          \"%.3f\" % bias(gdict[i][1], mdict[i][1], vocab2, gender_specific))\n",
      "187/74:\n",
      "print(labels[2])\n",
      "for i in gdict:\n",
      "    print(\"%10s\" % i, \\\n",
      "          \"%.3f\" % bias(gdict[i][2], mdict[i][2], vocab2, proflist), \\\n",
      "          \"%.3f\" % bias(gdict[i][2], mdict[i][2], vocab2, adjectives), \\\n",
      "          \"%.3f\" % bias(gdict[i][2], mdict[i][2], vocab2, cities), \\\n",
      "          \"%.3f\" % bias(gdict[i][2], mdict[i][2], vocab2, grammar), \\\n",
      "          \"%.3f\" % bias(gdict[i][2], mdict[i][2], vocab2, gender_specific))\n",
      "187/75:\n",
      "print(labels[3])\n",
      "for i in gdict:\n",
      "    print(\"%10s\" % i, \\\n",
      "          \"%.3f\" % bias(gdict[i][3], mdict[i][3], vocab2, proflist), \\\n",
      "          \"%.3f\" % bias(gdict[i][3], mdict[i][3], vocab2, adjectives), \\\n",
      "          \"%.3f\" % bias(gdict[i][3], mdict[i][3], vocab2, cities), \\\n",
      "          \"%.3f\" % bias(gdict[i][3], mdict[i][3], vocab2, grammar), \\\n",
      "          \"%.3f\" % bias(gdict[i][3], mdict[i][3], vocab2, gender_specific))\n",
      "187/76:\n",
      "grammar = ['a', 'an', 'the', \\\n",
      "           'for', 'and', 'nor', 'but', 'or', 'yet', 'so', \\\n",
      "           'after', 'as', 'before', 'if', 'inasmuch', 'now', \\\n",
      "           'once', 'since', 'supposing', 'though', 'until', \\\n",
      "           'whenever', 'whereas', 'wherever', 'which', \\\n",
      "           'although', 'because', 'even', 'lest', 'both', 'either', \\\n",
      "           'hardly', 'when', 'neither', 'than', 'whether', \\\n",
      "           'consequently', 'finally', 'furthermore', 'hence', \\\n",
      "           'however', 'incidentally', 'indeed', 'instead', 'likewise', \\\n",
      "           'meanwhile', 'about', 'above', 'across', 'after', 'ago', \\\n",
      "           'at', 'below', 'by', 'during', 'for', 'in', \\\n",
      "           'on', 'over', 'past', 'since', 'through', 'to', \\\n",
      "           'up', 'amid', 'atop', 'onto', \\\n",
      "           'throughout', 'upon', 'within', 'down']\n",
      "187/77:\n",
      "grammar = ['a', 'an', 'the', \\\n",
      "           'for', 'and', 'nor', 'but', 'or', 'yet', 'so', \\\n",
      "           'after', 'as', 'before', 'if', 'inasmuch', 'now', \\\n",
      "           'once', 'since', 'supposing', 'though', 'until', \\\n",
      "           'whenever', 'whereas', 'wherever', 'which', \\\n",
      "           'although', 'because', 'even', 'lest', 'both', 'either', \\\n",
      "           'hardly', 'when', 'neither', 'than', 'whether', \\\n",
      "           'consequently', 'finally', 'furthermore', 'hence', \\\n",
      "           'however', 'incidentally', 'indeed', 'instead', 'likewise', \\\n",
      "           'meanwhile', 'about', 'above', 'across', 'after', 'ago', \\\n",
      "           'at', 'below', 'by', 'during', 'for', 'in', \\\n",
      "           'on', 'over', 'past', 'since', 'through', 'to', \\\n",
      "           'up', 'amid', 'atop', 'onto', \\\n",
      "           'throughout', 'upon', 'within', 'without']\n",
      "187/78:\n",
      "with open('grammar.csv', mode='w') as f:\n",
      "    writer = csv.writer(f, delimiter=',')\n",
      "    writer.writerow(grammar)\n",
      "187/79:\n",
      "with open('grammar.csv') as csv_file:\n",
      "    grammar = csv_file.read().split(\",\")\n",
      "187/80:\n",
      "print(labels[0])\n",
      "for i in gdict:\n",
      "    print(\"%10s\" % i, \\\n",
      "          \"%.3f\" % bias(gdict[i][0], mdict[i][0], vocab, proflist), \\\n",
      "          \"%.3f\" % bias(gdict[i][0], mdict[i][0], vocab, adjectives), \\\n",
      "          \"%.3f\" % bias(gdict[i][0], mdict[i][0], vocab, cities), \\\n",
      "          \"%.3f\" % bias(gdict[i][0], mdict[i][0], vocab, grammar), \\\n",
      "          \"%.3f\" % bias(gdict[i][0], mdict[i][0], vocab, gender_specific))\n",
      "187/81:\n",
      "print(labels[1])\n",
      "for i in gdict:\n",
      "    print(\"%10s\" % i, \\\n",
      "          \"%.3f\" % bias(gdict[i][1], mdict[i][1], vocab2, proflist), \\\n",
      "          \"%.3f\" % bias(gdict[i][1], mdict[i][1], vocab2, adjectives), \\\n",
      "          \"%.3f\" % bias(gdict[i][1], mdict[i][1], vocab2, cities), \\\n",
      "          \"%.3f\" % bias(gdict[i][1], mdict[i][1], vocab2, grammar), \\\n",
      "          \"%.3f\" % bias(gdict[i][1], mdict[i][1], vocab2, gender_specific))\n",
      "187/82:\n",
      "print(labels[2])\n",
      "for i in gdict:\n",
      "    print(\"%10s\" % i, \\\n",
      "          \"%.3f\" % bias(gdict[i][2], mdict[i][2], vocab2, proflist), \\\n",
      "          \"%.3f\" % bias(gdict[i][2], mdict[i][2], vocab2, adjectives), \\\n",
      "          \"%.3f\" % bias(gdict[i][2], mdict[i][2], vocab2, cities), \\\n",
      "          \"%.3f\" % bias(gdict[i][2], mdict[i][2], vocab2, grammar), \\\n",
      "          \"%.3f\" % bias(gdict[i][2], mdict[i][2], vocab2, gender_specific))\n",
      "187/83:\n",
      "print(labels[3])\n",
      "for i in gdict:\n",
      "    print(\"%10s\" % i, \\\n",
      "          \"%.3f\" % bias(gdict[i][3], mdict[i][3], vocab2, proflist), \\\n",
      "          \"%.3f\" % bias(gdict[i][3], mdict[i][3], vocab2, adjectives), \\\n",
      "          \"%.3f\" % bias(gdict[i][3], mdict[i][3], vocab2, cities), \\\n",
      "          \"%.3f\" % bias(gdict[i][3], mdict[i][3], vocab2, grammar), \\\n",
      "          \"%.3f\" % bias(gdict[i][3], mdict[i][3], vocab2, gender_specific))\n",
      "189/1: len(adjectives), len(cities)\n",
      "189/2: len(politics), len(movies), len(books), len(sports)\n",
      "189/3:\n",
      "import os\n",
      "import time\n",
      "import dill\n",
      "from typing import List\n",
      "from functools import reduce\n",
      "from calendar import monthrange\n",
      "189/4:\n",
      "import json\n",
      "import csv\n",
      "import requests\n",
      "import xml.etree.ElementTree as ET\n",
      "from bs4 import BeautifulSoup\n",
      "import urllib.request as urllib2\n",
      "189/5:\n",
      "import matplotlib.pyplot as plt\n",
      "import numpy as np\n",
      "from scipy import stats\n",
      "from random import sample, randint, shuffle\n",
      "from sklearn.decomposition import PCA\n",
      "189/6:\n",
      "import nltk\n",
      "import text_summarizer\n",
      "from gensim.utils import simple_preprocess\n",
      "from gensim.summarization.summarizer import summarize\n",
      "from gensim.models import Word2Vec, LsiModel\n",
      "from gensim.test.utils import common_dictionary, common_corpus\n",
      "189/7: from time import process_time\n",
      "189/8:\n",
      "t = process_time()\n",
      "dill.load_session('thesis_env2.db')\n",
      "process_time() - t\n",
      "188/13:\n",
      "with open('grammar.csv') as csv_file:\n",
      "    grammar = csv_file.read().split(\",\")\n",
      "188/14: len(adjectives), len(grammar), len(cities)\n",
      "189/9: len(politics), len(movies), len(books), len(sports)\n",
      "189/10: len(all_)\n",
      "189/11: len(pairs), len(proflist), len(gender_specific)\n",
      "189/12:\n",
      "vocab = set(mpol.wv.vocab)\n",
      "for m in [mpol, mmov, mbooks, msports]:\n",
      "    vocab &= set(m.wv.vocab)\n",
      "len(vocab)\n",
      "189/13:\n",
      "vocab_all = set(vocab)\n",
      "for m in [mpol_s, mbks_s, mmov_s, mspts_s] + \\\n",
      "[mpol_b, mbks_b, mmov_b, mspts_b] + \\\n",
      "[mpol_c, mbks_c, mmov_c, mspts_c]:\n",
      "    vocab_all &= set(m.wv.vocab)\n",
      "len(vocab_all)\n",
      "189/14: vocab2 = vocab_all\n",
      "189/15: len(vocab), len(vocab2)\n",
      "189/16:\n",
      "with open('grammar.csv') as csv_file:\n",
      "    grammar = csv_file.read().split(\",\")\n",
      "189/17: len(adjectives), len(grammar), len(cities)\n",
      "189/18:\n",
      "def topK(m, word, K):\n",
      "    return [x[0] for x in m.wv.most_similar(positive=[word], topn=K)]\n",
      "188/15:\n",
      "def cos_sim(m, x, y):\n",
      "    wv1 = m.wv.get_vector(x)\n",
      "    wv2 = m.wv.get_vector(y)\n",
      "    sim = np.dot(x, y)/(np.linalg.norm(x)*np.linalg.norm(y))\n",
      "    return sim\n",
      "188/16:\n",
      "def s(m, w, A, B):\n",
      "    return np.mean([cos_sim(w, a) for a in A]) - np.mean([cos_sim(w, b) for b in B])\n",
      "188/17:\n",
      "def s_word(m, w, A, B):\n",
      "    return np.mean([cos_sim(w, a) for a in A]) - np.mean([cos_sim(w, b) for b in B])\n",
      "188/18:\n",
      "def s_set(m, X, Y, A, B):\n",
      "    return sum([s_word(m, x, A, B) for x in X]) - sum([s_word(m, y, A, B) for y in Y])\n",
      "185/191: top5(\"sell\")\n",
      "185/192: top5(\"produce\")\n",
      "185/193: top5(\"publish\")\n",
      "185/194: top5(\"recommend\")\n",
      "185/195: top5(\"distribute\")\n",
      "185/196:\n",
      "def top5(word):\n",
      "    for m in [mpol, mbooks, mmov, msports]:\n",
      "        print([x[0] for x in m.wv.most_similar(positive=[word], topn=5)])\n",
      "185/197: top5('monday')\n",
      "185/198: top5('house')\n",
      "185/199: top5('success')\n",
      "185/200: top5('seat')\n",
      "185/201: top5(\"best\")\n",
      "185/202: top5('healthy')\n",
      "187/84: grammar[:15]\n",
      "187/85: sorted(grammar)[:15]\n",
      "185/203:\n",
      "for x in [politics, books, movies, sports]:\n",
      "    print(np.median([len(nltk.word_tokenize(a.text)) for a in x]))\n",
      "185/204:\n",
      "for x in [politics, books, movies, sports]:\n",
      "    print(np.mean([len(nltk.word_tokenize(a.text)) for a in x]))\n",
      "188/19:\n",
      "female = [\"female\", \"woman\", \"girl\", \"sister\", \"she\", \"her\", \\\n",
      "          \"hers\", \"daughter\"]\n",
      "188/20: male = [\"male\", \"man\", \"boy\", \"brother\", \"he\", \"him\", \"his\", \"son\"]\n",
      "188/21:\n",
      "maths = [\"math\", \"algebra\", \"geometry\", \"calculus\", \"equations\", \\\n",
      "         \"computation\", \"numbers\", \"addition\"]\n",
      "188/22:\n",
      "arts = [\"poetry\", \"art\", \"dance\", \"literature\", \"novel\", \\\n",
      "        \"symphony\", \"drama\", \"sculpture\"]\n",
      "188/23:\n",
      "sciences = [\"science\", \"technology\", \"physics\", \"chemistry\", \\\n",
      "            \"einstein\", \"nasa\", \"experiment\", \"astronomy\"]\n",
      "188/24:\n",
      "career = [\"technician\", \"accountant\", \"supervisor\", \"engineer\", \\\n",
      "          \"worker\", \"educator\", \"clerk\", \"counselor\", \"inspector\", \\\n",
      "          \"mechanic\", \"manager\", \"therapist\", \"administrator\", \\\n",
      "          \"salesperson\", \"receptionist\", \"librarian\", \"advisor\", \\\n",
      "          \"pharmacist\", \"janitor\", \"psychologist\", \"physician\", \\\n",
      "          \"carpenter\", \"nurse\", \"investigator\", \"bartender\", \\\n",
      "          \"specialist\", \"electrician\", \"officer\", \"pathologist\", \\\n",
      "          \"teacher\", \"lawyer\", \"planner\", \"practitioner\", \"plumber\", \\\n",
      "          \"instructor\", \"surgeon\", \"veterinarian\", \"paramedic\", \\\n",
      "          \"examiner\", \"chemist\", \"machinist\", \"appraiser\", \\\n",
      "          \"nutritionist\", \"architect\", \"hairdresser\", \"baker\", \\\n",
      "          \"programmer\", \"paralegal\", \"hygienist\", \"scientist\"]\n",
      "188/25:\n",
      "family = [\"home\", \"parents\", \"children\", \"family\", \"cousins\", \\\n",
      "          \"marriage\", \"wedding\", \"relatives\"]\n",
      "188/26:\n",
      "def effect_size(m, X, Y, A, B):\n",
      "    num = np.mean([s_word(m, x, A, B) for x in X]) - np.mean([s_word(m, y, A, B) for y in Y])\n",
      "    return num/np.std()\n",
      "188/27: s_set(mpol, male, female, career, family)\n",
      "188/28:\n",
      "def s_word(m, w, A, B):\n",
      "    return np.mean([cos_sim(m, w, a) for a in A]) - np.mean([cos_sim(m, w, b) for b in B])\n",
      "188/29: s_set(mpol, male, female, career, family)\n",
      "188/30:\n",
      "def s_word(m, w, A, B):\n",
      "    return np.mean([cos_sim(m, w, a) for a in A]) - np.mean([cos_sim(m, w, b) for b in B])\n",
      "188/31: s_set(mpol, male, female, career, family)\n",
      "188/32:\n",
      "def cos_sim(m, x, y):\n",
      "    wv1 = m.wv.get_vector(x)\n",
      "    wv2 = m.wv.get_vector(y)\n",
      "    sim = np.dot(wv1, wv2)/(np.linalg.norm(wv1)*np.linalg.norm(wv2))\n",
      "    return sim\n",
      "188/33: s_set(mpol, male, female, career, family)\n",
      "188/34:\n",
      "def s_word(m, w, A, B):\n",
      "    for s in [X, Y, A, B]:\n",
      "        s = filter(lambda x: x in m.wv.vocab, s)\n",
      "    return np.mean([cos_sim(m, w, a) for a in A]) - np.mean([cos_sim(m, w, b) for b in B])\n",
      "188/35:\n",
      "def s_set(m, X, Y, A, B):\n",
      "    for s in [X, Y, A, B]:\n",
      "        s = filter(lambda x: x in m.wv.vocab, s)\n",
      "    return sum([s_word(m, x, A, B) for x in X]) - sum([s_word(m, y, A, B) for y in Y])\n",
      "188/36:\n",
      "def effect_size(m, X, Y, A, B):\n",
      "    for s in [X, Y, A, B]:\n",
      "        s = filter(lambda x: x in m.wv.vocab, s)\n",
      "    num = np.mean([s_word(m, x, A, B) for x in X]) - np.mean([s_word(m, y, A, B) for y in Y])\n",
      "    return num/np.std()\n",
      "188/37: s_set(mpol, male, female, career, family)\n",
      "188/38:\n",
      "def s_word(m, w, A, B):\n",
      "    for s in [A, B]:\n",
      "        s = filter(lambda x: x in m.wv.vocab, s)\n",
      "    return np.mean([cos_sim(m, w, a) for a in A]) - np.mean([cos_sim(m, w, b) for b in B])\n",
      "188/39: s_set(mpol, male, female, career, family)\n",
      "188/40: filter(lambda y: y < 0, [-1, 0, 1])\n",
      "188/41: list(filter(lambda y: y < 0, [-1, 0, 1]))\n",
      "188/42:\n",
      "def s_word(m, w, A, B):\n",
      "    for s in [A, B]:\n",
      "        s = filter(lambda x: x not in m.wv.vocab, s)\n",
      "    return np.mean([cos_sim(m, w, a) for a in A]) - np.mean([cos_sim(m, w, b) for b in B])\n",
      "188/43:\n",
      "def s_set(m, X, Y, A, B):\n",
      "    for s in [X, Y, A, B]:\n",
      "        s = filter(lambda x: x not in m.wv.vocab, s)\n",
      "    return sum([s_word(m, x, A, B) for x in X]) - sum([s_word(m, y, A, B) for y in Y])\n",
      "188/44:\n",
      "def effect_size(m, X, Y, A, B):\n",
      "    for s in [X, Y, A, B]:\n",
      "        s = filter(lambda x: x not in m.wv.vocab, s)\n",
      "    num = np.mean([s_word(m, x, A, B) for x in X]) - np.mean([s_word(m, y, A, B) for y in Y])\n",
      "    return num/np.std()\n",
      "188/45: s_set(mpol, male, female, career, family)\n",
      "188/46: 'therapist' not in m\n",
      "188/47: 'therapist' in m\n",
      "188/48: 'therapist' in mpol.wv.vocab\n",
      "188/49: mpol.wv.get_vector('therapist')\n",
      "188/50: mpol.wv.get_vector('worker')\n",
      "188/51: mpol.wv.get_vector('therapist')\n",
      "188/52: 'therapist' in mpol.wv.vocab\n",
      "188/53: 'therapist' not in mpol.wv.vocab\n",
      "188/54:\n",
      "def s_set(m, X, Y, A, B):\n",
      "    for s in [X, Y, A, B]:\n",
      "        s = filter(lambda x: x not in m.wv.vocab, s)\n",
      "    print(A)\n",
      "    return sum([s_word(m, x, A, B) for x in X]) - sum([s_word(m, y, A, B) for y in Y])\n",
      "188/55: s_set(mpol, male, female, career, family)\n",
      "188/56:\n",
      "def s_set(m, X, Y, A, B):\n",
      "    for s in [X, Y, A, B]:\n",
      "        s = list(filter(lambda x: x not in m.wv.vocab, s))\n",
      "    print(A)\n",
      "    return sum([s_word(m, x, A, B) for x in X]) - sum([s_word(m, y, A, B) for y in Y])\n",
      "188/57: s_set(mpol, male, female, career, family)\n",
      "188/58:\n",
      "def s_set(m, X, Y, A, B):\n",
      "    return sum([s_word(m, x, A, B) for x in X]) - sum([s_word(m, y, A, B) for y in Y])\n",
      "188/59:\n",
      "for s in [sciences]:\n",
      "    s = list(filter(lambda x: x not in m.wv.vocab, s))\n",
      "188/60:\n",
      "for s in [career]:\n",
      "    s = list(filter(lambda x: x not in m.wv.vocab, s))\n",
      "188/61: career\n",
      "188/62: career = list(filter(lambda x: x not in m.wv.vocab, career))\n",
      "188/63: career\n",
      "188/64:\n",
      "career = list(filter(lambda x: x not in m.wv.vocab, career))\n",
      "family = list(filter(lambda x: x not in m.wv.vocab, family))\n",
      "female = list(filter(lambda x: x not in m.wv.vocab, female))\n",
      "male = list(filter(lambda x: x not in m.wv.vocab, male))\n",
      "arts = list(filter(lambda x: x not in m.wv.vocab, arts))\n",
      "maths = list(filter(lambda x: x not in m.wv.vocab, maths))\n",
      "sciences = list(filter(lambda x: x not in m.wv.vocab, sciences))\n",
      "188/65:\n",
      "career = list(filter(lambda x: x not in vocab, career))\n",
      "family = list(filter(lambda x: x not in vocab, family))\n",
      "female = list(filter(lambda x: x not in vocab, female))\n",
      "male = list(filter(lambda x: x not in vocab, male))\n",
      "arts = list(filter(lambda x: x not in vocab, arts))\n",
      "maths = list(filter(lambda x: x not in vocab, maths))\n",
      "sciences = list(filter(lambda x: x not in vocab, sciences))\n",
      "188/66: s_set(mpol, male, female, career, family)\n",
      "188/67:\n",
      "for m in [male, female, career, family]:\n",
      "    print(m[:5])\n",
      "188/68: male\n",
      "188/69:\n",
      "sciences = [\"science\", \"technology\", \"physics\", \"chemistry\", \\\n",
      "            \"einstein\", \"nasa\", \"experiment\", \"astronomy\"]\n",
      "188/70:\n",
      "maths = [\"math\", \"algebra\", \"geometry\", \"calculus\", \"equations\", \\\n",
      "         \"computation\", \"numbers\", \"addition\"]\n",
      "188/71:\n",
      "arts = [\"poetry\", \"art\", \"dance\", \"literature\", \"novel\", \\\n",
      "        \"symphony\", \"drama\", \"sculpture\"]\n",
      "188/72:\n",
      "family = [\"home\", \"parents\", \"children\", \"family\", \"cousins\", \\\n",
      "          \"marriage\", \"wedding\", \"relatives\"]\n",
      "188/73:\n",
      "female = [\"female\", \"woman\", \"girl\", \"sister\", \"she\", \"her\", \\\n",
      "          \"hers\", \"daughter\"]\n",
      "188/74: male = [\"male\", \"man\", \"boy\", \"brother\", \"he\", \"him\", \"his\", \"son\"]\n",
      "188/75: male\n",
      "188/76: vocab\n",
      "188/77: len(vocab)\n",
      "188/78:\n",
      "career = list(filter(lambda x: x not in vocab, career))\n",
      "family = list(filter(lambda x: x not in vocab, family))\n",
      "female = list(filter(lambda x: x not in vocab, female))\n",
      "male = list(filter(lambda x: x not in vocab, male))\n",
      "arts = list(filter(lambda x: x not in vocab, arts))\n",
      "maths = list(filter(lambda x: x not in vocab, maths))\n",
      "sciences = list(filter(lambda x: x not in vocab, sciences))\n",
      "188/79: male\n",
      "188/80: 'male' in mpol.wv.vocab\n",
      "188/81:\n",
      "sciences = [\"science\", \"technology\", \"physics\", \"chemistry\", \\\n",
      "            \"einstein\", \"nasa\", \"experiment\", \"astronomy\"]\n",
      "188/82:\n",
      "maths = [\"math\", \"algebra\", \"geometry\", \"calculus\", \"equations\", \\\n",
      "         \"computation\", \"numbers\", \"addition\"]\n",
      "188/83:\n",
      "arts = [\"poetry\", \"art\", \"dance\", \"literature\", \"novel\", \\\n",
      "        \"symphony\", \"drama\", \"sculpture\"]\n",
      "188/84:\n",
      "career = [\"technician\", \"accountant\", \"supervisor\", \"engineer\", \\\n",
      "          \"worker\", \"educator\", \"clerk\", \"counselor\", \"inspector\", \\\n",
      "          \"mechanic\", \"manager\", \"therapist\", \"administrator\", \\\n",
      "          \"salesperson\", \"receptionist\", \"librarian\", \"advisor\", \\\n",
      "          \"pharmacist\", \"janitor\", \"psychologist\", \"physician\", \\\n",
      "          \"carpenter\", \"nurse\", \"investigator\", \"bartender\", \\\n",
      "          \"specialist\", \"electrician\", \"officer\", \"pathologist\", \\\n",
      "          \"teacher\", \"lawyer\", \"planner\", \"practitioner\", \"plumber\", \\\n",
      "          \"instructor\", \"surgeon\", \"veterinarian\", \"paramedic\", \\\n",
      "          \"examiner\", \"chemist\", \"machinist\", \"appraiser\", \\\n",
      "          \"nutritionist\", \"architect\", \"hairdresser\", \"baker\", \\\n",
      "          \"programmer\", \"paralegal\", \"hygienist\", \"scientist\"]\n",
      "188/85:\n",
      "family = [\"home\", \"parents\", \"children\", \"family\", \"cousins\", \\\n",
      "          \"marriage\", \"wedding\", \"relatives\"]\n",
      "188/86:\n",
      "female = [\"female\", \"woman\", \"girl\", \"sister\", \"she\", \"her\", \\\n",
      "          \"hers\", \"daughter\"]\n",
      "188/87: male = [\"male\", \"man\", \"boy\", \"brother\", \"he\", \"him\", \"his\", \"son\"]\n",
      "188/88:\n",
      "career = list(filter(lambda x: x in vocab, career))\n",
      "family = list(filter(lambda x: x in vocab, family))\n",
      "female = list(filter(lambda x: x in vocab, female))\n",
      "male = list(filter(lambda x: x in vocab, male))\n",
      "arts = list(filter(lambda x: x in vocab, arts))\n",
      "maths = list(filter(lambda x: x in vocab, maths))\n",
      "sciences = list(filter(lambda x: x in vocab, sciences))\n",
      "188/89:\n",
      "for m in [male, female, career, family]:\n",
      "    print(m[:5])\n",
      "188/90: career\n",
      "188/91: s_set(mpol, male, female, career, family)\n",
      "188/92:\n",
      "s_set(mpol, male, female, career, family), \\\n",
      "effect_size(mpol, male, female, career, family)\n",
      "188/93:\n",
      "def effect_size(m, X, Y, A, B):\n",
      "    num = np.mean([s_word(m, x, A, B) for x in X]) - np.mean([s_word(m, y, A, B) for y in Y])\n",
      "    return num/np.std([s_word(m, x, A, B) for x in X + Y])\n",
      "188/94:\n",
      "s_set(mpol, male, female, career, family), \\\n",
      "effect_size(mpol, male, female, career, family)\n",
      "188/95:\n",
      "def mdictadd(models):\n",
      "    for i in range(len(models)):\n",
      "        mdict[list(mdict)[i]].append(models[i])\n",
      "188/96:\n",
      "mdict = {\"politics\": [], \"books\": [], \"movies\": [], \"sports\": []}\n",
      "mdictadd([mpol, mmov, mbooks, msports])\n",
      "mdictadd([mpol_s, mbks_s, mmov_s, mspts_s])\n",
      "mdictadd([mpol_b, mbks_b, mmov_b, mspts_b])\n",
      "mdictadd([mpol_c, mbks_c, mmov_c, mspts_c])\n",
      "188/97:\n",
      "s_set(mbooks, male, female, career, family), \\\n",
      "effect_size(mbooks, male, female, career, family)\n",
      "188/98:\n",
      "s_set(mmov, male, female, career, family), \\\n",
      "effect_size(mmov, male, female, career, family)\n",
      "188/99:\n",
      "s_set(msports, male, female, career, family), \\\n",
      "effect_size(msports, male, female, career, family)\n",
      "188/100:\n",
      "s_set(mpol, male, female, maths, arts), \\\n",
      "effect_size(mpol, male, female, maths, arts)\n",
      "188/101:\n",
      "s_set(mbooks, male, female, maths, arts), \\\n",
      "effect_size(mbooks, male, female, maths, arts)\n",
      "188/102:\n",
      "s_set(mpol, male, female, sciences, arts), \\\n",
      "effect_size(mpol, male, female, sciences, arts)\n",
      "188/103:\n",
      "s_set(mbooks, male, female, sciences, arts), \\\n",
      "effect_size(mbooks, male, female, sciences, arts)\n",
      "188/104:\n",
      "stereotypes_m = ['strong', 'masculine', 'leader', 'president']\n",
      "stereotypes_f = ['feminine', 'nurse', 'receptionist', 'homemaker']\n",
      "188/105:\n",
      "s_set(mpol, male, female, stereotypes_m, stereotypes_f), \\\n",
      "effect_size(mpol, male, female, stereotypes_m, stereotypes_f)\n",
      "188/106:\n",
      "stereotypes_m = ['strong', 'tall', 'leader', 'president']\n",
      "stereotypes_f = ['small', 'nurse', 'receptionist', 'homemaker']\n",
      "188/107:\n",
      "s_set(mpol, male, female, stereotypes_m, stereotypes_f), \\\n",
      "effect_size(mpol, male, female, stereotypes_m, stereotypes_f)\n",
      "188/108:\n",
      "s_set(mbooks, male, female, stereotypes_m, stereotypes_f), \\\n",
      "effect_size(mbooks, male, female, stereotypes_m, stereotypes_f)\n",
      "188/109:\n",
      "stereotypes_m = ['leader', 'president', 'professor', 'soldier']\n",
      "stereotypes_f = ['teacher', 'nurse', 'receptionist', 'homemaker']\n",
      "188/110:\n",
      "s_set(mpol, male, female, stereotypes_m, stereotypes_f), \\\n",
      "effect_size(mpol, male, female, stereotypes_m, stereotypes_f)\n",
      "188/111:\n",
      "s_set(mbooks, male, female, stereotypes_m, stereotypes_f), \\\n",
      "effect_size(mbooks, male, female, stereotypes_m, stereotypes_f)\n",
      "188/112:\n",
      "s_set(mmov, male, female, stereotypes_m, stereotypes_f), \\\n",
      "effect_size(mmov, male, female, stereotypes_m, stereotypes_f)\n",
      "188/113:\n",
      "s_set(msports, male, female, stereotypes_m, stereotypes_f), \\\n",
      "effect_size(msports, male, female, stereotypes_m, stereotypes_f)\n",
      "188/114:\n",
      "stereotypes_m = ['captain', 'president', 'professor', 'soldier']\n",
      "stereotypes_f = ['teacher', 'nurse', 'receptionist', 'secretary']\n",
      "188/115:\n",
      "s_set(mpol, male, female, stereotypes_m, stereotypes_f), \\\n",
      "effect_size(mpol, male, female, stereotypes_m, stereotypes_f)\n",
      "188/116:\n",
      "s_set(mbooks, male, female, stereotypes_m, stereotypes_f), \\\n",
      "effect_size(mbooks, male, female, stereotypes_m, stereotypes_f)\n",
      "188/117:\n",
      "s_set(mmov, male, female, stereotypes_m, stereotypes_f), \\\n",
      "effect_size(mmov, male, female, stereotypes_m, stereotypes_f)\n",
      "188/118:\n",
      "s_set(msports, male, female, stereotypes_m, stereotypes_f), \\\n",
      "effect_size(msports, male, female, stereotypes_m, stereotypes_f)\n",
      "188/119: effect_size(mpol, male, female, stereotypes_m, stereotypes_f)\n",
      "188/120:\n",
      "def bias(m):\n",
      "    return effect_size(m, male, female, stereotypes_m, stereotypes_f)\n",
      "188/121:\n",
      "print(labels[0])\n",
      "for i in mdict:\n",
      "    print(\"%10s\" % i, \\\n",
      "          \"%.3f\" % bias(mdict[i][0]), \\\n",
      "          \"%.3f\" % bias(mdict[i][0]), \\\n",
      "          \"%.3f\" % bias(mdict[i][0]), \\\n",
      "          \"%.3f\" % bias(mdict[i][0]), \\\n",
      "          \"%.3f\" % bias(mdict[i][0]))\n",
      "188/122:\n",
      "print(labels[0])\n",
      "for i in mdict:\n",
      "    print(\"%10s\" % i, \\\n",
      "          \"%.3f\" % bias(mdict[i][0]))\n",
      "188/123:\n",
      "effect_size(mpol, male, female, stereotypes_m, stereotypes_f)\n",
      "effect_size(mbooks, male, female, stereotypes_m, stereotypes_f)\n",
      "effect_size(mmov, male, female, stereotypes_m, stereotypes_f)\n",
      "effect_size(msports, male, female, stereotypes_m, stereotypes_f)\n",
      "188/124:\n",
      "mdict = {\"politics\": [], \"books\": [], \"movies\": [], \"sports\": []}\n",
      "mdictadd([mpol, mbooks, mmov, msports])\n",
      "mdictadd([mpol_s, mbks_s, mmov_s, mspts_s])\n",
      "mdictadd([mpol_b, mbks_b, mmov_b, mspts_b])\n",
      "mdictadd([mpol_c, mbks_c, mmov_c, mspts_c])\n",
      "188/125:\n",
      "print(labels[0])\n",
      "for i in mdict:\n",
      "    print(\"%10s\" % i, \\\n",
      "          \"%.3f\" % bias(mdict[i][0]))\n",
      "187/86:\n",
      "mdict = {\"politics\": [], \"books\": [], \"movies\": [], \"sports\": []}\n",
      "mdictadd([mpol, mbooks, mmov, msports])\n",
      "mdictadd([mpol_s, mbks_s, mmov_s, mspts_s])\n",
      "mdictadd([mpol_b, mbks_b, mmov_b, mspts_b])\n",
      "mdictadd([mpol_c, mbks_c, mmov_c, mspts_c])\n",
      "187/87:\n",
      "labels = [\"original\", \"summary (TextRank)\", \"summary (Centroid-BOW)\", \"summary (control)\"]\n",
      "gdict = {\"politics\": [], \"books\": [], \"movies\": [], \"sports\": []}\n",
      "gdictadd([mpol, mbooks, mmov, msports])\n",
      "gdictadd([mpol_s, mbks_s, mmov_s, mspts_s])\n",
      "gdictadd([mpol_b, mbks_b, mmov_b, mspts_b])\n",
      "gdictadd([mpol_c, mbks_c, mmov_c, mspts_c])\n",
      "187/88:\n",
      "print(labels[0])\n",
      "for i in gdict:\n",
      "    print(\"%10s\" % i, \\\n",
      "          \"%.3f\" % bias(gdict[i][0], mdict[i][0], vocab, proflist), \\\n",
      "          \"%.3f\" % bias(gdict[i][0], mdict[i][0], vocab, adjectives), \\\n",
      "          \"%.3f\" % bias(gdict[i][0], mdict[i][0], vocab, cities), \\\n",
      "          \"%.3f\" % bias(gdict[i][0], mdict[i][0], vocab, grammar), \\\n",
      "          \"%.3f\" % bias(gdict[i][0], mdict[i][0], vocab, gender_specific))\n",
      "188/126:\n",
      "combined = politics+books+movies+sports\n",
      "len(combined)\n",
      "188/127: len(proflist)\n",
      "188/128:\n",
      "print(labels[0])\n",
      "for i in mdict:\n",
      "    print(\"%10s\" % i, \\\n",
      "          \"%.3f\" % bias(mdict[i][0]))\n",
      "print(labels[1])\n",
      "for i in mdict:\n",
      "    print(\"%10s\" % i, \\\n",
      "          \"%.3f\" % bias(mdict[i][1]))\n",
      "print(labels[2])\n",
      "for i in mdict:\n",
      "    print(\"%10s\" % i, \\\n",
      "          \"%.3f\" % bias(mdict[i][2]))\n",
      "print(labels[3])\n",
      "for i in mdict:\n",
      "    print(\"%10s\" % i, \\\n",
      "          \"%.3f\" % bias(mdict[i][3]))\n",
      "188/129:\n",
      "# train word embeddings\n",
      "mcombined = getw2v(combined)\n",
      "188/130: profs = set(proflist) & vocab2\n",
      "188/131: gcombined = getg(mcombined)\n",
      "188/132:\n",
      "def gproj(g, m, w1):\n",
      "    wvec = m.wv.get_vector(w1)\n",
      "    p = np.dot(g, wvec)/(np.linalg.norm(g)*np.linalg.norm(wvec))\n",
      "    return p\n",
      "188/133:\n",
      "for p in profs:\n",
      "    bias = gproj(gcombined, mcombined, p)\n",
      "    print(p, bias)\n",
      "188/134:\n",
      "for p in profs:\n",
      "    bias = gproj(gpol, mpol, p)\n",
      "    print(p, bias)\n",
      "188/135:\n",
      "for p in profs:\n",
      "    bias = gproj(gcombined, mcombined, p)\n",
      "    print(p, bias)\n",
      "188/136:\n",
      "for p in profs:\n",
      "    bias = gproj(gcombined, mcombined, p)\n",
      "    if bias > .1 or bias < -.01:\n",
      "        print(p, bias)\n",
      "188/137: nurse in vocab2\n",
      "188/138: 'nurse' in vocab2\n",
      "188/139: 'secretary' in vocab2\n",
      "188/140:\n",
      "stereotypes_m = ['captain', 'president', 'professor', 'soldier', \\\n",
      "                 'coach', 'director', 'principal', 'consultant', \\\n",
      "                 'officer', 'sergeant', 'commissioner', 'deputy', \\\n",
      "                 'manager', 'dean', 'minister']\n",
      "stereotypes_f = ['teacher', 'author', 'secretary', 'writer', \\\n",
      "                 'artist', 'singer']\n",
      "188/141:\n",
      "print(labels[0])\n",
      "for i in mdict:\n",
      "    print(\"%10s\" % i, \\\n",
      "          \"%.3f\" % bias(mdict[i][0]))\n",
      "print(labels[1])\n",
      "for i in mdict:\n",
      "    print(\"%10s\" % i, \\\n",
      "          \"%.3f\" % bias(mdict[i][1]))\n",
      "print(labels[2])\n",
      "for i in mdict:\n",
      "    print(\"%10s\" % i, \\\n",
      "          \"%.3f\" % bias(mdict[i][2]))\n",
      "print(labels[3])\n",
      "for i in mdict:\n",
      "    print(\"%10s\" % i, \\\n",
      "          \"%.3f\" % bias(mdict[i][3]))\n",
      "188/142:\n",
      "for p in profs:\n",
      "    b = gproj(gcombined, mcombined, p)\n",
      "    if b > .1 or b < -.01:\n",
      "        print(p, b)\n",
      "188/143:\n",
      "def bias(m):\n",
      "    return effect_size(m, male, female, stereotypes_m, stereotypes_f)\n",
      "188/144:\n",
      "print(labels[0])\n",
      "for i in mdict:\n",
      "    print(\"%10s\" % i, \\\n",
      "          \"%.3f\" % bias(mdict[i][0]))\n",
      "print(labels[1])\n",
      "for i in mdict:\n",
      "    print(\"%10s\" % i, \\\n",
      "          \"%.3f\" % bias(mdict[i][1]))\n",
      "print(labels[2])\n",
      "for i in mdict:\n",
      "    print(\"%10s\" % i, \\\n",
      "          \"%.3f\" % bias(mdict[i][2]))\n",
      "print(labels[3])\n",
      "for i in mdict:\n",
      "    print(\"%10s\" % i, \\\n",
      "          \"%.3f\" % bias(mdict[i][3]))\n",
      "188/145:\n",
      "female = list(filter(lambda x: x in vocab2, female))\n",
      "male = list(filter(lambda x: x in vocab2, male))\n",
      "188/146:\n",
      "print(labels[0])\n",
      "for i in mdict:\n",
      "    print(\"%10s\" % i, \\\n",
      "          \"%.3f\" % bias(mdict[i][0]))\n",
      "print(labels[1])\n",
      "for i in mdict:\n",
      "    print(\"%10s\" % i, \\\n",
      "          \"%.3f\" % bias(mdict[i][1]))\n",
      "print(labels[2])\n",
      "for i in mdict:\n",
      "    print(\"%10s\" % i, \\\n",
      "          \"%.3f\" % bias(mdict[i][2]))\n",
      "print(labels[3])\n",
      "for i in mdict:\n",
      "    print(\"%10s\" % i, \\\n",
      "          \"%.3f\" % bias(mdict[i][3]))\n",
      "188/147:\n",
      "def bias_c(m):\n",
      "    return effect_size(m, male, female, stereotypes_m, family)\n",
      "188/148:\n",
      "print(labels[0])\n",
      "for i in mdict:\n",
      "    print(\"%10s\" % i, \\\n",
      "          \"%.3f\" % bias_c(mdict[i][0]))\n",
      "print(labels[1])\n",
      "for i in mdict:\n",
      "    print(\"%10s\" % i, \\\n",
      "          \"%.3f\" % bias_c(mdict[i][1]))\n",
      "print(labels[2])\n",
      "for i in mdict:\n",
      "    print(\"%10s\" % i, \\\n",
      "          \"%.3f\" % bias_c(mdict[i][2]))\n",
      "print(labels[3])\n",
      "for i in mdict:\n",
      "    print(\"%10s\" % i, \\\n",
      "          \"%.3f\" % bias_c(mdict[i][3]))\n",
      "188/149:\n",
      "stereotypes_m = ['captain', 'president', 'professor', 'soldier', \\\n",
      "                 'coach', 'director']\n",
      "                 # , 'principal', 'consultant', \\\n",
      "                 'officer', 'sergeant', 'commissioner', 'deputy', \\\n",
      "                 'manager', 'dean', 'minister']\n",
      "stereotypes_f = ['teacher', 'author', 'secretary', 'writer', \\\n",
      "                 'artist', 'singer']\n",
      "188/150:\n",
      "stereotypes_m = ['captain', 'president', 'professor', 'soldier', \\\n",
      "                 'coach', 'director']\n",
      "                 # , 'principal', 'consultant', \\\n",
      "                 # 'officer', 'sergeant', 'commissioner', 'deputy', \\\n",
      "                 # 'manager', 'dean', 'minister']\n",
      "stereotypes_f = ['teacher', 'author', 'secretary', 'writer', \\\n",
      "                 'artist', 'singer']\n",
      "188/151:\n",
      "print(labels[0])\n",
      "for i in mdict:\n",
      "    print(\"%10s\" % i, \\\n",
      "          \"%.3f\" % bias(mdict[i][0]))\n",
      "print(labels[1])\n",
      "for i in mdict:\n",
      "    print(\"%10s\" % i, \\\n",
      "          \"%.3f\" % bias(mdict[i][1]))\n",
      "print(labels[2])\n",
      "for i in mdict:\n",
      "    print(\"%10s\" % i, \\\n",
      "          \"%.3f\" % bias(mdict[i][2]))\n",
      "print(labels[3])\n",
      "for i in mdict:\n",
      "    print(\"%10s\" % i, \\\n",
      "          \"%.3f\" % bias(mdict[i][3]))\n",
      "188/152:\n",
      "for p in profs:\n",
      "    b = gproj(gcombined, mcombined, p)\n",
      "    if b > .15 or b < -.01:\n",
      "        print(p, b)\n",
      "188/153:\n",
      "stereotypes_m = ['captain', 'president', 'professor', 'dean', \\\n",
      "                 'commissioner', 'coach', 'director', 'officer', \\\n",
      "                 'manager']\n",
      "stereotypes_f = ['teacher', 'author', 'secretary', 'writer', \\\n",
      "                 'artist', 'singer', 'teenager', 'editor', \\\n",
      "                 'photographer']\n",
      "188/154:\n",
      "print(labels[0])\n",
      "for i in mdict:\n",
      "    print(\"%10s\" % i, \\\n",
      "          \"%.3f\" % bias(mdict[i][0]))\n",
      "print(labels[1])\n",
      "for i in mdict:\n",
      "    print(\"%10s\" % i, \\\n",
      "          \"%.3f\" % bias(mdict[i][1]))\n",
      "print(labels[2])\n",
      "for i in mdict:\n",
      "    print(\"%10s\" % i, \\\n",
      "          \"%.3f\" % bias(mdict[i][2]))\n",
      "print(labels[3])\n",
      "for i in mdict:\n",
      "    print(\"%10s\" % i, \\\n",
      "          \"%.3f\" % bias(mdict[i][3]))\n",
      "188/155:\n",
      "def bias_c(m):\n",
      "    return effect_size(m, male, female, male, female)\n",
      "188/156:\n",
      "print(labels[0])\n",
      "for i in mdict:\n",
      "    print(\"%10s\" % i, \\\n",
      "          \"%.3f\" % bias_c(mdict[i][0]))\n",
      "print(labels[1])\n",
      "for i in mdict:\n",
      "    print(\"%10s\" % i, \\\n",
      "          \"%.3f\" % bias_c(mdict[i][1]))\n",
      "print(labels[2])\n",
      "for i in mdict:\n",
      "    print(\"%10s\" % i, \\\n",
      "          \"%.3f\" % bias_c(mdict[i][2]))\n",
      "print(labels[3])\n",
      "for i in mdict:\n",
      "    print(\"%10s\" % i, \\\n",
      "          \"%.3f\" % bias_c(mdict[i][3]))\n",
      "188/157: len(male)\n",
      "188/158: len(female)\n",
      "188/159: gender_specific\n",
      "188/160: male\n",
      "188/161: female\n",
      "188/162:\n",
      "female = [\"female\", \"woman\", \"girl\", \"sister\", \"she\", \"her\", \\\n",
      "          \"daughter\", \"mother\"]\n",
      "188/163: male = [\"male\", \"man\", \"boy\", \"brother\", \"he\", \"him\", \"son\", \"father\"]\n",
      "188/164:\n",
      "print(labels[0])\n",
      "for i in mdict:\n",
      "    print(\"%10s\" % i, \\\n",
      "          \"%.3f\" % bias(mdict[i][0]))\n",
      "print(labels[1])\n",
      "for i in mdict:\n",
      "    print(\"%10s\" % i, \\\n",
      "          \"%.3f\" % bias(mdict[i][1]))\n",
      "print(labels[2])\n",
      "for i in mdict:\n",
      "    print(\"%10s\" % i, \\\n",
      "          \"%.3f\" % bias(mdict[i][2]))\n",
      "print(labels[3])\n",
      "for i in mdict:\n",
      "    print(\"%10s\" % i, \\\n",
      "          \"%.3f\" % bias(mdict[i][3]))\n",
      "188/165: male\n",
      "188/166: female\n",
      "188/167: set(gender_specific) & set(vocab2)\n",
      "188/168:\n",
      "male2 = ['bachelor', 'husband', 'brothers', 'dad', 'king', \\\n",
      "         'grandfather', 'guy', 'spokesman', 'uncle']\n",
      "188/169: set(gender_specific) & set(vocab2) - male - female\n",
      "188/170: set(gender_specific) & set(vocab2) - set(male) - set(female)\n",
      "188/171: set(gender_specific_all) & set(vocab2) - set(male) - set(female)\n",
      "188/172: set(gender_specific_full) & set(vocab2) - set(male) - set(female)\n",
      "188/173:\n",
      "with open('debiaswe-master/data/gender_specific_full.json') as f:\n",
      "    gender_specific_full = json.loads(f.read())\n",
      "188/174: set(gender_specific_full) & set(vocab2) - set(male) - set(female)\n",
      "188/175:\n",
      "male2 = ['husband', 'brothers', 'king', \\\n",
      "         'grandfather', 'guy', 'spokesman', 'uncle', \\\n",
      "         'himself', 'men']\n",
      "188/176:\n",
      "female2 = ['wife', 'sisters', 'queen', ''\\\n",
      "           'grandmother', 'lady', 'spokeswoman', 'widow', \\\n",
      "           'herself', 'women']\n",
      "188/177:\n",
      "def bias_c(m):\n",
      "    return effect_size(m, male, female, male2, female2)\n",
      "188/178:\n",
      "print(labels[0])\n",
      "for i in mdict:\n",
      "    print(\"%10s\" % i, \\\n",
      "          \"%.3f\" % bias_c(mdict[i][0]))\n",
      "print(labels[1])\n",
      "for i in mdict:\n",
      "    print(\"%10s\" % i, \\\n",
      "          \"%.3f\" % bias_c(mdict[i][1]))\n",
      "print(labels[2])\n",
      "for i in mdict:\n",
      "    print(\"%10s\" % i, \\\n",
      "          \"%.3f\" % bias_c(mdict[i][2]))\n",
      "print(labels[3])\n",
      "for i in mdict:\n",
      "    print(\"%10s\" % i, \\\n",
      "          \"%.3f\" % bias_c(mdict[i][3]))\n",
      "188/179:\n",
      "def bias_c(m, a, b):\n",
      "    return effect_size(m, male, female, a, b)\n",
      "188/180:\n",
      "print(labels[0])\n",
      "for i in mdict:\n",
      "    print(\"%10s\" % i, \\\n",
      "          \"%.3f\" % bias_c(mdict[i][0], male2, female2))\n",
      "print(labels[1])\n",
      "for i in mdict:\n",
      "    print(\"%10s\" % i, \\\n",
      "          \"%.3f\" % bias_c(mdict[i][1], male2, female2))\n",
      "print(labels[2])\n",
      "for i in mdict:\n",
      "    print(\"%10s\" % i, \\\n",
      "          \"%.3f\" % bias_c(mdict[i][2], male2, female2))\n",
      "print(labels[3])\n",
      "for i in mdict:\n",
      "    print(\"%10s\" % i, \\\n",
      "          \"%.3f\" % bias_c(mdict[i][3], male2, female2))\n",
      "188/181: grammar\n",
      "188/182:\n",
      "l = [1,2,3,4,5]\n",
      "random.shuffle(l)\n",
      "188/183:\n",
      "l = [1,2,3,4,5]\n",
      "shuffle(l)\n",
      "188/184:\n",
      "l = [1,2,3,4,5]\n",
      "shuffle(l)\n",
      "l\n",
      "188/185:\n",
      "def bias_r(m, a, b):\n",
      "    tot = a + b\n",
      "    shuffle(tot)\n",
      "    return effect_size(m, male, female, tot[:len(a)], tot[len(a):])\n",
      "188/186:\n",
      "tot = male + female\n",
      "shuffle(tot)\n",
      "tot[:len(male)]\n",
      "188/187:\n",
      "female = [\"female\", \"woman\", \"girl\", \"girls\", \"sister\", \"she\", \"her\", \\\n",
      "          \"daughter\", \"mother\"]\n",
      "188/188: male = [\"male\", \"man\", \"boy\", \"boys\" \"brother\", \"he\", \"him\", \"son\", \"father\"]\n",
      "188/189:\n",
      "print(labels[0])\n",
      "for i in mdict:\n",
      "    print(\"%10s\" % i, \\\n",
      "          \"%.3f\" % bias(mdict[i][0]))\n",
      "print(labels[1])\n",
      "for i in mdict:\n",
      "    print(\"%10s\" % i, \\\n",
      "          \"%.3f\" % bias(mdict[i][1]))\n",
      "print(labels[2])\n",
      "for i in mdict:\n",
      "    print(\"%10s\" % i, \\\n",
      "          \"%.3f\" % bias(mdict[i][2]))\n",
      "print(labels[3])\n",
      "for i in mdict:\n",
      "    print(\"%10s\" % i, \\\n",
      "          \"%.3f\" % bias(mdict[i][3]))\n",
      "188/190: male = [\"male\", \"man\", \"boy\", \"boys\", \"brother\", \"he\", \"him\", \"son\", \"father\"]\n",
      "188/191:\n",
      "print(labels[0])\n",
      "for i in mdict:\n",
      "    print(\"%10s\" % i, \\\n",
      "          \"%.3f\" % bias(mdict[i][0]))\n",
      "print(labels[1])\n",
      "for i in mdict:\n",
      "    print(\"%10s\" % i, \\\n",
      "          \"%.3f\" % bias(mdict[i][1]))\n",
      "print(labels[2])\n",
      "for i in mdict:\n",
      "    print(\"%10s\" % i, \\\n",
      "          \"%.3f\" % bias(mdict[i][2]))\n",
      "print(labels[3])\n",
      "for i in mdict:\n",
      "    print(\"%10s\" % i, \\\n",
      "          \"%.3f\" % bias(mdict[i][3]))\n",
      "188/192:\n",
      "print(labels[0])\n",
      "for i in mdict:\n",
      "    print(\"%10s\" % i, \\\n",
      "          \"%.3f\" % bias_r(mdict[i][0], male2, female2))\n",
      "print(labels[1])\n",
      "for i in mdict:\n",
      "    print(\"%10s\" % i, \\\n",
      "          \"%.3f\" % bias_r(mdict[i][1], male2, female2))\n",
      "print(labels[2])\n",
      "for i in mdict:\n",
      "    print(\"%10s\" % i, \\\n",
      "          \"%.3f\" % bias_r(mdict[i][2], male2, female2))\n",
      "print(labels[3])\n",
      "for i in mdict:\n",
      "    print(\"%10s\" % i, \\\n",
      "          \"%.3f\" % bias_r(mdict[i][3], male2, female2))\n",
      "188/193:\n",
      "print(labels[0])\n",
      "for i in mdict:\n",
      "    print(\"%10s\" % i, \\\n",
      "          \"%.3f\" % bias_r(mdict[i][0], male2, female2))\n",
      "print(labels[1])\n",
      "for i in mdict:\n",
      "    print(\"%10s\" % i, \\\n",
      "          \"%.3f\" % bias_r(mdict[i][1], male2, female2))\n",
      "print(labels[2])\n",
      "for i in mdict:\n",
      "    print(\"%10s\" % i, \\\n",
      "          \"%.3f\" % bias_r(mdict[i][2], male2, female2))\n",
      "print(labels[3])\n",
      "for i in mdict:\n",
      "    print(\"%10s\" % i, \\\n",
      "          \"%.3f\" % bias_r(mdict[i][3], male2, female2))\n",
      "188/194:\n",
      "print(labels[0])\n",
      "for i in mdict:\n",
      "    print(\"%10s\" % i, \\\n",
      "          \"%.3f\" % bias_r(mdict[i][0], male2, female2))\n",
      "print(labels[1])\n",
      "for i in mdict:\n",
      "    print(\"%10s\" % i, \\\n",
      "          \"%.3f\" % bias_r(mdict[i][1], male2, female2))\n",
      "print(labels[2])\n",
      "for i in mdict:\n",
      "    print(\"%10s\" % i, \\\n",
      "          \"%.3f\" % bias_r(mdict[i][2], male2, female2))\n",
      "print(labels[3])\n",
      "for i in mdict:\n",
      "    print(\"%10s\" % i, \\\n",
      "          \"%.3f\" % bias_r(mdict[i][3], male2, female2))\n",
      "188/195:\n",
      "print(labels[0])\n",
      "for i in mdict:\n",
      "    print(\"%10s\" % i, \\\n",
      "          \"%.3f\" % bias_r(mdict[i][0], grammar[:len(grammar)/2], grammar[len(grammar):]))\n",
      "print(labels[1])\n",
      "for i in mdict:\n",
      "    print(\"%10s\" % i, \\\n",
      "          \"%.3f\" % bias_r(mdict[i][1], grammar[:len(grammar)/2], grammar[len(grammar):]))\n",
      "print(labels[2])\n",
      "for i in mdict:\n",
      "    print(\"%10s\" % i, \\\n",
      "          \"%.3f\" % bias_r(mdict[i][2], grammar[:len(grammar)/2], grammar[len(grammar):]))\n",
      "print(labels[3])\n",
      "for i in mdict:\n",
      "    print(\"%10s\" % i, \\\n",
      "          \"%.3f\" % bias_r(mdict[i][3], grammar[:len(grammar)/, grammar[len(grammar):]))\n",
      "188/196:\n",
      "print(labels[0])\n",
      "for i in mdict:\n",
      "    print(\"%10s\" % i, \\\n",
      "          \"%.3f\" % bias_r(mdict[i][0], grammar[:len(grammar)/2], grammar[len(grammar):]))\n",
      "print(labels[1])\n",
      "for i in mdict:\n",
      "    print(\"%10s\" % i, \\\n",
      "          \"%.3f\" % bias_r(mdict[i][1], grammar[:len(grammar)/2], grammar[len(grammar):]))\n",
      "print(labels[2])\n",
      "for i in mdict:\n",
      "    print(\"%10s\" % i, \\\n",
      "          \"%.3f\" % bias_r(mdict[i][2], grammar[:len(grammar)/2], grammar[len(grammar):]))\n",
      "print(labels[3])\n",
      "for i in mdict:\n",
      "    print(\"%10s\" % i, \\\n",
      "          \"%.3f\" % bias_r(mdict[i][3], grammar[:len(grammar)/2, grammar[len(grammar):])\n",
      "188/197:\n",
      "print(labels[0])\n",
      "for i in mdict:\n",
      "    print(\"%10s\" % i, \\\n",
      "          \"%.3f\" % bias_r(mdict[i][0], grammar[:len(grammar)/2], grammar[len(grammar):]))\n",
      "print(labels[1])\n",
      "for i in mdict:\n",
      "    print(\"%10s\" % i, \\\n",
      "          \"%.3f\" % bias_r(mdict[i][1], grammar[:len(grammar)/2], grammar[len(grammar):]))\n",
      "print(labels[2])\n",
      "for i in mdict:\n",
      "    print(\"%10s\" % i, \\\n",
      "          \"%.3f\" % bias_r(mdict[i][2], grammar[:len(grammar)/2], grammar[len(grammar):]))\n",
      "print(labels[3])\n",
      "for i in mdict:\n",
      "    print(\"%10s\" % i, \\\n",
      "          \"%.3f\" % bias_r(mdict[i][3], grammar[:len(grammar)/2, grammar[len(grammar):]))\n",
      "188/198:\n",
      "print(labels[0])\n",
      "for i in mdict:\n",
      "    print(\"%10s\" % i, \\\n",
      "          \"%.3f\" % bias_r(mdict[i][0], grammar[:len(grammar)/2], grammar[len(grammar):]))\n",
      "print(labels[1])\n",
      "for i in mdict:\n",
      "    print(\"%10s\" % i, \\\n",
      "          \"%.3f\" % bias_r(mdict[i][1], grammar[:len(grammar)/2], grammar[len(grammar):]))\n",
      "print(labels[2])\n",
      "for i in mdict:\n",
      "    print(\"%10s\" % i, \\\n",
      "          \"%.3f\" % bias_r(mdict[i][2], grammar[:len(grammar)/2], grammar[len(grammar):]))\n",
      "print(labels[3])\n",
      "for i in mdict:\n",
      "    print(\"%10s\" % i, \\\n",
      "          \"%.3f\" % bias_r(mdict[i][3], grammar[:len(grammar)/2], grammar[len(grammar):]))\n",
      "188/199:\n",
      "print(labels[0])\n",
      "for i in mdict:\n",
      "    print(\"%10s\" % i, \\\n",
      "          \"%.3f\" % bias_r(mdict[i][0], grammar[:len(grammar)//2], grammar[len(grammar):]))\n",
      "print(labels[1])\n",
      "for i in mdict:\n",
      "    print(\"%10s\" % i, \\\n",
      "          \"%.3f\" % bias_r(mdict[i][1], grammar[:len(grammar)//2], grammar[len(grammar):]))\n",
      "print(labels[2])\n",
      "for i in mdict:\n",
      "    print(\"%10s\" % i, \\\n",
      "          \"%.3f\" % bias_r(mdict[i][2], grammar[:len(grammar)//2], grammar[len(grammar):]))\n",
      "print(labels[3])\n",
      "for i in mdict:\n",
      "    print(\"%10s\" % i, \\\n",
      "          \"%.3f\" % bias_r(mdict[i][3], grammar[:len(grammar)//2], grammar[len(grammar):]))\n",
      "187/89:\n",
      "from sklearn.preprocessing import StandardScaler\n",
      "scaler = StandardScaler()\n",
      "187/90:\n",
      "matrix = []\n",
      "for x in vocab:\n",
      "    matrix.append(mpol.wv.get_vector(x))\n",
      "matrix = np.array(matrix)\n",
      "187/91:\n",
      "scaler.fit_transform(matrix)\n",
      "p = PCA(n_components = 10)\n",
      "p.fit(matrix)\n",
      "187/92: p.components_[0]\n",
      "187/93: # p.components_[0]\n",
      "187/94:\n",
      "def proj(a, b):\n",
      "    p = np.dot(a, b)/(np.linalg.norm(a)*np.linalg.norm(b))\n",
      "    return p\n",
      "187/95:\n",
      "# p.components_[0]\n",
      "words = ['her', 'him', 'mr', 'ms', 'women', 'men', 'president', 'captain', 'professor', 'teacher', 'receptionist']\n",
      "xs = [gproj(p.components_[0], mpol, x) for x in words]\n",
      "ys = [gproj(p.components_[1], mpol, x) for x in words]\n",
      "plt.plot(xs, ys)\n",
      "187/96:\n",
      "# p.components_[0]\n",
      "words = ['her', 'him', 'mr', 'ms', 'women', 'men', 'president', 'captain', 'professor', 'teacher', 'receptionist']\n",
      "xs = [gproj(p.components_[0], mpol, x) for x in words]\n",
      "ys = [gproj(p.components_[1], mpol, x) for x in words]\n",
      "plt.plot(xs, ys, 'o')\n",
      "187/97:\n",
      "# p.components_[0]\n",
      "words = ['her', 'him', 'mr', 'ms', 'women', 'men', 'president', 'captain', 'professor', 'teacher', 'receptionist']\n",
      "xs = [gproj(p.components_[0], mpol, x) for x in words]\n",
      "ys = [gproj(p.components_[1], mpol, x) for x in words]\n",
      "plt.plot(xs, ys, 'o')\n",
      "# label points\n",
      "for x,y,z in zip(xs,ys,words):\n",
      "    plt.annotate(z, # this is the text\n",
      "                 (x,y), # this is the point to label\n",
      "                 textcoords=\"offset points\", # how to position the text\n",
      "                 xytext=(0,10), # distance from text to points (x,y)\n",
      "                 ha='center') # horizontal alignment can be left, right or center\n",
      "plt.xticks(np.arange(0,10,1))\n",
      "plt.yticks(np.arange(0,5,0.5))\n",
      "plt.show()\n",
      "187/98:\n",
      "# p.components_[0]\n",
      "words = ['her', 'him', 'mr', 'ms', 'women', 'men', 'president', 'captain', 'professor', 'teacher', 'receptionist']\n",
      "xs = [gproj(p.components_[0], mpol, x) for x in words]\n",
      "ys = [gproj(p.components_[1], mpol, x) for x in words]\n",
      "plt.plot(xs, ys, 'o')\n",
      "# label points\n",
      "for x,y,z in zip(xs,ys,words):\n",
      "    plt.annotate(z, # this is the text\n",
      "                 (x,y), # this is the point to label\n",
      "                 textcoords=\"offset points\", # how to position the text\n",
      "                 xytext=(0,10), # distance from text to points (x,y)\n",
      "                 ha='center') # horizontal alignment can be left, right or center\n",
      "plt.xticks(np.arange(0,2,.1))\n",
      "plt.yticks(np.arange(0,2,.1))\n",
      "plt.show()\n",
      "187/99:\n",
      "# p.components_[0]\n",
      "words = ['her', 'him', 'mr', 'ms', 'women', 'men', 'president', 'captain', 'professor', 'teacher', 'receptionist']\n",
      "xs = [gproj(p.components_[0], mpol, x) for x in words]\n",
      "ys = [gproj(p.components_[1], mpol, x) for x in words]\n",
      "plt.plot(xs, ys, 'o')\n",
      "# label points\n",
      "for x,y,z in zip(xs,ys,words):\n",
      "    plt.annotate(z, # this is the text\n",
      "                 (x,y), # this is the point to label\n",
      "                 textcoords=\"offset points\", # how to position the text\n",
      "                 xytext=(0,10), # distance from text to points (x,y)\n",
      "                 ha='center') # horizontal alignment can be left, right or center\n",
      "plt.xticks(np.arange(0,1,.5))\n",
      "plt.yticks(np.arange(0,1,.5))\n",
      "plt.show()\n",
      "187/100:\n",
      "# p.components_[0]\n",
      "words = ['her', 'him', 'mr', 'ms', 'women', 'men', 'president', 'captain', 'professor', 'teacher', 'receptionist']\n",
      "xs = [gproj(p.components_[0], mpol, x) for x in words]\n",
      "ys = [gproj(p.components_[1], mpol, x) for x in words]\n",
      "plt.plot(xs, ys, 'o')\n",
      "# label points\n",
      "for x,y,z in zip(xs,ys,words):\n",
      "    plt.annotate(z, # this is the text\n",
      "                 (x,y), # this is the point to label\n",
      "                 textcoords=\"offset points\", # how to position the text\n",
      "                 xytext=(0,10), # distance from text to points (x,y)\n",
      "                 ha='center') # horizontal alignment can be left, right or center\n",
      "plt.xticks(np.arange(-1,1,.25))\n",
      "plt.yticks(np.arange(-1,1,.25))\n",
      "plt.show()\n",
      "187/101:\n",
      "# p.components_[0]\n",
      "words = ['her', 'him', 'mr', 'ms', 'women', 'men', 'president', 'captain', 'professor', 'teacher', 'receptionist']\n",
      "xs = [gproj(gpol, mpol, x) for x in words]\n",
      "ys = [gproj(p.components_[0], mpol, x) for x in words]\n",
      "plt.plot(xs, ys, 'o')\n",
      "# label points\n",
      "for x,y,z in zip(xs,ys,words):\n",
      "    plt.annotate(z, # this is the text\n",
      "                 (x,y), # this is the point to label\n",
      "                 textcoords=\"offset points\", # how to position the text\n",
      "                 xytext=(0,10), # distance from text to points (x,y)\n",
      "                 ha='center') # horizontal alignment can be left, right or center\n",
      "plt.xticks(np.arange(-1,1,.25))\n",
      "plt.yticks(np.arange(-1,1,.25))\n",
      "plt.show()\n",
      "187/102:\n",
      "def getgs(m):\n",
      "    matrix = []\n",
      "    for a, b in pairs:\n",
      "        if a not in m.wv.vocab or b not in m.wv.vocab:\n",
      "            continue\n",
      "        center = (m.wv.get_vector(a) + m.wv.get_vector(b))/2\n",
      "        matrix.append(m.wv.get_vector(a) - center)\n",
      "        matrix.append(m.wv.get_vector(b) - center)\n",
      "    matrix = np.array(matrix)\n",
      "    p = PCA(n_components = 10)\n",
      "    p.fit(matrix)\n",
      "    print(p.explained_variance_ratio_[0])\n",
      "#         plt.bar(range(10), p.explained_variance_ratio_)\n",
      "#         plt.title(\"Explained Variance vs Component Number (\" + t + \")\")\n",
      "    return p.components_[0], p.components_[1]\n",
      "187/103:\n",
      "# p.components_[0]\n",
      "words = ['her', 'him', 'mr', 'ms', 'women', 'men', 'president', 'captain', 'professor', 'teacher', 'receptionist']\n",
      "gpol1, gpol2 = getgpols(mpol)\n",
      "xs = [gproj(gpol1, mpol, x) for x in words]\n",
      "ys = [gproj(gpol2, mpol, x) for x in words]\n",
      "plt.plot(xs, ys, 'o')\n",
      "# label points\n",
      "for x,y,z in zip(xs,ys,words):\n",
      "    plt.annotate(z, # this is the text\n",
      "                 (x,y), # this is the point to label\n",
      "                 textcoords=\"offset points\", # how to position the text\n",
      "                 xytext=(0,10), # distance from text to points (x,y)\n",
      "                 ha='center') # horizontal alignment can be left, right or center\n",
      "plt.xticks(np.arange(-1,1,.25))\n",
      "plt.yticks(np.arange(-1,1,.25))\n",
      "plt.show()\n",
      "187/104:\n",
      "# p.components_[0]\n",
      "words = ['her', 'him', 'mr', 'ms', 'women', 'men', 'president', 'captain', 'professor', 'teacher', 'receptionist']\n",
      "gpol1, gpol2 = getgs(mpol)\n",
      "xs = [gproj(gpol1, mpol, x) for x in words]\n",
      "ys = [gproj(gpol2, mpol, x) for x in words]\n",
      "plt.plot(xs, ys, 'o')\n",
      "# label points\n",
      "for x,y,z in zip(xs,ys,words):\n",
      "    plt.annotate(z, # this is the text\n",
      "                 (x,y), # this is the point to label\n",
      "                 textcoords=\"offset points\", # how to position the text\n",
      "                 xytext=(0,10), # distance from text to points (x,y)\n",
      "                 ha='center') # horizontal alignment can be left, right or center\n",
      "plt.xticks(np.arange(-1,1,.25))\n",
      "plt.yticks(np.arange(-1,1,.25))\n",
      "plt.show()\n",
      "187/105:\n",
      "# p.components_[0]\n",
      "words = ['her', 'him', 'mr', 'ms', 'women', 'men', 'president', 'captain', 'professor', 'teacher', 'receptionist', 'family']\n",
      "gpol1, gpol2 = getgs(mpol)\n",
      "xs = [gproj(gpol1, mpol, x) for x in words]\n",
      "ys = [gproj(gpol2, mpol, x) for x in words]\n",
      "plt.plot(xs, ys, 'o')\n",
      "# label points\n",
      "for x,y,z in zip(xs,ys,words):\n",
      "    plt.annotate(z, # this is the text\n",
      "                 (x,y), # this is the point to label\n",
      "                 textcoords=\"offset points\", # how to position the text\n",
      "                 xytext=(0,10), # distance from text to points (x,y)\n",
      "                 ha='center') # horizontal alignment can be left, right or center\n",
      "plt.xticks(np.arange(-1,1,.25))\n",
      "plt.yticks(np.arange(-1,1,.25))\n",
      "plt.show()\n",
      "187/106:\n",
      "# p.components_[0]\n",
      "words = ['her', 'him', 'mr', 'ms', 'women', 'men', 'president', 'captain', 'professor', 'teacher', \\\n",
      "         'nurse', 'family']\n",
      "gpol1, gpol2 = getgs(mpol)\n",
      "xs = [gproj(gpol1, mpol, x) for x in words]\n",
      "ys = [gproj(gpol2, mpol, x) for x in words]\n",
      "plt.plot(xs, ys, 'o')\n",
      "# label points\n",
      "for x,y,z in zip(xs,ys,words):\n",
      "    plt.annotate(z, # this is the text\n",
      "                 (x,y), # this is the point to label\n",
      "                 textcoords=\"offset points\", # how to position the text\n",
      "                 xytext=(0,10), # distance from text to points (x,y)\n",
      "                 ha='center') # horizontal alignment can be left, right or center\n",
      "plt.xticks(np.arange(-1,1,.25))\n",
      "plt.yticks(np.arange(-1,1,.25))\n",
      "plt.show()\n",
      "187/107:\n",
      "# p.components_[0]\n",
      "words = ['her', 'him', 'mr', 'ms', 'women', 'men', 'president', 'captain', 'professor', 'teacher', \\\n",
      "         'nurse', 'family', 'man', 'woman']\n",
      "gpol1, gpol2 = getgs(mpol)\n",
      "xs = [gproj(gpol1, mpol, x) for x in words]\n",
      "ys = [gproj(gpol2, mpol, x) for x in words]\n",
      "plt.plot(xs, ys, 'o')\n",
      "# label points\n",
      "for x,y,z in zip(xs,ys,words):\n",
      "    plt.annotate(z, # this is the text\n",
      "                 (x,y), # this is the point to label\n",
      "                 textcoords=\"offset points\", # how to position the text\n",
      "                 xytext=(0,10), # distance from text to points (x,y)\n",
      "                 ha='center') # horizontal alignment can be left, right or center\n",
      "plt.xticks(np.arange(-1,1,.25))\n",
      "plt.yticks(np.arange(-1,1,.25))\n",
      "plt.show()\n",
      "187/108:\n",
      "# p.components_[0]\n",
      "words = ['her', 'him', 'mr', 'ms', 'father', 'mother', 'president', 'captain', 'professor', 'teacher', \\\n",
      "         'nurse', 'family', 'man', 'woman']\n",
      "gpol1, gpol2 = getgs(mpol)\n",
      "xs = [gproj(gpol1, mpol, x) for x in words]\n",
      "ys = [gproj(gpol2, mpol, x) for x in words]\n",
      "plt.plot(xs, ys, 'o')\n",
      "# label points\n",
      "for x,y,z in zip(xs,ys,words):\n",
      "    plt.annotate(z, # this is the text\n",
      "                 (x,y), # this is the point to label\n",
      "                 textcoords=\"offset points\", # how to position the text\n",
      "                 xytext=(0,10), # distance from text to points (x,y)\n",
      "                 ha='center') # horizontal alignment can be left, right or center\n",
      "plt.xticks(np.arange(-1,1,.25))\n",
      "plt.yticks(np.arange(-1,1,.25))\n",
      "plt.show()\n",
      "185/205:\n",
      "def getg(m):\n",
      "    matrix = []\n",
      "    for a, b in pairs:\n",
      "        if a not in m.wv.vocab or b not in m.wv.vocab:\n",
      "            continue\n",
      "        center = (m.wv.get_vector(a) + m.wv.get_vector(b))/2\n",
      "        matrix.append(m.wv.get_vector(a) - center)\n",
      "        matrix.append(m.wv.get_vector(b) - center)\n",
      "    matrix = np.array(matrix)\n",
      "    p = PCA(n_components = 10)\n",
      "    p.fit(matrix)\n",
      "    print(p.explained_variance_ratio_[0])\n",
      "#         plt.bar(range(10), p.explained_variance_ratio_)\n",
      "#         plt.title(\"Explained Variance vs Component Number (\" + t + \")\")\n",
      "    return p.components_[0]\n",
      "190/1:\n",
      "import os\n",
      "import time\n",
      "import dill\n",
      "from typing import List\n",
      "from functools import reduce\n",
      "from calendar import monthrange\n",
      "190/2:\n",
      "import json\n",
      "import csv\n",
      "import requests\n",
      "import xml.etree.ElementTree as ET\n",
      "from bs4 import BeautifulSoup\n",
      "import urllib.request as urllib2\n",
      "190/3:\n",
      "import matplotlib.pyplot as plt\n",
      "import numpy as np\n",
      "from scipy import stats\n",
      "from random import sample, randint, shuffle\n",
      "from sklearn.decomposition import PCA\n",
      "190/4:\n",
      "import nltk\n",
      "import text_summarizer\n",
      "from gensim.utils import simple_preprocess\n",
      "from gensim.summarization.summarizer import summarize\n",
      "from gensim.models import Word2Vec, LsiModel\n",
      "from gensim.test.utils import common_dictionary, common_corpus\n",
      "190/5: from time import process_time\n",
      "190/6:\n",
      "t = process_time()\n",
      "dill.load_session('thesis_env2.db')\n",
      "process_time() - t\n",
      "190/7: len(politics), len(movies), len(books), len(sports)\n",
      "190/8: len(all_)\n",
      "190/9: len(pairs), len(proflist), len(gender_specific)\n",
      "190/10:\n",
      "vocab = set(mpol.wv.vocab)\n",
      "for m in [mpol, mmov, mbooks, msports]:\n",
      "    vocab &= set(m.wv.vocab)\n",
      "len(vocab)\n",
      "190/11:\n",
      "vocab_all = set(vocab)\n",
      "for m in [mpol_s, mbks_s, mmov_s, mspts_s] + \\\n",
      "[mpol_b, mbks_b, mmov_b, mspts_b] + \\\n",
      "[mpol_c, mbks_c, mmov_c, mspts_c]:\n",
      "    vocab_all &= set(m.wv.vocab)\n",
      "len(vocab_all)\n",
      "190/12: vocab2 = vocab_all\n",
      "190/13: len(vocab), len(vocab2)\n",
      "190/14:\n",
      "grammar = ['a', 'an', 'the', \\\n",
      "           'for', 'and', 'nor', 'but', 'or', 'yet', 'so', \\\n",
      "           'after', 'as', 'before', 'if', 'inasmuch', 'now', \\\n",
      "           'once', 'since', 'supposing', 'though', 'until', \\\n",
      "           'whenever', 'whereas', 'wherever', 'which', \\\n",
      "           'although', 'because', 'even', 'lest', 'both', 'either', \\\n",
      "           'hardly', 'when', 'neither', 'than', 'whether', \\\n",
      "           'consequently', 'finally', 'furthermore', 'hence', \\\n",
      "           'however', 'incidentally', 'indeed', 'instead', 'likewise', \\\n",
      "           'meanwhile', 'about', 'above', 'across', 'after', 'ago', \\\n",
      "           'at', 'below', 'by', 'during', 'for', 'in', \\\n",
      "           'on', 'over', 'past', 'since', 'through', 'to', \\\n",
      "           'up', 'amid', 'atop', 'onto', \\\n",
      "           'throughout', 'upon', 'within', 'without']\n",
      "190/15: gpol = getg(mpol, \"Gender Pairs - Politics\", True)\n",
      "190/16: gpol = getg(mpol)\n",
      "190/17:\n",
      "def getg(m, title, show):\n",
      "    matrix = []\n",
      "    for a, b in pairs:\n",
      "        if a not in m.wv.vocab or b not in m.wv.vocab:\n",
      "            continue\n",
      "        center = (m.wv.get_vector(a) + m.wv.get_vector(b))/2\n",
      "        matrix.append(m.wv.get_vector(a) - center)\n",
      "        matrix.append(m.wv.get_vector(b) - center)\n",
      "    matrix = np.array(matrix)\n",
      "    p = PCA(n_components = 10)\n",
      "    p.fit(matrix)\n",
      "    print(p.explained_variance_ratio_[0])\n",
      "    if show:\n",
      "        plt.bar(range(10), p.explained_variance_ratio_)\n",
      "        plt.title(\"Explained Variance vs Component Number (\" + t + \")\")\n",
      "    return p.components_[0]\n",
      "190/18: gpol = getg(mpol, \"Gender Pairs - Politics\", True)\n",
      "190/19:\n",
      "def getg(m, t, show):\n",
      "    matrix = []\n",
      "    for a, b in pairs:\n",
      "        if a not in m.wv.vocab or b not in m.wv.vocab:\n",
      "            continue\n",
      "        center = (m.wv.get_vector(a) + m.wv.get_vector(b))/2\n",
      "        matrix.append(m.wv.get_vector(a) - center)\n",
      "        matrix.append(m.wv.get_vector(b) - center)\n",
      "    matrix = np.array(matrix)\n",
      "    p = PCA(n_components = 10)\n",
      "    p.fit(matrix)\n",
      "    print(p.explained_variance_ratio_[0])\n",
      "    if show:\n",
      "        plt.bar(range(10), p.explained_variance_ratio_)\n",
      "        plt.title(\"Explained Variance vs Component Number (\" + t + \")\")\n",
      "    return p.components_[0]\n",
      "190/20: gpol = getg(mpol, \"Gender Pairs - Politics\", True)\n",
      "190/21:\n",
      "def getg(m):\n",
      "    matrix = []\n",
      "    for a, b in pairs:\n",
      "        if a not in m.wv.vocab or b not in m.wv.vocab:\n",
      "            continue\n",
      "        center = (m.wv.get_vector(a) + m.wv.get_vector(b))/2\n",
      "        matrix.append(m.wv.get_vector(a) - center)\n",
      "        matrix.append(m.wv.get_vector(b) - center)\n",
      "    matrix = np.array(matrix)\n",
      "    p = PCA(n_components = 10)\n",
      "    p.fit(matrix)\n",
      "    print(p.explained_variance_ratio_[0])\n",
      "#         plt.bar(range(10), p.explained_variance_ratio_)\n",
      "#         plt.title(\"Explained Variance vs Component Number (\" + t + \")\")\n",
      "    return p.components_[0]\n",
      "190/22: msports\n",
      "190/23: gsports = getg(msports)\n",
      "190/24:\n",
      "res = sorted([(x, gproj(gsports, msports, x)) for x in filter(lambda x: x in vocab, vocab)], key=lambda x: x[1])\n",
      "for i in range(10):\n",
      "    print(f\"{res[i][0].ljust(10)} {res[i][1]:.2f} \\t\\t {res[-i-1][0].ljust(10)} {res[-i-1][1]:.2f}\")\n",
      "190/25: heshe = msports.wv.get_vector('he') - msports.wv.get_vector('she')\n",
      "190/26:\n",
      "res = sorted([(x, gproj(heshe, msports, x)) for x in filter(lambda x: x in vocab, vocab)], key=lambda x: x[1])\n",
      "for i in range(10):\n",
      "    print(f\"{res[i][0].ljust(10)} {res[i][1]:.2f} \\t\\t {res[-i-1][0].ljust(10)} {res[-i-1][1]:.2f}\")\n",
      "190/27: mpol.wv.get_vector('politician') + mpol.wv.get_vector('she')\n",
      "190/28:\n",
      "w = mpol.wv.get_vector('politician') + mpol.wv.get_vector('she')\n",
      "mpol.most_similar(positive=[w], topn=1))\n",
      "190/29:\n",
      "w = mpol.wv.get_vector('politician') + mpol.wv.get_vector('she')\n",
      "mpol.most_similar(positive=[w], topn=1)\n",
      "190/30:\n",
      "w = mpol.wv.get_vector('politician') + mpol.wv.get_vector('woman')\n",
      "mpol.most_similar(positive=[w], topn=1)\n",
      "190/31:\n",
      "w = mpol.wv.get_vector('politician') + mpol.wv.get_vector('woman')\n",
      "mpol.most_similar(positive=[w], topn=5)\n",
      "190/32:\n",
      "w = mpol.wv.get_vector('politician') + mpol.wv.get_vector('female')\n",
      "mpol.most_similar(positive=[w], topn=5)\n",
      "190/33:\n",
      "w = mpol.wv.get_vector('politician') + mpol.wv.get_vector('male')\n",
      "mpol.most_similar(positive=[w], topn=5)\n",
      "190/34:\n",
      "w = mpol.wv.get_vector('politician') + mpol.wv.get_vector('female')\n",
      "mpol.most_similar(positive=[w], topn=15)\n",
      "190/35:\n",
      "w = mpol.wv.get_vector('politician') + mpol.wv.get_vector('male')\n",
      "mpol.most_similar(positive=[w], topn=15)\n",
      "190/36:\n",
      "import os\n",
      "import time\n",
      "import dill\n",
      "from typing import List\n",
      "from functools import reduce\n",
      "from calendar import monthrange\n",
      "190/37:\n",
      "male = ['male', 'man', 'boy', 'brother', 'he', 'him', 'son', 'father', 'husband', 'brothers', 'king', \\\n",
      "        'grandfather', 'guy', 'spokesman', 'uncle', 'himself', 'men', 'mr']\n",
      "180/187: \"hi\"\n",
      "188/200: \"hi\"\n",
      "188/201:\n",
      "male2 = ['husband', 'brothers', 'king', \\\n",
      "         'grandfather', 'guy', 'spokesman', 'uncle', \\\n",
      "         'himself', 'men']\n",
      "188/202:\n",
      "female2 = ['wife', 'sisters', 'queen', \\\n",
      "           'grandmother', 'lady', 'spokeswoman', 'widow', \\\n",
      "           'herself', 'women']\n",
      "188/203:\n",
      "print(labels[0])\n",
      "for i in mdict:\n",
      "    print(\"%10s\" % i, \\\n",
      "          \"%.3f\" % bias_c(mdict[i][0], male2, female2))\n",
      "print(labels[1])\n",
      "for i in mdict:\n",
      "    print(\"%10s\" % i, \\\n",
      "          \"%.3f\" % bias_c(mdict[i][1], male2, female2))\n",
      "print(labels[2])\n",
      "for i in mdict:\n",
      "    print(\"%10s\" % i, \\\n",
      "          \"%.3f\" % bias_c(mdict[i][2], male2, female2))\n",
      "print(labels[3])\n",
      "for i in mdict:\n",
      "    print(\"%10s\" % i, \\\n",
      "          \"%.3f\" % bias_c(mdict[i][3], male2, female2))\n",
      "190/38:\n",
      "female = [\"female\", \"woman\", \"girl\", \"girls\", \"sister\", \"she\", \"her\", \"daughter\", \"mother\", 'wife', 'sisters', \\\n",
      "          'queen', 'grandmother', 'lady', 'spokeswoman', 'widow', 'herself', 'women', 'ms']\n",
      "190/39: mean([1,2])\n",
      "190/40: shuffled([1,2,3,4,5])\n",
      "190/41: shuffle([1,2,3,4,5])\n",
      "190/42:\n",
      "def valid(g, m):\n",
      "    normal = abs(np.mean([gproj(g, m, w) for w in male]) - np.mean([gproj(g, m, w) for w in female]))\n",
      "    shuffled = male + female\n",
      "    shuffle(shuffled)\n",
      "    randomized = abs(np.mean([gproj(g, m, w) for w in shuffled[:len(male)]]) \\\n",
      "                     - np.mean([gproj(g, m, w) for w in shuffled[len(male):]]))\n",
      "    return normal, shuffled, normal-shuffled\n",
      "190/43: valid(gpol, mpol)\n",
      "190/44:\n",
      "def valid(g, m):\n",
      "    normal = abs(np.mean([gproj(g, m, w) for w in male]) - np.mean([gproj(g, m, w) for w in female]))\n",
      "    shuffled = male + female\n",
      "    shuffle(shuffled)\n",
      "    randomized = abs(np.mean([gproj(g, m, w) for w in shuffled[:len(male)]]) \\\n",
      "                     - np.mean([gproj(g, m, w) for w in shuffled[len(male):]]))\n",
      "    return normal, shuffled\n",
      "190/45: valid(gpol, mpol)\n",
      "190/46:\n",
      "def valid(g, m):\n",
      "    normal = abs(np.mean([gproj(g, m, w) for w in male]) - np.mean([gproj(g, m, w) for w in female]))\n",
      "    shuffled = male + female\n",
      "    shuffle(shuffled)\n",
      "    randomized = abs(np.mean([gproj(g, m, w) for w in shuffled[:len(male)]]) \\\n",
      "                     - np.mean([gproj(g, m, w) for w in shuffled[len(male):]]))\n",
      "    return normal, randomized, normal-randomized\n",
      "190/47: valid(gpol, mpol)\n",
      "190/48: valid(gsports, msports)\n",
      "190/49: valid(gpol, mpol)\n",
      "190/50: valid(gsports, msports)\n",
      "190/51: valid(gpol_s, mpol_s)\n",
      "190/52: valid(getg(mpol_s), mpol_s)\n",
      "190/53: valid(gmov, mmov)\n",
      "190/54: valid(getg(mmov), mmov)\n",
      "190/55: valid(getg(mmov_s), mmov_s)\n",
      "190/56: valid(getg(msports_s), msports_s)\n",
      "190/57: valid(getg(mspts_s), mspts_s)\n",
      "190/58: valid(getg(mmov_s), mmov_s)\n",
      "190/59: valid(getg(mmov_s), mmov_s)\n",
      "190/60: valid(getg(mmov_s), mmov_s)\n",
      "190/61: valid(getg(mmov_s), mmov_s)\n",
      "190/62: valid(getg(mmov_s), mmov_s)\n",
      "190/63: valid(getg(mmov_s), mmov_s)\n",
      "190/64: valid(getg(mmov_s), mmov_s)\n",
      "190/65: valid(getg(mmov_s), mmov_s)\n",
      "190/66:\n",
      "shuffled = male + female\n",
      "shuffle(shuffled)\n",
      "190/67:\n",
      "def valid(g, m):\n",
      "    normal = abs(np.mean([gproj(g, m, w) for w in male]) - np.mean([gproj(g, m, w) for w in female]))\n",
      "    randomized = abs(np.mean([gproj(g, m, w) for w in shuffled[:len(male)]]) \\\n",
      "                     - np.mean([gproj(g, m, w) for w in shuffled[len(male):]]))\n",
      "    return normal, randomized, normal-randomized\n",
      "190/68: valid(gpol, mpol)\n",
      "190/69: valid(getg(mmov), mmov)\n",
      "190/70: valid(gsports, msports)\n",
      "190/71: valid(getg(mpol_s), mpol_s)\n",
      "190/72: valid(getg(mmov_s), mmov_s)\n",
      "190/73: valid(getg(mspts_s), mspts_s)\n",
      "190/74: shuffled\n",
      "190/75:\n",
      "shuffled = male + female\n",
      "shuffle(shuffled)\n",
      "190/76: shuffled\n",
      "190/77:\n",
      "male = ['male', 'man', 'boy', 'brother', 'he', 'him', 'son', 'father', 'husband', 'brothers', 'king', \\\n",
      "        'grandfather', 'uncle', 'himself', 'men', 'mr']\n",
      "190/78:\n",
      "female = [\"female\", \"woman\", \"girl\", \"girls\", \"sister\", \"she\", \"her\", \"daughter\", \"mother\", 'wife', 'sisters', \\\n",
      "          'queen', 'grandmother', 'herself', 'women', 'ms']\n",
      "190/79:\n",
      "shuffled = male + female\n",
      "shuffle(shuffled)\n",
      "190/80: shuffled\n",
      "190/81:\n",
      "shuffled = male + female\n",
      "shuffle(shuffled)\n",
      "190/82: shuffled\n",
      "190/83: shuffled[:16]\n",
      "190/84:\n",
      "shuffled = male + female\n",
      "shuffle(shuffled)\n",
      "190/85: shuffled[:16]\n",
      "190/86:\n",
      "shuffled = male + female\n",
      "shuffle(shuffled)\n",
      "190/87: shuffled[:16]\n",
      "190/88:\n",
      "shuffled = male + female\n",
      "shuffle(shuffled)\n",
      "190/89: shuffled[:16]\n",
      "190/90: valid(gpol, mpol)\n",
      "190/91: valid(getg(mmov), mmov)\n",
      "190/92: valid(gsports, msports)\n",
      "190/93: valid(getg(mpol_s), mpol_s)\n",
      "190/94: valid(getg(mmov_s), mmov_s)\n",
      "190/95: valid(getg(mspts_s), mspts_s)\n",
      "190/96:\n",
      "for i in mdict:\n",
      "    print(\"%10s\" % i, \\\n",
      "          \"%.3f\" % valid(gdict[i][0], mdict[i][0]))\n",
      "190/97:\n",
      "for i in mdict:\n",
      "    print(\"%10s\" % i, valid(gdict[i][0], mdict[i][0]))\n",
      "190/98:\n",
      "print(labels[0])\n",
      "for i in mdict:\n",
      "    print(\"%10s\" % i, valid(gdict[i][0], mdict[i][0]))\n",
      "print(labels[1])\n",
      "for i in mdict:\n",
      "    print(\"%10s\" % i, valid(gdict[i][1], mdict[i][1]))\n",
      "print(labels[2])\n",
      "for i in mdict:\n",
      "    print(\"%10s\" % i, valid(gdict[i][2], mdict[i][2]))\n",
      "print(labels[3])\n",
      "for i in mdict:\n",
      "    print(\"%10s\" % i, valid(gdict[i][3], mdict[i][3]))\n",
      "190/99: shuffled2 = shuffled\n",
      "190/100:\n",
      "shuffled = male + female\n",
      "shuffle(shuffled)\n",
      "190/101:\n",
      "print(labels[0])\n",
      "for i in mdict:\n",
      "    print(\"%10s\" % i, valid(gdict[i][0], mdict[i][0]))\n",
      "print(labels[1])\n",
      "for i in mdict:\n",
      "    print(\"%10s\" % i, valid(gdict[i][1], mdict[i][1]))\n",
      "print(labels[2])\n",
      "for i in mdict:\n",
      "    print(\"%10s\" % i, valid(gdict[i][2], mdict[i][2]))\n",
      "print(labels[3])\n",
      "for i in mdict:\n",
      "    print(\"%10s\" % i, valid(gdict[i][3], mdict[i][3]))\n",
      "190/102:\n",
      "shuffled = male + female\n",
      "shuffle(shuffled)\n",
      "190/103:\n",
      "print(labels[0])\n",
      "for i in mdict:\n",
      "    print(\"%10s\" % i, valid(gdict[i][0], mdict[i][0]))\n",
      "print(labels[1])\n",
      "for i in mdict:\n",
      "    print(\"%10s\" % i, valid(gdict[i][1], mdict[i][1]))\n",
      "print(labels[2])\n",
      "for i in mdict:\n",
      "    print(\"%10s\" % i, valid(gdict[i][2], mdict[i][2]))\n",
      "print(labels[3])\n",
      "for i in mdict:\n",
      "    print(\"%10s\" % i, valid(gdict[i][3], mdict[i][3]))\n",
      "190/104: valid(gpol, mpol)\n",
      "190/105: shuffled\n",
      "190/106:\n",
      "shuffled = male + female\n",
      "shuffle(shuffled)\n",
      "190/107: shuffled\n",
      "190/108:\n",
      "def valid(g, m):\n",
      "    normal = abs(np.mean([gproj(g, m, w) for w in male]) - np.mean([gproj(g, m, w) for w in female]))\n",
      "    randomized = abs(np.mean([gproj(g, m, w) for w in shuffled[:len(male)]]) \\\n",
      "                     - np.mean([gproj(g, m, w) for w in shuffled[len(male):]]))\n",
      "    return normal, randomized, normal-randomized\n",
      "190/109:\n",
      "print(labels[0])\n",
      "for i in mdict:\n",
      "    print(\"%10s\" % i, valid(gdict[i][0], mdict[i][0]))\n",
      "print(labels[1])\n",
      "for i in mdict:\n",
      "    print(\"%10s\" % i, valid(gdict[i][1], mdict[i][1]))\n",
      "print(labels[2])\n",
      "for i in mdict:\n",
      "    print(\"%10s\" % i, valid(gdict[i][2], mdict[i][2]))\n",
      "print(labels[3])\n",
      "for i in mdict:\n",
      "    print(\"%10s\" % i, valid(gdict[i][3], mdict[i][3]))\n",
      "190/110:\n",
      "shuffled = male + female\n",
      "shuffle(shuffled)\n",
      "190/111: valid(gpol, mpol)\n",
      "190/112:\n",
      "shuffled = male + female\n",
      "shuffle(shuffled)\n",
      "190/113: valid(gpol, mpol)\n",
      "190/114:\n",
      "male = ['male', 'man', 'boy', 'boys', 'brother', 'he', 'him', 'son', 'father', 'husband', 'brothers', 'king', \\\n",
      "        'grandfather', 'uncle', 'himself', 'men', 'mr']\n",
      "190/115:\n",
      "male = ['male', 'man', 'boy', 'boys', 'brother', 'brothers', 'he', 'him', 'son', 'sons', 'father', 'husband', \\\n",
      "        'brothers', 'king', 'grandfather', 'uncle', 'himself', 'men', 'mr', 'spokesman']\n",
      "190/116:\n",
      "female = [\"female\", \"woman\", \"girl\", \"girls\", \"sister\", 'sisters', \"she\", \"her\", \"daughter\", 'daughters', \\\n",
      "          \"mother\", 'wife', 'sisters', 'queen', 'grandmother', 'herself', 'women', 'ms', 'mrs', 'spokeswoman']\n",
      "190/117:\n",
      "shuffled = male + female\n",
      "shuffle(shuffled)\n",
      "190/118: shuffled2 = shuffled\n",
      "190/119:\n",
      "print(labels[0])\n",
      "for i in mdict:\n",
      "    print(\"%10s\" % i, valid(gdict[i][0], mdict[i][0]))\n",
      "print(labels[1])\n",
      "for i in mdict:\n",
      "    print(\"%10s\" % i, valid(gdict[i][1], mdict[i][1]))\n",
      "print(labels[2])\n",
      "for i in mdict:\n",
      "    print(\"%10s\" % i, valid(gdict[i][2], mdict[i][2]))\n",
      "print(labels[3])\n",
      "for i in mdict:\n",
      "    print(\"%10s\" % i, valid(gdict[i][3], mdict[i][3]))\n",
      "190/120:\n",
      "print(labels[0])\n",
      "for i in mdict:\n",
      "    print(\"%10s\" % i, valid(gdict[i][0], mdict[i][0]))\n",
      "print(labels[1])\n",
      "for i in mdict:\n",
      "    print(\"%10s\" % i, valid(gdict[i][1], mdict[i][1]))\n",
      "print(labels[2])\n",
      "for i in mdict:\n",
      "    print(\"%10s\" % i, valid(gdict[i][2], mdict[i][2]))\n",
      "print(labels[3])\n",
      "for i in mdict:\n",
      "    print(\"%10s\" % i, valid(gdict[i][3], mdict[i][3]))\n",
      "190/121:\n",
      "shuffled = male + female\n",
      "shuffle(shuffled)\n",
      "190/122:\n",
      "print(labels[0])\n",
      "for i in mdict:\n",
      "    print(\"%10s\" % i, valid(gdict[i][0], mdict[i][0]))\n",
      "print(labels[1])\n",
      "for i in mdict:\n",
      "    print(\"%10s\" % i, valid(gdict[i][1], mdict[i][1]))\n",
      "print(labels[2])\n",
      "for i in mdict:\n",
      "    print(\"%10s\" % i, valid(gdict[i][2], mdict[i][2]))\n",
      "print(labels[3])\n",
      "for i in mdict:\n",
      "    print(\"%10s\" % i, valid(gdict[i][3], mdict[i][3]))\n",
      "190/123:\n",
      "def valid(g, m):\n",
      "    normal = abs(np.mean([gproj(g, m, w) for w in male]) - np.mean([gproj(g, m, w) for w in female]))\n",
      "    shuffleds = [shuffle(male+female) for i in range(25)]\n",
      "    randomized = np.mean([abs(np.mean([gproj(g, m, w) for w in shuffled[:len(male)]]) \\\n",
      "                     - np.mean([gproj(g, m, w) for w in shuffled[len(male):]])) for shuffled in shuffleds])\n",
      "    return normal, randomized, normal-randomized\n",
      "190/124:\n",
      "print(labels[0])\n",
      "for i in mdict:\n",
      "    print(\"%10s\" % i, valid(gdict[i][0], mdict[i][0]))\n",
      "print(labels[1])\n",
      "for i in mdict:\n",
      "    print(\"%10s\" % i, valid(gdict[i][1], mdict[i][1]))\n",
      "print(labels[2])\n",
      "for i in mdict:\n",
      "    print(\"%10s\" % i, valid(gdict[i][2], mdict[i][2]))\n",
      "print(labels[3])\n",
      "for i in mdict:\n",
      "    print(\"%10s\" % i, valid(gdict[i][3], mdict[i][3]))\n",
      "190/125:\n",
      "def valid(g, m):\n",
      "    normal = abs(np.mean([gproj(g, m, w) for w in male]) - np.mean([gproj(g, m, w) for w in female]))\n",
      "    shuffleds = [shuffled for i in range(25) if not shuffle(shuffled)]\n",
      "    randomized = np.mean([abs(np.mean([gproj(g, m, w) for w in shuffled[:len(male)]]) \\\n",
      "                     - np.mean([gproj(g, m, w) for w in shuffled[len(male):]])) for shuffled in shuffleds])\n",
      "    return normal, randomized, normal-randomized\n",
      "190/126:\n",
      "print(labels[0])\n",
      "for i in mdict:\n",
      "    print(\"%10s\" % i, valid(gdict[i][0], mdict[i][0]))\n",
      "print(labels[1])\n",
      "for i in mdict:\n",
      "    print(\"%10s\" % i, valid(gdict[i][1], mdict[i][1]))\n",
      "print(labels[2])\n",
      "for i in mdict:\n",
      "    print(\"%10s\" % i, valid(gdict[i][2], mdict[i][2]))\n",
      "print(labels[3])\n",
      "for i in mdict:\n",
      "    print(\"%10s\" % i, valid(gdict[i][3], mdict[i][3]))\n",
      "190/127: shuffleds\n",
      "190/128: [shuffled for i in range(25) if not shuffle(shuffled)]\n",
      "190/129:\n",
      "def valid(g, m):\n",
      "    normal = abs(np.mean([gproj(g, m, w) for w in male]) - np.mean([gproj(g, m, w) for w in female]))\n",
      "    shuffleds = [shuffled for i in range(25) if shuffle(shuffled) == None]\n",
      "    randomized = np.mean([abs(np.mean([gproj(g, m, w) for w in shuffled[:len(male)]]) \\\n",
      "                     - np.mean([gproj(g, m, w) for w in shuffled[len(male):]])) for shuffled in shuffleds])\n",
      "    return normal, randomized, normal-randomized\n",
      "190/130:\n",
      "print(labels[0])\n",
      "for i in mdict:\n",
      "    print(\"%10s\" % i, valid(gdict[i][0], mdict[i][0]))\n",
      "print(labels[1])\n",
      "for i in mdict:\n",
      "    print(\"%10s\" % i, valid(gdict[i][1], mdict[i][1]))\n",
      "print(labels[2])\n",
      "for i in mdict:\n",
      "    print(\"%10s\" % i, valid(gdict[i][2], mdict[i][2]))\n",
      "print(labels[3])\n",
      "for i in mdict:\n",
      "    print(\"%10s\" % i, valid(gdict[i][3], mdict[i][3]))\n",
      "190/131:\n",
      "def valid(g, m):\n",
      "    normal = abs(np.mean([gproj(g, m, w) for w in male]) - np.mean([gproj(g, m, w) for w in female]))\n",
      "    shuffleds = [shuffled for i in range(50) if shuffle(shuffled) == None]\n",
      "    randomized = np.mean([abs(np.mean([gproj(g, m, w) for w in shuffled[:len(male)]]) \\\n",
      "                     - np.mean([gproj(g, m, w) for w in shuffled[len(male):]])) for shuffled in shuffleds])\n",
      "    return normal, randomized, normal-randomized\n",
      "190/132:\n",
      "print(labels[0])\n",
      "for i in mdict:\n",
      "    print(\"%10s\" % i, valid(gdict[i][0], mdict[i][0]))\n",
      "print(labels[1])\n",
      "for i in mdict:\n",
      "    print(\"%10s\" % i, valid(gdict[i][1], mdict[i][1]))\n",
      "print(labels[2])\n",
      "for i in mdict:\n",
      "    print(\"%10s\" % i, valid(gdict[i][2], mdict[i][2]))\n",
      "print(labels[3])\n",
      "for i in mdict:\n",
      "    print(\"%10s\" % i, valid(gdict[i][3], mdict[i][3]))\n",
      "190/133: [shuffled for i in range(25) if shuffle(shuffled) == None]\n",
      "190/134:\n",
      "def valid(g, m):\n",
      "    normal = abs(np.mean([gproj(g, m, w) for w in male]) - np.mean([gproj(g, m, w) for w in female]))\n",
      "    shuffleds = []\n",
      "    for i in range(50):\n",
      "        shuffled = male + female\n",
      "        shuffle(shuffled)\n",
      "        shuffleds.append(shuffled)\n",
      "    randomized = np.mean([abs(np.mean([gproj(g, m, w) for w in shuffled[:len(male)]]) \\\n",
      "                     - np.mean([gproj(g, m, w) for w in shuffled[len(male):]])) for shuffled in shuffleds])\n",
      "    return normal, randomized, normal-randomized\n",
      "190/135:\n",
      "print(labels[0])\n",
      "for i in mdict:\n",
      "    print(\"%10s\" % i, valid(gdict[i][0], mdict[i][0]))\n",
      "print(labels[1])\n",
      "for i in mdict:\n",
      "    print(\"%10s\" % i, valid(gdict[i][1], mdict[i][1]))\n",
      "print(labels[2])\n",
      "for i in mdict:\n",
      "    print(\"%10s\" % i, valid(gdict[i][2], mdict[i][2]))\n",
      "print(labels[3])\n",
      "for i in mdict:\n",
      "    print(\"%10s\" % i, valid(gdict[i][3], mdict[i][3]))\n",
      "190/136:\n",
      "def valid(g, m):\n",
      "    normal = abs(np.mean([gproj(g, m, w) for w in male]) - np.mean([gproj(g, m, w) for w in female]))\n",
      "    shuffleds = []\n",
      "    for i in range(100):\n",
      "        shuffled = male + female\n",
      "        shuffle(shuffled)\n",
      "        shuffleds.append(shuffled)\n",
      "    randomized = np.mean([abs(np.mean([gproj(g, m, w) for w in shuffled[:len(male)]]) \\\n",
      "                     - np.mean([gproj(g, m, w) for w in shuffled[len(male):]])) for shuffled in shuffleds])\n",
      "    return normal, randomized, normal-randomized\n",
      "190/137:\n",
      "print(labels[0])\n",
      "for i in mdict:\n",
      "    print(\"%10s\" % i, valid(gdict[i][0], mdict[i][0]))\n",
      "print(labels[1])\n",
      "for i in mdict:\n",
      "    print(\"%10s\" % i, valid(gdict[i][1], mdict[i][1]))\n",
      "print(labels[2])\n",
      "for i in mdict:\n",
      "    print(\"%10s\" % i, valid(gdict[i][2], mdict[i][2]))\n",
      "print(labels[3])\n",
      "for i in mdict:\n",
      "    print(\"%10s\" % i, valid(gdict[i][3], mdict[i][3]))\n",
      "190/138:\n",
      "def valid(g, m):\n",
      "    normal = abs(np.mean([gproj(g, m, w) for w in male]) - np.mean([gproj(g, m, w) for w in female]))\n",
      "    shuffleds = []\n",
      "    for i in range(100):\n",
      "        shuffled = male + female\n",
      "        shuffle(shuffled)\n",
      "        shuffleds.append(shuffled)\n",
      "    randomized = np.mean([abs(np.mean([gproj(g, m, w) for w in shuffled[:len(male)]]) \\\n",
      "                     - np.mean([gproj(g, m, w) for w in shuffled[len(male):]])) for shuffled in shuffleds])\n",
      "    return normal, randomized, normal-randomized\n",
      "190/139:\n",
      "print(labels[0])\n",
      "for i in mdict:\n",
      "    print(\"%10s\" % i, valid(gdict[i][0], mdict[i][0]))\n",
      "print(labels[1])\n",
      "for i in mdict:\n",
      "    print(\"%10s\" % i, valid(gdict[i][1], mdict[i][1]))\n",
      "print(labels[2])\n",
      "for i in mdict:\n",
      "    print(\"%10s\" % i, valid(gdict[i][2], mdict[i][2]))\n",
      "print(labels[3])\n",
      "for i in mdict:\n",
      "    print(\"%10s\" % i, valid(gdict[i][3], mdict[i][3]))\n",
      "190/140:\n",
      "male = ['male', 'man', 'boy', 'boys', 'brother', 'brothers', 'he', 'him', 'son', 'sons', 'father', 'husband', \\\n",
      "        'brothers', 'king', 'grandfather', 'uncle', 'himself', 'men', 'mr']\n",
      "190/141:\n",
      "female = [\"female\", \"woman\", \"girl\", \"girls\", \"sister\", 'sisters', \"she\", \"her\", \"daughter\", 'daughters', \\\n",
      "          \"mother\", 'wife', 'sisters', 'queen', 'grandmother', 'herself', 'women', 'ms', 'mrs']\n",
      "190/142:\n",
      "shuffled = male + female\n",
      "shuffle(shuffled)\n",
      "190/143:\n",
      "print(labels[0])\n",
      "for i in mdict:\n",
      "    print(\"%10s\" % i, valid(gdict[i][0], mdict[i][0]))\n",
      "print(labels[1])\n",
      "for i in mdict:\n",
      "    print(\"%10s\" % i, valid(gdict[i][1], mdict[i][1]))\n",
      "print(labels[2])\n",
      "for i in mdict:\n",
      "    print(\"%10s\" % i, valid(gdict[i][2], mdict[i][2]))\n",
      "print(labels[3])\n",
      "for i in mdict:\n",
      "    print(\"%10s\" % i, valid(gdict[i][3], mdict[i][3]))\n",
      "190/144:\n",
      "male = ['male', 'man', 'boy', 'boys', 'brother', 'brothers', 'he', 'him', 'son', 'sons', 'father', \\\n",
      "        'brothers', 'king', 'grandfather', 'uncle', 'himself', 'men', 'mr']\n",
      "190/145:\n",
      "female = [\"female\", \"woman\", \"girl\", \"girls\", \"sister\", 'sisters', \"she\", \"her\", \"daughter\", 'daughters', \\\n",
      "          \"mother\", 'sisters', 'queen', 'grandmother', 'herself', 'women', 'ms', 'mrs']\n",
      "190/146:\n",
      "shuffled = male + female\n",
      "shuffle(shuffled)\n",
      "190/147:\n",
      "print(labels[0])\n",
      "for i in mdict:\n",
      "    print(\"%10s\" % i, valid(gdict[i][0], mdict[i][0]))\n",
      "print(labels[1])\n",
      "for i in mdict:\n",
      "    print(\"%10s\" % i, valid(gdict[i][1], mdict[i][1]))\n",
      "print(labels[2])\n",
      "for i in mdict:\n",
      "    print(\"%10s\" % i, valid(gdict[i][2], mdict[i][2]))\n",
      "print(labels[3])\n",
      "for i in mdict:\n",
      "    print(\"%10s\" % i, valid(gdict[i][3], mdict[i][3]))\n",
      "190/148: bias(gsports, msports, vocab, gender_specific)\n",
      "190/149:\n",
      "import os\n",
      "import time\n",
      "import dill\n",
      "from typing import List\n",
      "from functools import reduce\n",
      "from calendar import monthrange\n",
      "188/204:\n",
      "print(labels[0])\n",
      "for i in mdict:\n",
      "    print(\"%10s\" % i, \\\n",
      "          \"%.3f\" % bias_c(mdict[i][0], male2, female2))\n",
      "print(labels[1])\n",
      "for i in mdict:\n",
      "    print(\"%10s\" % i, \\\n",
      "          \"%.3f\" % bias_c(mdict[i][1], male2, female2))\n",
      "print(labels[2])\n",
      "for i in mdict:\n",
      "    print(\"%10s\" % i, \\\n",
      "          \"%.3f\" % bias_c(mdict[i][2], male2, female2))\n",
      "print(labels[3])\n",
      "for i in mdict:\n",
      "    print(\"%10s\" % i, \\\n",
      "          \"%.3f\" % bias_c(mdict[i][3], male2, female2))\n",
      "190/150:\n",
      "for m in [mpol, mbooks, mmov, msports]:\n",
      "    c1 = m.wv.vocab['she'].count\n",
      "    c2 = m.wv.vocab['he'].count\n",
      "    print(c1, c2, c1/c2)\n",
      "190/151:\n",
      "def heshestats(w1, w2):\n",
      "    for m in [mpol, mbooks, mmov, msports]:\n",
      "        c1 = m.wv.vocab[w1].count\n",
      "        c2 = m.wv.vocab[w2].count\n",
      "        print(c1, c2, c1/c2)\n",
      "190/152: heshestats('he', 'she')\n",
      "190/153: heshestats('him', 'her')\n",
      "190/154: heshestats('man', 'woman')\n",
      "190/155:\n",
      "def heshestats(w1, w2):\n",
      "    for m in [mpol, mbooks, mmov, msports]:\n",
      "        c1 = m.wv.vocab[w1].count\n",
      "        c2 = m.wv.vocab[w2].count\n",
      "        print(c1, c2, c2/c1)\n",
      "190/156: heshestats('he', 'she')\n",
      "190/157: heshestats('him', 'her')\n",
      "190/158: heshestats('man', 'woman')\n",
      "190/159:\n",
      "for x in [summaries_pol, summaries_bks, summaries_mov, summaries_spts]:\n",
      "    print(np.mean([len(nltk.word_tokenize(a)) for a in x]))\n",
      "190/160:\n",
      "for x in [bow_pol, bow_bks, bow_mov, bow_spts]:\n",
      "    print(np.mean([len(nltk.word_tokenize(a)) for a in x]))\n",
      "190/161:\n",
      "for x in [ctrls_pol, ctrls_bks, ctrls_mov, ctrls_spts]:\n",
      "    print(np.mean([len(nltk.word_tokenize(a)) for a in x]))\n",
      "190/162: np.mean([1022, 1204, 1174, 975])\n",
      "190/163: np.mean([224, 234, 241, 230]), np.mean([224, 225, 228, 216]), np.mean([224, 219, 220, 216])\n",
      "188/205:\n",
      "for p in adjectives:\n",
      "    b = gproj(gcombined, mcombined, p)\n",
      "    if b > .15 or b < -.01:\n",
      "        print(p, b)\n",
      "188/206:\n",
      "for p in adjectives if p in mcombined.wv.vocab:\n",
      "    b = gproj(gcombined, mcombined, p)\n",
      "    if b > .15 or b < -.01:\n",
      "        print(p, b)\n",
      "188/207:\n",
      "for p in adjectives:\n",
      "    if p not in mcombined.wv.vocab:\n",
      "        continue\n",
      "    b = gproj(gcombined, mcombined, p)\n",
      "    if b > .15 or b < -.01:\n",
      "        print(p, b)\n",
      "188/208:\n",
      "adj = [(x, gproj(gcombined, mcombined, x)) for x in adjectives if x in mcombined.wv.vocab]\n",
      "sorted(adj, key=lambda x: x[1])[:15]\n",
      "188/209: sorted(adj, key=lambda x: x[1], reverse=True)[:15]\n",
      "188/210:\n",
      "adj = [(x, gproj(gcombined, mcombined, x)) for x in proflist if x in mcombined.wv.vocab]\n",
      "sorted(adj, key=lambda x: x[1])[:15]\n",
      "188/211:\n",
      "adj = [(x, gproj(gcombined, mcombined, x)) for x in proflist if x in mcombined.wv.vocab]\n",
      "sorted(adj, key=lambda x: x[1], reverse=True)[:15]\n",
      "188/212:\n",
      "stereotypes_m = ['captain', 'president', 'professor', 'dean', \\\n",
      "                 'commissioner', 'coach', 'director', 'officer', \\\n",
      "                 'manager', 'commander']\n",
      "stereotypes_f = ['teacher', 'author', 'secretary', 'writer', \\\n",
      "                 'artist', 'singer', 'teenager', 'editor', \\\n",
      "                 'photographer', 'maid', 'nurse', 'nanny', 'dancer', 'housekeeper', 'receptionist', 'librarian', 'hairdresser']\n",
      "188/213:\n",
      "print(labels[0])\n",
      "for i in mdict:\n",
      "    print(\"%10s\" % i, \\\n",
      "          \"%.3f\" % bias(mdict[i][0]))\n",
      "print(labels[1])\n",
      "for i in mdict:\n",
      "    print(\"%10s\" % i, \\\n",
      "          \"%.3f\" % bias(mdict[i][1]))\n",
      "print(labels[2])\n",
      "for i in mdict:\n",
      "    print(\"%10s\" % i, \\\n",
      "          \"%.3f\" % bias(mdict[i][2]))\n",
      "print(labels[3])\n",
      "for i in mdict:\n",
      "    print(\"%10s\" % i, \\\n",
      "          \"%.3f\" % bias(mdict[i][3]))\n",
      "188/214:\n",
      "stereotypes_m = ['captain', 'president', 'professor', 'dean', \\\n",
      "                 'commissioner', 'coach', 'director', 'officer', \\\n",
      "                 'manager', 'commander']\n",
      "stereotypes_f = ['teacher', 'author', 'secretary', 'writer', \\\n",
      "                 'artist', 'singer', 'teenager', 'editor', \\\n",
      "                 'photographer', 'maid', 'nurse', 'dancer', 'housekeeper', 'receptionist', 'librarian', 'hairdresser']\n",
      "188/215:\n",
      "print(labels[0])\n",
      "for i in mdict:\n",
      "    print(\"%10s\" % i, \\\n",
      "          \"%.3f\" % bias(mdict[i][0]))\n",
      "print(labels[1])\n",
      "for i in mdict:\n",
      "    print(\"%10s\" % i, \\\n",
      "          \"%.3f\" % bias(mdict[i][1]))\n",
      "print(labels[2])\n",
      "for i in mdict:\n",
      "    print(\"%10s\" % i, \\\n",
      "          \"%.3f\" % bias(mdict[i][2]))\n",
      "print(labels[3])\n",
      "for i in mdict:\n",
      "    print(\"%10s\" % i, \\\n",
      "          \"%.3f\" % bias(mdict[i][3]))\n",
      "188/216:\n",
      "stereotypes_m = ['captain', 'president', 'professor', 'dean', \\\n",
      "                 'commissioner', 'coach', 'director', 'officer', \\\n",
      "                 'manager', 'commander']\n",
      "stereotypes_f = ['teacher', 'author', 'secretary', 'writer', \\\n",
      "                 'artist', 'singer', 'teenager', 'editor', \\\n",
      "                 'photographer', 'maid', 'nurse', 'dancer', 'receptionist', 'librarian', 'hairdresser']\n",
      "188/217:\n",
      "print(labels[0])\n",
      "for i in mdict:\n",
      "    print(\"%10s\" % i, \\\n",
      "          \"%.3f\" % bias(mdict[i][0]))\n",
      "print(labels[1])\n",
      "for i in mdict:\n",
      "    print(\"%10s\" % i, \\\n",
      "          \"%.3f\" % bias(mdict[i][1]))\n",
      "print(labels[2])\n",
      "for i in mdict:\n",
      "    print(\"%10s\" % i, \\\n",
      "          \"%.3f\" % bias(mdict[i][2]))\n",
      "print(labels[3])\n",
      "for i in mdict:\n",
      "    print(\"%10s\" % i, \\\n",
      "          \"%.3f\" % bias(mdict[i][3]))\n",
      "188/218:\n",
      "stereotypes_m = ['captain', 'president', 'professor', 'dean', \\\n",
      "                 'commissioner', 'coach', 'director', 'officer', \\\n",
      "                 'manager', 'commander']\n",
      "stereotypes_f = ['teacher', 'author', 'secretary', 'writer', \\\n",
      "                 'artist', 'singer', 'teenager', 'editor', \\\n",
      "                 'photographer', 'maid', 'nurse', 'dancer', 'receptionist', 'hairdresser']\n",
      "188/219:\n",
      "print(labels[0])\n",
      "for i in mdict:\n",
      "    print(\"%10s\" % i, \\\n",
      "          \"%.3f\" % bias(mdict[i][0]))\n",
      "print(labels[1])\n",
      "for i in mdict:\n",
      "    print(\"%10s\" % i, \\\n",
      "          \"%.3f\" % bias(mdict[i][1]))\n",
      "print(labels[2])\n",
      "for i in mdict:\n",
      "    print(\"%10s\" % i, \\\n",
      "          \"%.3f\" % bias(mdict[i][2]))\n",
      "print(labels[3])\n",
      "for i in mdict:\n",
      "    print(\"%10s\" % i, \\\n",
      "          \"%.3f\" % bias(mdict[i][3]))\n",
      "188/220:\n",
      "stereotypes_m = ['captain', 'president', 'professor', 'dean', \\\n",
      "                 'commissioner', 'coach', 'director', 'officer', \\\n",
      "                 'manager', 'commander']\n",
      "stereotypes_f = ['teacher', 'author', 'secretary', 'writer', \\\n",
      "                 'artist', 'singer', 'teenager', 'editor', \\\n",
      "                 'photographer', 'maid', 'nurse', 'dancer', 'receptionist']\n",
      "188/221:\n",
      "print(labels[0])\n",
      "for i in mdict:\n",
      "    print(\"%10s\" % i, \\\n",
      "          \"%.3f\" % bias(mdict[i][0]))\n",
      "print(labels[1])\n",
      "for i in mdict:\n",
      "    print(\"%10s\" % i, \\\n",
      "          \"%.3f\" % bias(mdict[i][1]))\n",
      "print(labels[2])\n",
      "for i in mdict:\n",
      "    print(\"%10s\" % i, \\\n",
      "          \"%.3f\" % bias(mdict[i][2]))\n",
      "print(labels[3])\n",
      "for i in mdict:\n",
      "    print(\"%10s\" % i, \\\n",
      "          \"%.3f\" % bias(mdict[i][3]))\n",
      "188/222:\n",
      "stereotypes_m = ['captain', 'president', 'professor', 'dean', \\\n",
      "                 'commissioner', 'coach', 'director', 'officer', \\\n",
      "                 'manager', 'commander']\n",
      "stereotypes_f = ['teacher', 'author', 'secretary', 'writer', \\\n",
      "                 'artist', 'singer', 'teenager', 'editor', \\\n",
      "                 'photographer', 'nurse', 'dancer', 'receptionist']\n",
      "188/223:\n",
      "print(labels[0])\n",
      "for i in mdict:\n",
      "    print(\"%10s\" % i, \\\n",
      "          \"%.3f\" % bias(mdict[i][0]))\n",
      "print(labels[1])\n",
      "for i in mdict:\n",
      "    print(\"%10s\" % i, \\\n",
      "          \"%.3f\" % bias(mdict[i][1]))\n",
      "print(labels[2])\n",
      "for i in mdict:\n",
      "    print(\"%10s\" % i, \\\n",
      "          \"%.3f\" % bias(mdict[i][2]))\n",
      "print(labels[3])\n",
      "for i in mdict:\n",
      "    print(\"%10s\" % i, \\\n",
      "          \"%.3f\" % bias(mdict[i][3]))\n",
      "188/224:\n",
      "stereotypes_m = ['captain', 'president', 'professor', 'dean', \\\n",
      "                 'commissioner', 'coach', 'director', 'officer', \\\n",
      "                 'manager', 'commander']\n",
      "stereotypes_f = ['teacher', 'author', 'secretary', 'writer', \\\n",
      "                 'artist', 'singer', 'teenager', 'editor', \\\n",
      "                 'photographer', 'nurse', 'receptionist']\n",
      "188/225:\n",
      "print(labels[0])\n",
      "for i in mdict:\n",
      "    print(\"%10s\" % i, \\\n",
      "          \"%.3f\" % bias(mdict[i][0]))\n",
      "print(labels[1])\n",
      "for i in mdict:\n",
      "    print(\"%10s\" % i, \\\n",
      "          \"%.3f\" % bias(mdict[i][1]))\n",
      "print(labels[2])\n",
      "for i in mdict:\n",
      "    print(\"%10s\" % i, \\\n",
      "          \"%.3f\" % bias(mdict[i][2]))\n",
      "print(labels[3])\n",
      "for i in mdict:\n",
      "    print(\"%10s\" % i, \\\n",
      "          \"%.3f\" % bias(mdict[i][3]))\n",
      "188/226:\n",
      "stereotypes_m = ['captain', 'president', 'professor', 'dean', \\\n",
      "                 'commissioner', 'coach', 'director', 'officer', \\\n",
      "                 'manager', 'commander']\n",
      "stereotypes_f = ['teacher', 'author', 'secretary', 'writer', \\\n",
      "                 'artist', 'singer', 'teenager', 'editor', \\\n",
      "                 'photographer', 'nurse']\n",
      "188/227:\n",
      "print(labels[0])\n",
      "for i in mdict:\n",
      "    print(\"%10s\" % i, \\\n",
      "          \"%.3f\" % bias(mdict[i][0]))\n",
      "print(labels[1])\n",
      "for i in mdict:\n",
      "    print(\"%10s\" % i, \\\n",
      "          \"%.3f\" % bias(mdict[i][1]))\n",
      "print(labels[2])\n",
      "for i in mdict:\n",
      "    print(\"%10s\" % i, \\\n",
      "          \"%.3f\" % bias(mdict[i][2]))\n",
      "print(labels[3])\n",
      "for i in mdict:\n",
      "    print(\"%10s\" % i, \\\n",
      "          \"%.3f\" % bias(mdict[i][3]))\n",
      "188/228:\n",
      "adj = [(x, gproj(gcombined, mcombined, x)) for x in proflist if x in mcombined.wv.vocab]\n",
      "sorted(adj, key=lambda x: x[1])[:20]\n",
      "188/229:\n",
      "stereotypes_m = ['captain', 'president', 'professor', 'dean', \\\n",
      "                 'commissioner', 'coach', 'director', 'officer', \\\n",
      "                 'manager', 'commander']\n",
      "stereotypes_f = ['teacher', 'author', 'secretary', 'writer', \\\n",
      "                 'artist', 'singer', 'teenager', 'editor', \\\n",
      "                 'photographer', 'student']\n",
      "188/230:\n",
      "print(labels[0])\n",
      "for i in mdict:\n",
      "    print(\"%10s\" % i, \\\n",
      "          \"%.3f\" % bias(mdict[i][0]))\n",
      "print(labels[1])\n",
      "for i in mdict:\n",
      "    print(\"%10s\" % i, \\\n",
      "          \"%.3f\" % bias(mdict[i][1]))\n",
      "print(labels[2])\n",
      "for i in mdict:\n",
      "    print(\"%10s\" % i, \\\n",
      "          \"%.3f\" % bias(mdict[i][2]))\n",
      "print(labels[3])\n",
      "for i in mdict:\n",
      "    print(\"%10s\" % i, \\\n",
      "          \"%.3f\" % bias(mdict[i][3]))\n",
      "188/231:\n",
      "stereotypes_m = ['captain', 'president', 'professor', 'dean', \\\n",
      "                 'commissioner', 'coach', 'director', 'officer', \\\n",
      "                 'manager']\n",
      "stereotypes_f = ['teacher', 'author', 'secretary', 'writer', \\\n",
      "                 'artist', 'singer', 'teenager', 'editor', \\\n",
      "                 'photographer']\n",
      "188/232:\n",
      "print(labels[0])\n",
      "for i in mdict:\n",
      "    print(\"%10s\" % i, \\\n",
      "          \"%.3f\" % bias(mdict[i][0]))\n",
      "print(labels[1])\n",
      "for i in mdict:\n",
      "    print(\"%10s\" % i, \\\n",
      "          \"%.3f\" % bias(mdict[i][1]))\n",
      "print(labels[2])\n",
      "for i in mdict:\n",
      "    print(\"%10s\" % i, \\\n",
      "          \"%.3f\" % bias(mdict[i][2]))\n",
      "print(labels[3])\n",
      "for i in mdict:\n",
      "    print(\"%10s\" % i, \\\n",
      "          \"%.3f\" % bias(mdict[i][3]))\n",
      "188/233:\n",
      "adj = [(x, gproj(gcombined, mcombined, x)) for x in proflist if x in vocab]\n",
      "sorted(adj, key=lambda x: x[1])[:20]\n",
      "188/234:\n",
      "adj = [(x, gproj(gcombined, mcombined, x)) for x in proflist if x in vocab2]\n",
      "sorted(adj, key=lambda x: x[1])[:20]\n",
      "188/235:\n",
      "adj = [(x, gproj(gcombined, mcombined, x)) for x in proflist if x in vocab2]\n",
      "sorted(adj, key=lambda x: x[1], reverse=True)[:15]\n",
      "188/236:\n",
      "adj = [(x, gproj(gcombined, mcombined, x)) for x in adjectives if x in vocab2]\n",
      "sorted(adj, key=lambda x: x[1])[:15]\n",
      "188/237: sorted(adj, key=lambda x: x[1], reverse=True)[:15]\n",
      "188/238:\n",
      "adj_m = ['frank', 'direct', 'powerful', 'great', 'reliable', 'responsible', 'respected', 'impressive', 'efficient']\n",
      "adj_f = ['beautiful', 'passionate', 'spirited', 'pretty', 'normal', 'mature', 'patient', 'delicate', 'emotional']\n",
      "188/239:\n",
      "print(labels[0])\n",
      "for i in mdict:\n",
      "    print(\"%10s\" % i, \\\n",
      "          \"%.3f\" % bias_c(mdict[i][0], adj_m, adj_f))\n",
      "print(labels[1])\n",
      "for i in mdict:\n",
      "    print(\"%10s\" % i, \\\n",
      "          \"%.3f\" % bias_c(mdict[i][1], adj_m, adj_f))\n",
      "print(labels[2])\n",
      "for i in mdict:\n",
      "    print(\"%10s\" % i, \\\n",
      "          \"%.3f\" % bias_c(mdict[i][2], adj_m, adj_f))\n",
      "print(labels[3])\n",
      "for i in mdict:\n",
      "    print(\"%10s\" % i, \\\n",
      "          \"%.3f\" % bias_c(mdict[i][3], adj_m, adj_f))\n",
      "188/240:\n",
      "adj = [(x, gproj(gcombined, mcombined, x)) for x in proflist if x in vocab]\n",
      "sorted(adj, key=lambda x: x[1])[:20]\n",
      "188/241: stereo_f = ['maid', 'nurse', 'dancer', 'teenager', 'teacher', 'receptionist', 'student', 'chef', 'photographer']\n",
      "188/242:\n",
      "print(labels[0])\n",
      "for i in mdict:\n",
      "    print(\"%10s\" % i, \\\n",
      "          \"%.3f\" % bias_c(mdict[i][0], stereotypes_m, stereo_f))\n",
      "188/243: grammar\n",
      "188/244: shuffle(grammar)\n",
      "188/245: grammar\n",
      "188/246:\n",
      "gram1 = grammar[:9]\n",
      "gram2 = grammar[9:18]\n",
      "print(labels[0])\n",
      "for i in mdict:\n",
      "    print(\"%10s\" % i, \\\n",
      "          \"%.3f\" % bias_c(mdict[i][0], gram1, gram2))\n",
      "print(labels[1])\n",
      "for i in mdict:\n",
      "    print(\"%10s\" % i, \\\n",
      "          \"%.3f\" % bias_c(mdict[i][1], gram1, gram2))\n",
      "print(labels[2])\n",
      "for i in mdict:\n",
      "    print(\"%10s\" % i, \\\n",
      "          \"%.3f\" % bias_c(mdict[i][2], gram1, gram2))\n",
      "print(labels[3])\n",
      "for i in mdict:\n",
      "    print(\"%10s\" % i, \\\n",
      "          \"%.3f\" % bias_c(mdict[i][3], gram1, gram2))\n",
      "188/247:\n",
      "grammar2 = [x for x in grammar if x in vocab2]\n",
      "gram1 = grammar2[:9]\n",
      "gram2 = grammar2[9:18]\n",
      "print(labels[0])\n",
      "for i in mdict:\n",
      "    print(\"%10s\" % i, \\\n",
      "          \"%.3f\" % bias_c(mdict[i][0], gram1, gram2))\n",
      "print(labels[1])\n",
      "for i in mdict:\n",
      "    print(\"%10s\" % i, \\\n",
      "          \"%.3f\" % bias_c(mdict[i][1], gram1, gram2))\n",
      "print(labels[2])\n",
      "for i in mdict:\n",
      "    print(\"%10s\" % i, \\\n",
      "          \"%.3f\" % bias_c(mdict[i][2], gram1, gram2))\n",
      "print(labels[3])\n",
      "for i in mdict:\n",
      "    print(\"%10s\" % i, \\\n",
      "          \"%.3f\" % bias_c(mdict[i][3], gram1, gram2))\n",
      "188/248:\n",
      "adj = [(x, gproj(gcombined, mcombined, x)) for x in proflist]\n",
      "sorted(adj, key=lambda x: x[1])[:20]\n",
      "188/249:\n",
      "adj = [(x, gproj(gcombined, mcombined, x)) for x in proflist if x in vocab]\n",
      "sorted(adj, key=lambda x: x[1])[:20]\n",
      "188/250:\n",
      "stereotypes_m = ['captain', 'president', 'principal', 'dean', \\\n",
      "                 'commissioner', 'coach', 'director', 'officer', \\\n",
      "                 'manager']\n",
      "stereotypes_f = ['teacher', 'author', 'secretary', 'writer', \\\n",
      "                 'artist', 'singer', 'teenager', 'editor', \\\n",
      "                 'photographer']\n",
      "188/251:\n",
      "print(labels[0])\n",
      "for i in mdict:\n",
      "    print(\"%10s\" % i, \\\n",
      "          \"%.3f\" % bias(mdict[i][0]))\n",
      "print(labels[1])\n",
      "for i in mdict:\n",
      "    print(\"%10s\" % i, \\\n",
      "          \"%.3f\" % bias(mdict[i][1]))\n",
      "print(labels[2])\n",
      "for i in mdict:\n",
      "    print(\"%10s\" % i, \\\n",
      "          \"%.3f\" % bias(mdict[i][2]))\n",
      "print(labels[3])\n",
      "for i in mdict:\n",
      "    print(\"%10s\" % i, \\\n",
      "          \"%.3f\" % bias(mdict[i][3]))\n",
      "190/164: gbooks = getg(mbooks, \"Gender Pairs - Books\", True)\n",
      "190/165:\n",
      "def getg(m, t):\n",
      "    matrix = []\n",
      "    for a, b in pairs:\n",
      "        if a not in m.wv.vocab or b not in m.wv.vocab:\n",
      "            continue\n",
      "        center = (m.wv.get_vector(a) + m.wv.get_vector(b))/2\n",
      "        matrix.append(m.wv.get_vector(a) - center)\n",
      "        matrix.append(m.wv.get_vector(b) - center)\n",
      "    matrix = np.array(matrix)\n",
      "    p = PCA(n_components = 10)\n",
      "    p.fit(matrix)\n",
      "    print(p.explained_variance_ratio_[0])\n",
      "    if t:\n",
      "        plt.bar(range(10), p.explained_variance_ratio_)\n",
      "        plt.title(\"Explained Variance vs Component Number (\" + t + \")\")\n",
      "    return p.components_[0]\n",
      "190/166: gbooks = getg(mbooks, \"Gender Pairs - Books\", True)\n",
      "190/167:\n",
      "def getg(m, t, tr):\n",
      "    matrix = []\n",
      "    for a, b in pairs:\n",
      "        if a not in m.wv.vocab or b not in m.wv.vocab:\n",
      "            continue\n",
      "        center = (m.wv.get_vector(a) + m.wv.get_vector(b))/2\n",
      "        matrix.append(m.wv.get_vector(a) - center)\n",
      "        matrix.append(m.wv.get_vector(b) - center)\n",
      "    matrix = np.array(matrix)\n",
      "    p = PCA(n_components = 10)\n",
      "    p.fit(matrix)\n",
      "    print(p.explained_variance_ratio_[0])\n",
      "    if tr:\n",
      "        plt.bar(range(10), p.explained_variance_ratio_)\n",
      "        plt.title(\"Explained Variance vs Component Number (\" + t + \")\")\n",
      "    return p.components_[0]\n",
      "190/168: gbooks = getg(mbooks, \"Gender Pairs - Books\", True)\n",
      "190/169: gpol = getg(mpol, \"Gender Pairs - Politics\", True)\n",
      "190/170: gmov = getg(mmov, \"Gender Pairs - Movies\", True)\n",
      "190/171: politics[0]\n",
      "190/172: politics[0].title\n",
      "190/173: politics[0].id\n",
      "190/174: politics[0].text\n",
      "190/175: politics[0].id\n",
      "190/176: all_[0]\n",
      "190/177: gbooks = getg(mbooks, \"Gender Pairs - Books\", True)\n",
      "190/178: gmov = getg(mmov, \"Gender Pairs - Movies\", True)\n",
      "190/179: gsports = getg(msports, \"Gender Pairs - Sports\", True)\n",
      "190/180:\n",
      "def getb(m, t, tr):\n",
      "    matrix = []\n",
      "    for a, b in pairs:\n",
      "        if a not in m.wv.vocab or b not in m.wv.vocab:\n",
      "            continue\n",
      "        center = (m.wv.get_vector(a) + m.wv.get_vector(b))/2\n",
      "        matrix.append(m.wv.get_vector(a) - center)\n",
      "        matrix.append(m.wv.get_vector(b) - center)\n",
      "    matrix = np.array(matrix)\n",
      "    p = PCA(n_components = 10)\n",
      "    p.fit(matrix)\n",
      "    print(p.explained_variance_ratio_[0])\n",
      "    if tr:\n",
      "        plt.bar(range(10), p.explained_variance_ratio_)\n",
      "        plt.title(\"Explained Variance vs Component Number (\" + t + \")\")\n",
      "    return p.components_[0]\n",
      "192/1:\n",
      "import numpy as np\n",
      "\n",
      "# (ACT1-3) using input files compute the following\n",
      "train_file = open(\"train.txt\", \"r\")\n",
      "test_file = open(\"test.txt\", \"r\")\n",
      "docs_test = []\n",
      "docs_train = []\n",
      "labels = []\n",
      "even_line = True\n",
      "\n",
      "#input test lines into docs_test\n",
      "test_file.readline()\n",
      "word = ''\n",
      "for line in test_file:\n",
      "    words = []\n",
      "    for chars in line:\n",
      "        if(chars != ' ' and chars != '\\n'):\n",
      "            word += chars\n",
      "        elif (chars != '\\n'):\n",
      "            words.append(word)\n",
      "            word = ''\n",
      "    docs_test.append(words)\n",
      "\n",
      "#input training lines into docs_train and labels\n",
      "train_file.readline()\n",
      "for line in train_file:\n",
      "    if even_line:\n",
      "        words = []\n",
      "        \n",
      "        for chars in line:\n",
      "            if(chars != ' ' and chars != '\\n'):\n",
      "                word += chars\n",
      "            elif (chars != '\\n'):\n",
      "                words.append(word)\n",
      "                word = ''\n",
      "        docs_train.append(words)\n",
      "        even_line = False\n",
      "    else:\n",
      "        labels.append(int(line))\n",
      "        even_line = True\n",
      "\n",
      "n_train = len(docs_train)\n",
      "n_test = len(docs_test)\n",
      "print(docs_train[40])\n",
      "print(docs_test[40])\n",
      "\n",
      "\n",
      "\n",
      "#CHECKS\n",
      "assert((n_train+n_test)==1500)\n",
      "assert(np.sum(np.array(labels))== -348)\n",
      "assert(len(docs_train[1])+len(docs_test[1])==12)\n",
      "192/2:\n",
      "# (ACT5) design a function that takes \n",
      "# (1) docs: list of documents (i.e. list of list of words)\n",
      "# (2) w2i: a dictionary that maps words to index\n",
      "# output numpy matrix X as described above with shape of (n,d+1) -- where d is size of vocabulary\n",
      "# (DO NOT forget last column of X which is all 1)\n",
      "\n",
      "def make_matrix(docs, w2i):\n",
      "    #print(np.shape(w2i))\n",
      "    A = np.zeros((len(docs), len(w2i) + 1))\n",
      "    for idx, row in enumerate(docs):\n",
      "        for word in row:\n",
      "            A[idx:(idx + 1), w2i[word]: (w2i[word] + 1)] += 1;\n",
      "        A[idx:(idx + 1), len(w2i)] = 1   \n",
      "    return A\n",
      "\n",
      "X_train = make_matrix(docs_train,w2i)\n",
      "X_test = make_matrix(docs_test,w2i)\n",
      "y_train = np.array(labels)\n",
      "\n",
      "# (ACT6-8)\n",
      "print (\"First 5 rows of X_train: \")\n",
      "print(X_train[:5, :])\n",
      "print (\"Last 5 rows of X_test: \")\n",
      "print(X_test[len(docs_test) - 5:, :])\n",
      "print (\"First 10 labels of training set:\")\n",
      "print (labels[:10])\n",
      "\n",
      "#CHECKS\n",
      "assert(np.sum(X_train)==6871)\n",
      "assert(np.sum(X_test)==3462)\n",
      "assert(np.sum(X_test[10,:]+X_train[10,:])==11)\n",
      "192/3:\n",
      "# (ACT9) Design a function that takes y (list of true labels) and y_hat (list of predicted labels)\n",
      "# and computes the error\n",
      "def err(y,y_hat):\n",
      "    error = 0.0\n",
      "    for idx, label in enumerate(y):\n",
      "        if (label != y_hat[idx]): \n",
      "            error+= 1\n",
      "    error = error / len(y)\n",
      "    return error\n",
      "\n",
      "#CHECKS\n",
      "assert(err([-1,+1,-1,+1],[+1,+1,+1,-1])==0.75)\n",
      "assert(err([+1,+1,+1,+1],[+1,+1,+1,-1])==0.25)\n",
      "assert(err([-1,-1,-1,+1],[+1,+1,+1,-1])==1)\n",
      "194/1:\n",
      "import numpy as np\n",
      "\n",
      "# (ACT1-3) using input files compute the following\n",
      "train_file = open(\"train.txt\", \"r\")\n",
      "test_file = open(\"test.txt\", \"r\")\n",
      "docs_test = []\n",
      "docs_train = []\n",
      "labels = []\n",
      "even_line = True\n",
      "\n",
      "#input test lines into docs_test\n",
      "test_file.readline()\n",
      "word = ''\n",
      "for line in test_file:\n",
      "    words = []\n",
      "    for chars in line:\n",
      "        if(chars != ' ' and chars != '\\n'):\n",
      "            word += chars\n",
      "        elif (chars != '\\n'):\n",
      "            words.append(word)\n",
      "            word = ''\n",
      "    docs_test.append(words)\n",
      "\n",
      "#input training lines into docs_train and labels\n",
      "train_file.readline()\n",
      "for line in train_file:\n",
      "    if even_line:\n",
      "        words = []\n",
      "        \n",
      "        for chars in line:\n",
      "            if(chars != ' ' and chars != '\\n'):\n",
      "                word += chars\n",
      "            elif (chars != '\\n'):\n",
      "                words.append(word)\n",
      "                word = ''\n",
      "        docs_train.append(words)\n",
      "        even_line = False\n",
      "    else:\n",
      "        labels.append(int(line))\n",
      "        even_line = True\n",
      "\n",
      "n_train = len(docs_train)\n",
      "n_test = len(docs_test)\n",
      "print(docs_train[40])\n",
      "print(docs_test[40])\n",
      "\n",
      "\n",
      "\n",
      "#CHECKS\n",
      "assert((n_train+n_test)==1500)\n",
      "assert(np.sum(np.array(labels))== -348)\n",
      "assert(len(docs_train[1])+len(docs_test[1])==12)\n",
      "194/2:\n",
      "# (ACT4) design a function that takes list of documents (list of list of words) \n",
      "# as input and returns sorted list of distinct words \n",
      "# use built-in sort in python for sorting strings\n",
      "def make_vocabulary(docs):\n",
      "    vocab = []\n",
      "    for doc in docs:\n",
      "        for word in doc:\n",
      "            if (vocab.count(word) == 0): \n",
      "                vocab.append(word) \n",
      "    vocab.sort()\n",
      "    return vocab\n",
      "    \n",
      "    \n",
      "vocab = make_vocabulary(docs_train)\n",
      "d = len(vocab) \n",
      "w2i = {vocab[i]:i for i in range(len(vocab))} # maps words in dictionary to corresponding index\n",
      "print(vocab)\n",
      "print(w2i)\n",
      "\n",
      "#CHECKS\n",
      "assert(vocab[2]==\"LnGi\")\n",
      "assert(vocab == sorted(vocab))\n",
      "194/3:\n",
      "# (ACT5) design a function that takes \n",
      "# (1) docs: list of documents (i.e. list of list of words)\n",
      "# (2) w2i: a dictionary that maps words to index\n",
      "# output numpy matrix X as described above with shape of (n,d+1) -- where d is size of vocabulary\n",
      "# (DO NOT forget last column of X which is all 1)\n",
      "\n",
      "def make_matrix(docs, w2i):\n",
      "    #print(np.shape(w2i))\n",
      "    A = np.zeros((len(docs), len(w2i) + 1))\n",
      "    for idx, row in enumerate(docs):\n",
      "        for word in row:\n",
      "            A[idx:(idx + 1), w2i[word]: (w2i[word] + 1)] += 1;\n",
      "        A[idx:(idx + 1), len(w2i)] = 1   \n",
      "    return A\n",
      "\n",
      "X_train = make_matrix(docs_train,w2i)\n",
      "X_test = make_matrix(docs_test,w2i)\n",
      "y_train = np.array(labels)\n",
      "\n",
      "# (ACT6-8)\n",
      "print (\"First 5 rows of X_train: \")\n",
      "print(X_train[:5, :])\n",
      "print (\"Last 5 rows of X_test: \")\n",
      "print(X_test[len(docs_test) - 5:, :])\n",
      "print (\"First 10 labels of training set:\")\n",
      "print (labels[:10])\n",
      "\n",
      "#CHECKS\n",
      "assert(np.sum(X_train)==6871)\n",
      "assert(np.sum(X_test)==3462)\n",
      "assert(np.sum(X_test[10,:]+X_train[10,:])==11)\n",
      "194/4:\n",
      "# (ACT9) Design a function that takes y (list of true labels) and y_hat (list of predicted labels)\n",
      "# and computes the error\n",
      "def err(y,y_hat):\n",
      "    error = 0.0\n",
      "    for idx, label in enumerate(y):\n",
      "        if (label != y_hat[idx]): \n",
      "            error+= 1\n",
      "    error = error / len(y)\n",
      "    return error\n",
      "\n",
      "#CHECKS\n",
      "assert(err([-1,+1,-1,+1],[+1,+1,+1,-1])==0.75)\n",
      "assert(err([+1,+1,+1,+1],[+1,+1,+1,-1])==0.25)\n",
      "assert(err([-1,-1,-1,+1],[+1,+1,+1,-1])==1)\n",
      "194/5:\n",
      "# (ACT10) Design a function that takes as input\n",
      "# (1) document matrix X --- numpy array shape =  (n,d+1)\n",
      "# (2) vector w --- numpy array shape = (d+1,1) or (d+1,)\n",
      "# and output \n",
      "# (1) predictions y_hat --- numpy array shape = (n,1)\n",
      "def predict(X,w):\n",
      "    product = np.dot(X, w)\n",
      "    product[product < 0] = -1\n",
      "    product[product >= 0] = 1\n",
      "    return product\n",
      "\n",
      "#CHECKS\n",
      "assert(np.sum(predict(X_train,np.ones(d+1).reshape(-1,1)).reshape(-1,1))==n_train)\n",
      "194/6:\n",
      "# Loading candidates list candids = [w0,w1,...]\n",
      "import pickle\n",
      "with open('candids.pkl', 'rb') as f:\n",
      "    candids = pickle.load(f)\n",
      "print(\"size of candidates lists %d\"%len(candids))\n",
      "print(candids[0])\n",
      "194/7:\n",
      "# (ACT11) fill err_list with training error of each candidate w\n",
      "err_list = []\n",
      "for candid in candids:\n",
      "    err_list.append(err(labels, predict(X_train, candid)))\n",
      "\n",
      "# (ACT12) index of w with smallest error over training set \n",
      "best_index = np.argmin(err_list)\n",
      "\n",
      "print(\"Training Error of candidates:\")\n",
      "for i,err in enumerate(err_list):\n",
      "    print(f\"Candidate # %d has error %f\" % (i,err))\n",
      "\n",
      "print(\"Index of best predictor: %d\"%best_index)\n",
      "print(\"Best Predictor:\")\n",
      "print(candids[best_index])\n",
      "\n",
      "#CHECKS\n",
      "assert(np.sum(err_list)<=2.5)\n",
      "194/8:\n",
      "# Best predictor\n",
      "w_best = candids[best_index]\n",
      "\n",
      "# (ACT13) Use w_best to predict labels for X_test \n",
      "y_test = np.dot(X_test, w_best)\n",
      "y_test[y_test >= 0] = 1\n",
      "y_test[y_test < 0] = -1\n",
      "\n",
      "# (ACT14) print first 10 labels predicted for test set\n",
      "print(y_test[0:10])\n",
      "\n",
      "#CHECKS\n",
      "def my_hash(y):\n",
      "    p1 = 28433\n",
      "    p2 = 577\n",
      "    ret = 0\n",
      "    for e in range(len(y)):\n",
      "        ret = ((ret*p2+int(e)) % p1 + p1) % p1\n",
      "    return ret\n",
      "assert(my_hash(y_test) == 19262)\n",
      "194/9:\n",
      "# (ACT15) using X_train and X_test\n",
      "# plot frequency (number of times it appeared) of each word using index of words as x-axis \n",
      "# using two subplots (bar plots) one for X_train and one for X_test\n",
      "# you might find plt.bar useful\n",
      "import matplotlib.pyplot as plt\n",
      "\n",
      "x = np.arange(len(X_train[0]) - 1)            \n",
      "y_train = np.sum(X_train, axis = 0)[:10]\n",
      "y_test = np.sum(X_test, axis = 0)[:10]\n",
      "\n",
      "plt.subplot(2, 1, 1)\n",
      "plt.bar(x, y_train)\n",
      "plt.title(\"frequency of words in training set\")\n",
      "\n",
      "plt.subplot(2, 1, 2)\n",
      "plt.bar(x, y_test)\n",
      "plt.title(\"frequency of words in test set\")\n",
      "\n",
      "plt.show()\n",
      "194/10:\n",
      "# (ACT15) using X_train and X_test\n",
      "# plot frequency (number of times it appeared) of each word using index of words as x-axis \n",
      "# using two subplots (bar plots) one for X_train and one for X_test\n",
      "# you might find plt.bar useful\n",
      "import matplotlib.pyplot as plt\n",
      "\n",
      "x = np.arange(len(X_train[0]) - 1)            \n",
      "y_train = np.sum(X_train, axis = 0)[:10]\n",
      "y_test = np.sum(X_test, axis = 0)[:10]\n",
      "\n",
      "plt.subplot(2, 1, 1)\n",
      "plt.bar(x, y_train)\n",
      "plt.title(\"frequency of words in training set\")\n",
      "\n",
      "plt.subplot(2, 1, 2)\n",
      "plt.bar(x, y_test)\n",
      "plt.title(\"frequency of words in test set\")\n",
      "\n",
      "plt.show()\n",
      "194/11:\n",
      "import numpy as np\n",
      "\n",
      "# Open the two files \"train\" and \"test\"\n",
      "train = open(\"train.txt\", \"r\")\n",
      "test = open(\"test.txt\", \"r\")\n",
      "\n",
      "training = train.readlines()\n",
      "testing = test.readlines()\n",
      "\n",
      "# Fill in the training and label lists\n",
      "docs_train = []\n",
      "label = []\n",
      "\n",
      "i = 1 # Start at i = 1 to avoid the first line, which is just the number of docs\n",
      "while i < len(training):\n",
      "    if (i % 2 == 1):\n",
      "        docs_train.append(training[i]) # In the training text, odd lines are doc names\n",
      "    if (i % 2 == 0):\n",
      "        label.append(training[i]) # In the training text, even lines are labels\n",
      "    i = i + 1\n",
      "\n",
      "\n",
      "# Convert the labels to a numerical matrix\n",
      "labels = np.zeros(len(label))\n",
      "i = 0\n",
      "while i < len(label):\n",
      "    if (label[i] == '1\\n'):\n",
      "        labels[i] = 1\n",
      "    if (label[i] == '-1\\n'):\n",
      "        labels[i] = -1\n",
      "    i = i + 1\n",
      "    \n",
      "# Convert the docs_train list to a \"list of a list\"\n",
      "i = 0\n",
      "while i < len(docs_train):\n",
      "    words = []\n",
      "    words.append(docs_train[i])\n",
      "    docs_train[i] = docs_train[i].split()\n",
      "    i = i + 1\n",
      "\n",
      "# Fill in the test list\n",
      "docs_test = []\n",
      "\n",
      "i = 1 # Start at i = 1 to avoid the first line, which is just the number of docs\n",
      "while i < len(testing):\n",
      "    docs_test.append(testing[i])\n",
      "    i = i + 1\n",
      "\n",
      "# Convert the docs_train list to a \"list of a list\"\n",
      "i = 0\n",
      "while i < len(docs_test):\n",
      "    words = []\n",
      "    words.append(docs_test[i])\n",
      "    docs_test[i] = docs_test[i].split()\n",
      "    i = i + 1\n",
      "\n",
      "n_train = len(docs_train)\n",
      "n_test = len(docs_test)\n",
      "print(docs_train[40])\n",
      "print(docs_test[40])\n",
      "\n",
      "\n",
      "#CHECKS\n",
      "assert((n_train+n_test)==1500)\n",
      "assert(np.sum(np.array(labels))== -348)\n",
      "assert(len(docs_train[1])+len(docs_test[1])==12)\n",
      "194/12:\n",
      "# (ACT4) design a function that takes list of documents (list of list of words) \n",
      "# as input and returns sorted list of distinct words \n",
      "# use built-in sort in python for sorting strings\n",
      "def make_vocabulary(docs):\n",
      "    vocabulary = []\n",
      "    i = 0\n",
      "    while i < len(docs):\n",
      "        temp = docs[i]\n",
      "        j = 0\n",
      "        while j < len(docs[i]):\n",
      "            vocabulary.append(temp[j])\n",
      "            j = j + 1\n",
      "        i = i + 1\n",
      "        vocabulary = list(dict.fromkeys(vocabulary))\n",
      "        vocabulary.sort()\n",
      "    return vocabulary\n",
      "    \n",
      "    \n",
      "vocab = make_vocabulary(docs_train)\n",
      "d = len(vocab) \n",
      "w2i = {vocab[i]:i for i in range(len(vocab))} # maps words in dictionary to corresponding index\n",
      "print(vocab)\n",
      "print(w2i)\n",
      "\n",
      "#CHECKS\n",
      "assert(vocab[2]==\"LnGi\")\n",
      "assert(vocab == sorted(vocab))\n",
      "194/13:\n",
      "# (ACT5) design a function that takes \n",
      "# (1) docs: list of documents (i.e. list of list of words)\n",
      "# (2) w2i: a dictionary that maps words to index\n",
      "# output numpy matrix X as described above with shape of (n,d+1) -- where d is size of vocabulary\n",
      "# (DO NOT forget last column of X which is all 1)\n",
      "\n",
      "def make_matrix(docs, w2i):\n",
      "    d = len(w2i) # Size of the vocabulary\n",
      "    n = len(docs) # Number of documents\n",
      "    X = np.zeros((n, d+1)) # Initialize matrix with appropriate dimensions\n",
      "    i = 0\n",
      "    while i < n: # Consider each row within the X matrix\n",
      "        document = docs[i] # Consider one document at a time\n",
      "        j = 0\n",
      "        while j < d+1: # Consider each column of the specific row\n",
      "            if (j == d): # Satisfies constraint for last column = 1 for all rows\n",
      "                X[i,j] = 1\n",
      "            if (j < d): \n",
      "                vocab = list(w2i.keys()) # Pulls the keys (words) from the dictionary \n",
      "                word = vocab[j] # Consider one vocab word at a time\n",
      "                k = 0\n",
      "            while k < len(document): # Consider all words within a document\n",
      "                check = document[k] # Consdier one word at a time\n",
      "                if (check == word):\n",
      "                    X[i,j] = X[i,j] + 1 # Increment if the words match\n",
      "                k = k + 1\n",
      "            j = j + 1\n",
      "        i = i + 1\n",
      "    return X\n",
      "\n",
      "X_train = make_matrix(docs_train,w2i)\n",
      "X_test = make_matrix(docs_test,w2i)\n",
      "y_train = np.array(labels)\n",
      "\n",
      "# (ACT6-8)\n",
      "print (\"First 5 rows of X_train: \")\n",
      "print(X_train[0:4,]) # We start with row 0 in Python\n",
      "print (\"Last 5 rows of X_test: \")\n",
      "print(X_test[len(docs_test)-1,]) # We start with our last row (length - 1) and move backwards\n",
      "print(X_test[len(docs_test)-2,])\n",
      "print(X_test[len(docs_test)-3,])\n",
      "print(X_test[len(docs_test)-4,])\n",
      "print(X_test[len(docs_test)-5,])\n",
      "print (\"First 10 labels of training set:\")\n",
      "print(y_train[0:9,])\n",
      "\n",
      "\n",
      "#CHECKS\n",
      "assert(np.sum(X_train)==6871)\n",
      "assert(np.sum(X_test)==3462)\n",
      "assert(np.sum(X_test[10,:]+X_train[10,:])==11)\n",
      "194/14:\n",
      "# (ACT9) Design a function that takes y (list of true labels) and y_hat (list of predicted labels)\n",
      "# and computes the error\n",
      "def err(y,y_hat):\n",
      "    n = len(y) # n for the above formula\n",
      "    total = 0 # Initialize the sum of the error\n",
      "    i = 0\n",
      "    while i < n: # Consider all y's in the given set\n",
      "        test_y = y[i]\n",
      "        test_yhat = y_hat[i]\n",
      "        if (test_y == test_yhat): # If they are equal, l = 0\n",
      "            total = total\n",
      "        else:\n",
      "            total = total + 1 # If they are not equal, l = 1\n",
      "        i = i + 1\n",
      "    error = total / n # Compute the error\n",
      "    return error\n",
      "        \n",
      "\n",
      "#CHECKS\n",
      "assert(err([-1,+1,-1,+1],[+1,+1,+1,-1])==0.75)\n",
      "assert(err([+1,+1,+1,+1],[+1,+1,+1,-1])==0.25)\n",
      "assert(err([-1,-1,-1,+1],[+1,+1,+1,-1])==1)\n",
      "194/15:\n",
      "# (ACT10) Design a function that takes as input\n",
      "# (1) document matrix X --- numpy array shape =  (n,d+1)\n",
      "# (2) vector w --- numpy array shape = (d+1,1) or (d+1,)\n",
      "# and output \n",
      "# (1) predictions y_hat --- numpy array shape = (n,1)\n",
      "def predict(X,w):\n",
      "    n = X.shape[0] # Gather size n \n",
      "    y_hat = np.zeros((n,1)) # Initialize y_hat with appropriate dimensions\n",
      "    i = 0\n",
      "    while i < n:\n",
      "        j = 0\n",
      "        temp = 0 # Initialize a temporary value for a given y_hat element\n",
      "        while j < d + 1: # Consider all weights individually \n",
      "            temp = temp + X[i,j]*w[j] # Matrix multiplication\n",
      "            j = j + 1\n",
      "        if (sum(temp) >= 0): # Sign check \n",
      "            y_hat[i] = 1\n",
      "        if (sum(temp) < 0):\n",
      "            y_hat[i] = -1\n",
      "        i = i + 1\n",
      "    return y_hat\n",
      "\n",
      "#CHECKS\n",
      "assert(np.sum(predict(X_train,np.ones(d+1).reshape(-1,1)).reshape(-1,1))==n_train)\n",
      "194/16:\n",
      "# Loading candidates list candids = [w0,w1,...]\n",
      "import pickle\n",
      "with open('candids.pkl', 'rb') as f:\n",
      "    candids = pickle.load(f)\n",
      "print(\"size of candidates lists %d\"%len(candids))\n",
      "print(candids[0])\n",
      "194/17:\n",
      "# (ACT11) fill err_list with training error of each candidate w\n",
      "err_list = np.zeros(len(candids))\n",
      "i = 0\n",
      "while i < len(err_list):\n",
      "    y_hat = predict(X_train,candids[i])\n",
      "    error = err(y_train,y_hat)\n",
      "    err_list[i] = error\n",
      "    i = i + 1\n",
      "    \n",
      "#(ACT12) index of w with smallest error over training set \n",
      "best_index = np.argmin(err_list)\n",
      "print(\"Training Error of candidates:\")\n",
      "for i,err in enumerate(err_list):\n",
      "    print(f\"Candidate # %d has error %f\" % (i,err))\n",
      "\n",
      "print(\"Index of best predictor: %d\"%best_index)\n",
      "print(\"Best Predictor:\")\n",
      "print(candids[best_index])\n",
      "\n",
      "#CHECKS\n",
      "assert(np.sum(err_list)<=2.5)\n",
      "194/18:\n",
      "# Best predictor\n",
      "w_best = candids[best_index]\n",
      "\n",
      "# (ACT13) Use w_best to predict labels for X_test \n",
      "y_test = predict(X_test,w_best)\n",
      "\n",
      "# (ACT14) print first 10 labels predicted for test set\n",
      "print(y_test[0:9])\n",
      "\n",
      "#CHECKS\n",
      "def my_hash(y):\n",
      "    p1 = 28433\n",
      "    p2 = 577\n",
      "    ret = 0\n",
      "    for e in range(len(y)):\n",
      "        ret = ((ret*p2+int(e)) % p1 + p1) % p1\n",
      "    return ret\n",
      "assert(my_hash(y_test) == 19262)\n",
      "194/19:\n",
      "# (ACT15) using X_train and X_test\n",
      "# plot frequency (number of times it appeared) of each word using index of words as x-axis \n",
      "# using two subplots (bar plots) one for X_train and one for X_test\n",
      "# you might find plt.bar useful\n",
      "import matplotlib.pyplot as plt\n",
      "\n",
      "words = list(w2i.keys()) \n",
      "train_frequency = np.zeros(len(words))\n",
      "test_frequency = np.zeros(len(words))\n",
      "\n",
      "# Use a loop to fill the frequency vectors using the X matrices calculated above \n",
      "i = 0\n",
      "while i < len(words):\n",
      "    j = 0\n",
      "    total_train = 0\n",
      "    total_test = 0\n",
      "    while j < X_train.shape[0]:\n",
      "        total_train = total_train + X_train[j,i]\n",
      "        j = j + 1\n",
      "    train_frequency[i] = total_train\n",
      "    j = 0\n",
      "    while j < X_test.shape[0]:\n",
      "        total_test = total_test + X_test[j,i]\n",
      "        j = j + 1\n",
      "    test_frequency[i] = total_test        \n",
      "    i = i + 1\n",
      "\n",
      "# Plot the values in a bar plot (code adopted from MatPlotLib Example)\n",
      "axis = np.arange(len(words))  \n",
      "width = 0.4\n",
      "fig, graph = plt.subplots()\n",
      "train = graph.bar(axis - width/2,train_frequency,width,label ='Training Set')\n",
      "test = graph.bar(axis + width/2,test_frequency,width,label ='Test Set')\n",
      "graph.set_ylabel('Frequency')\n",
      "graph.set_title('Frequency of Vocabulary in Training and Test Sets')\n",
      "graph.set_xticks(axis)\n",
      "graph.set_xticklabels(words)\n",
      "graph.legend()\n",
      "fig.tight_layout()\n",
      "plt.show()\n",
      "194/20:\n",
      "import numpy as np\n",
      "\n",
      "# Open the two files \"train\" and \"test\"\n",
      "train = open(\"train.txt\", \"r\")\n",
      "test = open(\"test.txt\", \"r\")\n",
      "\n",
      "training = train.readlines()\n",
      "testing = test.readlines()\n",
      "\n",
      "# Fill in the training and label lists\n",
      "docs_train = []\n",
      "label = []\n",
      "\n",
      "i = 1 # Start at i = 1 to avoid the first line, which is just the number of docs\n",
      "while i < len(training):\n",
      "    if (i % 2 == 1):\n",
      "        docs_train.append(training[i]) # In the training text, odd lines are doc names\n",
      "    if (i % 2 == 0):\n",
      "        label.append(training[i]) # In the training text, even lines are labels\n",
      "    i = i + 1\n",
      "\n",
      "\n",
      "# Convert the labels to a numerical matrix\n",
      "labels = np.zeros(len(label))\n",
      "i = 0\n",
      "while i < len(label):\n",
      "    if (label[i] == '1\\n'):\n",
      "        labels[i] = 1\n",
      "    if (label[i] == '-1\\n'):\n",
      "        labels[i] = -1\n",
      "    i = i + 1\n",
      "    \n",
      "# Convert the docs_train list to a \"list of a list\"\n",
      "i = 0\n",
      "while i < len(docs_train):\n",
      "    words = []\n",
      "    words.append(docs_train[i])\n",
      "    docs_train[i] = docs_train[i].split()\n",
      "    i = i + 1\n",
      "\n",
      "# Fill in the test list\n",
      "docs_test = []\n",
      "\n",
      "i = 1 # Start at i = 1 to avoid the first line, which is just the number of docs\n",
      "while i < len(testing):\n",
      "    docs_test.append(testing[i])\n",
      "    i = i + 1\n",
      "\n",
      "# Convert the docs_train list to a \"list of a list\"\n",
      "i = 0\n",
      "while i < len(docs_test):\n",
      "    words = []\n",
      "    words.append(docs_test[i])\n",
      "    docs_test[i] = docs_test[i].split()\n",
      "    i = i + 1\n",
      "\n",
      "n_train = len(docs_train)\n",
      "n_test = len(docs_test)\n",
      "print(docs_train[40])\n",
      "print(docs_test[40])\n",
      "\n",
      "\n",
      "#CHECKS\n",
      "assert((n_train+n_test)==1500)\n",
      "assert(np.sum(np.array(labels))== -348)\n",
      "assert(len(docs_train[1])+len(docs_test[1])==12)\n",
      "194/21:\n",
      "# (ACT4) design a function that takes list of documents (list of list of words) \n",
      "# as input and returns sorted list of distinct words \n",
      "# use built-in sort in python for sorting strings\n",
      "def make_vocabulary(docs):\n",
      "    vocabulary = []\n",
      "    i = 0\n",
      "    while i < len(docs):\n",
      "        temp = docs[i]\n",
      "        j = 0\n",
      "        while j < len(docs[i]):\n",
      "            vocabulary.append(temp[j])\n",
      "            j = j + 1\n",
      "        i = i + 1\n",
      "        vocabulary = list(dict.fromkeys(vocabulary))\n",
      "        vocabulary.sort()\n",
      "    return vocabulary\n",
      "    \n",
      "    \n",
      "vocab = make_vocabulary(docs_train)\n",
      "d = len(vocab) \n",
      "w2i = {vocab[i]:i for i in range(len(vocab))} # maps words in dictionary to corresponding index\n",
      "print(vocab)\n",
      "print(w2i)\n",
      "\n",
      "#CHECKS\n",
      "assert(vocab[2]==\"LnGi\")\n",
      "assert(vocab == sorted(vocab))\n",
      "194/22:\n",
      "# (ACT5) design a function that takes \n",
      "# (1) docs: list of documents (i.e. list of list of words)\n",
      "# (2) w2i: a dictionary that maps words to index\n",
      "# output numpy matrix X as described above with shape of (n,d+1) -- where d is size of vocabulary\n",
      "# (DO NOT forget last column of X which is all 1)\n",
      "\n",
      "def make_matrix(docs, w2i):\n",
      "    d = len(w2i) # Size of the vocabulary\n",
      "    n = len(docs) # Number of documents\n",
      "    X = np.zeros((n, d+1)) # Initialize matrix with appropriate dimensions\n",
      "    i = 0\n",
      "    while i < n: # Consider each row within the X matrix\n",
      "        document = docs[i] # Consider one document at a time\n",
      "        j = 0\n",
      "        while j < d+1: # Consider each column of the specific row\n",
      "            if (j == d): # Satisfies constraint for last column = 1 for all rows\n",
      "                X[i,j] = 1\n",
      "            if (j < d): \n",
      "                vocab = list(w2i.keys()) # Pulls the keys (words) from the dictionary \n",
      "                word = vocab[j] # Consider one vocab word at a time\n",
      "                k = 0\n",
      "            while k < len(document): # Consider all words within a document\n",
      "                check = document[k] # Consdier one word at a time\n",
      "                if (check == word):\n",
      "                    X[i,j] = X[i,j] + 1 # Increment if the words match\n",
      "                k = k + 1\n",
      "            j = j + 1\n",
      "        i = i + 1\n",
      "    return X\n",
      "\n",
      "X_train = make_matrix(docs_train,w2i)\n",
      "X_test = make_matrix(docs_test,w2i)\n",
      "y_train = np.array(labels)\n",
      "\n",
      "# (ACT6-8)\n",
      "print (\"First 5 rows of X_train: \")\n",
      "print(X_train[0:4,]) # We start with row 0 in Python\n",
      "print (\"Last 5 rows of X_test: \")\n",
      "print(X_test[len(docs_test)-1,]) # We start with our last row (length - 1) and move backwards\n",
      "print(X_test[len(docs_test)-2,])\n",
      "print(X_test[len(docs_test)-3,])\n",
      "print(X_test[len(docs_test)-4,])\n",
      "print(X_test[len(docs_test)-5,])\n",
      "print (\"First 10 labels of training set:\")\n",
      "print(y_train[0:9,])\n",
      "\n",
      "\n",
      "#CHECKS\n",
      "assert(np.sum(X_train)==6871)\n",
      "assert(np.sum(X_test)==3462)\n",
      "assert(np.sum(X_test[10,:]+X_train[10,:])==11)\n",
      "194/23:\n",
      "# (ACT9) Design a function that takes y (list of true labels) and y_hat (list of predicted labels)\n",
      "# and computes the error\n",
      "def err(y,y_hat):\n",
      "    n = len(y) # n for the above formula\n",
      "    total = 0 # Initialize the sum of the error\n",
      "    i = 0\n",
      "    while i < n: # Consider all y's in the given set\n",
      "        test_y = y[i]\n",
      "        test_yhat = y_hat[i]\n",
      "        if (test_y == test_yhat): # If they are equal, l = 0\n",
      "            total = total\n",
      "        else:\n",
      "            total = total + 1 # If they are not equal, l = 1\n",
      "        i = i + 1\n",
      "    error = total / n # Compute the error\n",
      "    return error\n",
      "        \n",
      "\n",
      "#CHECKS\n",
      "assert(err([-1,+1,-1,+1],[+1,+1,+1,-1])==0.75)\n",
      "assert(err([+1,+1,+1,+1],[+1,+1,+1,-1])==0.25)\n",
      "assert(err([-1,-1,-1,+1],[+1,+1,+1,-1])==1)\n",
      "194/24:\n",
      "# (ACT10) Design a function that takes as input\n",
      "# (1) document matrix X --- numpy array shape =  (n,d+1)\n",
      "# (2) vector w --- numpy array shape = (d+1,1) or (d+1,)\n",
      "# and output \n",
      "# (1) predictions y_hat --- numpy array shape = (n,1)\n",
      "def predict(X,w):\n",
      "    n = X.shape[0] # Gather size n \n",
      "    y_hat = np.zeros((n,1)) # Initialize y_hat with appropriate dimensions\n",
      "    i = 0\n",
      "    while i < n:\n",
      "        j = 0\n",
      "        temp = 0 # Initialize a temporary value for a given y_hat element\n",
      "        while j < d + 1: # Consider all weights individually \n",
      "            temp = temp + X[i,j]*w[j] # Matrix multiplication\n",
      "            j = j + 1\n",
      "        if (sum(temp) >= 0): # Sign check \n",
      "            y_hat[i] = 1\n",
      "        if (sum(temp) < 0):\n",
      "            y_hat[i] = -1\n",
      "        i = i + 1\n",
      "    return y_hat\n",
      "\n",
      "#CHECKS\n",
      "assert(np.sum(predict(X_train,np.ones(d+1).reshape(-1,1)).reshape(-1,1))==n_train)\n",
      "194/25:\n",
      "# Loading candidates list candids = [w0,w1,...]\n",
      "import pickle\n",
      "with open('candids.pkl', 'rb') as f:\n",
      "    candids = pickle.load(f)\n",
      "print(\"size of candidates lists %d\"%len(candids))\n",
      "print(candids[0])\n",
      "194/26:\n",
      "# (ACT11) fill err_list with training error of each candidate w\n",
      "err_list = np.zeros(len(candids))\n",
      "i = 0\n",
      "while i < len(err_list):\n",
      "    y_hat = predict(X_train,candids[i])\n",
      "    error = err(y_train,y_hat)\n",
      "    err_list[i] = error\n",
      "    i = i + 1\n",
      "    \n",
      "#(ACT12) index of w with smallest error over training set \n",
      "best_index = np.argmin(err_list)\n",
      "print(\"Training Error of candidates:\")\n",
      "for i,err in enumerate(err_list):\n",
      "    print(f\"Candidate # %d has error %f\" % (i,err))\n",
      "\n",
      "print(\"Index of best predictor: %d\"%best_index)\n",
      "print(\"Best Predictor:\")\n",
      "print(candids[best_index])\n",
      "\n",
      "#CHECKS\n",
      "assert(np.sum(err_list)<=2.5)\n",
      "194/27:\n",
      "# Best predictor\n",
      "w_best = candids[best_index]\n",
      "\n",
      "# (ACT13) Use w_best to predict labels for X_test \n",
      "y_test = predict(X_test,w_best)\n",
      "\n",
      "# (ACT14) print first 10 labels predicted for test set\n",
      "print(y_test[0:9])\n",
      "\n",
      "#CHECKS\n",
      "def my_hash(y):\n",
      "    p1 = 28433\n",
      "    p2 = 577\n",
      "    ret = 0\n",
      "    for e in range(len(y)):\n",
      "        ret = ((ret*p2+int(e)) % p1 + p1) % p1\n",
      "    return ret\n",
      "assert(my_hash(y_test) == 19262)\n",
      "194/28:\n",
      "# (ACT15) using X_train and X_test\n",
      "# plot frequency (number of times it appeared) of each word using index of words as x-axis \n",
      "# using two subplots (bar plots) one for X_train and one for X_test\n",
      "# you might find plt.bar useful\n",
      "import matplotlib.pyplot as plt\n",
      "\n",
      "words = list(w2i.keys()) \n",
      "train_frequency = np.zeros(len(words))\n",
      "test_frequency = np.zeros(len(words))\n",
      "\n",
      "# Use a loop to fill the frequency vectors using the X matrices calculated above \n",
      "i = 0\n",
      "while i < len(words):\n",
      "    j = 0\n",
      "    total_train = 0\n",
      "    total_test = 0\n",
      "    while j < X_train.shape[0]:\n",
      "        total_train = total_train + X_train[j,i]\n",
      "        j = j + 1\n",
      "    train_frequency[i] = total_train\n",
      "    j = 0\n",
      "    while j < X_test.shape[0]:\n",
      "        total_test = total_test + X_test[j,i]\n",
      "        j = j + 1\n",
      "    test_frequency[i] = total_test        \n",
      "    i = i + 1\n",
      "\n",
      "# Plot the values in a bar plot (code adopted from MatPlotLib Example)\n",
      "axis = np.arange(len(words))  \n",
      "width = 0.4\n",
      "fig, graph = plt.subplots()\n",
      "train = graph.bar(axis - width/2,train_frequency,width,label ='Training Set')\n",
      "test = graph.bar(axis + width/2,test_frequency,width,label ='Test Set')\n",
      "graph.set_ylabel('Frequency')\n",
      "graph.set_title('Frequency of Vocabulary in Training and Test Sets')\n",
      "graph.set_xticks(axis)\n",
      "graph.set_xticklabels(words)\n",
      "graph.legend()\n",
      "fig.tight_layout()\n",
      "plt.show()\n",
      "195/1:\n",
      "import numpy as np\n",
      "\n",
      "# (ACT1-3) using input files compute the following\n",
      "# docs_train = list of documents where each document is a list of words\n",
      "docs_train = []\n",
      "train_file = open(\"train.txt\", \"r\")\n",
      "for x in range(0, int(train_file.readline())*2):\n",
      "  line = train_file.readline().strip().split(' ')\n",
      "  if (x % 2 == 1): continue\n",
      "  word_list = []\n",
      "  for word in line:\n",
      "    word_list.append(word)\n",
      "  docs_train.append(word_list)\n",
      "\n",
      "# docs_test = ACT2 #list of documents where each document is a list of words\n",
      "docs_test = []\n",
      "test_file = open(\"test.txt\", \"r\")\n",
      "for x in range(0, int(test_file.readline())):\n",
      "  line = test_file.readline().strip().split(' ')\n",
      "  word_list = []\n",
      "  for word in line:\n",
      "    word_list.append(word)\n",
      "  docs_test.append(word_list)\n",
      "\n",
      "# labels = ACT3   #list of labels each either -1 or +1 \n",
      "labels = []\n",
      "train_file = open(\"train.txt\", \"r\")\n",
      "for x in range(0, int(train_file.readline())*2):\n",
      "  val = train_file.readline()\n",
      "  if (x % 2 == 0): continue\n",
      "  labels.append(int(val))\n",
      "\n",
      "n_train = len(docs_train)\n",
      "n_test = len(docs_test)\n",
      "print(docs_train[40])\n",
      "print(docs_test[40])\n",
      "\n",
      "#CHECKS\n",
      "assert((n_train+n_test)==1500)\n",
      "assert(np.sum(np.array(labels))== -348)\n",
      "assert(len(docs_train[1])+len(docs_test[1])==12)\n",
      "195/2:\n",
      "# (ACT4) design a function that takes list of documents (list of list of words) \n",
      "# as input and returns sorted list of distinct words \n",
      "# use built-in sort in python for sorting strings\n",
      "def make_vocabulary(docs):\n",
      "    sorted_list = []\n",
      "    for i in range(len(docs)):\n",
      "      for j in range(len(docs[i])):\n",
      "        if docs[i][j] not in sorted_list:\n",
      "          sorted_list.append(docs[i][j])\n",
      "    return sorted(sorted_list)   \n",
      "    \n",
      "vocab = make_vocabulary(docs_train)\n",
      "d = len(vocab) \n",
      "w2i = {vocab[i]:i for i in range(len(vocab))} # maps words in dictionary to corresponding index\n",
      "print(vocab)\n",
      "print(w2i)\n",
      "\n",
      "#CHECKS\n",
      "assert(vocab[2]==\"LnGi\")\n",
      "assert(vocab == sorted(vocab))\n",
      "195/3:\n",
      "# (ACT5) design a function that takes \n",
      "# (1) docs: list of documents (i.e. list of list of words)\n",
      "# (2) w2i: a dictionary that maps words to index\n",
      "# output numpy matrix X as described above with shape of (n,d+1) -- where d is size of vocabulary\n",
      "# (DO NOT forget last column of X which is all 1)\n",
      "\n",
      "def make_matrix(docs, w2i):\n",
      "    a = np.empty(shape=(len(docs), len(w2i) + 1))   # matrix to be returned\n",
      "    for i in range(len(docs)):    \n",
      "      row = []\n",
      "      for x in w2i:\n",
      "        row.append(docs[i].count(str(x)))\n",
      "      row.append(1)\n",
      "      a[i] = row                               # add row i to the matrix\n",
      "    return a \n",
      "\n",
      "X_train = make_matrix(docs_train,w2i)\n",
      "X_test = make_matrix(docs_test,w2i)\n",
      "y_train = np.array(labels)\n",
      "\n",
      "# (ACT6-8)\n",
      "print (\"First 5 rows of X_train: \")\n",
      "print(X_train[:5])\n",
      "print (\"Last 5 rows of X_test: \")\n",
      "print(X_test[-5:])\n",
      "print (\"First 10 labels of training set:\")\n",
      "print (y_train[:10])\n",
      "\n",
      "#CHECKS\n",
      "assert(np.sum(X_train)==6871)\n",
      "assert(np.sum(X_test)==3462)\n",
      "assert(np.sum(X_test[10,:]+X_train[10,:])==11)\n",
      "195/4:\n",
      "# (ACT9) Design a function that takes y (list of true labels) and y_hat (list of predicted labels)\n",
      "# and computes the error\n",
      "\n",
      "def err(y,y_hat):\n",
      "    loss = 0\n",
      "    for i in range(len(y)):\n",
      "      loss += (y[i] != y_hat[i])\n",
      "    return loss / len(y)\n",
      "\n",
      "#CHECKS\n",
      "assert(err([-1,+1,-1,+1],[+1,+1,+1,-1])==0.75)\n",
      "assert(err([+1,+1,+1,+1],[+1,+1,+1,-1])==0.25)\n",
      "assert(err([-1,-1,-1,+1],[+1,+1,+1,-1])==1)\n",
      "195/5:\n",
      "# (ACT10) Design a function that takes as input\n",
      "# (1) document matrix X --- numpy array shape =  (n,d+1)\n",
      "# (2) vector w --- numpy array shape = (d+1,1) or (d+1,)\n",
      "# and output \n",
      "# (1) predictions y_hat --- numpy array shape = (n,1)\n",
      "def predict(X,w):\n",
      "  # y_hat = sign(Xw)\n",
      "  y_hat = np.empty(len(X))\n",
      "  for i in range(len(X)):\n",
      "    y_hat[i] = np.sign(np.matmul(X, w)[i])\n",
      "    if y_hat[i] == 0: \n",
      "      y_hat[i] = 1\n",
      "  return y_hat\n",
      "  \n",
      "#CHECKS\n",
      "assert(np.sum(predict(X_train,np.ones(d+1).reshape(-1,1)).reshape(-1,1))==n_train)\n",
      "195/6:\n",
      "# Loading candidates list candids = [w0,w1,...]\n",
      "import pickle\n",
      "with open('candids.pkl', 'rb') as f:\n",
      "    candids = pickle.load(f)\n",
      "print(\"size of candidates lists %d\"%len(candids))\n",
      "print(candids[0])\n",
      "195/7:\n",
      "# (ACT11) fill err_list with training error of each candidate w\n",
      "# err_list = \n",
      "err_list = []\n",
      "for i in range(len(candids)):\n",
      "  err_list.append(err(y_train, predict(X_train, candids[i])))\n",
      "\n",
      "# (ACT12) index of w with smallest error over training set \n",
      "best_index = err_list.index(min(err_list))\n",
      "\n",
      "print(\"Training Error of candidates:\")\n",
      "for i,cand_err in enumerate(err_list):\n",
      "    print(f\"Candidate # %d has error %f\" % (i,cand_err))\n",
      "\n",
      "print(\"Index of best predictor: %d\"%best_index)\n",
      "print(\"Best Predictor:\")\n",
      "print(candids[best_index])\n",
      "\n",
      "#CHECKS\n",
      "assert(np.sum(err_list)<=2.5)\n",
      "195/8:\n",
      "# Best predictor\n",
      "w_best = candids[best_index]\n",
      "\n",
      "# (ACT13) Use w_best to predict labels for X_test \n",
      "y_test = predict(X_test, w_best)\n",
      "\n",
      "# (ACT14) print first 10 labels predicted for test set\n",
      "print(y_test[:10])\n",
      "\n",
      "#CHECKS\n",
      "def my_hash(y):\n",
      "    p1 = 28433\n",
      "    p2 = 577\n",
      "    ret = 0\n",
      "    for e in range(len(y)):\n",
      "        ret = ((ret*p2+int(e)) % p1 + p1) % p1\n",
      "    return ret\n",
      "assert(my_hash(y_test) == 19262)\n",
      "195/9:\n",
      "# (ACT15) using X_train and X_test\n",
      "# plot frequency (number of times it appeared) of each word using index of words as x-axis \n",
      "# using two subplots (bar plots) one for X_train and one for X_test\n",
      "# you might find plt.bar useful\n",
      "import matplotlib.pyplot as plt\n",
      "\n",
      "# set up axes for X_train\n",
      "x_axis = np.arange(len(vocab))\n",
      "x_train_freqs = np.zeros(len(vocab))\n",
      "for i in range(len(X_train)): \n",
      "  for j in range(len(vocab)):\n",
      "    x_train_freqs[j] += X_train[i][j]\n",
      "\n",
      "# set up axes for X_test\n",
      "x_test_freqs = np.zeros(len(vocab))\n",
      "for i in range(len(X_test)): \n",
      "  for j in range(len(vocab)):\n",
      "    x_test_freqs[j] += X_test[i][j]\n",
      "\n",
      "# plot X_train\n",
      "fig, (ax1, ax2) = plt.subplots(2)\n",
      "fig.suptitle('Frequency of each word in X_train (blue) and X_test (red)')\n",
      "ax1.bar(x_axis, x_train_freqs, color = 'b', width = 0.5)\n",
      "ax2.bar(x_axis, x_test_freqs, color = 'r', width = 0.5)\n",
      "plt.show()\n",
      "195/10:\n",
      "# (ACT15) using X_train and X_test\n",
      "# plot frequency (number of times it appeared) of each word using index of words as x-axis \n",
      "# using two subplots (bar plots) one for X_train and one for X_test\n",
      "# you might find plt.bar useful\n",
      "import matplotlib.pyplot as plt\n",
      "\n",
      "# set up axes for X_train\n",
      "x_axis = np.arange(len(vocab))\n",
      "x_train_freqs = np.zeros(len(vocab))\n",
      "for i in range(len(X_train)): \n",
      "  for j in range(len(vocab)):\n",
      "    x_train_freqs[j] += X_train[i][j]\n",
      "\n",
      "# set up axes for X_test\n",
      "x_test_freqs = np.zeros(len(vocab))\n",
      "for i in range(len(X_test)): \n",
      "  for j in range(len(vocab)):\n",
      "    x_test_freqs[j] += X_test[i][j]\n",
      "\n",
      "# plot X_train\n",
      "fig, (ax1, ax2) = plt.subplots(2)\n",
      "fig.suptitle('Frequency of each word in X_train (blue) and X_test (red)')\n",
      "ax1.bar(x_axis, x_train_freqs, color = 'b', width = 0.5)\n",
      "ax2.bar(x_axis, x_test_freqs, color = 'r', width = 0.5)\n",
      "plt.show()\n",
      "195/11:\n",
      "import numpy as np\n",
      "\n",
      "# (ACT1-3) using input files compute the following\n",
      "docs_train = ACT1 #list of documents where each document is a list of words\n",
      "docs_test = ACT2 #list of documents where each document is a list of words\n",
      "labels = ACT3   #list of labels each either -1 or +1 \n",
      "\n",
      "\n",
      "n_train = len(docs_train)\n",
      "n_test = len(docs_test)\n",
      "print(docs_train[40])\n",
      "print(docs_test[40])\n",
      "\n",
      "#CHECKS\n",
      "assert((n_train+n_test)==1500)\n",
      "assert(np.sum(np.array(labels))== -348)\n",
      "assert(len(docs_train[1])+len(docs_test[1])==12)\n",
      "195/12:\n",
      "import numpy as np\n",
      "import re\n",
      "\n",
      "ftrain = open(\"train.txt\")\n",
      "ftest = open(\"test.txt\")\n",
      "\n",
      "# (ACT1-3) using input files compute the following\n",
      "ltrain = 2*int(ftrain.readline())\n",
      "\n",
      "docs_train = [] #list of documents where each document is a list of words\n",
      "labels = [] #list of labels each either -1 or +1\n",
      "\n",
      "\n",
      "# Reading in from text.txt to construct a list of documents and labels                  \n",
      "while ltrain > 0:\n",
      "    if(ltrain%2 == 0):\n",
      "        string = ftrain.readline().split('\\n')\n",
      "        docs_train.append(string[0].split())\n",
      "    else:\n",
      "        labels.append(int(ftrain.readline()))\n",
      "    ltrain -= 1\n",
      "    \n",
      "\n",
      "# Reading in from test.txt and consrtucting a list od documents\n",
      "ltest = int(ftest.readline())\n",
      "docs_test = [] #list of documents where each document is a list of words\n",
      "while ltest > 0:\n",
      "    string = ftest.readline().split('\\n')\n",
      "    docs_test.append(string[0].split())\n",
      "    ltest -= 1   \n",
      "\n",
      "ftrain.close()\n",
      "ftest.close()\n",
      "\n",
      "n_train = len(docs_train)\n",
      "n_test = len(docs_test)\n",
      "print(docs_train[40])\n",
      "print(docs_test[40])\n",
      "\n",
      "#CHECKS\n",
      "assert((n_train+n_test)==1500)\n",
      "assert(np.sum(np.array(labels))== -348)\n",
      "assert(len(docs_train[1])+len(docs_test[1])==12)\n",
      "195/13:\n",
      "# (ACT4) design a function that takes list of documents (list of list of words) \n",
      "# as input and returns sorted list of distinct words \n",
      "# use built-in sort in python for sorting strings\n",
      "def make_vocabulary(docs):\n",
      "    vocab = []\n",
      "    seen = []\n",
      "    l_docs = len(docs)\n",
      "    for i in range(0, l_docs):\n",
      "        for word in docs[i]: \n",
      "            if not (word in seen):\n",
      "                vocab.append(word)\n",
      "                seen.append(word)\n",
      "    vocab.sort()\n",
      "    return vocab\n",
      "    \n",
      "vocab = make_vocabulary(docs_train)\n",
      "d = len(vocab) \n",
      "w2i = {vocab[i]:i for i in range(len(vocab))} # maps words in dictionary to corresponding index\n",
      "print(vocab)\n",
      "print(w2i)\n",
      "\n",
      "#CHECKS\n",
      "assert(vocab[2]==\"LnGi\")\n",
      "assert(vocab == sorted(vocab))\n",
      "195/14:\n",
      "# (ACT5) design a function that takes \n",
      "# (1) docs: list of documents (i.e. list of list of words)\n",
      "# (2) w2i: a dictionary that maps words to index\n",
      "# output numpy matrix X as described above with shape of (n,d+1) -- where d is size of vocabulary\n",
      "# (DO NOT forget last column of X which is all 1)\n",
      "\n",
      "def make_matrix(docs, w2i):\n",
      "  d = len(w2i)\n",
      "  X = np.zeros(len(docs)*( d +1)).reshape(len(docs), d+1)\n",
      "  ind = 0\n",
      "  for doc in docs:\n",
      "    xarray = np.zeros(d+1)\n",
      "    for word in doc:\n",
      "      if word in w2i:\n",
      "        xarray[w2i[word]] += 1\n",
      "    X[ind] = xarray\n",
      "    ind += 1\n",
      "  for array in X:\n",
      "    array[d] = 1\n",
      "  return X\n",
      "\n",
      " \n",
      "X_train = make_matrix(docs_train,w2i)\n",
      "X_test = make_matrix(docs_test,w2i)\n",
      "y_train = np.array(labels)\n",
      "\n",
      "# (ACT6-8)\n",
      "print (\"First 5 rows of X_train: \")\n",
      "print(X_train[:5])\n",
      "print (\"Last 5 rows of X_test: \")\n",
      "print(X_test[:5])\n",
      "print (\"First 10 labels of training set:\")\n",
      "print(labels[:10])\n",
      "\n",
      "#CHECKS\n",
      "assert(np.sum(X_train)==6871)\n",
      "assert(np.sum(X_test)==3462)\n",
      "assert(np.sum(X_test[10,:]+X_train[10,:])==11)\n",
      "195/15:\n",
      "# (ACT9) Design a function that takes y (list of true labels) and y_hat (list of predicted labels)\n",
      "# and computes the error\n",
      "def err(y,y_hat):\n",
      "    n = len(y)\n",
      "    error = 0\n",
      "    for i in range(n):\n",
      "      if (y[i] != y_hat[i]):\n",
      "        error += 1\n",
      "    return error/n\n",
      "      \n",
      "\n",
      "#CHECKS\n",
      "assert(err([-1,+1,-1,+1],[+1,+1,+1,-1])==0.75)\n",
      "assert(err([+1,+1,+1,+1],[+1,+1,+1,-1])==0.25)\n",
      "assert(err([-1,-1,-1,+1],[+1,+1,+1,-1])==1)\n",
      "assert(err([1,2,3,4,5],[1,2,3,4,5])==0)\n",
      "195/16:\n",
      "# (ACT10) Design a function that takes as input\n",
      "# (1) document matrix X --- numpy array shape =  (n,d+1)\n",
      "# (2) vector w --- numpy array shape = (d+1,1) or (d+1,)\n",
      "# and output \n",
      "# (1) predictions y_hat --- numpy array shape = (n,1)\n",
      "def predict(X,w):\n",
      "  # y_hat = sign(Xw) where sign(r) = 1 if r>= 0 and =0 if r<=0\n",
      "  n = len(X)\n",
      "  y_hat = np.zeros(n).reshape(n,1)\n",
      "  for i in range(n):\n",
      "    if (np.dot(X[i], w) >= 0):\n",
      "      y_hat[i] = 1\n",
      "    else:\n",
      "      y_hat[i] = -1\n",
      "  return y_hat\n",
      "\n",
      "\n",
      "#CHECKS\n",
      "assert(np.sum(predict(X_train,np.ones(d+1).reshape(-1,1)).reshape(-1,1))==n_train)\n",
      "195/17:\n",
      "# Loading candidates list candids = [w0,w1,...]\n",
      "import pickle\n",
      "with open('candids.pkl', 'rb') as f:\n",
      "    candids = pickle.load(f)\n",
      "print(\"size of candidates lists %d\"%len(candids))\n",
      "print(candids[0])\n",
      "195/18:\n",
      "# (ACT11) fill err_list with training error of each candidate w\n",
      "err_list = []\n",
      "for candidate in candids:\n",
      "  y_hat = predict(X_train, candidate)\n",
      "  error = err(y_train, y_hat)\n",
      "  err_list.append(error)\n",
      "\n",
      "  \n",
      "# (ACT12) index of w with smallest error over training set \n",
      "best_index = 0\n",
      "for k in range(len(err_list)):\n",
      "  if (err_list[k] < err_list[best_index]):\n",
      "    best_index = k\n",
      "\n",
      "print(\"Training Error of candidates:\")\n",
      "for i,cand_err in enumerate(err_list):\n",
      "    print(f\"Candidate # %d has error %f\" % (i,cand_err))\n",
      "\n",
      "print(\"Index of best predictor: %d\"%best_index)\n",
      "print(\"Best Predictor:\")\n",
      "print(candids[best_index])\n",
      "\n",
      "#CHECKS\n",
      "assert(np.sum(err_list)<=2.5)\n",
      "195/19:\n",
      "# Best predictor\n",
      "w_best = candids[best_index]\n",
      "\n",
      "# (ACT13) Use w_best to predict labels for X_test \n",
      "y_test = predict(X_test, w_best)\n",
      "\n",
      "# (ACT14) print first 10 labels predicted for test set\n",
      "print(y_test[:10])\n",
      "\n",
      "#CHECKS\n",
      "def my_hash(y):\n",
      "    p1 = 28433\n",
      "    p2 = 577\n",
      "    ret = 0\n",
      "    for e in range(len(y)):\n",
      "        ret = ((ret*p2+int(e)) % p1 + p1) % p1\n",
      "    return ret\n",
      "assert(my_hash(y_test) == 19262)\n",
      "195/20:\n",
      "# (ACT15) using X_train and X_test\n",
      "# plot frequency (number of times it appeared) of each word using index of words as x-axis \n",
      "# using two subplots (bar plots) one for X_train and one for X_test\n",
      "# you might find plt.bar useful\n",
      "import matplotlib.pyplot as plt\n",
      "\n",
      "x_axis = np.arange(len(w2i))\n",
      "\n",
      "nrows = len(X_test)\n",
      "ncols = len(X_test[0])\n",
      "\n",
      "# calculating frequency of words in X_test\n",
      "freq_test = []\n",
      "for j in range(ncols-1):\n",
      "  word = 0\n",
      "  for i in range(nrows):\n",
      "    word += X_test[i][j] \n",
      "  freq_test.append(word)\n",
      "\n",
      "# calculating frequency of words in X_test\n",
      "freq_train = []\n",
      "for j in range(ncols -1):\n",
      "  word = 0\n",
      "  for i in range(nrows):\n",
      "    word += X_train[i][j] \n",
      "  freq_train.append(word)\n",
      "\n",
      "\n",
      "# Set up sub plots\n",
      "plot_train, plot_test = plt.subplots()\n",
      "bar_width = 0.35\n",
      "opacity = 0.8\n",
      "\n",
      "bar_train = plt.bar(x_axis, freq_train, bar_width, alpha=opacity, color='r', label='X_Train')\n",
      "bar_test = plt.bar(x_axis + bar_width, freq_test, bar_width, alpha=opacity, color='b', label='X_Test')\n",
      "\n",
      "plt.xlabel('Words')\n",
      "plt.ylabel('Frequency')\n",
      "\n",
      "plt.xticks(x_axis + bar_width/2, x_axis)\n",
      "plt.legend()\n",
      "\n",
      "plt.tight_layout()\n",
      "plt.show()\n",
      "195/21: import numpy as np\n",
      "195/22:\n",
      "# ACT1-3\n",
      "# format of train, test data: list of lists/documents, where each doc contains the words\n",
      "\n",
      "docs_train = []\n",
      "labels = []\n",
      "with open('train.txt') as f:\n",
      "    for idx, line in enumerate(f,1):\n",
      "        if idx%2==0:\n",
      "            docs_train.append(line.strip().split())\n",
      "        elif idx!=1:\n",
      "            labels.append(int(line.strip()))\n",
      "            \n",
      "with open('test.txt') as f:\n",
      "    docs_test = [line.strip().split() for line in f]\n",
      "docs_test.remove(docs_test[0])\n",
      "\n",
      "n_test = len(docs_test)\n",
      "n_train = len(docs_train)\n",
      "\n",
      "#CHECKS\n",
      "assert((n_train+n_test)==1500)\n",
      "assert(np.sum(np.array(labels))== -348)\n",
      "assert(len(docs_train[1])+len(docs_test[1])==12)\n",
      "195/23:\n",
      "print(docs_train[40])\n",
      "print(docs_test[40])\n",
      "195/24:\n",
      "import numpy as np\n",
      "\n",
      "# (ACT1-3) using input files compute the following\n",
      "docs=open(\"train.txt\", \"r\")\n",
      "n_train=int(docs.readline())\n",
      "i=0\n",
      "docs_train=[]\n",
      "labels= []\n",
      "while i<n_train:\n",
      "    docu=docs.readline()\n",
      "    word_list=docu.split()\n",
      "    docs_train.append(word_list)\n",
      "    value=docs.readline()\n",
      "    labels.append(int(value))\n",
      "    i=i+1\n",
      "docs.close()\n",
      "docs2=open(\"test.txt\",\"r\")\n",
      "n_test=int (docs2.readline())\n",
      "docs_test=[]\n",
      "j=0\n",
      "while j<n_test:\n",
      "    docu=docs2.readline()\n",
      "    word_list=docu.split()\n",
      "    docs_test.append(word_list)\n",
      "    j=j+1\n",
      "docs2.close\n",
      "\n",
      "\n",
      "n_train = len(docs_train)\n",
      "n_test = len(docs_test)\n",
      "print(docs_train[40])\n",
      "print(docs_test[40])\n",
      "n_train+n_test\n",
      "\n",
      "#CHECKS\n",
      "assert((n_train+n_test)==1500)\n",
      "assert(np.sum(np.array(labels))== -348)\n",
      "assert(len(docs_train[1])+len(docs_test[1])==12)\n",
      "195/25:\n",
      "# (ACT4) design a function that takes list of documents (list of list of words) \n",
      "# as input and returns sorted list of distinct words \n",
      "# use built-in sort in python for sorting strings\n",
      "def make_vocabulary(docs):\n",
      "    all_words=[]\n",
      "    dict_words=[]\n",
      "    for sublist in docs:\n",
      "        for item in sublist:\n",
      "            all_words.append(item)\n",
      "    for item in all_words:\n",
      "        if item not in dict_words:\n",
      "            dict_words.append(item)\n",
      "        \n",
      "    return sorted(dict_words)\n",
      "    \n",
      "vocab = make_vocabulary(docs_train)\n",
      "d = len(vocab) \n",
      "w2i = {vocab[i]:i for i in range(len(vocab))} # maps words in dictionary to corresponding index\n",
      "print(vocab)\n",
      "print(w2i)\n",
      "\n",
      "#CHECKS\n",
      "assert(vocab[2]==\"LnGi\")\n",
      "assert(vocab == sorted(vocab))\n",
      "195/26:\n",
      "# (ACT5) design a function that takes \n",
      "# (1) docs: list of documents (i.e. list of list of words)\n",
      "# (2) w2i: a dictionary that maps words to index\n",
      "# output numpy matrix X as described above with shape of (n,d+1) -- where d is size of vocabulary\n",
      "# (DO NOT forget last column of X which is all 1)\n",
      "\n",
      "\n",
      "def make_matrix(docs, w2i):\n",
      "    count_f=[]\n",
      "    i=0\n",
      "    for i in range (0,len(docs)):\n",
      "        j=0\n",
      "        count_x=[0]*(len(w2i)+1)\n",
      "        for j in range(0,len(w2i)):\n",
      "            for item in docs[i]:\n",
      "                if w2i[item] == j:\n",
      "                    count_x[j]=count_x[j]+1\n",
      "            j=j+1\n",
      "            count_x[len(w2i)]=1\n",
      "        count_f.append(count_x)\n",
      "        i=i+1\n",
      "        #return (np.matrix(count_f))\n",
      "    return (np.matrix(count_f))\n",
      "        \n",
      "X_train = make_matrix(docs_train,w2i)\n",
      "X_test = make_matrix(docs_test,w2i)\n",
      "y_train = np.array(labels)\n",
      "\n",
      "# (ACT6-8)\n",
      "# print (\"First 5 rows of X_train: \")\n",
      "print (X_train[0:5])\n",
      "# print (\"Last 5 rows of X_test: \")\n",
      "print (X_test[-6:-1])\n",
      "# print (\"First 10 labels of training set:\")\n",
      "print (y_train[0:10])\n",
      "\n",
      "\n",
      "\n",
      "#CHECKS\n",
      "assert(np.sum(X_train)==6871)\n",
      "assert(np.sum(X_test)==3462)\n",
      "assert(np.sum(X_test[10,:]+X_train[10,:])==11)\n",
      "195/27:\n",
      "# (ACT9) Design a function that takes y (list of true labels) and y_hat (list of predicted labels)\n",
      "# and computes the error\n",
      "def err(y,y_hat):\n",
      "    i=0\n",
      "    l_tot=0\n",
      "    l=0\n",
      "    for i in range (0,len(y)):\n",
      "        if y[i]==y_hat[i]:\n",
      "            l=0\n",
      "        else:\n",
      "            l=1\n",
      "        l_tot=l_tot+l\n",
      "        i=i+1\n",
      "    return ((1/len(y))*l_tot)\n",
      "                \n",
      "\n",
      "#CHECKS\n",
      "assert(err([-1,+1,-1,+1],[+1,+1,+1,-1])==0.75)\n",
      "assert(err([+1,+1,+1,+1],[+1,+1,+1,-1])==0.25)\n",
      "assert(err([-1,-1,-1,+1],[+1,+1,+1,-1])==1)\n",
      "195/28:\n",
      "# (ACT10) Design a function that takes as input\n",
      "# (1) document matrix X --- numpy array shape =  (n,d+1)\n",
      "# (2) vector w --- numpy array shape = (d+1,1) or (d+1,)\n",
      "# and output \n",
      "# (1) predictions y_hat --- numpy array shape = (n,1)\n",
      "def predict(X,w):\n",
      "    y=[]\n",
      "    y_new1=np.dot(X,w,out=None)\n",
      "    for sublist in y_new1:\n",
      "        for item in sublist:\n",
      "            y.append(item)\n",
      "    return np.array(y)\n",
      "        \n",
      "\n",
      "print(np.sum(predict(X_train,np.ones(d+1).reshape(-1,1)).reshape(-1,1)))\n",
      "print(n_train)\n",
      "assert(np.sum(predict(X_train,np.ones(d+1).reshape(-1,1)).reshape(-1,1))==n_train)\n",
      "195/29:\n",
      "# Loading candidates list candids = [w0,w1,...]\n",
      "import pickle\n",
      "with open('candids.pkl', 'rb') as f:\n",
      "    candids = pickle.load(f)\n",
      "print(\"size of candidates lists %d\"%len(candids))\n",
      "print(candids[0])\n",
      "195/30:\n",
      "# (ACT11) fill err_list with training error of each candidate w\n",
      "err_i=0.0\n",
      "err_list=[]\n",
      "y_pre=[]\n",
      "y_pre2=[]\n",
      "i=0\n",
      "min_error=0.0\n",
      "for i in range (0,len(candids)):\n",
      "    y_pre=predict(X_train,candids[i])\n",
      "    err_i=err(y_train,y_pre)\n",
      "    err_list.append(err_i)\n",
      "    i=i+1\n",
      "    \n",
      "# (ACT12) index of w with smallest error over training set \n",
      "err_ind = {err_list[i]:i for i in range(len(err_list))}\n",
      "min_error=min(err_list)\n",
      "best_index = err_ind[min_error]\n",
      "\n",
      "\n",
      "print(\"Training Error of candidates:\")\n",
      "for i,err in enumerate(err_list):\n",
      "    print(f\"Candidate # %d has error %f\" % (i,err))\n",
      "\n",
      "print(\"Index of best predictor: %d\"%best_index)\n",
      "print(\"Best Predictor:\")\n",
      "print(candids[best_index])\n",
      "\n",
      "#CHECKS\n",
      "assert(np.sum(err_list)<=2.5)\n",
      "195/31:\n",
      "import numpy as np\n",
      "\n",
      "# (ACT1-3) using input files compute the following\n",
      "#docs_train = ACT1 #list of documents where each document is a list of words\n",
      "#docs_test = ACT2 #list of documents where each document is a list of words\n",
      "#labels = ACT3   #list of labels each either -1 or +1 \n",
      "\n",
      "train_set = open(\"train.txt\").read().splitlines() #open the file\n",
      "test_set = open(\"test.txt\").read().splitlines() #open the file\n",
      "docs_train = [];docs_test = [];labels = []\n",
      "n_train = int(train_set[0]);n_test = int(test_set[0]) #store the number of train and test sets\n",
      "\n",
      "#store the information for docs\n",
      "for i in range(1,len(train_set)):\n",
      "    if(i % 2 == 1):\n",
      "        docs_train.append(train_set[i].split())\n",
      "    else:\n",
      "        labels.append(int(train_set[i]))\n",
      "\n",
      "for i in range(1,len(test_set)):\n",
      "    docs_test.append(test_set[i].split())\n",
      "        \n",
      "labels = np.array(labels)\n",
      "docs_train = np.array(docs_train)\n",
      "docs_test = np.array(docs_test)\n",
      "\n",
      "#n_train = len(docs_train)\n",
      "#n_test = len(docs_test)\n",
      "print(docs_train[40])\n",
      "print(docs_test[40])\n",
      "\n",
      "#CHECKS\n",
      "assert((n_train+n_test)==1500)\n",
      "assert(np.sum(np.array(labels))== -348)\n",
      "assert(len(docs_train[1])+len(docs_test[1])==12)\n",
      "195/32:\n",
      "# (ACT4) design a function that takes list of documents (list of list of words) \n",
      "# as input and returns sorted list of distinct words \n",
      "# use built-in sort in python for sorting strings\n",
      "def make_vocabulary(docs):\n",
      "    #ACT4\n",
      "    n = len(docs)\n",
      "    vocab = []\n",
      "    for i in range(n):\n",
      "        vocab = vocab + docs[i]\n",
      "    #filter only different words\n",
      "    vocab = np.unique(vocab)\n",
      "    return vocab\n",
      "    \n",
      "vocab = make_vocabulary(docs_train)\n",
      "d = len(vocab) \n",
      "w2i = {vocab[i]:i for i in range(len(vocab))} # maps words in dictionary to corresponding index\n",
      "print(vocab)\n",
      "print(w2i)\n",
      "\n",
      "#CHECKS\n",
      "assert(vocab[2]==\"LnGi\")\n",
      "print((vocab == sorted(vocab)).all())\n",
      "assert((vocab == sorted(vocab)).all())\n",
      "195/33:\n",
      "# (ACT5) design a function that takes \n",
      "# (1) docs: list of documents (i.e. list of list of words)\n",
      "# (2) w2i: a dictionary that maps words to index\n",
      "# output numpy matrix X as described above with shape of (n,d+1) -- where d is size of vocabulary\n",
      "# (DO NOT forget last column of X which is all 1)\n",
      "\n",
      "def make_matrix(docs, w2i):\n",
      "    #ACT5\n",
      "    n = len(docs);d = len(w2i)\n",
      "    X = np.zeros((n, d+1))\n",
      "    \n",
      "    for i in range(n):\n",
      "        for vocab in w2i.keys():\n",
      "            for w in docs[i]:\n",
      "                if(w == vocab):\n",
      "                    X[i][w2i.get(vocab)] += 1\n",
      "    #last column             \n",
      "    for i in range(n):\n",
      "        X[i][d] = 1\n",
      "    return X\n",
      "                \n",
      "\n",
      "X_train = make_matrix(docs_train,w2i)\n",
      "X_test = make_matrix(docs_test,w2i)\n",
      "y_train = np.array(labels)\n",
      "\n",
      "# (ACT6-8)\n",
      "print (\"First 5 rows of X_train: \")\n",
      "print(X_train[0:5])\n",
      "print (\"Last 5 rows of X_test: \")\n",
      "print(X_test[n_test-6:n_test-1])\n",
      "print (\"First 10 labels of training set:\")\n",
      "print (y_train[0:10])\n",
      "\n",
      "#CHECKS\n",
      "assert(np.sum(X_train)==6871)\n",
      "assert(np.sum(X_test)==3462)\n",
      "assert(np.sum(X_test[10,:]+X_train[10,:])==11)\n",
      "195/34:\n",
      "# (ACT9) Design a function that takes y (list of true labels) and y_hat (list of predicted labels)\n",
      "# and computes the error\n",
      "def err(y,y_hat):\n",
      "    #ACT9\n",
      "    n = len(y)\n",
      "    #definition of loss\n",
      "    loss = np.zeros(n)\n",
      "    for i in range(n):\n",
      "        if(y[i] != y_hat[i]):\n",
      "            loss[i] = 1\n",
      "    return np.mean(loss)\n",
      "\n",
      "#CHECKS\n",
      "assert(err([-1,+1,-1,+1],[+1,+1,+1,-1])==0.75)\n",
      "assert(err([+1,+1,+1,+1],[+1,+1,+1,-1])==0.25)\n",
      "assert(err([-1,-1,-1,+1],[+1,+1,+1,-1])==1)\n",
      "195/35:\n",
      "# (ACT10) Design a function that takes as input\n",
      "# (1) document matrix X --- numpy array shape =  (n,d+1)\n",
      "# (2) vector w --- numpy array shape = (d+1,1) or (d+1,)\n",
      "# and output \n",
      "# (1) predictions y_hat --- numpy array shape = (n,1)\n",
      "def predict(X,w):\n",
      "    #ACT10\n",
      "    Xw = np.matmul(X,w)\n",
      "    n = np.shape(X)[0] # number of rows\n",
      "    y_hat = np.zeros(n)\n",
      "    \n",
      "    for i in range(n):\n",
      "        if(Xw[i] >= 0):\n",
      "            y_hat[i] = 1\n",
      "        else:\n",
      "            y_hat[i] = -1\n",
      "    return y_hat\n",
      "\n",
      "#CHECKS\n",
      "assert(np.sum(predict(X_train,np.ones(d+1).reshape(-1,1)).reshape(-1,1))==n_train)\n",
      "195/36:\n",
      "# Loading candidates list candids = [w0,w1,...]\n",
      "import pickle\n",
      "with open('candids.pkl', 'rb') as f:\n",
      "    candids = pickle.load(f)\n",
      "print(\"size of candidates lists %d\"%len(candids))\n",
      "print(candids[0])\n",
      "195/37:\n",
      "# (ACT11) fill err_list with training error of each candidate w\n",
      "#err_list = ACT11\n",
      "err_list = np.zeros(len(candids))\n",
      "for i in range(len(candids)):\n",
      "    err_list[i] = err(y_train,predict(X_train,candids[i]))\n",
      "# (ACT12) index of w with smallest error over training set \n",
      "#best_index = ACT12\n",
      "best_index = np.where(err_list == min(err_list))[0][0]\n",
      "\n",
      "print(\"Training Error of candidates:\")\n",
      "for i,cand_err in enumerate(err_list):\n",
      "    print(f\"Candidate # %d has error %f\" % (i,cand_err))\n",
      "\n",
      "print(\"Index of best predictor: %d\"%best_index)\n",
      "print(\"Best Predictor:\")\n",
      "print(candids[best_index])\n",
      "\n",
      "#CHECKS\n",
      "assert(np.sum(err_list)<=2.5)\n",
      "195/38:\n",
      "# Best predictor\n",
      "w_best = candids[best_index]\n",
      "\n",
      "# (ACT13) Use w_best to predict labels for X_test \n",
      "#y_test = ACT13\n",
      "y_test = predict(X_test,w_best)\n",
      "\n",
      "# (ACT14) print first 10 labels predicted for test set\n",
      "#print(ACT14)\n",
      "print(y_test[0:10])\n",
      "\n",
      "#CHECKS\n",
      "def my_hash(y):\n",
      "    p1 = 28433\n",
      "    p2 = 577\n",
      "    ret = 0\n",
      "    for e in range(len(y)):\n",
      "        ret = ((ret*p2+int(e)) % p1 + p1) % p1\n",
      "    return ret\n",
      "assert(my_hash(y_test) == 19262)\n",
      "195/39:\n",
      "# (ACT15) using X_train and X_test\n",
      "# plot frequency (number of times it appeared) of each word using index of words as x-axis \n",
      "# using two subplots (bar plots) one for X_train and one for X_test\n",
      "# you might find plt.bar useful\n",
      "import matplotlib.pyplot as plt\n",
      "\n",
      "#ACT15\n",
      "plt.figure(figsize = (10,7))\n",
      "plt.title('Plot of Frequency vs Words')\n",
      "\n",
      "train_freq = np.delete(np.sum(X_train,axis=0),[9]) #remove the last element\n",
      "test_freq = np.delete(np.sum(X_test,axis=0),[9]) #remove the last element\n",
      "x_axis = np.arange(10)\n",
      "\n",
      "\n",
      "plt.bar(x_axis, train_freq, color = 'r', width = 0.3, label = 'Train')\n",
      "plt.bar(x_axis + 0.3, test_freq, color = 'b', width = 0.3, label = 'Test')\n",
      "\n",
      "plt.ylabel('Frequency')\n",
      "plt.xlabel('Word Index')\n",
      "\n",
      "plt.legend()\n",
      "\n",
      "plt.show()\n",
      "195/40:\n",
      "import numpy as np\n",
      "\n",
      "# (ACT1-3) using input files compute the following\n",
      "train_file = open(\"train.txt\", \"r\")\n",
      "test_file = open(\"test.txt\", \"r\")\n",
      "docs_test = []\n",
      "docs_train = []\n",
      "labels = []\n",
      "even_line = True\n",
      "\n",
      "#input test lines into docs_test\n",
      "test_file.readline()\n",
      "word = ''\n",
      "for line in test_file:\n",
      "    words = []\n",
      "    for chars in line:\n",
      "        if(chars != ' ' and chars != '\\n'):\n",
      "            word += chars\n",
      "        elif (chars != '\\n'):\n",
      "            words.append(word)\n",
      "            word = ''\n",
      "    docs_test.append(words)\n",
      "\n",
      "#input training lines into docs_train and labels\n",
      "train_file.readline()\n",
      "for line in train_file:\n",
      "    if even_line:\n",
      "        words = []\n",
      "        \n",
      "        for chars in line:\n",
      "            if(chars != ' ' and chars != '\\n'):\n",
      "                word += chars\n",
      "            elif (chars != '\\n'):\n",
      "                words.append(word)\n",
      "                word = ''\n",
      "        docs_train.append(words)\n",
      "        even_line = False\n",
      "    else:\n",
      "        labels.append(int(line))\n",
      "        even_line = True\n",
      "\n",
      "n_train = len(docs_train)\n",
      "n_test = len(docs_test)\n",
      "print(docs_train[40])\n",
      "print(docs_test[40])\n",
      "\n",
      "\n",
      "\n",
      "#CHECKS\n",
      "assert((n_train+n_test)==1500)\n",
      "assert(np.sum(np.array(labels))== -348)\n",
      "assert(len(docs_train[1])+len(docs_test[1])==12)\n",
      "195/41:\n",
      "# (ACT4) design a function that takes list of documents (list of list of words) \n",
      "# as input and returns sorted list of distinct words \n",
      "# use built-in sort in python for sorting strings\n",
      "def make_vocabulary(docs):\n",
      "    vocab = []\n",
      "    for doc in docs:\n",
      "        for word in doc:\n",
      "            if (vocab.count(word) == 0): \n",
      "                vocab.append(word) \n",
      "    vocab.sort()\n",
      "    return vocab\n",
      "    \n",
      "    \n",
      "vocab = make_vocabulary(docs_train)\n",
      "d = len(vocab) \n",
      "w2i = {vocab[i]:i for i in range(len(vocab))} # maps words in dictionary to corresponding index\n",
      "print(vocab)\n",
      "print(w2i)\n",
      "\n",
      "#CHECKS\n",
      "assert(vocab[2]==\"LnGi\")\n",
      "assert(vocab == sorted(vocab))\n",
      "195/42:\n",
      "# (ACT5) design a function that takes \n",
      "# (1) docs: list of documents (i.e. list of list of words)\n",
      "# (2) w2i: a dictionary that maps words to index\n",
      "# output numpy matrix X as described above with shape of (n,d+1) -- where d is size of vocabulary\n",
      "# (DO NOT forget last column of X which is all 1)\n",
      "\n",
      "def make_matrix(docs, w2i):\n",
      "    #print(np.shape(w2i))\n",
      "    A = np.zeros((len(docs), len(w2i) + 1))\n",
      "    for idx, row in enumerate(docs):\n",
      "        for word in row:\n",
      "            A[idx:(idx + 1), w2i[word]: (w2i[word] + 1)] += 1;\n",
      "        A[idx:(idx + 1), len(w2i)] = 1   \n",
      "    return A\n",
      "\n",
      "X_train = make_matrix(docs_train,w2i)\n",
      "X_test = make_matrix(docs_test,w2i)\n",
      "y_train = np.array(labels)\n",
      "\n",
      "# (ACT6-8)\n",
      "print (\"First 5 rows of X_train: \")\n",
      "print(X_train[:5, :])\n",
      "print (\"Last 5 rows of X_test: \")\n",
      "print(X_test[len(docs_test) - 5:, :])\n",
      "print (\"First 10 labels of training set:\")\n",
      "print (labels[:10])\n",
      "\n",
      "#CHECKS\n",
      "assert(np.sum(X_train)==6871)\n",
      "assert(np.sum(X_test)==3462)\n",
      "assert(np.sum(X_test[10,:]+X_train[10,:])==11)\n",
      "195/43:\n",
      "# (ACT9) Design a function that takes y (list of true labels) and y_hat (list of predicted labels)\n",
      "# and computes the error\n",
      "def err(y,y_hat):\n",
      "    error = 0.0\n",
      "    for idx, label in enumerate(y):\n",
      "        if (label != y_hat[idx]): \n",
      "            error+= 1\n",
      "    error = error / len(y)\n",
      "    return error\n",
      "\n",
      "#CHECKS\n",
      "assert(err([-1,+1,-1,+1],[+1,+1,+1,-1])==0.75)\n",
      "assert(err([+1,+1,+1,+1],[+1,+1,+1,-1])==0.25)\n",
      "assert(err([-1,-1,-1,+1],[+1,+1,+1,-1])==1)\n",
      "195/44:\n",
      "# (ACT10) Design a function that takes as input\n",
      "# (1) document matrix X --- numpy array shape =  (n,d+1)\n",
      "# (2) vector w --- numpy array shape = (d+1,1) or (d+1,)\n",
      "# and output \n",
      "# (1) predictions y_hat --- numpy array shape = (n,1)\n",
      "def predict(X,w):\n",
      "    product = np.dot(X, w)\n",
      "    product[product < 0] = -1\n",
      "    product[product >= 0] = 1\n",
      "    return product\n",
      "\n",
      "#CHECKS\n",
      "assert(np.sum(predict(X_train,np.ones(d+1).reshape(-1,1)).reshape(-1,1))==n_train)\n",
      "195/45:\n",
      "# Loading candidates list candids = [w0,w1,...]\n",
      "import pickle\n",
      "with open('candids.pkl', 'rb') as f:\n",
      "    candids = pickle.load(f)\n",
      "print(\"size of candidates lists %d\"%len(candids))\n",
      "print(candids[0])\n",
      "195/46:\n",
      "# (ACT11) fill err_list with training error of each candidate w\n",
      "err_list = []\n",
      "for candid in candids:\n",
      "    err_list.append(err(labels, predict(X_train, candid)))\n",
      "\n",
      "# (ACT12) index of w with smallest error over training set \n",
      "best_index = np.argmin(err_list)\n",
      "\n",
      "print(\"Training Error of candidates:\")\n",
      "for i,err in enumerate(err_list):\n",
      "    print(f\"Candidate # %d has error %f\" % (i,err))\n",
      "\n",
      "print(\"Index of best predictor: %d\"%best_index)\n",
      "print(\"Best Predictor:\")\n",
      "print(candids[best_index])\n",
      "\n",
      "#CHECKS\n",
      "assert(np.sum(err_list)<=2.5)\n",
      "195/47:\n",
      "# Best predictor\n",
      "w_best = candids[best_index]\n",
      "\n",
      "# (ACT13) Use w_best to predict labels for X_test \n",
      "y_test = np.dot(X_test, w_best)\n",
      "y_test[y_test >= 0] = 1\n",
      "y_test[y_test < 0] = -1\n",
      "\n",
      "# (ACT14) print first 10 labels predicted for test set\n",
      "print(y_test[0:10])\n",
      "\n",
      "#CHECKS\n",
      "def my_hash(y):\n",
      "    p1 = 28433\n",
      "    p2 = 577\n",
      "    ret = 0\n",
      "    for e in range(len(y)):\n",
      "        ret = ((ret*p2+int(e)) % p1 + p1) % p1\n",
      "    return ret\n",
      "assert(my_hash(y_test) == 19262)\n",
      "195/48:\n",
      "# (ACT15) using X_train and X_test\n",
      "# plot frequency (number of times it appeared) of each word using index of words as x-axis \n",
      "# using two subplots (bar plots) one for X_train and one for X_test\n",
      "# you might find plt.bar useful\n",
      "import matplotlib.pyplot as plt\n",
      "\n",
      "x = np.arange(len(X_train[0]) - 1)            \n",
      "y_train = np.sum(X_train, axis = 0)[:10]\n",
      "y_test = np.sum(X_test, axis = 0)[:10]\n",
      "\n",
      "plt.subplot(2, 1, 1)\n",
      "plt.bar(x, y_train)\n",
      "plt.title(\"frequency of words in training set\")\n",
      "\n",
      "plt.subplot(2, 1, 2)\n",
      "plt.bar(x, y_test)\n",
      "plt.title(\"frequency of words in test set\")\n",
      "\n",
      "plt.show()\n",
      "197/1:\n",
      "import os\n",
      "import time\n",
      "import dill\n",
      "from typing import List\n",
      "from functools import reduce\n",
      "from calendar import monthrange\n",
      "197/2:\n",
      "import json\n",
      "import csv\n",
      "import requests\n",
      "import xml.etree.ElementTree as ET\n",
      "from bs4 import BeautifulSoup\n",
      "import urllib.request as urllib2\n",
      "197/3:\n",
      "import matplotlib.pyplot as plt\n",
      "import numpy as np\n",
      "from scipy import stats\n",
      "from random import sample, randint, shuffle\n",
      "from sklearn.decomposition import PCA\n",
      "197/4:\n",
      "import nltk\n",
      "import text_summarizer\n",
      "from gensim.utils import simple_preprocess\n",
      "from gensim.summarization.summarizer import summarize\n",
      "from gensim.models import Word2Vec, LsiModel\n",
      "from gensim.test.utils import common_dictionary, common_corpus\n",
      "197/5: from time import process_time\n",
      "197/6:\n",
      "t = process_time()\n",
      "dill.load_session('thesis_env2.db')\n",
      "process_time() - t\n",
      "197/7:\n",
      "gpol_c = getg(mpol_c)\n",
      "res = sorted([(x, gproj(gpol_c, mpol_c, x)) for x in filter(lambda x: x in vocab2, vocab2)], key=lambda x: x[1])\n",
      "for i in range(10):\n",
      "    print(f\"{res[i][0].ljust(10)} {res[i][1]:.2f} \\t\\t {res[-i-1][0].ljust(10)} {res[-i-1][1]:.2f}\")\n",
      "gbks_c = getg(mbks_c)\n",
      "res = sorted([(x, gproj(gbks_c, mbks_c, x)) for x in filter(lambda x: x in vocab2, vocab2)], key=lambda x: x[1])\n",
      "for i in range(10):\n",
      "    print(f\"{res[i][0].ljust(10)} {res[i][1]:.2f} \\t\\t {res[-i-1][0].ljust(10)} {res[-i-1][1]:.2f}\")\n",
      "gmov_c = getg(mmov_c)\n",
      "res = sorted([(x, gproj(gmov_c, mmov_c, x)) for x in filter(lambda x: x in vocab2, vocab2)], key=lambda x: x[1])\n",
      "for i in range(10):\n",
      "    print(f\"{res[i][0].ljust(10)} {res[i][1]:.2f} \\t\\t {res[-i-1][0].ljust(10)} {res[-i-1][1]:.2f}\")\n",
      "gspts_c = getg(mspts_c)\n",
      "res = sorted([(x, gproj(gspts_c, mspts_c, x)) for x in filter(lambda x: x in vocab2, vocab2)], key=lambda x: x[1])\n",
      "for i in range(10):\n",
      "    print(f\"{res[i][0].ljust(10)} {res[i][1]:.2f} \\t\\t {res[-i-1][0].ljust(10)} {res[-i-1][1]:.2f}\")\n",
      "200/1: #PA2 by Eric Dogariu\n",
      "200/2:\n",
      "import matplotlib.pyplot as plt #import matplotlib\n",
      "import numpy as np #import numpy\n",
      "200/3:\n",
      "# Mean squared error from residual\n",
      "def mse_from_e(residual):\n",
      "    length = len(residual) #get length of residual vector\n",
      "    squared = np.power(residual, 2) #square all vector elements\n",
      "    sum2 = np.sum(squared) #add elements\n",
      "    mse = sum2 / length #divide by length of vector\n",
      "    return mse #return mse\n",
      "\n",
      "# Mean Squared Error of X.w - y\n",
      "def mean_squared_error(X, y, w):\n",
      "    dot = np.dot(X, w) #subtract y from dot product of X and w\n",
      "    y_neg = np.multiply(-1, y) #multiply y by -1\n",
      "    sum2 = np.add(dot, y_neg) #X.w - y\n",
      "    mse2 = mse_from_e(sum2) #find mse of X.w - y\n",
      "    return mse2 #return mse\n",
      "\n",
      "\n",
      "#CHECKS\n",
      "r = [8, 10, 4] #test residual\n",
      "X2 = np.zeros((3,2)) #declare and fill test X array\n",
      "X2[0][0] = 2\n",
      "X2[0][1] = 6\n",
      "X2[1][0] = 6\n",
      "X2[1][1] = 8\n",
      "X2[2][0] = 10\n",
      "X2[2][1] = 12\n",
      "W2 = [1, 2]  #test weight array\n",
      "Y2 = [3, 6, 6] #test y array\n",
      "assert(mse_from_e(r) == 60) #check mse value for residual\n",
      "assert(mean_squared_error(X2, Y2, W2) == 387) #check mse value for X.w - y\n",
      "200/4:\n",
      "# Generate a sawtooth weight vector\n",
      "def genu(d, m):\n",
      "    u = np.arange(d) * (2 * (np.arange(d) % 2) - 1)\n",
      "    u = m * u / np.sqrt(np.dot(u, u))\n",
      "    return u\n",
      "\n",
      "# Generate random data\n",
      "def genx(n, d):\n",
      "    X = np.random.randint(0, 2, (n, d))\n",
      "    X[:,int(d/2)] = 1\n",
      "    return X\n",
      "\n",
      "# Generate targets and add noise\n",
      "def gent(X, u, noise):\n",
      "    n = X.shape[0]\n",
      "    y = np.dot(X, u).reshape(n, 1)\n",
      "    y += noise * np.var(y) * np.random.randn(n, 1)\n",
      "    return y\n",
      "\n",
      "# Generate data, weights, and targets\n",
      "def gimme_data_regres(n, d, noise=0.1):\n",
      "    u = genu(d, 1.0)\n",
      "    X = genx(n, d)\n",
      "    y = gent(X, u, noise)\n",
      "    mse_gen = mean_squared_error(X, y, u)\n",
      "    print('Generator Loss={0:8.5f}\\n'.format(mse_gen))\n",
      "    return X, u, y\n",
      "200/5:\n",
      "# Plot loss as a function of epoch\n",
      "def loss_plotter(vlist, fname):\n",
      "    vr = vlist[0]\n",
      "    vn = vlist[1]\n",
      "    plt.plot(range(1, 1+len(vr)), vr,\n",
      "           range(1, 1+len(vn)), vn,\n",
      "           linewidth=2, linestyle='-', marker='o')\n",
      "    plt.legend(('rep', 'nor'))\n",
      "    plt.grid()\n",
      "    xt = np.arange(1, 1 + max(len(vr), len(vn)))\n",
      "    _ = plt.xticks(xt)\n",
      "    _ = plt.xlabel('Epoch', fontsize=14)\n",
      "    _ = plt.ylabel(fname, fontsize=14)\n",
      "    plt.show()\n",
      "    return\n",
      "\n",
      "# Scatter plot of predicted vs. observed targets\n",
      "def loss_scatter(X, y, w, fname):\n",
      "    plt.scatter(y, X.dot(w), edgecolors=(0,0,0))\n",
      "    plt.plot([y.min(), y.max()], [y.min(), y.max()], 'k--', lw=4)\n",
      "    plt.grid()\n",
      "    plt.xlabel('$y$', fontsize=14)\n",
      "    plt.ylabel('$\\hat{y}$', fontsize=14)\n",
      "    plt.show()\n",
      "200/6:\n",
      "# sample new index (w/ or w/o replacement)\n",
      "def sample_new_index(d, replace):\n",
      "    ind = 0\n",
      "    if replace == 1:  #replacement\n",
      "        ind = np.random.permutation(d)[0] #get random integer in range 0 to d-1\n",
      "        return ind #return index\n",
      "    else:\n",
      "        if 'prm' not in sample_new_index.__dict__: #declare new permuted array and take index 0 value\n",
      "            sample_new_index.prm = np.random.permutation(d) #new permuted array\n",
      "            sample_new_index.head = 0  #set position to 0\n",
      "            ind = sample_new_index.prm[sample_new_index.head]  #take 0 position index value\n",
      "            sample_new_index.head = sample_new_index.head + 1   #increment position \n",
      "            return ind  #return index\n",
      "            \n",
      "        \n",
      "        if (sample_new_index.head > 0 and sample_new_index.head < d-1): #if position greater than 0 and less than d-1\n",
      "            ind = sample_new_index.prm[sample_new_index.head]  #take index value at position\n",
      "            sample_new_index.head = sample_new_index.head + 1  #increment position \n",
      "            return ind  #return index\n",
      "            \n",
      "        \n",
      "        if (sample_new_index.head == d-1): #if position equals d-1\n",
      "            ind = sample_new_index.prm[sample_new_index.head] #take index value at position\n",
      "            del sample_new_index.prm  #delete object (reached end index)\n",
      "            return ind  #return index\n",
      "        \n",
      "            \n",
      "array = []\n",
      "x = sample_new_index(10, replace=1) #sample index 0-9 with replacement\n",
      "assert(x >= 0) #check index greater than or equal to 0\n",
      "assert (x < 10) #check index less than 10\n",
      "\n",
      "\n",
      "for i in range(0, 10):  #sample index 10 times\n",
      "    x = sample_new_index(10, replace=0) #sample index 0-9 without replacement\n",
      "    array.append(x) #add index to array\n",
      "assert(len(array) == 10) #check array length is 10 (index sampled 10 times)\n",
      "array.sort()  #sort array\n",
      "current = 0  #set current\n",
      "prev = array[0] #set previous\n",
      "for i in range(1, 10): #check that each element is one larger than previous element in sorted array, to make sure\n",
      "    #all index values used only once\n",
      "    current = array[i] #set current\n",
      "    assert(current == prev + 1) #check that current is one larger than previous element\n",
      "    prev = current #set previous for next loop\n",
      "200/7:\n",
      "# calculate the change to w[j] wrt current margins z\n",
      "# xjs is the squared norm of the jth column of X, a.k.a. ||xj||^2\n",
      "def delta_wj(e, xj, xjs):\n",
      "    e2 = e[:,0] #reshape e from (n, 1) to (n, )\n",
      "    xj2 = xj[:,0] #reshape Xj from (n, 1) to (n, )\n",
      "    mult = np.dot(e2, xj2) #dot product of e and xj (e.Xj)\n",
      "    a = (mult * -1) / (xjs) #a = -e.Xj / ||xj||^2\n",
      "    return a  #return a\n",
      "\n",
      "\n",
      "\n",
      "# Return new values for w[j] and residual\n",
      "def update(wj, e, xj, xjs):\n",
      "    a = delta_wj(e, xj, xjs) #get value of a\n",
      "    wj_new = wj + a  #update wj (Wjnew = Wj + a)\n",
      "    xj_new = np.multiply(a, xj) #a*Xj\n",
      "    e_new = np.add(e, xj_new) #update e (e_new = e + a*Xj)\n",
      "    return wj_new, e_new  #return Wjnew, e_new\n",
      "\n",
      "e2 = [1, 2, 3, 4] #create e residual vector\n",
      "xj2 = [4, 6, 8, 10] #create Xj vector\n",
      "length = len(e2) #length of e and Xj\n",
      "e2 = np.reshape(e2, (length, 1)) #reshape e from (n, ) to (n, 1)\n",
      "xj2 = np.reshape(xj2, (length, 1)) #reshape Xj from (n, ) to (n, 1) \n",
      "xjs2 = np.sum(np.power(xj2, 2))    #||xj||^2\n",
      "assert(xjs2 == 216) #make sure ||xj||^2 has right value\n",
      "wj2 = 2 #set Wj\n",
      "assert(round(delta_wj(e2, xj2, xjs2), 2) == -0.37) #check delta_wj function returns correct a value\n",
      "wj_new2, e_new2 = update(wj2, e2, xj2, xjs2) #get Wj_new and e_new from update function\n",
      "assert(round(wj_new2, 2) == 1.63) #check that Wj_new has right value\n",
      "e_new2 = list(e_new2) #convert e_new to list and round entries\n",
      "e_new2[0] = round(float(e_new2[0]), 2)\n",
      "e_new2[1] = round(float(e_new2[1]), 2)\n",
      "e_new2[2] = round(float(e_new2[2]), 2)\n",
      "e_new2[3] = round(float(e_new2[3]), 2)\n",
      "assert(e_new2 == [-0.48, -0.22, 0.04, 0.30]) #check that elements of e_new have right values\n",
      "200/8:\n",
      "# Initialize all variables using the zero vector for w\n",
      "# (Initialize w as the zero vector)\n",
      "# You should return w, xjs, residual\n",
      "def initialize(X, y):\n",
      "    length = np.shape(X)[1] #number of columns (d)\n",
      "    w = np.zeros(length) #create weight vector of all zeros with size (d, 1)\n",
      "    w = w.reshape(length, 1)\n",
      "    X2 = np.power(X, 2)\n",
      "    xjs = np.sum(X2, axis = 0)\n",
      "    xjs = xjs.reshape(length, 1)\n",
      "    sum1 = np.dot(X, w)\n",
      "    sum1 = sum1.reshape(np.shape(X)[0], 1)\n",
      "    neg_y = np.multiply(-1, y)\n",
      "    neg_y = neg_y.reshape(np.shape(X)[0], 1)\n",
      "    e = np.add(sum1, neg_y)\n",
      "    return w, xjs, e\n",
      "    \n",
      "\n",
      "X2 = np.zeros((3,2)) #declare and fill test X array with dimensions (n, d)\n",
      "X2[0][0] = 2\n",
      "X2[0][1] = 6\n",
      "X2[1][0] = 6\n",
      "X2[1][1] = 8\n",
      "X2[2][0] = 10\n",
      "X2[2][1] = 12\n",
      "Y2 = [3, 6, 6] #test y array \n",
      "w2, Xjs2, e2 = initialize(X2, Y2) #initialize w, xjs, and residual\n",
      "assert(np.sum(w2) == 0) #make sure sum of all elements in w is 0 (every number is 0)\n",
      "assert(np.shape(w2)[0] == 2 and np.shape(w2)[1] == 1) #make sure w has shape (d,1)\n",
      "assert(np.shape(Xjs2)[0] == 2 and np.shape(Xjs2)[1] == 1) #make sure xjs has shape (d,1)\n",
      "assert(np.shape(e2)[0] == 3 and np.shape(e2)[1] == 1) #make sure e has shape (n,1)\n",
      "200/9:\n",
      "# Check whether termination condition is met\n",
      "def mse_check(mse_p, mse_c, eps):\n",
      "    b = False\n",
      "    sum1 = (mse_p - mse_c) / mse_c\n",
      "    if (sum1 <= eps):\n",
      "        b = True\n",
      "    return b\n",
      "\n",
      "assert(mse_check(1, 0.2, 0.001) == False) #check false condition\n",
      "assert(mse_check(0.001, 0.01, 0.001) == True) #check true condition\n",
      "200/10:\n",
      "# Linear regression using coordinate decent\n",
      "def linear_regression_cd(X, y, epochs=100, eps=0.001, replace=1):\n",
      "    w, xjs, residual = initialize(X, y)\n",
      "    mse_cd = [mse_from_e(residual)]\n",
      "    n, d = X.shape\n",
      "    for e in range(d * epochs):\n",
      "        j = sample_new_index(d, replace)\n",
      "        xj = X[:,j].reshape(n, 1)\n",
      "        w[j], residual = update(w[j], residual, xj, xjs[j])\n",
      "        if (e + 1) % d == 0:\n",
      "            mse_cd.append(mse_from_e(residual))\n",
      "            print('Epoch: {0:2d}  MSE: {1:5.3f}'.format(int((e+1)/d), mse_cd[-1]))\n",
      "            if mse_check(mse_cd[-2], mse_cd[-1], eps): break\n",
      "    return w, mse_cd\n",
      "200/11:\n",
      "# ---------------- Main for linear regression using Coordinate Descent --------------\n",
      "\n",
      "np.random.seed(17)\n",
      "n, d, noise = 1000, 20, 1.0\n",
      "myeps = 1e-4\n",
      "\n",
      "[X, u, y] = gimme_data_regres(n, d, noise)\n",
      "\n",
      "mse_list = []\n",
      "[wr, mse_r] = linear_regression_cd(X, y, eps=myeps)\n",
      "mse_list.append(mse_r)\n",
      "[wn, mse_n] = linear_regression_cd(X, y, eps=myeps, replace=0)\n",
      "mse_list.append(mse_n)\n",
      "\n",
      "loss_plotter(mse_list, 'MSE')\n",
      "loss_scatter(X, y, wn, 'True vs. Predicted Outcome')\n",
      "203/1:\n",
      "import os\n",
      "import time\n",
      "import dill\n",
      "from typing import List\n",
      "from functools import reduce\n",
      "from calendar import monthrange\n",
      "203/2:\n",
      "import json\n",
      "import csv\n",
      "import requests\n",
      "import xml.etree.ElementTree as ET\n",
      "from bs4 import BeautifulSoup\n",
      "import urllib.request as urllib2\n",
      "203/3:\n",
      "import matplotlib.pyplot as plt\n",
      "import numpy as np\n",
      "from scipy import stats\n",
      "from random import sample, randint, shuffle\n",
      "from sklearn.decomposition import PCA\n",
      "203/4:\n",
      "import nltk\n",
      "import text_summarizer\n",
      "from gensim.utils import simple_preprocess\n",
      "from gensim.summarization.summarizer import summarize\n",
      "from gensim.models import Word2Vec, LsiModel\n",
      "from gensim.test.utils import common_dictionary, common_corpus\n",
      "203/5: from time import process_time\n",
      "203/6:\n",
      "t = process_time()\n",
      "dill.load_session('thesis_env2.db')\n",
      "process_time() - t\n",
      "204/1:\n",
      "import os\n",
      "import time\n",
      "import dill\n",
      "from typing import List\n",
      "from functools import reduce\n",
      "from calendar import monthrange\n",
      "204/2:\n",
      "import json\n",
      "import csv\n",
      "import requests\n",
      "import xml.etree.ElementTree as ET\n",
      "from bs4 import BeautifulSoup\n",
      "import urllib.request as urllib2\n",
      "204/3:\n",
      "import matplotlib.pyplot as plt\n",
      "import numpy as np\n",
      "from scipy import stats\n",
      "from random import sample, randint, shuffle\n",
      "from sklearn.decomposition import PCA\n",
      "204/4:\n",
      "import nltk\n",
      "import text_summarizer\n",
      "from gensim.utils import simple_preprocess\n",
      "from gensim.summarization.summarizer import summarize\n",
      "from gensim.models import Word2Vec, LsiModel\n",
      "from gensim.test.utils import common_dictionary, common_corpus\n",
      "204/5: from time import process_time\n",
      "204/6:\n",
      "t = process_time()\n",
      "dill.load_session('thesis_env2.db')\n",
      "process_time() - t\n",
      "205/1:\n",
      "import os\n",
      "import time\n",
      "import dill\n",
      "from typing import List\n",
      "from functools import reduce\n",
      "from calendar import monthrange\n",
      "205/2:\n",
      "import json\n",
      "import csv\n",
      "import requests\n",
      "import xml.etree.ElementTree as ET\n",
      "from bs4 import BeautifulSoup\n",
      "import urllib.request as urllib2\n",
      "205/3:\n",
      "import matplotlib.pyplot as plt\n",
      "import numpy as np\n",
      "from scipy import stats\n",
      "from random import sample, randint, shuffle\n",
      "from sklearn.decomposition import PCA\n",
      "205/4:\n",
      "import nltk\n",
      "import text_summarizer\n",
      "from gensim.utils import simple_preprocess\n",
      "from gensim.summarization.summarizer import summarize\n",
      "from gensim.models import Word2Vec, LsiModel\n",
      "from gensim.test.utils import common_dictionary, common_corpus\n",
      "205/5: from time import process_time\n",
      "205/6:\n",
      "t = process_time()\n",
      "dill.load_session('thesis_env.db')\n",
      "process_time() - t\n",
      "205/7: len(adjectives)\n",
      "205/8: len(all_)\n",
      "204/7: len(politics), len(movies), len(books), len(sports)\n",
      "204/8: len(all_)\n",
      "203/7: len(politics), len(movies), len(books), len(sports)\n",
      "203/8: len(all_)\n",
      "203/9:\n",
      "def heshestats(w1, w2):\n",
      "    for m in [mpol, mbooks, mmov, msports]:\n",
      "        c1 = m.wv.vocab[w1].count\n",
      "        c2 = m.wv.vocab[w2].count\n",
      "        print(c1, c2, c2/c1)\n",
      "203/10: heshestats('he', 'she')\n",
      "204/9: 'african american' in vocab\n",
      "204/10: 'asian american' in vocab\n",
      "204/11: 'asian' in vocab\n",
      "204/12: 'asian-american' in vocab\n",
      "204/13: 'african-american' in vocab\n",
      "203/11: mpol.most_similar(positive=[\"black\"], topn=1))\n",
      "203/12: mpol.most_similar(positive=[\"black\"], topn=1)\n",
      "203/13: mpol.most_similar(positive=[\"black\"], topn=10)\n",
      "203/14: mpol.most_similar(positive=[\"white\"], topn=10)\n",
      "203/15:  mpol.most_similar(positive=[\"caucasian\"], topn=10)\n",
      "203/16:  mpol.most_similar(positive=[\"asian\"], topn=10)\n",
      "203/17: # Race\n",
      "203/18: mpol.most_similar(positive=[\"native\"], topn=10)\n",
      "203/19: mpol.most_similar(positive=[\"native american\"], topn=10)\n",
      "203/20: mpol.most_similar(positive=[\"indian\"], topn=10)\n",
      "203/21: mpol.most_similar(positive=[\"hispanic\"], topn=10)\n",
      "203/22: mpol.most_similar(positive=[\"jew\"], topn=10)\n",
      "203/23: mpol.most_similar(positive=[\"christian\"], topn=10)\n",
      "203/24: mpol.most_similar(positive=[\"muslim\"], topn=10)\n",
      "205/9: top5('monday')\n",
      "205/10:\n",
      "def top5(word):\n",
      "    for m in [mpol, mbooks, mmov, msports]:\n",
      "        print([x[0] for x in m.wv.most_similar(positive=[word], topn=5)])\n",
      "205/11: top5('monday')\n",
      "204/14: len(all_)\n",
      "203/25: len(all_)\n",
      "203/26: len(all_)\n",
      "206/1:\n",
      "import os\n",
      "import time\n",
      "import dill\n",
      "from typing import List\n",
      "from functools import reduce\n",
      "from calendar import monthrange\n",
      "206/2:\n",
      "import json\n",
      "import csv\n",
      "import requests\n",
      "import xml.etree.ElementTree as ET\n",
      "from bs4 import BeautifulSoup\n",
      "import urllib.request as urllib2\n",
      "206/3:\n",
      "import matplotlib.pyplot as plt\n",
      "import numpy as np\n",
      "from scipy import stats\n",
      "from random import sample, randint, shuffle\n",
      "from sklearn.decomposition import PCA\n",
      "206/4:\n",
      "import nltk\n",
      "import text_summarizer\n",
      "from gensim.utils import simple_preprocess\n",
      "from gensim.summarization.summarizer import summarize\n",
      "from gensim.models import Word2Vec, LsiModel\n",
      "from gensim.test.utils import common_dictionary, common_corpus\n",
      "206/5: from time import process_time\n",
      "206/6:\n",
      "t = process_time()\n",
      "dill.load_session('thesis_env2.db')\n",
      "process_time() - t\n",
      "206/7: len(politics), len(movies), len(books), len(sports)\n",
      "206/8: len(all_)\n",
      "206/9: len(pairs), len(proflist), len(gender_specific)\n",
      "206/10:\n",
      "vocab = set(mpol.wv.vocab)\n",
      "for m in [mpol, mmov, mbooks, msports]:\n",
      "    vocab &= set(m.wv.vocab)\n",
      "len(vocab)\n",
      "206/11:\n",
      "vocab_all = set(vocab)\n",
      "for m in [mpol_s, mbks_s, mmov_s, mspts_s] + \\\n",
      "[mpol_b, mbks_b, mmov_b, mspts_b] + \\\n",
      "[mpol_c, mbks_c, mmov_c, mspts_c]:\n",
      "    vocab_all &= set(m.wv.vocab)\n",
      "len(vocab_all)\n",
      "206/12: len(vocab), len(vocab2)\n",
      "206/13: vocab2 = vocab_all\n",
      "206/14: len(vocab), len(vocab2)\n",
      "206/15: len(adjectives), len(cities), len(grammar)\n",
      "206/16:\n",
      "grammar = ['a', 'an', 'the', \\\n",
      "           'for', 'and', 'nor', 'but', 'or', 'yet', 'so', \\\n",
      "           'after', 'as', 'before', 'if', 'inasmuch', 'now', \\\n",
      "           'once', 'since', 'supposing', 'though', 'until', \\\n",
      "           'whenever', 'whereas', 'wherever', 'which', \\\n",
      "           'although', 'because', 'even', 'lest', 'both', 'either', \\\n",
      "           'hardly', 'when', 'neither', 'than', 'whether', \\\n",
      "           'consequently', 'finally', 'furthermore', 'hence', \\\n",
      "           'however', 'incidentally', 'indeed', 'instead', 'likewise', \\\n",
      "           'meanwhile', 'about', 'above', 'across', 'after', 'ago', \\\n",
      "           'at', 'below', 'by', 'during', 'for', 'in', \\\n",
      "           'on', 'over', 'past', 'since', 'through', 'to', \\\n",
      "           'up', 'amid', 'atop', 'onto', \\\n",
      "           'throughout', 'upon', 'within', 'without']\n",
      "206/17: len(adjectives), len(cities), len(grammar)\n",
      "207/1:\n",
      "import os\n",
      "import time\n",
      "import dill\n",
      "from typing import List\n",
      "from functools import reduce\n",
      "from calendar import monthrange\n",
      "207/2:\n",
      "import json\n",
      "import csv\n",
      "import requests\n",
      "import xml.etree.ElementTree as ET\n",
      "from bs4 import BeautifulSoup\n",
      "import urllib.request as urllib2\n",
      "207/3:\n",
      "import matplotlib.pyplot as plt\n",
      "import numpy as np\n",
      "from scipy import stats\n",
      "from random import sample, randint, shuffle\n",
      "from sklearn.decomposition import PCA\n",
      "207/4:\n",
      "import nltk\n",
      "import text_summarizer\n",
      "from gensim.utils import simple_preprocess\n",
      "from gensim.summarization.summarizer import summarize\n",
      "from gensim.models import Word2Vec, LsiModel\n",
      "from gensim.test.utils import common_dictionary, common_corpus\n",
      "207/5: from time import process_time\n",
      "207/6:\n",
      "t = process_time()\n",
      "dill.load_session('thesis_env.db')\n",
      "process_time() - t\n",
      "207/7: len(adjectives)\n",
      "207/8: len(all_)\n",
      "207/9: alld\n",
      "207/10:\n",
      "t = time.process_time()\n",
      "alld = {}\n",
      "for doc in all_:\n",
      "    doc = Doc(doc.id, doc.gdescriptors, doc.descriptors, doc.text)\n",
      "    for d in doc.descriptors:\n",
      "        count, docs = alld.get(d, (0, set()))\n",
      "        docs.add(doc)\n",
      "        alld[d] = (count + 1, docs)\n",
      "time.process_time() - t\n",
      "207/11:\n",
      "alldlist = sorted([(d, alld[d][0], alld[d][1]) for d in alld], key=lambda x: x[1], reverse=True)\n",
      "for x in range(15):\n",
      "    print(alldlist[x][0:2])\n",
      "207/12:\n",
      "def exclude(e, x):\n",
      "    for d in x.descriptors:\n",
      "        if d == e:\n",
      "            continue\n",
      "        if d in ['Politics and Government', 'Motion Pictures', 'Books and Literature']:\n",
      "            return False\n",
      "    return True\n",
      "207/13:\n",
      "def getlist(topic):\n",
      "    return list(filter(lambda x: exclude(topic, x), alld[topic][1]))\n",
      "207/14: politics = getlist('Politics and Government')\n",
      "207/15: movies = getlist('Motion Pictures')\n",
      "207/16: books = getlist('Books and Literature')\n",
      "207/17: sports = getlist('Baseball') + getlist('Football')\n",
      "207/18: len(politics), len(movies), len(books), len(sports)\n",
      "207/19: politics = sample(politics, 7370)\n",
      "207/20: movies = sample(movies, 7370)\n",
      "207/21: books = sample(books, 7370)\n",
      "207/22: sports = sample(sports, 7370)\n",
      "207/23:\n",
      "def getw2v(category):\n",
      "    sentences = []\n",
      "    for t in category:\n",
      "        sentences += nltk.sent_tokenize(t.text)\n",
      "    docs = [simple_preprocess(s) for s in sentences]\n",
      "    return Word2Vec(docs)\n",
      "207/24:\n",
      "t = time.process_time()\n",
      "msports = getw2v(sports)\n",
      "time.process_time() - t\n",
      "207/25:\n",
      "t = time.process_time()\n",
      "msports2 = getw2v(sports + sports)\n",
      "time.process_time() - t\n",
      "207/26:\n",
      "def gdictadd(models):\n",
      "    for i in range(4):\n",
      "        gdict[list(gdict)[i]].append(getg(models[i]))\n",
      "207/27:\n",
      "def mdictadd(models):\n",
      "    for i in range(4):\n",
      "        mdict[list(mdict)[i]].append(models[i])\n",
      "207/28:\n",
      "def gproj(g, m, w1):\n",
      "    wvec = m.wv.get_vector(w1)\n",
      "    p = np.dot(g, wvec)/(np.linalg.norm(g)*np.linalg.norm(wvec))\n",
      "    return p\n",
      "207/29:\n",
      "def bias(g, m, vocab, wordlist):\n",
      "    b = 0\n",
      "    l = list(filter(lambda x: x in vocab, wordlist))\n",
      "    for x in l:\n",
      "        b += abs(gproj(g, m, x))\n",
      "    return b/len(l)\n",
      "207/30:\n",
      "def getg(m):\n",
      "    matrix = []\n",
      "    for a, b in pairs:\n",
      "        if a not in m.wv.vocab or b not in m.wv.vocab:\n",
      "            continue\n",
      "        center = (m.wv.get_vector(a) + m.wv.get_vector(b))/2\n",
      "        matrix.append(m.wv.get_vector(a) - center)\n",
      "        matrix.append(m.wv.get_vector(b) - center)\n",
      "    matrix = np.array(matrix)\n",
      "    p = PCA(n_components = 10)\n",
      "    p.fit(matrix)\n",
      "    print(p.explained_variance_ratio_[0])\n",
      "#         plt.bar(range(10), p.explained_variance_ratio_)\n",
      "#         plt.title(\"Explained Variance vs Component Number (\" + t + \")\")\n",
      "    return p.components_[0]\n",
      "207/31:\n",
      "gdict = {}\n",
      "models = [msports, msports2]\n",
      "gdictadd(models)\n",
      "mdictadd(models)\n",
      "for i in gdict:\n",
      "    print(\"%10s\" % i, \\\n",
      "          \"%.3f\" % bias(gdict[i][0], mdict[i][0], vocab, proflist), \\\n",
      "          \"%.3f\" % bias(gdict[i][0], mdict[i][0], vocab, adjectives), \\\n",
      "          \"%.3f\" % bias(gdict[i][0], mdict[i][0], vocab, cities), \\\n",
      "          \"%.3f\" % bias(gdict[i][0], mdict[i][0], vocab, objects), \\\n",
      "          \"%.3f\" % bias(gdict[i][0], mdict[i][0], vocab, gender_specific))\n",
      "207/32:\n",
      "def gdictadd(models):\n",
      "    for i in range(len(models)):\n",
      "        gdict[list(gdict)[i]].append(getg(models[i]))\n",
      "207/33:\n",
      "def mdictadd(models):\n",
      "    for i in range(len(models)):\n",
      "        mdict[list(mdict)[i]].append(models[i])\n",
      "207/34:\n",
      "gdict = {}\n",
      "models = [msports, msports2]\n",
      "gdictadd(models)\n",
      "mdictadd(models)\n",
      "for i in gdict:\n",
      "    print(\"%10s\" % i, \\\n",
      "          \"%.3f\" % bias(gdict[i][0], mdict[i][0], vocab, proflist), \\\n",
      "          \"%.3f\" % bias(gdict[i][0], mdict[i][0], vocab, adjectives), \\\n",
      "          \"%.3f\" % bias(gdict[i][0], mdict[i][0], vocab, cities), \\\n",
      "          \"%.3f\" % bias(gdict[i][0], mdict[i][0], vocab, objects), \\\n",
      "          \"%.3f\" % bias(gdict[i][0], mdict[i][0], vocab, gender_specific))\n",
      "207/35: gdict\n",
      "207/36: models\n",
      "207/37:\n",
      "gdict = {\"sports\": [], \"sports2\": []}\n",
      "models = [msports, msports2]\n",
      "gdictadd(models)\n",
      "mdictadd(models)\n",
      "for i in gdict:\n",
      "    print(\"%10s\" % i, \\\n",
      "          \"%.3f\" % bias(gdict[i][0], mdict[i][0], vocab, proflist), \\\n",
      "          \"%.3f\" % bias(gdict[i][0], mdict[i][0], vocab, adjectives), \\\n",
      "          \"%.3f\" % bias(gdict[i][0], mdict[i][0], vocab, cities), \\\n",
      "          \"%.3f\" % bias(gdict[i][0], mdict[i][0], vocab, objects), \\\n",
      "          \"%.3f\" % bias(gdict[i][0], mdict[i][0], vocab, gender_specific))\n",
      "207/38:\n",
      "with open('debiaswe-master/data/equalize_pairs.json') as f:\n",
      "    pairs = json.loads(f.read())\n",
      "pairs = list(map(lambda x: [x[0].lower(), x[1].lower()], pairs))\n",
      "with open('debiaswe-master/data/professions.json') as f:\n",
      "    profs = json.loads(f.read())\n",
      "proflist = list(filter(lambda x: x in mpol.wv.vocab, [p[0] for p in profs if abs(p[1]) < 0.6]))\n",
      "with open('debiaswe-master/data/gender_specific_seed.json') as f:\n",
      "    gender_specific = json.loads(f.read())\n",
      "with open('objects.csv') as csv_file:\n",
      "    objects = csv_file.read().split(\"\\n\")\n",
      "objects[0] = 'angle'\n",
      "with open('cities.csv') as csv_file:\n",
      "    cities = csv_file.read().split(\",\")\n",
      "cities = [x.lower() for x in cities]\n",
      "207/39:\n",
      "with open('debiaswe-master/data/equalize_pairs.json') as f:\n",
      "    pairs = json.loads(f.read())\n",
      "pairs = list(map(lambda x: [x[0].lower(), x[1].lower()], pairs))\n",
      "with open('debiaswe-master/data/professions.json') as f:\n",
      "    profs = json.loads(f.read())\n",
      "# proflist = list(filter(lambda x: x in mpol.wv.vocab, [p[0] for p in profs if abs(p[1]) < 0.6]))\n",
      "with open('debiaswe-master/data/gender_specific_seed.json') as f:\n",
      "    gender_specific = json.loads(f.read())\n",
      "with open('objects.csv') as csv_file:\n",
      "    objects = csv_file.read().split(\"\\n\")\n",
      "objects[0] = 'angle'\n",
      "with open('cities.csv') as csv_file:\n",
      "    cities = csv_file.read().split(\",\")\n",
      "cities = [x.lower() for x in cities]\n",
      "207/40:\n",
      "gdict = {\"sports\": [], \"sports2\": []}\n",
      "models = [msports, msports2]\n",
      "gdictadd(models)\n",
      "mdictadd(models)\n",
      "for i in gdict:\n",
      "    print(\"%10s\" % i, \\\n",
      "          \"%.3f\" % bias(gdict[i][0], mdict[i][0], vocab, proflist), \\\n",
      "          \"%.3f\" % bias(gdict[i][0], mdict[i][0], vocab, adjectives), \\\n",
      "          \"%.3f\" % bias(gdict[i][0], mdict[i][0], vocab, cities), \\\n",
      "          \"%.3f\" % bias(gdict[i][0], mdict[i][0], vocab, objects), \\\n",
      "          \"%.3f\" % bias(gdict[i][0], mdict[i][0], vocab, gender_specific))\n",
      "207/41:\n",
      "gdict = {\"sports\": [], \"sports2\": []}\n",
      "mdict = {}\n",
      "models = [msports, msports2]\n",
      "gdictadd(models)\n",
      "mdictadd(models)\n",
      "for i in gdict:\n",
      "    print(\"%10s\" % i, \\\n",
      "          \"%.3f\" % bias(gdict[i][0], mdict[i][0], vocab, proflist), \\\n",
      "          \"%.3f\" % bias(gdict[i][0], mdict[i][0], vocab, adjectives), \\\n",
      "          \"%.3f\" % bias(gdict[i][0], mdict[i][0], vocab, cities), \\\n",
      "          \"%.3f\" % bias(gdict[i][0], mdict[i][0], vocab, objects), \\\n",
      "          \"%.3f\" % bias(gdict[i][0], mdict[i][0], vocab, gender_specific))\n",
      "207/42:\n",
      "gdict = {\"sports\": [], \"sports2\": []}\n",
      "mdict = {\"sports\": [], \"sports2\": []}\n",
      "models = [msports, msports2]\n",
      "gdictadd(models)\n",
      "mdictadd(models)\n",
      "for i in gdict:\n",
      "    print(\"%10s\" % i, \\\n",
      "          \"%.3f\" % bias(gdict[i][0], mdict[i][0], vocab, proflist), \\\n",
      "          \"%.3f\" % bias(gdict[i][0], mdict[i][0], vocab, adjectives), \\\n",
      "          \"%.3f\" % bias(gdict[i][0], mdict[i][0], vocab, cities), \\\n",
      "          \"%.3f\" % bias(gdict[i][0], mdict[i][0], vocab, objects), \\\n",
      "          \"%.3f\" % bias(gdict[i][0], mdict[i][0], vocab, gender_specific))\n",
      "207/43:\n",
      "vocab = set(mpol.wv.vocab)\n",
      "for m in [mpol, mmov, mbooks, msports]:\n",
      "    vocab &= set(m.wv.vocab)\n",
      "len(vocab)\n",
      "207/44:\n",
      "vocab = set(msports.wv.vocab)\n",
      "len(vocab)\n",
      "207/45:\n",
      "gdict = {\"sports\": [], \"sports2\": []}\n",
      "mdict = {\"sports\": [], \"sports2\": []}\n",
      "models = [msports, msports2]\n",
      "gdictadd(models)\n",
      "mdictadd(models)\n",
      "for i in gdict:\n",
      "    print(\"%10s\" % i, \\\n",
      "          \"%.3f\" % bias(gdict[i][0], mdict[i][0], vocab, proflist), \\\n",
      "          \"%.3f\" % bias(gdict[i][0], mdict[i][0], vocab, adjectives), \\\n",
      "          \"%.3f\" % bias(gdict[i][0], mdict[i][0], vocab, cities), \\\n",
      "          \"%.3f\" % bias(gdict[i][0], mdict[i][0], vocab, objects), \\\n",
      "          \"%.3f\" % bias(gdict[i][0], mdict[i][0], vocab, gender_specific))\n",
      "207/46:\n",
      "vocab = set(msports.wv.vocab)\n",
      "len(vocab)\n",
      "proflist = list(filter(lambda x: x in msports.wv.vocab, [p[0] for p in profs if abs(p[1]) < 0.6]))\n",
      "207/47:\n",
      "gdict = {\"sports\": [], \"sports2\": []}\n",
      "mdict = {\"sports\": [], \"sports2\": []}\n",
      "models = [msports, msports2]\n",
      "gdictadd(models)\n",
      "mdictadd(models)\n",
      "for i in gdict:\n",
      "    print(\"%10s\" % i, \\\n",
      "          \"%.3f\" % bias(gdict[i][0], mdict[i][0], vocab, proflist), \\\n",
      "          \"%.3f\" % bias(gdict[i][0], mdict[i][0], vocab, adjectives), \\\n",
      "          \"%.3f\" % bias(gdict[i][0], mdict[i][0], vocab, cities), \\\n",
      "          \"%.3f\" % bias(gdict[i][0], mdict[i][0], vocab, objects), \\\n",
      "          \"%.3f\" % bias(gdict[i][0], mdict[i][0], vocab, gender_specific))\n",
      "207/48:\n",
      "t = time.process_time()\n",
      "msports2 = getw2v(sports + sports + sports + sports)\n",
      "time.process_time() - t\n",
      "207/49:\n",
      "gdict = {\"sports\": [], \"sports2\": []}\n",
      "mdict = {\"sports\": [], \"sports2\": []}\n",
      "models = [msports, msports2]\n",
      "gdictadd(models)\n",
      "mdictadd(models)\n",
      "for i in gdict:\n",
      "    print(\"%10s\" % i, \\\n",
      "          \"%.3f\" % bias(gdict[i][0], mdict[i][0], vocab, proflist), \\\n",
      "          \"%.3f\" % bias(gdict[i][0], mdict[i][0], vocab, adjectives), \\\n",
      "          \"%.3f\" % bias(gdict[i][0], mdict[i][0], vocab, cities), \\\n",
      "          \"%.3f\" % bias(gdict[i][0], mdict[i][0], vocab, objects), \\\n",
      "          \"%.3f\" % bias(gdict[i][0], mdict[i][0], vocab, gender_specific))\n",
      "207/50:\n",
      "gdict = {\"sports\": [], \"sports2\": []}\n",
      "mdict = {\"sports\": [], \"sports2\": []}\n",
      "models = [msports, msports2]\n",
      "gdictadd(models)\n",
      "mdictadd(models)\n",
      "for i in gdict:\n",
      "    print(\"%10s\" % i, \\\n",
      "          \"%.3f\" % bias(gdict[i][0], mdict[i][0], vocab, proflist), \\\n",
      "          \"%.3f\" % bias(gdict[i][0], mdict[i][0], vocab, adjectives), \\\n",
      "          \"%.3f\" % bias(gdict[i][0], mdict[i][0], vocab, grammar), \\\n",
      "          \"%.3f\" % bias(gdict[i][0], mdict[i][0], vocab, gender_specific))\n",
      "207/51:\n",
      "grammar = ['a', 'an', 'the', \\\n",
      "           'for', 'and', 'nor', 'but', 'or', 'yet', 'so', \\\n",
      "           'after', 'as', 'before', 'if', 'inasmuch', 'now', \\\n",
      "           'once', 'since', 'supposing', 'though', 'until', \\\n",
      "           'whenever', 'whereas', 'wherever', 'which', \\\n",
      "           'although', 'because', 'even', 'lest', 'both', 'either', \\\n",
      "           'hardly', 'when', 'neither', 'than', 'whether', \\\n",
      "           'consequently', 'finally', 'furthermore', 'hence', \\\n",
      "           'however', 'incidentally', 'indeed', 'instead', 'likewise', \\\n",
      "           'meanwhile', 'about', 'above', 'across', 'after', 'ago', \\\n",
      "           'at', 'below', 'by', 'during', 'for', 'in', \\\n",
      "           'on', 'over', 'past', 'since', 'through', 'to', \\\n",
      "           'up', 'amid', 'atop', 'onto', \\\n",
      "           'throughout', 'upon', 'within', 'without']\n",
      "207/52:\n",
      "gdict = {\"sports\": [], \"sports2\": []}\n",
      "mdict = {\"sports\": [], \"sports2\": []}\n",
      "models = [msports, msports2]\n",
      "gdictadd(models)\n",
      "mdictadd(models)\n",
      "for i in gdict:\n",
      "    print(\"%10s\" % i, \\\n",
      "          \"%.3f\" % bias(gdict[i][0], mdict[i][0], vocab, proflist), \\\n",
      "          \"%.3f\" % bias(gdict[i][0], mdict[i][0], vocab, adjectives), \\\n",
      "          \"%.3f\" % bias(gdict[i][0], mdict[i][0], vocab, grammar), \\\n",
      "          \"%.3f\" % bias(gdict[i][0], mdict[i][0], vocab, gender_specific))\n",
      "207/53: proflist\n",
      "207/54: len(proflist), len(profs)\n",
      "207/55: proflist\n",
      "207/56: [p for p in proflist if p in msports.wv.vocab]\n",
      "207/57: len([p for p in proflist if p in msports.wv.vocab])\n",
      "207/58:\n",
      "t = time.process_time()\n",
      "msports1 = getw2v(sports + sports)\n",
      "time.process_time() - t\n",
      "207/59:\n",
      "t = time.process_time()\n",
      "mmov = getw2v(movies)\n",
      "time.process_time() - t\n",
      "207/60:\n",
      "t = time.process_time()\n",
      "mpol = getw2v(politics)\n",
      "time.process_time() - t\n",
      "207/61:\n",
      "t = time.process_time()\n",
      "mbooks = getw2v(books)\n",
      "time.process_time() - t\n",
      "207/62:\n",
      "t = time.process_time()\n",
      "mbooks1 = getw2v(books + books)\n",
      "time.process_time() - t\n",
      "207/63:\n",
      "t = time.process_time()\n",
      "mbooks2 = getw2v(books + books + books + books)\n",
      "time.process_time() - t\n",
      "207/64:\n",
      "vocab = set(mpol.wv.vocab)\n",
      "for m in [mpol, mmov, mbooks, msports]:\n",
      "    vocab &= set(m.wv.vocab)\n",
      "len(vocab)\n",
      "207/65:\n",
      "gdict = {\"sports\": [], \"sports1\": [], \"sports2\": []}\n",
      "mdict = {\"sports\": [],  \"sports2\": []}\n",
      "models = [msports, msports1, msports2]\n",
      "gdictadd(models)\n",
      "mdictadd(models)\n",
      "for i in gdict:\n",
      "    print(\"%10s\" % i, \\\n",
      "          \"%.3f\" % bias(gdict[i][0], mdict[i][0], vocab, proflist), \\\n",
      "          \"%.3f\" % bias(gdict[i][0], mdict[i][0], vocab, adjectives), \\\n",
      "          \"%.3f\" % bias(gdict[i][0], mdict[i][0], vocab, grammar), \\\n",
      "          \"%.3f\" % bias(gdict[i][0], mdict[i][0], vocab, gender_specific))\n",
      "207/66:\n",
      "gdict = {\"sports\": [], \"sports1\": [], \"sports2\": []}\n",
      "mdict = {\"sports\": [], \"sports1\": [], \"sports2\": []}\n",
      "models = [msports, msports1, msports2]\n",
      "gdictadd(models)\n",
      "mdictadd(models)\n",
      "for i in gdict:\n",
      "    print(\"%10s\" % i, \\\n",
      "          \"%.3f\" % bias(gdict[i][0], mdict[i][0], vocab, proflist), \\\n",
      "          \"%.3f\" % bias(gdict[i][0], mdict[i][0], vocab, adjectives), \\\n",
      "          \"%.3f\" % bias(gdict[i][0], mdict[i][0], vocab, grammar), \\\n",
      "          \"%.3f\" % bias(gdict[i][0], mdict[i][0], vocab, gender_specific))\n",
      "207/67:\n",
      "gdict = {\"books\": [], \"books1\": [], \"books2\": []}\n",
      "mdict = {\"books\": [], \"books1\": [], \"books2\": []}\n",
      "models = [mbooks, mbooks1, mbooks2]\n",
      "gdictadd(models)\n",
      "mdictadd(models)\n",
      "for i in gdict:\n",
      "    print(\"%10s\" % i, \\\n",
      "          \"%.3f\" % bias(gdict[i][0], mdict[i][0], vocab, proflist), \\\n",
      "          \"%.3f\" % bias(gdict[i][0], mdict[i][0], vocab, adjectives), \\\n",
      "          \"%.3f\" % bias(gdict[i][0], mdict[i][0], vocab, grammar), \\\n",
      "          \"%.3f\" % bias(gdict[i][0], mdict[i][0], vocab, gender_specific))\n",
      "207/68:\n",
      "t = time.process_time()\n",
      "mpol1 = getw2v(politics + politics)\n",
      "time.process_time() - t\n",
      "207/69:\n",
      "t = time.process_time()\n",
      "mpol2 = getw2v(politics + politics + politics + politics)\n",
      "time.process_time() - t\n",
      "207/70:\n",
      "t = time.process_time()\n",
      "mmov1 = getw2v(movies + movies)\n",
      "time.process_time() - t\n",
      "207/71:\n",
      "t = time.process_time()\n",
      "mmov2 = getw2v(movies + movies + movies + movies)\n",
      "time.process_time() - t\n",
      "207/72:\n",
      "gdict = {\"mov\": [], \"mov1\": [], \"mov2\": []}\n",
      "mdict = {\"mov\": [], \"mov1\": [], \"mov2\": []}\n",
      "models = [mmov, mmov1, mmov2]\n",
      "gdictadd(models)\n",
      "mdictadd(models)\n",
      "for i in gdict:\n",
      "    print(\"%10s\" % i, \\\n",
      "          \"%.3f\" % bias(gdict[i][0], mdict[i][0], vocab, proflist), \\\n",
      "          \"%.3f\" % bias(gdict[i][0], mdict[i][0], vocab, adjectives), \\\n",
      "          \"%.3f\" % bias(gdict[i][0], mdict[i][0], vocab, grammar), \\\n",
      "          \"%.3f\" % bias(gdict[i][0], mdict[i][0], vocab, gender_specific))\n",
      "207/73:\n",
      "gdict = {\"pol\": [], \"pol1\": [], \"pol2\": []}\n",
      "mdict = {\"pol\": [], \"pol1\": [], \"pol2\": []}\n",
      "models = [mpol, mpol1, mpol2]\n",
      "gdictadd(models)\n",
      "mdictadd(models)\n",
      "for i in gdict:\n",
      "    print(\"%10s\" % i, \\\n",
      "          \"%.3f\" % bias(gdict[i][0], mdict[i][0], vocab, proflist), \\\n",
      "          \"%.3f\" % bias(gdict[i][0], mdict[i][0], vocab, adjectives), \\\n",
      "          \"%.3f\" % bias(gdict[i][0], mdict[i][0], vocab, grammar), \\\n",
      "          \"%.3f\" % bias(gdict[i][0], mdict[i][0], vocab, gender_specific))\n",
      "206/18: heshestats('his', 'her')\n",
      "206/19:\n",
      "def heshestats(w1, w2):\n",
      "    for m in [mpol, mbooks, mmov, msports]:\n",
      "        c1 = m.wv.vocab[w1].count\n",
      "        c2 = m.wv.vocab[w2].count\n",
      "        print(c1, c2, c2/c1)\n",
      "206/20: heshestats('his', 'her')\n",
      "206/21:\n",
      "def heshestats(w1, w1_, w2):\n",
      "    for m in [mpol, mbooks, mmov, msports]:\n",
      "        c1 = m.wv.vocab[w1].count + m.wv.vocab[w1_].count\n",
      "        c2 = m.wv.vocab[w2].count\n",
      "        print(c1, c2, c2/c1)\n",
      "206/22: heshestats('him', 'his', 'her')\n",
      "206/23: heshestats('his', 'hers')\n",
      "206/24:\n",
      "def heshestats(w1, w1_, w2, w2_):\n",
      "    for m in [mpol, mbooks, mmov, msports]:\n",
      "        c1 = m.wv.vocab[w1].count + m.wv.vocab[w1_].count\n",
      "        c2 = m.wv.vocab[w2].count + m.wv.vocab[w2_].count\n",
      "        print(c1, c2, c2/c1)\n",
      "206/25: heshestats('him', 'his', 'her', 'hers')\n",
      "206/26: heshestats('him', 'his', 'her', 'her')\n",
      "206/27: heshestats('him', 'his', 'her')\n",
      "206/28: heshestats('him', 'his', 'her', 'asdf')\n",
      "206/29: heshestats('him', 'his', 'her')\n",
      "206/30: heshestats('him', 'his', 'her', 'hers')\n",
      "206/31:\n",
      "def heshestats(w1, w1_, w2, w2_):\n",
      "    for m in [mpol, mbooks, mmov, msports]:\n",
      "        c1 = m.wv.vocab[w1].count + m.wv.vocab[w1_].count\n",
      "        c2 = m.wv.vocab[w2].count + m.wv.vocab[w2_].count\n",
      "        print(c1, c2, c2/c1)\n",
      "206/32: heshestats('him', 'his', 'her', 'hers')\n",
      "206/33:\n",
      "def heshestats(w1, w1_, w2, w2_):\n",
      "    for m in [mpol, mbooks, mmov, msports]:\n",
      "        c1 = m.wv.vocab[w1].count + m.wv.vocab[w1_].count\n",
      "        c2 = m.wv.vocab[w2].count + m.wv.vocab[w2_].count\n",
      "        print(m.wv.vocab[w2_].count)\n",
      "        print(c1, c2, c2/c1)\n",
      "206/34: heshestats('him', 'his', 'her', 'hers')\n",
      "206/35:\n",
      "def heshestats(w1, w1_, w2, w2_):\n",
      "    for m in [mpol, mbooks, mmov, msports]:\n",
      "        c1 = m.wv.vocab[w1].count + m.wv.vocab[w1_].count\n",
      "        c2 = m.wv.vocab[w2].count #+ m.wv.vocab[w2_].count\n",
      "        print(c1, c2, c2/c1)\n",
      "206/36: heshestats('him', 'his', 'her', 'hers')\n",
      "206/37:\n",
      "def heshestats(w1, w1_, w2, w2_):\n",
      "    for m in [mpol, mbooks, mmov, msports]:\n",
      "        c1 = m.wv.vocab[w1].count + m.wv.vocab[w1_].count\n",
      "        c2 = m.wv.vocab[w2].count + m.wv.vocab[w2_].count\n",
      "        print(c1, c2, c2/c1)\n",
      "206/38: heshestats('him', 'his', 'her', 'hers')\n",
      "206/39:\n",
      "def heshestats(w1, w2):\n",
      "    for m in [mpol, mbooks, mmov, msports]:\n",
      "        c1 = m.wv.vocab[w1].count #+ m.wv.vocab[w1_].count\n",
      "        c2 = m.wv.vocab[w2].count #+ m.wv.vocab[w2_].count\n",
      "        print(c1, c2, c2/c1)\n",
      "206/40: heshestats('boy', 'girl')\n",
      "207/74: books = getlist('Books and Literature')\n",
      "207/75: len(politics), len(movies), len(books), len(sports)\n",
      "207/76:\n",
      "bookslist = []\n",
      "for i in range(1, 21):\n",
      "    bookslist.append(sample(books, i*500))\n",
      "207/77: len(bookslist)\n",
      "207/78: bookslist[2]\n",
      "207/79: len(bookslist[2])\n",
      "207/80: len(bookslist[19])\n",
      "207/81:\n",
      "def getw2v(category):\n",
      "    sentences = []\n",
      "    for t in category:\n",
      "        sentences += nltk.sent_tokenize(t.text)\n",
      "    docs = [simple_preprocess(s) for s in sentences]\n",
      "    return Word2Vec(docs)\n",
      "207/82: booksmodels = [getw2v(b) for b in bookslist]\n",
      "207/83:\n",
      "def getg(m):\n",
      "    matrix = []\n",
      "    for a, b in pairs:\n",
      "        if a not in m.wv.vocab or b not in m.wv.vocab:\n",
      "            continue\n",
      "        center = (m.wv.get_vector(a) + m.wv.get_vector(b))/2\n",
      "        matrix.append(m.wv.get_vector(a) - center)\n",
      "        matrix.append(m.wv.get_vector(b) - center)\n",
      "    matrix = np.array(matrix)\n",
      "    p = PCA(n_components = 10)\n",
      "    p.fit(matrix)\n",
      "    # print(p.explained_variance_ratio_[0])\n",
      "#         plt.bar(range(10), p.explained_variance_ratio_)\n",
      "#         plt.title(\"Explained Variance vs Component Number (\" + t + \")\")\n",
      "    return p.components_[0]\n",
      "207/84:\n",
      "def bias(g, m, vocab, wordlist):\n",
      "    b = 0\n",
      "    l = list(filter(lambda x: x in vocab, wordlist))\n",
      "    for x in l:\n",
      "        b += abs(gproj(g, m, x))\n",
      "    return b/len(l)\n",
      "207/85: profs\n",
      "207/86: proflist\n",
      "207/87: proflist = list(filter(lambda x: x in mbooks.wv.vocab, [p[0] for p in profs if abs(p[1]) < 0.6]))\n",
      "207/88: proflist\n",
      "207/89: profs\n",
      "207/90: proflist = list(filter(lambda x: x in mbooks.wv.vocab, [p[0] for p in profs]))\n",
      "207/91: bias(g, m, vocab, wordlist)\n",
      "207/92: bias(getg(bookslist[0]), bookslist[0], bookslist[0].wv.vocab, proflist)\n",
      "207/93: bias(getg(booksmodels[0]), booksmodels[0], booksmodels[0].wv.vocab, proflist)\n",
      "207/94:\n",
      "bookbias = []\n",
      "for bm in booksmodels:\n",
      "    b = bias(getg(bm), bm, bm.wv.vocab, proflist)\n",
      "    print(b)\n",
      "    bookbias.append(b)\n",
      "207/95: plt.plot(bookbias)\n",
      "207/96: plt.plot(bookbias[1:])\n",
      "207/97:\n",
      "plt.plot([1:20]*20, bookbias[1:])\n",
      "plt.title(\"Projection Bias vs \")\n",
      "207/98: 1:20\n",
      "207/99: [1:20]\n",
      "207/100: [range(1,20)]\n",
      "207/101: list(range(1,20))\n",
      "207/102:\n",
      "plt.plot(list(range(1,21))*500, bookbias[1:])\n",
      "plt.title(\"Projection Bias vs \")\n",
      "207/103:\n",
      "plt.plot(list(range(2,21))*500, bookbias[1:])\n",
      "plt.title(\"Projection Bias vs \")\n",
      "207/104:\n",
      "plt.plot(list(range(2,21)), bookbias[1:])\n",
      "plt.title(\"Projection Bias vs \")\n",
      "207/105: list(range(2,21))*2\n",
      "207/106: list(range(2,21)*2)\n",
      "207/107: list(range(2,21)).*2\n",
      "207/108:\n",
      "plt.plot([i*500 for i in range(2,21)], bookbias[1:])\n",
      "plt.title(\"Projection Bias vs \")\n",
      "207/109:\n",
      "plt.plot([i*500 for i in range(2,21)], bookbias[1:])\n",
      "plt.title(\"Projection Bias vs Number of Articles\")\n",
      "207/110: politics = getlist('Politics and Government')\n",
      "207/111: movies = getlist('Motion Pictures')\n",
      "207/112: len(politics), len(movies), len(books), len(sports)\n",
      "207/113:\n",
      "plist = []\n",
      "for i in range(2, 20):\n",
      "    plist.append(sample(politics, i*500))\n",
      "plist.append(politics)\n",
      "207/114: pmodels = [getw2v(p) for p in plist]\n",
      "207/115:\n",
      "pbias = []\n",
      "for m in pmodels:\n",
      "    b = bias(getg(m), m, m.wv.vocab, proflist)\n",
      "    print(b)\n",
      "    pbias.append(b)\n",
      "207/116:\n",
      "plt.plot([i*500 for i in range(2,21)], bookbias[1:], label=\"books\")\n",
      "plt.plot([i*500 for i in range(2,21)], pbias[1:], label=\"politics\")\n",
      "plt.legend()\n",
      "plt.title(\"Projection Bias vs Number of Articles\")\n",
      "207/117:\n",
      "plt.plot([i*500 for i in range(2,21)], bookbias[1:], label=\"books\")\n",
      "plt.plot([i*500 for i in range(2,21)], pbias, label=\"politics\")\n",
      "plt.legend()\n",
      "plt.title(\"Projection Bias vs Number of Articles\")\n",
      "207/118:\n",
      "plt.plot([i*500 for i in range(3,21)], bookbias[2:], label=\"books\")\n",
      "plt.plot([i*500 for i in range(3,21)], pbias, label=\"politics\")\n",
      "plt.legend()\n",
      "plt.title(\"Projection Bias vs Number of Articles\")\n",
      "207/119:\n",
      "plt.plot([i*500 for i in range(3,21)], bookbias[2:], label=\"books\")\n",
      "plt.plot([i*500 for i in range(3,21)], pbias[1:], label=\"politics\")\n",
      "plt.legend()\n",
      "plt.title(\"Projection Bias vs Number of Articles\")\n",
      "207/120:\n",
      "plt.plot([i*500 for i in range(3,21)], bookbias[2:], label=\"books\")\n",
      "plt.plot([i*500 for i in range(3,21)], pbias[1:], label=\"politics\")\n",
      "plt.legend()\n",
      "plt.title(\"Projection Bias vs Number of Articles in Training Set\")\n",
      "207/121: books1 = sample(books, 2000)\n",
      "207/122:\n",
      "def heshestats(w1, w2):\n",
      "    for m in [mpol, mbooks, mmov, msports]:\n",
      "        c1 = m.wv.vocab[w1].count #+ m.wv.vocab[w1_].count\n",
      "        c2 = m.wv.vocab[w2].count #+ m.wv.vocab[w2_].count\n",
      "        print(c1, c2, c2/c1)\n",
      "207/123:\n",
      "def heshestats(m, w1, w2):\n",
      "    c1 = m.wv.vocab[w1].count #+ m.wv.vocab[w1_].count\n",
      "    c2 = m.wv.vocab[w2].count #+ m.wv.vocab[w2_].count\n",
      "    print(c1, c2, c2/c1)\n",
      "207/124:\n",
      "b1 = sample(books, 2000)\n",
      "mb1 = getw2v(b1)\n",
      "heshestats(mb1, 'he', 'she')\n",
      "207/125:\n",
      "b2 = sample(books, 2000)\n",
      "mb2 = getw2v(b2)\n",
      "heshestats(mb2, 'he', 'she')\n",
      "207/126: b1\n",
      "207/127: b1.text\n",
      "207/128: [b.text for b in b1]\n",
      "207/129:\n",
      "lists = []\n",
      "for c in alldlist[:20]:\n",
      "    print(c[0], c[1])\n",
      "    lists.append(getlist(c[0]))\n",
      "207/130:\n",
      "lists = []\n",
      "for c in alldlist[:20]:\n",
      "    l = getlist(c[0])\n",
      "    print(c[0], len(l))\n",
      "    lists.append(l)\n",
      "207/131:\n",
      "lists = []\n",
      "for c in alldlist[:25]:\n",
      "    l = getlist(c[0])\n",
      "    print(c[0], len(l))\n",
      "    lists.append(l)\n",
      "207/132:\n",
      "def exclude(e, x):\n",
      "    for d in x.descriptors:\n",
      "        if d == e:\n",
      "            continue\n",
      "        if d in [t[0] for t in alldlist[:20]]:\n",
      "            return False\n",
      "    return True\n",
      "207/133:\n",
      "lists = []\n",
      "for c in alldlist[:25]:\n",
      "    l = getlist(c[0])\n",
      "    print(c[0], len(l))\n",
      "    lists.append(l)\n",
      "207/134:\n",
      "lists = []\n",
      "for c in alldlist[:30]:\n",
      "    l = getlist(c[0])\n",
      "    if len(l) > 2500:\n",
      "        print(c[0], len(l))\n",
      "        lists.append(l)\n",
      "207/135:\n",
      "lists = []\n",
      "for c in alldlist[:30]:\n",
      "    l = getlist(c[0])\n",
      "    if len(l) > 2500:\n",
      "        print(c[0], len(l))\n",
      "        lists.append(l)\n",
      "len(lists)\n",
      "207/136:\n",
      "lists = []\n",
      "for c in alldlist[:30]:\n",
      "    l = getlist(c[0])\n",
      "    if len(l) > 3000:\n",
      "        print(c[0], len(l))\n",
      "        lists.append(l)\n",
      "len(lists)\n",
      "207/137:\n",
      "lists = []\n",
      "for c in alldlist[:30]:\n",
      "    l = getlist(c[0])\n",
      "    if len(l) > 2900:\n",
      "        print(c[0], len(l))\n",
      "        lists.append(l)\n",
      "len(lists)\n",
      "207/138:\n",
      "lists = []\n",
      "for c in alldlist[:30]:\n",
      "    l = getlist(c[0])\n",
      "    if len(l) > 2600:\n",
      "        print(c[0], len(l))\n",
      "        lists.append(l)\n",
      "len(lists)\n",
      "207/139:\n",
      "lists = []\n",
      "for c in alldlist[:30]:\n",
      "    l = getlist(c[0])\n",
      "    if len(l) > 2400:\n",
      "        print(c[0], len(l))\n",
      "        lists.append(l)\n",
      "len(lists)\n",
      "207/140:\n",
      "lists = []\n",
      "for c in alldlist[:30]:\n",
      "    l = getlist(c[0])\n",
      "    if len(l) > 2000:\n",
      "        print(c[0], len(l))\n",
      "        lists.append(l)\n",
      "len(lists)\n",
      "207/141:\n",
      "def exclude(e, x):\n",
      "    for d in x.descriptors:\n",
      "        if d == e:\n",
      "            continue\n",
      "        if d in [t[0] for t in alldlist[:30]]:\n",
      "            return False\n",
      "    return True\n",
      "207/142:\n",
      "lists = []\n",
      "for c in alldlist[:30]:\n",
      "    l = getlist(c[0])\n",
      "    if len(l) > 2000:\n",
      "        print(c[0], len(l))\n",
      "        lists.append(l)\n",
      "len(lists)\n",
      "207/143:\n",
      "ms = []\n",
      "for l in lists:\n",
      "    m = getw2v(sample(l, 2000))\n",
      "    heshestats(m, 'he', 'she')\n",
      "207/144:\n",
      "ms = []\n",
      "for l in lists:\n",
      "    m = getw2v(sample(l, 2000))\n",
      "    heshestats(m, 'he', 'she')\n",
      "    ms.append(m)\n",
      "207/145:\n",
      "def heshestats(m, w1, w2):\n",
      "    c1 = m.wv.vocab[w1].count #+ m.wv.vocab[w1_].count\n",
      "    c2 = m.wv.vocab[w2].count #+ m.wv.vocab[w2_].count\n",
      "    print(c1, c2, c2/c1)\n",
      "    return c2/c1\n",
      "207/146:\n",
      "def heshestats(m, w1, w2):\n",
      "    c1 = m.wv.vocab[w1].count #+ m.wv.vocab[w1_].count\n",
      "    c2 = m.wv.vocab[w2].count #+ m.wv.vocab[w2_].count\n",
      "    #print(c1, c2, c2/c1)\n",
      "    return c2/c1\n",
      "207/147:\n",
      "heshes = []\n",
      "for m in ms:\n",
      "    heshes.append(heshestats(m, 'he', 'she'))\n",
      "207/148:\n",
      "biases = []\n",
      "for m in ms:\n",
      "    b = bias(getg(m), m, m.wv.vocab, proflist)\n",
      "    print(b, heshestats(m, 'he', 'she'))\n",
      "    biases.append(b)\n",
      "207/149: plt.plot(heshes, biases)\n",
      "207/150: plt.plot(heshes, biases, \"o\")\n",
      "210/1:\n",
      "# Implement logistic regression with SGD\n",
      "# h is the handle you defined above\n",
      "def sgd(X, y, h):\n",
      "    epoch_number = 0\n",
      "    loss, error, grad, eta = h['loss'], h['error'], h['grad'], h['eta']\n",
      "    epochs, bs = h['epochs'], h['batch_size']\n",
      "    eps, pstr = h['eps'], h['pstr']\n",
      "    n, d = X.shape\n",
      "    nbs = int(n / bs)\n",
      "    sampler = IndexSampler(nbs)\n",
      "\n",
      "    w = np.zeros(d)\n",
      "    losses = [loss(X,y,w)]\n",
      "    errors = [error(X,y,w)]\n",
      "    gamma = 0.6\n",
      "    for e in range(1, epochs * nbs):\n",
      "        # these two lines get a batch of examples and labels for you\n",
      "        head = sampler.sample_new_index(replace=0) * bs\n",
      "        Xt, yt = X[head:head + bs], y[head:head + bs]\n",
      "\n",
      "        w_new = w - (grad(Xt, yt, w) * eta[epoch_number])\n",
      "        w = w_new\n",
      "#         w = (1-gamma) * w + gamma * w_new\n",
      "        if e % nbs == 0:\n",
      "            epoch_number += 1\n",
      "            errors.append(error(X,y,w))\n",
      "            losses.append(loss(X,y,w))\n",
      "            print(pstr.format(e // nbs, losses[-1], errors[-1] * 100))\n",
      "            if terminate(losses[-2], losses[-1], eps): break\n",
      "    print('\\n')\n",
      "    return w, losses, errors\n",
      "210/2: [w_sgd, loss_sgd, error_sgd] = sgd(Xtr, ytr8, prepare_logistic_sgd_handle())\n",
      "210/3:\n",
      "# Import the MNIST data\n",
      "from tensorflow.keras.datasets import mnist as keras_mnist\n",
      "(X_train, y_train), (X_test, y_test) = keras_mnist.load_data()\n",
      "210/4:\n",
      "import numpy as np\n",
      "\n",
      "# Calculate loss from margins\n",
      "# Here, z is equal to y times X dot w\n",
      "# That is, z_i = y_i * x_i dot w\n",
      "def logloss_from_z(z):\n",
      "    return np.mean(np.log(1+np.exp(-z)))\n",
      "\n",
      "# Calculate error from margins\n",
      "# the error is the proportion of examples that you \"get wrong\".\n",
      "# a.k.a. the examples where you assign less than 50% chance to the correct label\n",
      "# This is equivalent to the proportion of examples where z_i is less than 0\n",
      "def error_from_z(z):\n",
      "    return np.mean(z <= 0)\n",
      "\n",
      "def get_z(X, y, w):\n",
      "    temp = (X @ w)\n",
      "    temp = temp.reshape(y.shape)\n",
      "    return y * temp\n",
      "    \n",
      "# Calculate loss from parameter vector\n",
      "def logloss(X, y, w):\n",
      "    return logloss_from_z(get_z(X, y, w))\n",
      "\n",
      "# Calculate error from parameter vector\n",
      "def error(X, y, w):\n",
      "    return error_from_z(get_z(X, y, w))\n",
      "\n",
      "def get_q(X, y, w):\n",
      "    return 1 / (1 + np.exp(get_z(X, y, w)))\n",
      "    \n",
      "# Gradient of LogLoss w.r.t. w\n",
      "def logloss_gradient(X, y, w):\n",
      "    q = get_q(X, y, w)\n",
      "    return -np.mean((q * y) * X,axis=0)\n",
      "\n",
      "# Calculate error & loss from parameter vector\n",
      "# Return two floats in the format (loss, error)\n",
      "def logloss_and_error(X, y, w):\n",
      "    z = get_z(X, y, w)\n",
      "    return logloss_from_z(z), error_from_z(z)\n",
      "210/5:\n",
      "# Calculate the initial bias\n",
      "def init_bias(pos, neg):\n",
      "    return np.log(pos / neg)\n",
      "210/6:\n",
      "def flatten_images(X):\n",
      "    n, x, y = np.shape(X)\n",
      "    return np.reshape(X, (n, (x * y)))\n",
      "  \n",
      "# test flatten_images\n",
      "testArray = np.random.random((2, 3, 4))\n",
      "reshaped = flatten_images(testArray)\n",
      "a,b,c = np.shape(testArray)\n",
      "d,e = np.shape(reshaped)\n",
      "assert(a * b * c == d * e)\n",
      "210/7:\n",
      "# Normalize each example as described above. If bias is non-zero, add a bias term by appending `bias` to\n",
      "# each example\n",
      "\n",
      "def function(x):\n",
      "    m = np.mean(x)\n",
      "    s = np.std(x)\n",
      "    return (x - m) / s\n",
      "\n",
      "def normalize(X, bias=0):\n",
      "    newArray = np.apply_along_axis(function, 1, X)\n",
      "    \n",
      "    if bias != 0:\n",
      "        n, d = np.shape(newArray)\n",
      "        biasVector = np.full((n, 1), bias)\n",
      "        newArray = np.append(newArray, biasVector, axis = 1)\n",
      "        \n",
      "    return newArray\n",
      "210/8:\n",
      "# Normalize and flatten the training and test images\n",
      "# You may print Xtr and Xte, or view their shape, to get a feeling for their structure\n",
      "Xtr = normalize(flatten_images(X_train), bias=1)\n",
      "Xte = normalize(flatten_images(X_test), bias=1)\n",
      "210/9:\n",
      "ytr8 = (2 * (y_train == 8) - 1).reshape(len(y_train), 1)\n",
      "yte8 = (2 * (y_test == 8) - 1).reshape(len(y_test), 1)\n",
      "210/10:\n",
      "# `x` is the j-th column of X (a.k.a. the v_j from above)\n",
      "# `y` is the vector of labels\n",
      "# `z` is a vector with entries z_i\n",
      "# `cj` is c_j from above, the squared norm of `x`\n",
      "def delta_wj(x, y, z, cj):\n",
      "    q = 1/(1 + np.exp(z))\n",
      "    return 4/cj * np.sum(q * y * x)\n",
      "210/11:\n",
      "from numpy.random import randint\n",
      "from numpy.random import permutation\n",
      "\n",
      "# You don't need to use this class yourself: we provided the index-sampling code for you\n",
      "# sample new index (with or without replacement)\n",
      "# d is the max index\n",
      "class IndexSampler:\n",
      "    def __init__(self, d):\n",
      "        self.d = d\n",
      "        self.prm = None\n",
      "    \n",
      "    def sample_new_index(self, replace = 1):\n",
      "        if replace:\n",
      "            return randint(self.d)\n",
      "        if self.prm is None:\n",
      "            self.prm = permutation(self.d)\n",
      "            self.head = 0\n",
      "        ind = self.prm[self.head]\n",
      "        self.head += 1\n",
      "        if self.head == self.d:\n",
      "            self.head = 0\n",
      "            self.prm = None\n",
      "        return ind\n",
      "210/12:\n",
      "# epochs is maximum number of epochs to train\n",
      "# eps is your termination condition number, similar to in PA2's linear regression\n",
      "# Every epoch, report the loss (we've provided some code that reports loss for you)\n",
      "# (An epoch consists of d updates)\n",
      "\n",
      "### ACT 16\n",
      "def logistic_regression_cd(X, y, epochs=100, eps=0.001):\n",
      "    pstr = 'Epoch: {0:2d}  Loss: {1:5.3f}  Error: {2:5.3f}'\n",
      "    n, d = X.shape\n",
      "    \n",
      "    y=np.reshape(y, (y.shape[0]))\n",
      "    # initialize\n",
      "    w = np.zeros(d)\n",
      "    w[-1] = init_bias(np.sum(y > 0), np.sum(y < 0)) # initialize the last entry of w as initial bias\n",
      "\n",
      "    z = get_z(X, y, w)\n",
      "    c = np.square(np.linalg.norm(X, axis = 0))\n",
      "    losses = [logloss_from_z(z)]\n",
      "    errors = [error_from_z(z)]\n",
      "    X_col = X.T\n",
      "    \n",
      "    cur_epoch = 0\n",
      "    sampler = IndexSampler(d)\n",
      "    for e in range(1, d * epochs + 1):\n",
      "        j = sampler.sample_new_index()\n",
      "        w[j] += delta_wj(X_col[j], y, z, c[j])\n",
      "        z = get_z(X, y, w)\n",
      "        if e % d == 0:\n",
      "            losses.append(logloss_from_z(z))\n",
      "            errors.append(error_from_z(z))\n",
      "            cur_epoch += 1\n",
      "            print(pstr.format(cur_epoch, losses[-1], errors[-1] * 100))\n",
      "            if (losses[-2] - losses[-1]) / losses[-1] < eps: break\n",
      "    print('\\n')\n",
      "    return w, losses, errors\n",
      "210/13: import matplotlib.pyplot as plt\n",
      "210/14:\n",
      "#SGD handler\n",
      "def prepare_logistic_sgd_handle():\n",
      "    h = dict()\n",
      "    h['pstr'] = 'Epoch: {0:2d}  Loss: {1:5.3f}  Error: {2:5.3f}'\n",
      "    h['epochs'] = 100\n",
      "    h['eta'] = 1/np.sqrt((10 + np.arange(h['epochs']))) #0.05 * np.ones(h['epochs'])\n",
      "    h['grad'] = logloss_gradient\n",
      "    h['loss'] = logloss\n",
      "    h['error'] = error\n",
      "    h['batch_size'] = 1000\n",
      "    h['eps'] = 0.001\n",
      "    return h\n",
      "210/15:\n",
      "# Implement the termination condition (it can be similar to your\n",
      "#   termination condition for coordinate descent)\n",
      "# We recommend you don't terminate when c_loss is greater than p_loss, though\n",
      "def terminate(p_loss, c_loss, eps):\n",
      "    return c_loss < p_loss and (p_loss - c_loss) / c_loss < eps\n",
      "210/16:\n",
      "# Implement logistic regression with SGD\n",
      "# h is the handle you defined above\n",
      "def sgd(X, y, h):\n",
      "    epoch_number = 0\n",
      "    loss, error, grad, eta = h['loss'], h['error'], h['grad'], h['eta']\n",
      "    epochs, bs = h['epochs'], h['batch_size']\n",
      "    eps, pstr = h['eps'], h['pstr']\n",
      "    n, d = X.shape\n",
      "    nbs = int(n / bs)\n",
      "    sampler = IndexSampler(nbs)\n",
      "\n",
      "    w = np.zeros(d)\n",
      "    losses = [loss(X,y,w)]\n",
      "    errors = [error(X,y,w)]\n",
      "    gamma = 0.6\n",
      "    for e in range(1, epochs * nbs):\n",
      "        # these two lines get a batch of examples and labels for you\n",
      "        head = sampler.sample_new_index(replace=0) * bs\n",
      "        Xt, yt = X[head:head + bs], y[head:head + bs]\n",
      "\n",
      "        w_new = w - (grad(Xt, yt, w) * eta[epoch_number])\n",
      "        w = w_new\n",
      "#         w = (1-gamma) * w + gamma * w_new\n",
      "        if e % nbs == 0:\n",
      "            epoch_number += 1\n",
      "            errors.append(error(X,y,w))\n",
      "            losses.append(loss(X,y,w))\n",
      "            print(pstr.format(e // nbs, losses[-1], errors[-1] * 100))\n",
      "            if terminate(losses[-2], losses[-1], eps): break\n",
      "    print('\\n')\n",
      "    return w, losses, errors\n",
      "210/17: [w_sgd, loss_sgd, error_sgd] = sgd(Xtr, ytr8, prepare_logistic_sgd_handle())\n",
      "207/151:\n",
      "plt.plot(heshes, biases, \"o\")\n",
      "plt.title(\"Ratio of 'she' to 'he' vs Projection Bias on Professions\")\n",
      "plt.annotate(label, # this is the text\n",
      "                 (x,y), # this is the point to label\n",
      "                 textcoords=\"offset points\", # how to position the text\n",
      "                 xytext=(0,10), # distance from text to points (x,y)\n",
      "                 ha='center') # horizontal alignment can be left, right or center\n",
      "plt.show()\n",
      "207/152:\n",
      "plt.plot([i*500 for i in range(3,21)], bookbias[2:], label=\"books\")\n",
      "plt.plot([i*500 for i in range(3,21)], pbias[1:], label=\"politics\")\n",
      "plt.legend()\n",
      "plt.title(\"Projection Bias on Professions vs Number of Articles in Training Set\")\n",
      "plt.show()\n",
      "207/153:\n",
      "biases = []\n",
      "for m in ms:\n",
      "    b = bias(getg(m), m, m.wv.vocab, proflist)\n",
      "    print(heshestats(m, 'he', 'she'), b)\n",
      "    biases.append(b)\n",
      "207/154:\n",
      "plt.plot(heshes, biases, \"o\")\n",
      "plt.title(\"Ratio of 'she' to 'he' vs Projection Bias on Professions\")\n",
      "plt.annotate(\"Automobiles\", (0.19,0.34), textcoords=\"offset points\", xytext=(0,10), # distance from text to points (x,y)\n",
      "             ha='center') # horizontal alignment can be left, right or center\n",
      "plt.show()\n",
      "207/155:\n",
      "plt.plot(heshes, biases, \"o\")\n",
      "plt.title(\"Ratio of 'she' to 'he' vs Projection Bias on Professions\")\n",
      "plt.annotate(\"Automobiles\", (0.19,0.34), textcoords=\"offset points\", xytext=(10,0), # distance from text to points (x,y)\n",
      "             ha='center') # horizontal alignment can be left, right or center\n",
      "plt.show()\n",
      "207/156:\n",
      "plt.plot(heshes, biases, \"o\")\n",
      "plt.title(\"Ratio of 'she' to 'he' vs Projection Bias on Professions\")\n",
      "plt.annotate(\"Automobiles\", (0.19,0.34), textcoords=\"offset points\", xytext=(20,0), # distance from text to points (x,y)\n",
      "             ha='center') # horizontal alignment can be left, right or center\n",
      "plt.show()\n",
      "207/157:\n",
      "plt.plot(heshes, biases, \"o\")\n",
      "plt.title(\"Ratio of 'she' to 'he' vs Projection Bias on Professions\")\n",
      "plt.annotate(\"Automobiles\", (0.19,0.34), textcoords=\"offset points\", xytext=(50,0), # distance from text to points (x,y)\n",
      "             ha='center') # horizontal alignment can be left, right or center\n",
      "plt.show()\n",
      "207/158:\n",
      "plt.plot(heshes, biases, \"o\")\n",
      "plt.title(\"Ratio of 'she' to 'he' vs Projection Bias on Professions\")\n",
      "plt.annotate(\"Automobiles\", (0.19,0.34), textcoords=\"offset points\", xytext=(50,0), ha='right') # horizontal alignment can be left, right or center\n",
      "plt.show()\n",
      "207/159:\n",
      "plt.plot(heshes, biases, \"o\")\n",
      "plt.title(\"Ratio of 'she' to 'he' vs Projection Bias on Professions\")\n",
      "plt.annotate(\"Automobiles\", (0.19,0.34), textcoords=\"offset points\", xytext=(50,0)) # horizontal alignment can be left, right or center\n",
      "plt.show()\n",
      "207/160:\n",
      "plt.plot(heshes, biases, \"o\")\n",
      "plt.title(\"Ratio of 'she' to 'he' vs Projection Bias on Professions\")\n",
      "plt.annotate(\"Automobiles\", (0.19,0.34), textcoords=\"offset points\", xytext=(10,0)) # horizontal alignment can be left, right or center\n",
      "plt.show()\n",
      "207/161:\n",
      "plt.plot(heshes, biases, \"o\")\n",
      "plt.title(\"Ratio of 'she' to 'he' vs Projection Bias on Professions\")\n",
      "plt.annotate(\"Automobiles\", (0.19,0.34), textcoords=\"offset points\", xytext=(5,0)) # horizontal alignment can be left, right or center\n",
      "plt.show()\n",
      "207/162:\n",
      "plt.plot(heshes, biases, \"o\")\n",
      "plt.title(\"Ratio of 'she' to 'he' vs Projection Bias on Professions\")\n",
      "plt.annotate(\"Automobiles\", (0.19,0.34), textcoords=\"offset points\", xytext=(5,0))\n",
      "plt.annotate(\"Weddings\", (1.01,0.15), textcoords=\"offset points\", xytext=(-5,0))\n",
      "\n",
      "plt.show()\n",
      "207/163:\n",
      "plt.plot(heshes, biases, \"o\")\n",
      "plt.title(\"Ratio of 'she' to 'he' vs Projection Bias on Professions\")\n",
      "plt.annotate(\"Automobiles\", (0.19,0.34), textcoords=\"offset points\", xytext=(5,0))\n",
      "plt.annotate(\"Weddings\", (1.01,0.15), textcoords=\"offset points\", xytext=(-20,0))\n",
      "\n",
      "plt.show()\n",
      "207/164:\n",
      "plt.plot(heshes, biases, \"o\")\n",
      "plt.title(\"Ratio of 'she' to 'he' vs Projection Bias on Professions\")\n",
      "plt.annotate(\"Automobiles\", (0.19,0.34), textcoords=\"offset points\", xytext=(5,0))\n",
      "plt.annotate(\"Weddings\", (1.01,0.15), textcoords=\"offset points\", xytext=(-50,0))\n",
      "\n",
      "plt.show()\n",
      "207/165:\n",
      "plt.plot(heshes, biases, \"o\")\n",
      "plt.title(\"Ratio of 'she' to 'he' vs Projection Bias on Professions\")\n",
      "plt.annotate(\"Automobiles\", (0.19,0.34), textcoords=\"offset points\", xytext=(5,0))\n",
      "plt.annotate(\"Weddings\", (1.01,0.15), textcoords=\"offset points\", xytext=(-55,0))\n",
      "\n",
      "plt.show()\n",
      "207/166:\n",
      "plt.plot(heshes, biases, \"o\")\n",
      "plt.title(\"Ratio of 'she' to 'he' vs Projection Bias on Professions\")\n",
      "plt.annotate(\"Automobiles\", (0.19,0.34), textcoords=\"offset points\", xytext=(5,0))\n",
      "plt.annotate(\"Weddings\", (1.01,0.15), textcoords=\"offset points\", xytext=(-52,0))\n",
      "\n",
      "plt.show()\n",
      "207/167:\n",
      "plt.plot(heshes, biases, \"o\")\n",
      "plt.title(\"Ratio of 'she' to 'he' vs Projection Bias on Professions\")\n",
      "plt.annotate(\"Automobiles\", (0.19,0.34), textcoords=\"offset points\", xytext=(5,0))\n",
      "plt.annotate(\"Weddings\", (1.01,0.149), textcoords=\"offset points\", xytext=(-52,0))\n",
      "\n",
      "plt.show()\n",
      "207/168:\n",
      "biases = []\n",
      "i = 0\n",
      "for m in ms:\n",
      "    b = bias(getg(m), m, m.wv.vocab, proflist)\n",
      "    print(heshestats(m, 'he', 'she'), b)\n",
      "    print(alldlist[i][0])\n",
      "    i += 1\n",
      "    biases.append(b)\n",
      "207/169:\n",
      "biases = []\n",
      "i = 0\n",
      "for m in ms:\n",
      "    b = bias(getg(m), m, m.wv.vocab, proflist)\n",
      "    print(alldlist[i][0], heshestats(m, 'he', 'she'), b)\n",
      "    i += 1\n",
      "    biases.append(b)\n",
      "207/170:\n",
      "biases = []\n",
      "i = 0\n",
      "for m in ms:\n",
      "    b = bias(getg(m), m, m.wv.vocab, proflist)\n",
      "    print(heshestats(m, 'he', 'she'), b, alldlist[i][0])\n",
      "    i += 1\n",
      "    biases.append(b)\n",
      "207/171:\n",
      "biases = []\n",
      "i = 0\n",
      "labels = [x[0] for x in alldlist[i]]\n",
      "for m in ms:\n",
      "    b = bias(getg(m), m, m.wv.vocab, proflist)\n",
      "    print(heshestats(m, 'he', 'she'), b, alldlist[i][0])\n",
      "    i += 1\n",
      "    biases.append(b)\n",
      "207/172:\n",
      "biases = []\n",
      "i = 0\n",
      "labels = [x[0] for x in alldlist]\n",
      "for m in ms:\n",
      "    b = bias(getg(m), m, m.wv.vocab, proflist)\n",
      "    print(heshestats(m, 'he', 'she'), b, alldlist[i][0])\n",
      "    i += 1\n",
      "    biases.append(b)\n",
      "207/173:\n",
      "def coords(label):\n",
      "    i = labels.index(label)\n",
      "    return (heshes[i], biases[i])\n",
      "plt.plot(heshes, biases, \"o\")\n",
      "plt.title(\"Ratio of 'she' to 'he' vs Projection Bias on Professions\")\n",
      "plt.annotate(\"Automobiles\", coords(\"Automobiles\"), textcoords=\"offset points\", xytext=(5,0))\n",
      "plt.annotate(\"Weddings\", (1.01,0.149), textcoords=\"offset points\", xytext=(-52,0))\n",
      "plt.annotate(\"Baseball\", (1.01,0.149), textcoords=\"offset points\", xytext=(-52,0))\n",
      "plt.annotate(\"Football\", (1.01,0.149), textcoords=\"offset points\", xytext=(-52,0))\n",
      "plt.annotate(\"Basketball\", (1.01,0.149), textcoords=\"offset points\", xytext=(-52,0))\n",
      "plt.annotate(\"International Relations\", (1.01,0.149), textcoords=\"offset points\", xytext=(-52,0))\n",
      "plt.annotate(\"Children\", (1.01,0.149), textcoords=\"offset points\", xytext=(-52,0))\n",
      "plt.annotate(\"Education\", (1.01,0.149), textcoords=\"offset points\", xytext=(-52,0))\n",
      "plt.annotate(\"Computers\", (1.01,0.149), textcoords=\"offset points\", xytext=(-52,0))\n",
      "plt.annotate(\"Politics\", (1.01,0.149), textcoords=\"offset points\", xytext=(-52,0))\n",
      "plt.annotate(\"Travel\", (1.01,0.149), textcoords=\"offset points\", xytext=(-52,0))\n",
      "plt.annotate(\"Art\", (1.01,0.149), textcoords=\"offset points\", xytext=(-52,0))\n",
      "plt.show()\n",
      "207/174:\n",
      "def coords(label):\n",
      "    i = labels.index(label)\n",
      "    return (heshes[i], biases[i])\n",
      "plt.plot(heshes, biases, \"o\")\n",
      "plt.title(\"Ratio of 'she' to 'he' vs Projection Bias on Professions\")\n",
      "plt.annotate(\"Automobiles\", coords(\"Automobiles\"), textcoords=\"offset points\", xytext=(5,0))\n",
      "plt.annotate(\"Weddings\", coords(\"Weddings\"), textcoords=\"offset points\", xytext=(-52,0))\n",
      "plt.annotate(\"Baseball\", coords(\"Baseball\"), textcoords=\"offset points\", xytext=(-52,0))\n",
      "plt.annotate(\"Football\", coords(\"Football\"), textcoords=\"offset points\", xytext=(-52,0))\n",
      "plt.annotate(\"Basketball\", coords(\"Basketball\"), textcoords=\"offset points\", xytext=(-52,0))\n",
      "plt.annotate(\"International Relations\", coords(\"International Relations\"), textcoords=\"offset points\", xytext=(-52,0))\n",
      "plt.annotate(\"Children\", coords(\"Children\"), textcoords=\"offset points\", xytext=(-52,0))\n",
      "plt.annotate(\"Education\", coords(\"Education\"), textcoords=\"offset points\", xytext=(-52,0))\n",
      "plt.annotate(\"Computers\", coords(\"Computers\"), textcoords=\"offset points\", xytext=(-52,0))\n",
      "plt.annotate(\"Politics\", coords(\"Politics\"), textcoords=\"offset points\", xytext=(-52,0))\n",
      "plt.annotate(\"Travel\", coords(\"Travel\"), textcoords=\"offset points\", xytext=(-52,0))\n",
      "plt.annotate(\"Art\", coords(\"Art\"), textcoords=\"offset points\", xytext=(-52,0))\n",
      "plt.show()\n",
      "207/175: coords(\"Automobiles\")\n",
      "207/176: labels\n",
      "207/177: labels.index(\"Automobiles\")\n",
      "207/178:\n",
      "biases = []\n",
      "i = 0\n",
      "labels = []\n",
      "for m in ms:\n",
      "    labels.append(alldlist[i][0])\n",
      "    b = bias(getg(m), m, m.wv.vocab, proflist)\n",
      "    print(heshestats(m, 'he', 'she'), b, alldlist[i][0])\n",
      "    i += 1\n",
      "    biases.append(b)\n",
      "207/179: labels.index(\"Automobiles\")\n",
      "207/180:\n",
      "lists = []\n",
      "labels = []\n",
      "for c in alldlist[:30]:\n",
      "    l = getlist(c[0])\n",
      "    if len(l) > 2000:\n",
      "        print(c[0], len(l))\n",
      "        lists.append(l)\n",
      "        labels.append(c[0])\n",
      "len(lists)\n",
      "207/181:\n",
      "biases = []\n",
      "i = 0\n",
      "for m in ms:\n",
      "    b = bias(getg(m), m, m.wv.vocab, proflist)\n",
      "    print(heshestats(m, 'he', 'she'), b, labels[i])\n",
      "    i += 1\n",
      "    biases.append(b)\n",
      "207/182: labels.index(\"Automobiles\")\n",
      "207/183:\n",
      "def coords(label):\n",
      "    i = labels.index(label)\n",
      "    return (heshes[i], biases[i])\n",
      "plt.plot(heshes, biases, \"o\")\n",
      "plt.title(\"Ratio of 'she' to 'he' vs Projection Bias on Professions\")\n",
      "plt.annotate(\"Automobiles\", coords(\"Automobiles\"), textcoords=\"offset points\", xytext=(5,0))\n",
      "plt.annotate(\"Weddings\", coords(\"Weddings\"), textcoords=\"offset points\", xytext=(-52,0))\n",
      "plt.annotate(\"Baseball\", coords(\"Baseball\"), textcoords=\"offset points\", xytext=(-52,0))\n",
      "plt.annotate(\"Football\", coords(\"Football\"), textcoords=\"offset points\", xytext=(-52,0))\n",
      "plt.annotate(\"Basketball\", coords(\"Basketball\"), textcoords=\"offset points\", xytext=(-52,0))\n",
      "plt.annotate(\"International Relations\", coords(\"International Relations\"), textcoords=\"offset points\", xytext=(-52,0))\n",
      "plt.annotate(\"Children\", coords(\"Children\"), textcoords=\"offset points\", xytext=(-52,0))\n",
      "plt.annotate(\"Education\", coords(\"Education\"), textcoords=\"offset points\", xytext=(-52,0))\n",
      "plt.annotate(\"Computers\", coords(\"Computers\"), textcoords=\"offset points\", xytext=(-52,0))\n",
      "plt.annotate(\"Politics\", coords(\"Politics\"), textcoords=\"offset points\", xytext=(-52,0))\n",
      "plt.annotate(\"Travel\", coords(\"Travel\"), textcoords=\"offset points\", xytext=(-52,0))\n",
      "plt.annotate(\"Art\", coords(\"Art\"), textcoords=\"offset points\", xytext=(-52,0))\n",
      "plt.show()\n",
      "207/184:\n",
      "def coords(label):\n",
      "    i = labels.index(label)\n",
      "    return (heshes[i], biases[i])\n",
      "plt.plot(heshes, biases, \"o\")\n",
      "plt.title(\"Ratio of 'she' to 'he' vs Projection Bias on Professions\")\n",
      "plt.annotate(\"Automobiles\", coords(\"Automobiles\"), textcoords=\"offset points\", xytext=(5,0))\n",
      "plt.annotate(\"Weddings\", coords(\"Weddings and Engagements\"), textcoords=\"offset points\", xytext=(-52,0))\n",
      "plt.annotate(\"Baseball\", coords(\"Baseball\"), textcoords=\"offset points\", xytext=(-52,0))\n",
      "plt.annotate(\"Football\", coords(\"Football\"), textcoords=\"offset points\", xytext=(-52,0))\n",
      "plt.annotate(\"Basketball\", coords(\"Basketball\"), textcoords=\"offset points\", xytext=(-52,0))\n",
      "plt.annotate(\"International Relations\", coords(\"United States International Relations\"), textcoords=\"offset points\", xytext=(-52,0))\n",
      "plt.annotate(\"Children\", coords(\"Children and Youth\"), textcoords=\"offset points\", xytext=(-52,0))\n",
      "plt.annotate(\"Education\", coords(\"Education and Schools\"), textcoords=\"offset points\", xytext=(-52,0))\n",
      "plt.annotate(\"Computers\", coords(\"Computers and the Internet\"), textcoords=\"offset points\", xytext=(-52,0))\n",
      "plt.annotate(\"Politics\", coords(\"Politics and Government\"), textcoords=\"offset points\", xytext=(-52,0))\n",
      "plt.annotate(\"Travel\", coords(\"Travel and Vacations\"), textcoords=\"offset points\", xytext=(-52,0))\n",
      "plt.annotate(\"Art\", coords(\"Art\"), textcoords=\"offset points\", xytext=(-52,0))\n",
      "plt.show()\n",
      "207/185:\n",
      "def coords(label):\n",
      "    i = labels.index(label)\n",
      "    return (heshes[i], biases[i])\n",
      "plt.plot(heshes, biases, \"o\")\n",
      "plt.title(\"Ratio of 'she' to 'he' vs Projection Bias on Professions\")\n",
      "plt.annotate(\"Automobiles\", coords(\"Automobiles\"), textcoords=\"offset points\", xytext=(5,0))\n",
      "plt.annotate(\"Weddings\", coords(\"Weddings and Engagements\"), textcoords=\"offset points\", xytext=(-52,0))\n",
      "plt.annotate(\"Baseball\", coords(\"Baseball\"), textcoords=\"offset points\", xytext=(10,0))\n",
      "plt.annotate(\"Football\", coords(\"Football\"), textcoords=\"offset points\", xytext=(10,0))\n",
      "plt.annotate(\"Basketball\", coords(\"Basketball\"), textcoords=\"offset points\", xytext=(-52,0))\n",
      "plt.annotate(\"International Relations\", coords(\"United States International Relations\"), textcoords=\"offset points\", xytext=(-52,0))\n",
      "plt.annotate(\"Children\", coords(\"Children and Youth\"), textcoords=\"offset points\", xytext=(10,0))\n",
      "plt.annotate(\"Education\", coords(\"Education and Schools\"), textcoords=\"offset points\", xytext=(10,0))\n",
      "plt.annotate(\"Computers\", coords(\"Computers and the Internet\"), textcoords=\"offset points\", xytext=(10,0))\n",
      "plt.annotate(\"Politics\", coords(\"Politics and Government\"), textcoords=\"offset points\", xytext=(-52,0))\n",
      "plt.annotate(\"Travel\", coords(\"Travel and Vacations\"), textcoords=\"offset points\", xytext=(-52,0))\n",
      "plt.annotate(\"Art\", coords(\"Art\"), textcoords=\"offset points\", xytext=(-52,0))\n",
      "plt.show()\n",
      "207/186:\n",
      "def coords(label):\n",
      "    i = labels.index(label)\n",
      "    return (heshes[i], biases[i])\n",
      "plt.plot(heshes, biases, \"o\")\n",
      "plt.title(\"Ratio of 'she' to 'he' vs Projection Bias on Professions\")\n",
      "plt.annotate(\"Automobiles\", coords(\"Automobiles\"), textcoords=\"offset points\", xytext=(5,0))\n",
      "plt.annotate(\"Weddings\", coords(\"Weddings and Engagements\"), textcoords=\"offset points\", xytext=(-52,0))\n",
      "plt.annotate(\"Baseball\", coords(\"Baseball\"), textcoords=\"offset points\", xytext=(10,0))\n",
      "plt.annotate(\"Football\", coords(\"Football\"), textcoords=\"offset points\", xytext=(10,0))\n",
      "plt.annotate(\"Basketball\", coords(\"Basketball\"), textcoords=\"offset points\", xytext=(-52,0))\n",
      "plt.annotate(\"Children\", coords(\"Children and Youth\"), textcoords=\"offset points\", xytext=(10,0))\n",
      "plt.annotate(\"Education\", coords(\"Education and Schools\"), textcoords=\"offset points\", xytext=(10,0))\n",
      "plt.annotate(\"Computers\", coords(\"Computers and the Internet\"), textcoords=\"offset points\", xytext=(10,0))\n",
      "plt.annotate(\"Politics\", coords(\"Politics and Government\"), textcoords=\"offset points\", xytext=(-52,0))\n",
      "plt.annotate(\"Travel\", coords(\"Travel and Vacations\"), textcoords=\"offset points\", xytext=(-52,0))\n",
      "plt.annotate(\"Art\", coords(\"Art\"), textcoords=\"offset points\", xytext=(-52,0))\n",
      "plt.show()\n",
      "207/187:\n",
      "def coords(label):\n",
      "    i = labels.index(label)\n",
      "    return (heshes[i], biases[i])\n",
      "plt.plot(heshes, biases, \"o\")\n",
      "plt.title(\"Ratio of 'she' to 'he' vs Projection Bias on Professions\")\n",
      "plt.annotate(\"Automobiles\", coords(\"Automobiles\"), textcoords=\"offset points\", xytext=(5,0))\n",
      "plt.annotate(\"Weddings\", coords(\"Weddings and Engagements\"), textcoords=\"offset points\", xytext=(-52,0))\n",
      "plt.annotate(\"Baseball\", coords(\"Baseball\"), textcoords=\"offset points\", xytext=(10,0))\n",
      "plt.annotate(\"Football\", coords(\"Football\"), textcoords=\"offset points\", xytext=(10,0))\n",
      "plt.annotate(\"Basketball\", coords(\"Basketball\"), textcoords=\"offset points\", xytext=(-52,0))\n",
      "plt.annotate(\"Children\", coords(\"Children and Youth\"), textcoords=\"offset points\", xytext=(10,0))\n",
      "plt.annotate(\"Education\", coords(\"Education and Schools\"), textcoords=\"offset points\", xytext=(10,0))\n",
      "plt.annotate(\"Computers\", coords(\"Computers and the Internet\"), textcoords=\"offset points\", xytext=(10,0))\n",
      "plt.annotate(\"Politics\", coords(\"Politics and Government\"), textcoords=\"offset points\", xytext=(-5,0))\n",
      "plt.annotate(\"Travel\", coords(\"Travel and Vacations\"), textcoords=\"offset points\", xytext=(-52,0))\n",
      "plt.annotate(\"Art\", coords(\"Art\"), textcoords=\"offset points\", xytext=(-52,0))\n",
      "plt.show()\n",
      "207/188:\n",
      "def coords(label):\n",
      "    i = labels.index(label)\n",
      "    return (heshes[i], biases[i])\n",
      "plt.plot(heshes, biases, \"o\")\n",
      "plt.title(\"Ratio of 'she' to 'he' vs Projection Bias on Professions\")\n",
      "plt.annotate(\"Automobiles\", coords(\"Automobiles\"), textcoords=\"offset points\", xytext=(5,0))\n",
      "plt.annotate(\"Weddings\", coords(\"Weddings and Engagements\"), textcoords=\"offset points\", xytext=(-52,0))\n",
      "plt.annotate(\"Baseball\", coords(\"Baseball\"), textcoords=\"offset points\", xytext=(10,0))\n",
      "plt.annotate(\"Football\", coords(\"Football\"), textcoords=\"offset points\", xytext=(10,0))\n",
      "plt.annotate(\"Basketball\", coords(\"Basketball\"), textcoords=\"offset points\", xytext=(-52,0))\n",
      "plt.annotate(\"Children\", coords(\"Children and Youth\"), textcoords=\"offset points\", xytext=(10,0))\n",
      "plt.annotate(\"Education\", coords(\"Education and Schools\"), textcoords=\"offset points\", xytext=(10,0))\n",
      "plt.annotate(\"Computers\", coords(\"Computers and the Internet\"), textcoords=\"offset points\", xytext=(10,0))\n",
      "plt.annotate(\"Politics\", coords(\"Politics and Government\"), textcoords=\"offset points\", xytext=(0,0))\n",
      "plt.annotate(\"Travel\", coords(\"Travel and Vacations\"), textcoords=\"offset points\", xytext=(-52,0))\n",
      "plt.annotate(\"Art\", coords(\"Art\"), textcoords=\"offset points\", xytext=(-52,0))\n",
      "plt.show()\n",
      "207/189:\n",
      "def coords(label):\n",
      "    i = labels.index(label)\n",
      "    return (heshes[i], biases[i])\n",
      "plt.plot(heshes, biases, \"o\")\n",
      "plt.title(\"Ratio of 'she' to 'he' vs Projection Bias on Professions\")\n",
      "plt.annotate(\"Automobiles\", coords(\"Automobiles\"), textcoords=\"offset points\", xytext=(5,0))\n",
      "plt.annotate(\"Weddings\", coords(\"Weddings and Engagements\"), textcoords=\"offset points\", xytext=(-52,0))\n",
      "plt.annotate(\"Baseball\", coords(\"Baseball\"), textcoords=\"offset points\", xytext=(10,0))\n",
      "plt.annotate(\"Football\", coords(\"Football\"), textcoords=\"offset points\", xytext=(10,0))\n",
      "plt.annotate(\"Basketball\", coords(\"Basketball\"), textcoords=\"offset points\", xytext=(-52,0))\n",
      "plt.annotate(\"Children\", coords(\"Children and Youth\"), textcoords=\"offset points\", xytext=(10,0))\n",
      "plt.annotate(\"Education\", coords(\"Education and Schools\"), textcoords=\"offset points\", xytext=(10,0))\n",
      "plt.annotate(\"Computers\", coords(\"Computers and the Internet\"), textcoords=\"offset points\", xytext=(10,0))\n",
      "plt.annotate(\"Politics\", coords(\"Politics and Government\"), textcoords=\"offset points\", xytext=(10,0))\n",
      "plt.annotate(\"Travel\", coords(\"Travel and Vacations\"), textcoords=\"offset points\", xytext=(-52,0))\n",
      "plt.annotate(\"Art\", coords(\"Art\"), textcoords=\"offset points\", xytext=(-52,0))\n",
      "plt.show()\n",
      "207/190:\n",
      "def coords(label):\n",
      "    i = labels.index(label)\n",
      "    return (heshes[i], biases[i])\n",
      "plt.plot(heshes, biases, \"o\")\n",
      "plt.title(\"Ratio of 'she' to 'he' vs Projection Bias on Professions\")\n",
      "plt.annotate(\"Automobiles\", coords(\"Automobiles\"), textcoords=\"offset points\", xytext=(5,0))\n",
      "plt.annotate(\"Weddings\", coords(\"Weddings and Engagements\"), textcoords=\"offset points\", xytext=(-52,0))\n",
      "plt.annotate(\"Baseball\", coords(\"Baseball\"), textcoords=\"offset points\", xytext=(10,0))\n",
      "plt.annotate(\"Football\", coords(\"Football\"), textcoords=\"offset points\", xytext=(10,0))\n",
      "plt.annotate(\"Basketball\", coords(\"Basketball\"), textcoords=\"offset points\", xytext=(-52,0))\n",
      "plt.annotate(\"Children\", coords(\"Children and Youth\"), textcoords=\"offset points\", xytext=(10,0))\n",
      "plt.annotate(\"Education\", coords(\"Education and Schools\"), textcoords=\"offset points\", xytext=(10,0))\n",
      "plt.annotate(\"Computers\", coords(\"Computers and the Internet\"), textcoords=\"offset points\", xytext=(10,0))\n",
      "plt.annotate(\"Politics\", coords(\"Politics and Government\"), textcoords=\"offset points\", xytext=(15,0))\n",
      "plt.annotate(\"Travel\", coords(\"Travel and Vacations\"), textcoords=\"offset points\", xytext=(-52,0))\n",
      "plt.annotate(\"Art\", coords(\"Art\"), textcoords=\"offset points\", xytext=(-52,0))\n",
      "plt.show()\n",
      "207/191:\n",
      "def coords(label):\n",
      "    i = labels.index(label)\n",
      "    return (heshes[i], biases[i])\n",
      "plt.plot(heshes, biases, \"o\")\n",
      "plt.title(\"Ratio of 'she' to 'he' vs Projection Bias on Professions\")\n",
      "plt.annotate(\"Automobiles\", coords(\"Automobiles\"), textcoords=\"offset points\", xytext=(5,0))\n",
      "plt.annotate(\"Weddings\", coords(\"Weddings and Engagements\"), textcoords=\"offset points\", xytext=(-52,0))\n",
      "plt.annotate(\"Baseball\", coords(\"Baseball\"), textcoords=\"offset points\", xytext=(10,0))\n",
      "plt.annotate(\"Football\", coords(\"Football\"), textcoords=\"offset points\", xytext=(10,0))\n",
      "plt.annotate(\"Basketball\", coords(\"Basketball\"), textcoords=\"offset points\", xytext=(-52,0))\n",
      "plt.annotate(\"Children\", coords(\"Children and Youth\"), textcoords=\"offset points\", xytext=(10,0))\n",
      "plt.annotate(\"Education\", coords(\"Education and Schools\"), textcoords=\"offset points\", xytext=(10,0))\n",
      "plt.annotate(\"Computers\", coords(\"Computers and the Internet\"), textcoords=\"offset points\", xytext=(10,0))\n",
      "plt.annotate(\"Politics\", coords(\"Politics and Government\"), textcoords=\"offset points\", xytext=(20,0))\n",
      "plt.annotate(\"Travel\", coords(\"Travel and Vacations\"), textcoords=\"offset points\", xytext=(-52,0))\n",
      "plt.annotate(\"Art\", coords(\"Art\"), textcoords=\"offset points\", xytext=(-52,0))\n",
      "plt.show()\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "207/192:\n",
      "def coords(label):\n",
      "    i = labels.index(label)\n",
      "    return (heshes[i], biases[i])\n",
      "plt.plot(heshes, biases, \"o\")\n",
      "plt.title(\"Ratio of 'she' to 'he' vs Projection Bias on Professions\")\n",
      "plt.annotate(\"Automobiles\", coords(\"Automobiles\"), textcoords=\"offset points\", xytext=(5,0))\n",
      "plt.annotate(\"Weddings\", coords(\"Weddings and Engagements\"), textcoords=\"offset points\", xytext=(-52,0))\n",
      "plt.annotate(\"Baseball\", coords(\"Baseball\"), textcoords=\"offset points\", xytext=(10,0))\n",
      "plt.annotate(\"Football\", coords(\"Football\"), textcoords=\"offset points\", xytext=(10,0))\n",
      "plt.annotate(\"Basketball\", coords(\"Basketball\"), textcoords=\"offset points\", xytext=(-52,0))\n",
      "plt.annotate(\"Children\", coords(\"Children and Youth\"), textcoords=\"offset points\", xytext=(10,0))\n",
      "plt.annotate(\"Education\", coords(\"Education and Schools\"), textcoords=\"offset points\", xytext=(10,0))\n",
      "plt.annotate(\"Computers\", coords(\"Computers and the Internet\"), textcoords=\"offset points\", xytext=(0,0))\n",
      "plt.annotate(\"Politics\", coords(\"Politics and Government\"), textcoords=\"offset points\", xytext=(20,0))\n",
      "plt.annotate(\"Travel\", coords(\"Travel and Vacations\"), textcoords=\"offset points\", xytext=(-52,0))\n",
      "plt.annotate(\"Art\", coords(\"Art\"), textcoords=\"offset points\", xytext=(-52,0))\n",
      "plt.show()\n",
      "207/193:\n",
      "def coords(label):\n",
      "    i = labels.index(label)\n",
      "    return (heshes[i], biases[i])\n",
      "plt.plot(heshes, biases, \"o\")\n",
      "plt.title(\"Ratio of 'she' to 'he' vs Projection Bias on Professions\")\n",
      "plt.annotate(\"Automobiles\", coords(\"Automobiles\"), textcoords=\"offset points\", xytext=(5,0))\n",
      "plt.annotate(\"Weddings\", coords(\"Weddings and Engagements\"), textcoords=\"offset points\", xytext=(-52,0))\n",
      "plt.annotate(\"Baseball\", coords(\"Baseball\"), textcoords=\"offset points\", xytext=(10,0))\n",
      "plt.annotate(\"Football\", coords(\"Football\"), textcoords=\"offset points\", xytext=(10,0))\n",
      "plt.annotate(\"Basketball\", coords(\"Basketball\"), textcoords=\"offset points\", xytext=(-52,0))\n",
      "plt.annotate(\"Children\", coords(\"Children and Youth\"), textcoords=\"offset points\", xytext=(10,0))\n",
      "plt.annotate(\"Education\", coords(\"Education and Schools\"), textcoords=\"offset points\", xytext=(10,0))\n",
      "plt.annotate(\"Computers\", coords(\"Computers and the Internet\"), textcoords=\"offset points\", xytext=(0,0))\n",
      "plt.annotate(\"Politics\", coords(\"Politics and Government\"), textcoords=\"offset points\", xytext=(20,0))\n",
      "plt.annotate(\"Travel\", coords(\"Travel and Vacations\"), textcoords=\"offset points\", xytext=(-52,0))\n",
      "plt.annotate(\"Art\", coords(\"Art\"), textcoords=\"offset points\", xytext=(-5,0))\n",
      "plt.show()\n",
      "207/194:\n",
      "def coords(label):\n",
      "    i = labels.index(label)\n",
      "    return (heshes[i], biases[i])\n",
      "plt.plot(heshes, biases, \"o\")\n",
      "plt.title(\"Ratio of 'she' to 'he' vs Projection Bias on Professions\")\n",
      "plt.annotate(\"Automobiles\", coords(\"Automobiles\"), textcoords=\"offset points\", xytext=(5,0))\n",
      "plt.annotate(\"Weddings\", coords(\"Weddings and Engagements\"), textcoords=\"offset points\", xytext=(-52,0))\n",
      "plt.annotate(\"Baseball\", coords(\"Baseball\"), textcoords=\"offset points\", xytext=(10,0))\n",
      "plt.annotate(\"Football\", coords(\"Football\"), textcoords=\"offset points\", xytext=(10,0))\n",
      "plt.annotate(\"Basketball\", coords(\"Basketball\"), textcoords=\"offset points\", xytext=(-52,0))\n",
      "plt.annotate(\"Children\", coords(\"Children and Youth\"), textcoords=\"offset points\", xytext=(10,0))\n",
      "plt.annotate(\"Education\", coords(\"Education and Schools\"), textcoords=\"offset points\", xytext=(10,0))\n",
      "plt.annotate(\"Computers\", coords(\"Computers and the Internet\"), textcoords=\"offset points\", xytext=(0,0))\n",
      "plt.annotate(\"Politics\", coords(\"Politics and Government\"), textcoords=\"offset points\", xytext=(20,0))\n",
      "plt.annotate(\"Travel\", coords(\"Travel and Vacations\"), textcoords=\"offset points\", xytext=(-5,0))\n",
      "plt.annotate(\"Art\", coords(\"Art\"), textcoords=\"offset points\", xytext=(-5,0))\n",
      "plt.show()\n",
      "207/195:\n",
      "def coords(label):\n",
      "    i = labels.index(label)\n",
      "    return (heshes[i], biases[i])\n",
      "plt.plot(heshes, biases, \"o\")\n",
      "plt.title(\"Ratio of 'she' to 'he' vs Projection Bias on Professions\")\n",
      "plt.annotate(\"Automobiles\", coords(\"Automobiles\"), textcoords=\"offset points\", xytext=(5,0))\n",
      "plt.annotate(\"Weddings\", coords(\"Weddings and Engagements\"), textcoords=\"offset points\", xytext=(-52,0))\n",
      "plt.annotate(\"Baseball\", coords(\"Baseball\"), textcoords=\"offset points\", xytext=(5,0))\n",
      "plt.annotate(\"Football\", coords(\"Football\"), textcoords=\"offset points\", xytext=(5,-5))\n",
      "plt.annotate(\"Basketball\", coords(\"Basketball\"), textcoords=\"offset points\", xytext=(-52,0))\n",
      "plt.annotate(\"Children\", coords(\"Children and Youth\"), textcoords=\"offset points\", xytext=(10,0))\n",
      "plt.annotate(\"Education\", coords(\"Education and Schools\"), textcoords=\"offset points\", xytext=(10,0))\n",
      "plt.annotate(\"Computers\", coords(\"Computers and the Internet\"), textcoords=\"offset points\", xytext=(0,0))\n",
      "plt.annotate(\"Politics\", coords(\"Politics and Government\"), textcoords=\"offset points\", xytext=(20,0))\n",
      "plt.annotate(\"Travel\", coords(\"Travel and Vacations\"), textcoords=\"offset points\", xytext=(-5,0))\n",
      "plt.annotate(\"Art\", coords(\"Art\"), textcoords=\"offset points\", xytext=(-5,0))\n",
      "plt.show()\n",
      "207/196:\n",
      "def coords(label):\n",
      "    i = labels.index(label)\n",
      "    return (heshes[i], biases[i])\n",
      "plt.plot(heshes, biases, \"o\")\n",
      "plt.title(\"Ratio of 'she' to 'he' vs Projection Bias on Professions\")\n",
      "plt.annotate(\"Automobiles\", coords(\"Automobiles\"), textcoords=\"offset points\", xytext=(5,0))\n",
      "plt.annotate(\"Weddings\", coords(\"Weddings and Engagements\"), textcoords=\"offset points\", xytext=(-52,0))\n",
      "plt.annotate(\"Baseball\", coords(\"Baseball\"), textcoords=\"offset points\", xytext=(5,0))\n",
      "plt.annotate(\"Football\", coords(\"Football\"), textcoords=\"offset points\", xytext=(5,-5))\n",
      "plt.annotate(\"Basketball\", coords(\"Basketball\"), textcoords=\"offset points\", xytext=(0,5))\n",
      "plt.annotate(\"Children\", coords(\"Children and Youth\"), textcoords=\"offset points\", xytext=(10,0))\n",
      "plt.annotate(\"Education\", coords(\"Education and Schools\"), textcoords=\"offset points\", xytext=(10,0))\n",
      "plt.annotate(\"Computers\", coords(\"Computers and the Internet\"), textcoords=\"offset points\", xytext=(0,0))\n",
      "plt.annotate(\"Politics\", coords(\"Politics and Government\"), textcoords=\"offset points\", xytext=(20,0))\n",
      "plt.annotate(\"Travel\", coords(\"Travel and Vacations\"), textcoords=\"offset points\", xytext=(-5,0))\n",
      "plt.annotate(\"Art\", coords(\"Art\"), textcoords=\"offset points\", xytext=(-5,0))\n",
      "plt.show()\n",
      "207/197:\n",
      "def coords(label):\n",
      "    i = labels.index(label)\n",
      "    return (heshes[i], biases[i])\n",
      "plt.plot(heshes, biases, \"o\")\n",
      "plt.title(\"Ratio of 'she' to 'he' vs Projection Bias on Professions\")\n",
      "plt.annotate(\"Automobiles\", coords(\"Automobiles\"), textcoords=\"offset points\", xytext=(5,0))\n",
      "plt.annotate(\"Weddings\", coords(\"Weddings and Engagements\"), textcoords=\"offset points\", xytext=(-52,0))\n",
      "plt.annotate(\"Baseball\", coords(\"Baseball\"), textcoords=\"offset points\", xytext=(5,0))\n",
      "plt.annotate(\"Football\", coords(\"Football\"), textcoords=\"offset points\", xytext=(5,-5))\n",
      "plt.annotate(\"Basketball\", coords(\"Basketball\"), textcoords=\"offset points\", xytext=(0,5))\n",
      "plt.annotate(\"Children\", coords(\"Children and Youth\"), textcoords=\"offset points\", xytext=(10,0))\n",
      "plt.annotate(\"Education\", coords(\"Education and Schools\"), textcoords=\"offset points\", xytext=(10,0))\n",
      "plt.annotate(\"Computers\", coords(\"Computers and the Internet\"), textcoords=\"offset points\", xytext=(2,0))\n",
      "plt.annotate(\"Politics\", coords(\"Politics and Government\"), textcoords=\"offset points\", xytext=(22,0))\n",
      "plt.annotate(\"Travel\", coords(\"Travel and Vacations\"), textcoords=\"offset points\", xytext=(-5,0))\n",
      "plt.annotate(\"Art\", coords(\"Art\"), textcoords=\"offset points\", xytext=(-5,0))\n",
      "plt.show()\n",
      "207/198:\n",
      "def coords(label):\n",
      "    i = labels.index(label)\n",
      "    return (heshes[i], biases[i])\n",
      "plt.plot(heshes, biases, \"o\")\n",
      "plt.title(\"Ratio of 'she' to 'he' vs Projection Bias on Professions\")\n",
      "plt.annotate(\"Automobiles\", coords(\"Automobiles\"), textcoords=\"offset points\", xytext=(5,0))\n",
      "plt.annotate(\"Weddings\", coords(\"Weddings and Engagements\"), textcoords=\"offset points\", xytext=(-52,0))\n",
      "plt.annotate(\"Baseball\", coords(\"Baseball\"), textcoords=\"offset points\", xytext=(5,0))\n",
      "plt.annotate(\"Football\", coords(\"Football\"), textcoords=\"offset points\", xytext=(5,-5))\n",
      "plt.annotate(\"Basketball\", coords(\"Basketball\"), textcoords=\"offset points\", xytext=(0,5))\n",
      "plt.annotate(\"Children\", coords(\"Children and Youth\"), textcoords=\"offset points\", xytext=(10,0))\n",
      "plt.annotate(\"Education\", coords(\"Education and Schools\"), textcoords=\"offset points\", xytext=(10,0))\n",
      "plt.annotate(\"Computers\", coords(\"Computers and the Internet\"), textcoords=\"offset points\", xytext=(5,0))\n",
      "plt.annotate(\"Politics\", coords(\"Politics and Government\"), textcoords=\"offset points\", xytext=(22,0))\n",
      "plt.annotate(\"Travel\", coords(\"Travel and Vacations\"), textcoords=\"offset points\", xytext=(-5,0))\n",
      "plt.annotate(\"Art\", coords(\"Art\"), textcoords=\"offset points\", xytext=(-10,0))\n",
      "plt.show()\n",
      "207/199:\n",
      "def coords(label):\n",
      "    i = labels.index(label)\n",
      "    return (heshes[i], biases[i])\n",
      "plt.plot(heshes, biases, \"o\")\n",
      "plt.title(\"Ratio of 'she' to 'he' vs Projection Bias on Professions\")\n",
      "plt.annotate(\"Automobiles\", coords(\"Automobiles\"), textcoords=\"offset points\", xytext=(5,0))\n",
      "plt.annotate(\"Weddings\", coords(\"Weddings and Engagements\"), textcoords=\"offset points\", xytext=(-52,0))\n",
      "plt.annotate(\"Baseball\", coords(\"Baseball\"), textcoords=\"offset points\", xytext=(5,0))\n",
      "plt.annotate(\"Football\", coords(\"Football\"), textcoords=\"offset points\", xytext=(5,-5))\n",
      "plt.annotate(\"Basketball\", coords(\"Basketball\"), textcoords=\"offset points\", xytext=(0,5))\n",
      "plt.annotate(\"Children\", coords(\"Children and Youth\"), textcoords=\"offset points\", xytext=(10,0))\n",
      "plt.annotate(\"Education\", coords(\"Education and Schools\"), textcoords=\"offset points\", xytext=(10,0))\n",
      "plt.annotate(\"Computers\", coords(\"Computers and the Internet\"), textcoords=\"offset points\", xytext=(5,0))\n",
      "plt.annotate(\"Politics\", coords(\"Politics and Government\"), textcoords=\"offset points\", xytext=(22,0))\n",
      "plt.annotate(\"Travel\", coords(\"Travel and Vacations\"), textcoords=\"offset points\", xytext=(-5,0))\n",
      "plt.annotate(\"Art\", coords(\"Art\"), textcoords=\"offset points\", xytext=(-15,0))\n",
      "plt.show()\n",
      "207/200:\n",
      "def coords(label):\n",
      "    i = labels.index(label)\n",
      "    return (heshes[i], biases[i])\n",
      "plt.plot(heshes, biases, \"o\")\n",
      "plt.title(\"Ratio of 'she' to 'he' vs Projection Bias on Professions\")\n",
      "plt.annotate(\"Automobiles\", coords(\"Automobiles\"), textcoords=\"offset points\", xytext=(5,0))\n",
      "plt.annotate(\"Weddings\", coords(\"Weddings and Engagements\"), textcoords=\"offset points\", xytext=(-52,0))\n",
      "plt.annotate(\"Baseball\", coords(\"Baseball\"), textcoords=\"offset points\", xytext=(5,0))\n",
      "plt.annotate(\"Football\", coords(\"Football\"), textcoords=\"offset points\", xytext=(5,-5))\n",
      "plt.annotate(\"Basketball\", coords(\"Basketball\"), textcoords=\"offset points\", xytext=(0,5))\n",
      "plt.annotate(\"Children\", coords(\"Children and Youth\"), textcoords=\"offset points\", xytext=(10,0))\n",
      "plt.annotate(\"Education\", coords(\"Education and Schools\"), textcoords=\"offset points\", xytext=(10,0))\n",
      "plt.annotate(\"Computers\", coords(\"Computers and the Internet\"), textcoords=\"offset points\", xytext=(5,0))\n",
      "plt.annotate(\"Politics\", coords(\"Politics and Government\"), textcoords=\"offset points\", xytext=(22,0))\n",
      "plt.annotate(\"Travel\", coords(\"Travel and Vacations\"), textcoords=\"offset points\", xytext=(-5,0))\n",
      "plt.annotate(\"Art\", coords(\"Art\"), textcoords=\"offset points\", xytext=(-20,0))\n",
      "plt.show()\n",
      "207/201:\n",
      "def coords(label):\n",
      "    i = labels.index(label)\n",
      "    return (heshes[i], biases[i])\n",
      "plt.plot(heshes, biases, \"o\")\n",
      "plt.title(\"Ratio of 'she' to 'he' vs Projection Bias on Professions\")\n",
      "plt.annotate(\"Automobiles\", coords(\"Automobiles\"), textcoords=\"offset points\", xytext=(5,0))\n",
      "plt.annotate(\"Weddings\", coords(\"Weddings and Engagements\"), textcoords=\"offset points\", xytext=(-52,0))\n",
      "plt.annotate(\"Baseball\", coords(\"Baseball\"), textcoords=\"offset points\", xytext=(5,0))\n",
      "plt.annotate(\"Football\", coords(\"Football\"), textcoords=\"offset points\", xytext=(5,-5))\n",
      "plt.annotate(\"Basketball\", coords(\"Basketball\"), textcoords=\"offset points\", xytext=(0,5))\n",
      "plt.annotate(\"Children\", coords(\"Children and Youth\"), textcoords=\"offset points\", xytext=(10,0))\n",
      "plt.annotate(\"Education\", coords(\"Education and Schools\"), textcoords=\"offset points\", xytext=(10,0))\n",
      "plt.annotate(\"Computers\", coords(\"Computers and the Internet\"), textcoords=\"offset points\", xytext=(5,0))\n",
      "plt.annotate(\"Politics\", coords(\"Politics and Government\"), textcoords=\"offset points\", xytext=(22,0))\n",
      "plt.annotate(\"Travel\", coords(\"Travel and Vacations\"), textcoords=\"offset points\", xytext=(-5,0))\n",
      "plt.annotate(\"Art\", coords(\"Art\"), textcoords=\"offset points\", xytext=(-16,0))\n",
      "plt.show()\n",
      "207/202:\n",
      "def coords(label):\n",
      "    i = labels.index(label)\n",
      "    return (heshes[i], biases[i])\n",
      "plt.plot(heshes, biases, \"o\")\n",
      "plt.title(\"Ratio of 'she' to 'he' vs Projection Bias on Professions\")\n",
      "plt.annotate(\"Automobiles\", coords(\"Automobiles\"), textcoords=\"offset points\", xytext=(5,0))\n",
      "plt.annotate(\"Weddings\", coords(\"Weddings and Engagements\"), textcoords=\"offset points\", xytext=(-52,0))\n",
      "plt.annotate(\"Baseball\", coords(\"Baseball\"), textcoords=\"offset points\", xytext=(5,0))\n",
      "plt.annotate(\"Football\", coords(\"Football\"), textcoords=\"offset points\", xytext=(5,-5))\n",
      "plt.annotate(\"Basketball\", coords(\"Basketball\"), textcoords=\"offset points\", xytext=(0,5))\n",
      "plt.annotate(\"Children\", coords(\"Children and Youth\"), textcoords=\"offset points\", xytext=(10,0))\n",
      "plt.annotate(\"Education\", coords(\"Education and Schools\"), textcoords=\"offset points\", xytext=(10,0))\n",
      "plt.annotate(\"Computers\", coords(\"Computers and the Internet\"), textcoords=\"offset points\", xytext=(5,0))\n",
      "plt.annotate(\"Politics\", coords(\"Politics and Government\"), textcoords=\"offset points\", xytext=(22,0))\n",
      "plt.annotate(\"Travel\", coords(\"Travel and Vacations\"), textcoords=\"offset points\", xytext=(-5,0))\n",
      "plt.annotate(\"Art\", coords(\"Art\"), textcoords=\"offset points\", xytext=(-18,0))\n",
      "plt.show()\n",
      "207/203:\n",
      "def coords(label):\n",
      "    i = labels.index(label)\n",
      "    return (heshes[i], biases[i])\n",
      "plt.plot(heshes, biases, \"o\")\n",
      "plt.title(\"Ratio of 'she' to 'he' vs Projection Bias on Professions\")\n",
      "plt.annotate(\"Automobiles\", coords(\"Automobiles\"), textcoords=\"offset points\", xytext=(5,0))\n",
      "plt.annotate(\"Weddings\", coords(\"Weddings and Engagements\"), textcoords=\"offset points\", xytext=(-52,0))\n",
      "plt.annotate(\"Baseball\", coords(\"Baseball\"), textcoords=\"offset points\", xytext=(5,0))\n",
      "plt.annotate(\"Football\", coords(\"Football\"), textcoords=\"offset points\", xytext=(5,-5))\n",
      "plt.annotate(\"Basketball\", coords(\"Basketball\"), textcoords=\"offset points\", xytext=(0,5))\n",
      "plt.annotate(\"Children\", coords(\"Children and Youth\"), textcoords=\"offset points\", xytext=(10,0))\n",
      "plt.annotate(\"Education\", coords(\"Education and Schools\"), textcoords=\"offset points\", xytext=(10,0))\n",
      "plt.annotate(\"Computers\", coords(\"Computers and the Internet\"), textcoords=\"offset points\", xytext=(5,0))\n",
      "plt.annotate(\"Politics\", coords(\"Politics and Government\"), textcoords=\"offset points\", xytext=(22,0))\n",
      "plt.annotate(\"Travel\", coords(\"Travel and Vacations\"), textcoords=\"offset points\", xytext=(-5,0))\n",
      "plt.annotate(\"Art\", coords(\"Art\"), textcoords=\"offset points\", xytext=(-18,-2))\n",
      "plt.show()\n",
      "207/204:\n",
      "def coords(label):\n",
      "    i = labels.index(label)\n",
      "    return (heshes[i], biases[i])\n",
      "plt.plot(heshes, biases, \"o\")\n",
      "plt.title(\"Ratio of 'she' to 'he' vs Projection Bias on Professions\")\n",
      "plt.annotate(\"Automobiles\", coords(\"Automobiles\"), textcoords=\"offset points\", xytext=(5,0))\n",
      "plt.annotate(\"Weddings\", coords(\"Weddings and Engagements\"), textcoords=\"offset points\", xytext=(-52,0))\n",
      "plt.annotate(\"Baseball\", coords(\"Baseball\"), textcoords=\"offset points\", xytext=(5,0))\n",
      "plt.annotate(\"Football\", coords(\"Football\"), textcoords=\"offset points\", xytext=(5,-5))\n",
      "plt.annotate(\"Basketball\", coords(\"Basketball\"), textcoords=\"offset points\", xytext=(0,5))\n",
      "plt.annotate(\"Children\", coords(\"Children and Youth\"), textcoords=\"offset points\", xytext=(10,0))\n",
      "plt.annotate(\"Education\", coords(\"Education and Schools\"), textcoords=\"offset points\", xytext=(10,0))\n",
      "plt.annotate(\"Computers\", coords(\"Computers and the Internet\"), textcoords=\"offset points\", xytext=(5,0))\n",
      "plt.annotate(\"Politics\", coords(\"Politics and Government\"), textcoords=\"offset points\", xytext=(22,0))\n",
      "plt.annotate(\"Travel\", coords(\"Travel and Vacations\"), textcoords=\"offset points\", xytext=(-5,0))\n",
      "plt.annotate(\"Art\", coords(\"Art\"), textcoords=\"offset points\", xytext=(-18,-5))\n",
      "plt.show()\n",
      "207/205:\n",
      "def coords(label):\n",
      "    i = labels.index(label)\n",
      "    return (heshes[i], biases[i])\n",
      "plt.plot(heshes, biases, \"o\")\n",
      "plt.title(\"Ratio of 'she' to 'he' vs Projection Bias on Professions\")\n",
      "plt.annotate(\"Automobiles\", coords(\"Automobiles\"), textcoords=\"offset points\", xytext=(5,0))\n",
      "plt.annotate(\"Weddings\", coords(\"Weddings and Engagements\"), textcoords=\"offset points\", xytext=(-52,0))\n",
      "plt.annotate(\"Baseball\", coords(\"Baseball\"), textcoords=\"offset points\", xytext=(5,0))\n",
      "plt.annotate(\"Football\", coords(\"Football\"), textcoords=\"offset points\", xytext=(5,-5))\n",
      "plt.annotate(\"Basketball\", coords(\"Basketball\"), textcoords=\"offset points\", xytext=(0,5))\n",
      "plt.annotate(\"Children\", coords(\"Children and Youth\"), textcoords=\"offset points\", xytext=(5,-2))\n",
      "plt.annotate(\"Education\", coords(\"Education and Schools\"), textcoords=\"offset points\", xytext=(5,2))\n",
      "plt.annotate(\"Computers\", coords(\"Computers and the Internet\"), textcoords=\"offset points\", xytext=(5,0))\n",
      "plt.annotate(\"Politics\", coords(\"Politics and Government\"), textcoords=\"offset points\", xytext=(22,0))\n",
      "plt.annotate(\"Travel\", coords(\"Travel and Vacations\"), textcoords=\"offset points\", xytext=(-5,0))\n",
      "plt.annotate(\"Art\", coords(\"Art\"), textcoords=\"offset points\", xytext=(-18,-5))\n",
      "plt.show()\n",
      "207/206:\n",
      "def coords(label):\n",
      "    i = labels.index(label)\n",
      "    return (heshes[i], biases[i])\n",
      "plt.plot(heshes, biases, \"o\")\n",
      "plt.title(\"Ratio of 'she' to 'he' vs Projection Bias on Professions\")\n",
      "plt.annotate(\"Automobiles\", coords(\"Automobiles\"), textcoords=\"offset points\", xytext=(5,0))\n",
      "plt.annotate(\"Weddings\", coords(\"Weddings and Engagements\"), textcoords=\"offset points\", xytext=(-52,0))\n",
      "plt.annotate(\"Baseball\", coords(\"Baseball\"), textcoords=\"offset points\", xytext=(5,0))\n",
      "plt.annotate(\"Football\", coords(\"Football\"), textcoords=\"offset points\", xytext=(5,-5))\n",
      "plt.annotate(\"Basketball\", coords(\"Basketball\"), textcoords=\"offset points\", xytext=(0,5))\n",
      "plt.annotate(\"Children\", coords(\"Children and Youth\"), textcoords=\"offset points\", xytext=(5,-5))\n",
      "plt.annotate(\"Education\", coords(\"Education and Schools\"), textcoords=\"offset points\", xytext=(5,2))\n",
      "plt.annotate(\"Computers\", coords(\"Computers and the Internet\"), textcoords=\"offset points\", xytext=(5,0))\n",
      "plt.annotate(\"Politics\", coords(\"Politics and Government\"), textcoords=\"offset points\", xytext=(22,0))\n",
      "plt.annotate(\"Travel\", coords(\"Travel and Vacations\"), textcoords=\"offset points\", xytext=(-5,0))\n",
      "plt.annotate(\"Art\", coords(\"Art\"), textcoords=\"offset points\", xytext=(-18,-5))\n",
      "plt.show()\n",
      "207/207:\n",
      "def coords(label):\n",
      "    i = labels.index(label)\n",
      "    return (heshes[i], biases[i])\n",
      "plt.plot(heshes, biases, \"o\")\n",
      "plt.title(\"Ratio of 'she' to 'he' vs Projection Bias on Professions\")\n",
      "plt.annotate(\"Automobiles\", coords(\"Automobiles\"), textcoords=\"offset points\", xytext=(5,0))\n",
      "plt.annotate(\"Weddings\", coords(\"Weddings and Engagements\"), textcoords=\"offset points\", xytext=(-52,0))\n",
      "plt.annotate(\"Baseball\", coords(\"Baseball\"), textcoords=\"offset points\", xytext=(5,0))\n",
      "plt.annotate(\"Football\", coords(\"Football\"), textcoords=\"offset points\", xytext=(5,-5))\n",
      "plt.annotate(\"Basketball\", coords(\"Basketball\"), textcoords=\"offset points\", xytext=(0,5))\n",
      "plt.annotate(\"Children\", coords(\"Children and Youth\"), textcoords=\"offset points\", xytext=(5,-7))\n",
      "plt.annotate(\"Education\", coords(\"Education and Schools\"), textcoords=\"offset points\", xytext=(5,2))\n",
      "plt.annotate(\"Computers\", coords(\"Computers and the Internet\"), textcoords=\"offset points\", xytext=(5,0))\n",
      "plt.annotate(\"Politics\", coords(\"Politics and Government\"), textcoords=\"offset points\", xytext=(22,0))\n",
      "plt.annotate(\"Travel\", coords(\"Travel and Vacations\"), textcoords=\"offset points\", xytext=(-5,0))\n",
      "plt.annotate(\"Art\", coords(\"Art\"), textcoords=\"offset points\", xytext=(-18,-5))\n",
      "plt.show()\n",
      "207/208:\n",
      "def coords(label):\n",
      "    i = labels.index(label)\n",
      "    return (heshes[i], biases[i])\n",
      "plt.plot(heshes, biases, \"o\")\n",
      "plt.title(\"Ratio of 'she' to 'he' vs Projection Bias on Professions\")\n",
      "plt.annotate(\"Automobiles\", coords(\"Automobiles\"), textcoords=\"offset points\", xytext=(5,0))\n",
      "plt.annotate(\"Weddings\", coords(\"Weddings and Engagements\"), textcoords=\"offset points\", xytext=(-52,0))\n",
      "plt.annotate(\"Baseball\", coords(\"Baseball\"), textcoords=\"offset points\", xytext=(5,0))\n",
      "plt.annotate(\"Football\", coords(\"Football\"), textcoords=\"offset points\", xytext=(5,-5))\n",
      "plt.annotate(\"Basketball\", coords(\"Basketball\"), textcoords=\"offset points\", xytext=(0,5))\n",
      "plt.annotate(\"Children\", coords(\"Children and Youth\"), textcoords=\"offset points\", xytext=(5,-7))\n",
      "plt.annotate(\"Education\", coords(\"Education and Schools\"), textcoords=\"offset points\", xytext=(5,5))\n",
      "plt.annotate(\"Computers\", coords(\"Computers and the Internet\"), textcoords=\"offset points\", xytext=(5,0))\n",
      "plt.annotate(\"Politics\", coords(\"Politics and Government\"), textcoords=\"offset points\", xytext=(22,0))\n",
      "plt.annotate(\"Travel\", coords(\"Travel and Vacations\"), textcoords=\"offset points\", xytext=(-5,0))\n",
      "plt.annotate(\"Art\", coords(\"Art\"), textcoords=\"offset points\", xytext=(-18,-5))\n",
      "plt.show()\n",
      "207/209:\n",
      "def coords(label):\n",
      "    i = labels.index(label)\n",
      "    return (heshes[i], biases[i])\n",
      "plt.plot(heshes, biases, \"o\")\n",
      "plt.title(\"Ratio of 'she' to 'he' vs Projection Bias on Professions\")\n",
      "plt.annotate(\"Automobiles\", coords(\"Automobiles\"), textcoords=\"offset points\", xytext=(5,0))\n",
      "plt.annotate(\"Weddings\", coords(\"Weddings and Engagements\"), textcoords=\"offset points\", xytext=(-52,0))\n",
      "plt.annotate(\"Baseball\", coords(\"Baseball\"), textcoords=\"offset points\", xytext=(5,0))\n",
      "plt.annotate(\"Football\", coords(\"Football\"), textcoords=\"offset points\", xytext=(5,-5))\n",
      "plt.annotate(\"Basketball\", coords(\"Basketball\"), textcoords=\"offset points\", xytext=(0,5))\n",
      "plt.annotate(\"Children\", coords(\"Children and Youth\"), textcoords=\"offset points\", xytext=(5,-7))\n",
      "plt.annotate(\"Education\", coords(\"Education and Schools\"), textcoords=\"offset points\", xytext=(2,5))\n",
      "plt.annotate(\"Computers\", coords(\"Computers and the Internet\"), textcoords=\"offset points\", xytext=(5,0))\n",
      "plt.annotate(\"Politics\", coords(\"Politics and Government\"), textcoords=\"offset points\", xytext=(22,0))\n",
      "plt.annotate(\"Travel\", coords(\"Travel and Vacations\"), textcoords=\"offset points\", xytext=(-5,0))\n",
      "plt.annotate(\"Art\", coords(\"Art\"), textcoords=\"offset points\", xytext=(-18,-5))\n",
      "plt.show()\n",
      "207/210:\n",
      "def coords(label):\n",
      "    i = labels.index(label)\n",
      "    return (heshes[i], biases[i])\n",
      "plt.plot(heshes, biases, \"o\")\n",
      "plt.title(\"Ratio of 'she' to 'he' vs Projection Bias on Professions\")\n",
      "plt.annotate(\"Automobiles\", coords(\"Automobiles\"), textcoords=\"offset points\", xytext=(5,0))\n",
      "plt.annotate(\"Weddings\", coords(\"Weddings and Engagements\"), textcoords=\"offset points\", xytext=(-52,0))\n",
      "plt.annotate(\"Baseball\", coords(\"Baseball\"), textcoords=\"offset points\", xytext=(5,0))\n",
      "plt.annotate(\"Football\", coords(\"Football\"), textcoords=\"offset points\", xytext=(5,-5))\n",
      "plt.annotate(\"Basketball\", coords(\"Basketball\"), textcoords=\"offset points\", xytext=(0,5))\n",
      "plt.annotate(\"Children\", coords(\"Children and Youth\"), textcoords=\"offset points\", xytext=(5,-7))\n",
      "plt.annotate(\"Education\", coords(\"Education and Schools\"), textcoords=\"offset points\", xytext=(2,5))\n",
      "plt.annotate(\"Computers\", coords(\"Computers and the Internet\"), textcoords=\"offset points\", xytext=(5,0))\n",
      "plt.annotate(\"Politics\", coords(\"Politics and Government\"), textcoords=\"offset points\", xytext=(22,0))\n",
      "plt.annotate(\"Travel\", coords(\"Travel and Vacations\"), textcoords=\"offset points\", xytext=(-5,-5))\n",
      "plt.annotate(\"Art\", coords(\"Art\"), textcoords=\"offset points\", xytext=(-18,-5))\n",
      "plt.show()\n",
      "207/211:\n",
      "def coords(label):\n",
      "    i = labels.index(label)\n",
      "    return (heshes[i], biases[i])\n",
      "plt.plot(heshes, biases, \"o\")\n",
      "plt.title(\"Ratio of 'she' to 'he' vs Projection Bias on Professions\")\n",
      "plt.annotate(\"Automobiles\", coords(\"Automobiles\"), textcoords=\"offset points\", xytext=(5,0))\n",
      "plt.annotate(\"Weddings\", coords(\"Weddings and Engagements\"), textcoords=\"offset points\", xytext=(-52,0))\n",
      "plt.annotate(\"Baseball\", coords(\"Baseball\"), textcoords=\"offset points\", xytext=(5,0))\n",
      "plt.annotate(\"Football\", coords(\"Football\"), textcoords=\"offset points\", xytext=(5,-5))\n",
      "plt.annotate(\"Basketball\", coords(\"Basketball\"), textcoords=\"offset points\", xytext=(0,5))\n",
      "plt.annotate(\"Children\", coords(\"Children and Youth\"), textcoords=\"offset points\", xytext=(5,-7))\n",
      "plt.annotate(\"Education\", coords(\"Education and Schools\"), textcoords=\"offset points\", xytext=(2,5))\n",
      "plt.annotate(\"Computers\", coords(\"Computers and the Internet\"), textcoords=\"offset points\", xytext=(5,0))\n",
      "plt.annotate(\"Politics\", coords(\"Politics and Government\"), textcoords=\"offset points\", xytext=(22,0))\n",
      "plt.annotate(\"Travel\", coords(\"Travel and Vacations\"), textcoords=\"offset points\", xytext=(-5,-10))\n",
      "plt.annotate(\"Art\", coords(\"Art\"), textcoords=\"offset points\", xytext=(-18,-5))\n",
      "plt.show()\n",
      "207/212:\n",
      "def coords(label):\n",
      "    i = labels.index(label)\n",
      "    return (heshes[i], biases[i])\n",
      "plt.plot(heshes, biases, \"o\")\n",
      "plt.title(\"Ratio of 'she' to 'he' vs Projection Bias on Professions\")\n",
      "plt.annotate(\"Automobiles\", coords(\"Automobiles\"), textcoords=\"offset points\", xytext=(5,0))\n",
      "plt.annotate(\"Weddings\", coords(\"Weddings and Engagements\"), textcoords=\"offset points\", xytext=(-52,0))\n",
      "plt.annotate(\"Baseball\", coords(\"Baseball\"), textcoords=\"offset points\", xytext=(5,0))\n",
      "plt.annotate(\"Football\", coords(\"Football\"), textcoords=\"offset points\", xytext=(5,-5))\n",
      "plt.annotate(\"Basketball\", coords(\"Basketball\"), textcoords=\"offset points\", xytext=(0,5))\n",
      "plt.annotate(\"Children\", coords(\"Children and Youth\"), textcoords=\"offset points\", xytext=(5,-7))\n",
      "plt.annotate(\"Education\", coords(\"Education and Schools\"), textcoords=\"offset points\", xytext=(2,5))\n",
      "plt.annotate(\"Computers\", coords(\"Computers and the Internet\"), textcoords=\"offset points\", xytext=(5,0))\n",
      "plt.annotate(\"Politics\", coords(\"Politics and Government\"), textcoords=\"offset points\", xytext=(22,0))\n",
      "plt.annotate(\"Travel\", coords(\"Travel and Vacations\"), textcoords=\"offset points\", xytext=(-2,-10))\n",
      "plt.annotate(\"Art\", coords(\"Art\"), textcoords=\"offset points\", xytext=(-18,-5))\n",
      "plt.show()\n",
      "207/213:\n",
      "def coords(label):\n",
      "    i = labels.index(label)\n",
      "    return (heshes[i], biases[i])\n",
      "plt.plot(heshes, biases, \"o\")\n",
      "plt.title(\"Ratio of 'she' to 'he' vs Projection Bias on Professions\")\n",
      "plt.annotate(\"Automobiles\", coords(\"Automobiles\"), textcoords=\"offset points\", xytext=(5,0))\n",
      "plt.annotate(\"Weddings\", coords(\"Weddings and Engagements\"), textcoords=\"offset points\", xytext=(-52,0))\n",
      "plt.annotate(\"Baseball\", coords(\"Baseball\"), textcoords=\"offset points\", xytext=(5,0))\n",
      "plt.annotate(\"Football\", coords(\"Football\"), textcoords=\"offset points\", xytext=(5,-5))\n",
      "plt.annotate(\"Basketball\", coords(\"Basketball\"), textcoords=\"offset points\", xytext=(0,5))\n",
      "plt.annotate(\"Children\", coords(\"Children and Youth\"), textcoords=\"offset points\", xytext=(5,-7))\n",
      "plt.annotate(\"Education\", coords(\"Education and Schools\"), textcoords=\"offset points\", xytext=(2,5))\n",
      "plt.annotate(\"Computers\", coords(\"Computers and the Internet\"), textcoords=\"offset points\", xytext=(5,0))\n",
      "plt.annotate(\"Politics\", coords(\"Politics and Government\"), textcoords=\"offset points\", xytext=(22,0))\n",
      "plt.annotate(\"Travel\", coords(\"Travel and Vacations\"), textcoords=\"offset points\", xytext=(-1,-10))\n",
      "plt.annotate(\"Art\", coords(\"Art\"), textcoords=\"offset points\", xytext=(-18,-5))\n",
      "plt.show()\n",
      "207/214:\n",
      "def coords(label):\n",
      "    i = labels.index(label)\n",
      "    return (heshes[i], biases[i])\n",
      "plt.plot(heshes, biases, \"o\")\n",
      "plt.title(\"Ratio of 'she' to 'he' vs Projection Bias on Professions\")\n",
      "plt.annotate(\"Automobiles\", coords(\"Automobiles\"), textcoords=\"offset points\", xytext=(5,0))\n",
      "plt.annotate(\"Weddings\", coords(\"Weddings and Engagements\"), textcoords=\"offset points\", xytext=(-52,0))\n",
      "plt.annotate(\"Baseball\", coords(\"Baseball\"), textcoords=\"offset points\", xytext=(5,0))\n",
      "plt.annotate(\"Football\", coords(\"Football\"), textcoords=\"offset points\", xytext=(5,-5))\n",
      "plt.annotate(\"Basketball\", coords(\"Basketball\"), textcoords=\"offset points\", xytext=(0,5))\n",
      "plt.annotate(\"Children\", coords(\"Children and Youth\"), textcoords=\"offset points\", xytext=(5,-7))\n",
      "plt.annotate(\"Education\", coords(\"Education and Schools\"), textcoords=\"offset points\", xytext=(2,5))\n",
      "plt.annotate(\"Computers\", coords(\"Computers and the Internet\"), textcoords=\"offset points\", xytext=(5,0))\n",
      "plt.annotate(\"Politics\", coords(\"Politics and Government\"), textcoords=\"offset points\", xytext=(22,0))\n",
      "plt.annotate(\"Travel\", coords(\"Travel and Vacations\"), textcoords=\"offset points\", xytext=(1,-10))\n",
      "plt.annotate(\"Art\", coords(\"Art\"), textcoords=\"offset points\", xytext=(-18,-5))\n",
      "plt.show()\n",
      "207/215:\n",
      "def coords(label):\n",
      "    i = labels.index(label)\n",
      "    return (heshes[i], biases[i])\n",
      "plt.plot(heshes, biases, \"o\")\n",
      "plt.title(\"Ratio of 'she' to 'he' vs Projection Bias on Professions\")\n",
      "plt.annotate(\"Automobiles\", coords(\"Automobiles\"), textcoords=\"offset points\", xytext=(5,0))\n",
      "plt.annotate(\"Weddings\", coords(\"Weddings and Engagements\"), textcoords=\"offset points\", xytext=(-52,0))\n",
      "plt.annotate(\"Baseball\", coords(\"Baseball\"), textcoords=\"offset points\", xytext=(5,0))\n",
      "plt.annotate(\"Football\", coords(\"Football\"), textcoords=\"offset points\", xytext=(5,-5))\n",
      "plt.annotate(\"Basketball\", coords(\"Basketball\"), textcoords=\"offset points\", xytext=(0,5))\n",
      "plt.annotate(\"Children\", coords(\"Children and Youth\"), textcoords=\"offset points\", xytext=(5,-7))\n",
      "plt.annotate(\"Education\", coords(\"Education and Schools\"), textcoords=\"offset points\", xytext=(2,5))\n",
      "plt.annotate(\"Computers\", coords(\"Computers and the Internet\"), textcoords=\"offset points\", xytext=(5,0))\n",
      "plt.annotate(\"Politics\", coords(\"Politics and Government\"), textcoords=\"offset points\", xytext=(22,0))\n",
      "plt.annotate(\"Travel\", coords(\"Travel and Vacations\"), textcoords=\"offset points\", xytext=(2,-10))\n",
      "plt.annotate(\"Art\", coords(\"Art\"), textcoords=\"offset points\", xytext=(-18,-5))\n",
      "plt.show()\n",
      "207/216:\n",
      "ptest = getw2v(sample(politics, 100))\n",
      "bias(getg(ptest), ptest, ptest.wv.vocab, proflist)\n",
      "207/217:\n",
      "ptest = getw2v(sample(politics, 200))\n",
      "bias(getg(ptest), ptest, ptest.wv.vocab, proflist)\n",
      "207/218:\n",
      "ptest = getw2v(sample(politics, 500))\n",
      "bias(getg(ptest), ptest, ptest.wv.vocab, proflist)\n",
      "207/219:\n",
      "ptest = getw2v(sample(politics, 1000))\n",
      "bias(getg(ptest), ptest, ptest.wv.vocab, proflist)\n",
      "207/220:\n",
      "ptest = getw2v(sample(politics, 1500))\n",
      "bias(getg(ptest), ptest, ptest.wv.vocab, proflist)\n",
      "207/221:\n",
      "plistshort = []\n",
      "for i in range(1, 10):\n",
      "    plistshort.append(sample(politics, i*200))\n",
      "pmodelsshort = [getw2v(p) for p in plistshort]\n",
      "pbiasshort = [bias(getg(m), m, m.wv.vocab, proflist) for m in pmodelsshort]\n",
      "207/222:\n",
      "plt.plot([i*200 for i in range(1,10)], bookbiasshort, label=\"books\")\n",
      "plt.plot([i*200 for i in range(1,10)], pbiasshort, label=\"politics\")\n",
      "plt.legend()\n",
      "plt.title(\"Projection Bias on Professions vs Number of Articles in Training Set\")\n",
      "plt.show()\n",
      "207/223:\n",
      "# plt.plot([i*200 for i in range(1,10)], bookbiasshort, label=\"books\")\n",
      "plt.plot([i*200 for i in range(1,10)], pbiasshort, label=\"politics\")\n",
      "plt.legend()\n",
      "plt.title(\"Projection Bias on Professions vs Number of Articles in Training Set\")\n",
      "plt.show()\n",
      "207/224:\n",
      "blistshort = []\n",
      "for i in range(1, 10):\n",
      "    blistshort.append(sample(books, i*200))\n",
      "bmodelsshort = [getw2v(b) for b in blistshort]\n",
      "bbiasshort = [bias(getg(m), m, m.wv.vocab, proflist) for m in bmodelsshort]\n",
      "207/225:\n",
      "plt.plot([i*200 for i in range(1,10)], bookbiasshort, label=\"books\")\n",
      "plt.plot([i*200 for i in range(1,10)], pbiasshort, label=\"politics\")\n",
      "plt.legend()\n",
      "plt.title(\"Projection Bias on Professions vs Number of Articles in Training Set (200-1800)\")\n",
      "plt.show()\n",
      "207/226:\n",
      "plt.plot([i*200 for i in range(1,10)], bbiasshort, label=\"books\")\n",
      "plt.plot([i*200 for i in range(1,10)], pbiasshort, label=\"politics\")\n",
      "plt.legend()\n",
      "plt.title(\"Projection Bias on Professions vs Number of Articles in Training Set (200-1800)\")\n",
      "plt.show()\n",
      "207/227:\n",
      "plt.plot([i*500 for i in range(3,21)], bookbias[2:], label=\"books\")\n",
      "plt.plot([i*500 for i in range(3,21)], pbias[1:], label=\"politics\")\n",
      "plt.legend()\n",
      "plt.title(\"Projection Bias on Professions vs Number of Articles in Training Set (1500-10000)\")\n",
      "plt.show()\n",
      "207/228:\n",
      "plt.plot([i*200 for i in range(1,10)], bbiasshort, label=\"books\")\n",
      "plt.plot([i*200 for i in range(1,10)], pbiasshort, label=\"politics\")\n",
      "plt.legend()\n",
      "plt.title(\"Projection Bias on Professions vs Number of Articles in Training Set \\n (200-1800 articles)\")\n",
      "plt.show()\n",
      "207/229:\n",
      "plt.plot([i*200 for i in range(1,10)], bbiasshort, label=\"books\")\n",
      "plt.plot([i*200 for i in range(1,10)], pbiasshort, label=\"politics\")\n",
      "plt.legend()\n",
      "plt.title(\"Projection Bias on Professions vs Number of Articles \\n in Training Set (200-1800 articles)\")\n",
      "plt.show()\n",
      "207/230:\n",
      "plt.plot([i*500 for i in range(3,21)], bookbias[2:], label=\"books\")\n",
      "plt.plot([i*500 for i in range(3,21)], pbias[1:], label=\"politics\")\n",
      "plt.legend()\n",
      "plt.title(\"Projection Bias on Professions vs Number of Articles \\n in Training Set (1500-10000)\")\n",
      "plt.show()\n",
      "207/231:\n",
      "plt.plot([i*500 for i in range(3,21)], bookbias[2:], label=\"books\")\n",
      "plt.plot([i*500 for i in range(3,21)], pbias[1:], label=\"politics\")\n",
      "plt.legend()\n",
      "plt.title(\"Projection Bias on Professions vs Number of Articles \\n in Training Set (1500-10000 articles)\")\n",
      "plt.show()\n",
      "206/41:\n",
      "pca_r = PCA(n_components = 10)\n",
      "pca_r.fit(np.random.rand(70,100))\n",
      "plt.bar(range(10), pca_r.explained_variance_ratio_)\n",
      "plt.title(\"Explained Variance vs Component Number (Random)\")\n",
      "206/42:\n",
      "pca_r = PCA(n_components = 10)\n",
      "pca_r.fit(np.random.rand(70,100))\n",
      "plt.bar(range(10), pca_r.explained_variance_ratio_)\n",
      "plt.ylim(0.35)\n",
      "plt.title(\"Explained Variance vs Component Number (Random)\")\n",
      "206/43:\n",
      "pca_r = PCA(n_components = 10)\n",
      "pca_r.fit(np.random.rand(70,100))\n",
      "plt.bar(range(10), pca_r.explained_variance_ratio_)\n",
      "plt.ylim(0, 0.35)\n",
      "plt.title(\"Explained Variance vs Component Number (Random)\")\n",
      "207/232:\n",
      "stereotypes_m = ['captain', 'president', 'principal', 'dean', \\\n",
      "                 'commissioner', 'coach', 'director', 'officer', \\\n",
      "                 'manager']\n",
      "stereotypes_f = ['teacher', 'author', 'secretary', 'writer', \\\n",
      "                 'artist', 'singer', 'teenager', 'editor', \\\n",
      "                 'photographer']\n",
      "male = [\"male\", \"man\", \"boy\", \"boys\", \"brother\", \"he\", \"him\", \"son\", \"father\"]\n",
      "female = [\"female\", \"woman\", \"girl\", \"girls\", \"sister\", \"she\", \"her\", \\\n",
      "          \"daughter\", \"mother\"]\n",
      "def cos_sim(m, x, y):\n",
      "    wv1 = m.wv.get_vector(x)\n",
      "    wv2 = m.wv.get_vector(y)\n",
      "    sim = np.dot(wv1, wv2)/(np.linalg.norm(wv1)*np.linalg.norm(wv2))\n",
      "    return sim\n",
      "def s_word(m, w, A, B):\n",
      "    return np.mean([cos_sim(m, w, a) for a in A]) - np.mean([cos_sim(m, w, b) for b in B])\n",
      "def effect_size(m, X, Y, A, B):\n",
      "    num = np.mean([s_word(m, x, A, B) for x in X]) - np.mean([s_word(m, y, A, B) for y in Y])\n",
      "    return num/np.std([s_word(m, x, A, B) for x in X + Y])\n",
      "def wbias(m):\n",
      "    return effect_size(m, male, female, stereotypes_m, stereotypes_f)\n",
      "207/233:\n",
      "bookwbias = []\n",
      "for bm in booksmodels:\n",
      "    b = wbias(bm)\n",
      "    print(b)\n",
      "    bookwbias.append(b)\n",
      "207/234:\n",
      "pwbias = []\n",
      "for m in pmodels:\n",
      "    b = wbias(m)\n",
      "    print(b)\n",
      "    pwbias.append(b)\n",
      "207/235:\n",
      "stereotypes_m = ['captain', 'president', 'principal', 'dean', \\\n",
      "                 'commissioner', 'coach', 'director', 'officer', \\\n",
      "                 'manager']\n",
      "stereotypes_f = ['teacher', 'author', 'secretary', 'writer', \\\n",
      "                 'singer', 'teenager', 'editor', 'nurse' \\\n",
      "                 'photographer']\n",
      "male = [\"male\", \"man\", \"boy\", \"boys\", \"brother\", \"he\", \"him\", \"son\", \"father\"]\n",
      "female = [\"female\", \"woman\", \"girl\", \"girls\", \"sister\", \"she\", \"her\", \\\n",
      "          \"daughter\", \"mother\"]\n",
      "def cos_sim(m, x, y):\n",
      "    wv1 = m.wv.get_vector(x)\n",
      "    wv2 = m.wv.get_vector(y)\n",
      "    sim = np.dot(wv1, wv2)/(np.linalg.norm(wv1)*np.linalg.norm(wv2))\n",
      "    return sim\n",
      "def s_word(m, w, A, B):\n",
      "    return np.mean([cos_sim(m, w, a) for a in A]) - np.mean([cos_sim(m, w, b) for b in B])\n",
      "def effect_size(m, X, Y, A, B):\n",
      "    num = np.mean([s_word(m, x, A, B) for x in X]) - np.mean([s_word(m, y, A, B) for y in Y])\n",
      "    return num/np.std([s_word(m, x, A, B) for x in X + Y])\n",
      "def wbias(m):\n",
      "    return effect_size(m, male, female, stereotypes_m, stereotypes_f)\n",
      "207/236:\n",
      "bookwbias = []\n",
      "for bm in booksmodels:\n",
      "    b = wbias(bm)\n",
      "    print(b)\n",
      "    bookwbias.append(b)\n",
      "207/237:\n",
      "stereotypes_m = ['captain', 'president', 'principal', 'dean', \\\n",
      "                 'commissioner', 'coach', 'director', 'officer', \\\n",
      "                 'manager']\n",
      "stereotypes_f = ['teacher', 'author', 'secretary', 'writer', \\\n",
      "                 'singer', 'teenager', 'editor', 'nurse', \\\n",
      "                 'photographer']\n",
      "male = [\"male\", \"man\", \"boy\", \"boys\", \"brother\", \"he\", \"him\", \"son\", \"father\"]\n",
      "female = [\"female\", \"woman\", \"girl\", \"girls\", \"sister\", \"she\", \"her\", \\\n",
      "          \"daughter\", \"mother\"]\n",
      "def cos_sim(m, x, y):\n",
      "    wv1 = m.wv.get_vector(x)\n",
      "    wv2 = m.wv.get_vector(y)\n",
      "    sim = np.dot(wv1, wv2)/(np.linalg.norm(wv1)*np.linalg.norm(wv2))\n",
      "    return sim\n",
      "def s_word(m, w, A, B):\n",
      "    return np.mean([cos_sim(m, w, a) for a in A]) - np.mean([cos_sim(m, w, b) for b in B])\n",
      "def effect_size(m, X, Y, A, B):\n",
      "    num = np.mean([s_word(m, x, A, B) for x in X]) - np.mean([s_word(m, y, A, B) for y in Y])\n",
      "    return num/np.std([s_word(m, x, A, B) for x in X + Y])\n",
      "def wbias(m):\n",
      "    return effect_size(m, male, female, stereotypes_m, stereotypes_f)\n",
      "207/238:\n",
      "bookwbias = []\n",
      "for bm in booksmodels:\n",
      "    b = wbias(bm)\n",
      "    print(b)\n",
      "    bookwbias.append(b)\n",
      "207/239:\n",
      "pwbias = []\n",
      "for m in pmodels:\n",
      "    b = wbias(m)\n",
      "    print(b)\n",
      "    pwbias.append(b)\n",
      "207/240:\n",
      "stereotypes_m = ['captain', 'president', 'principal', 'dean', \\\n",
      "                 'commissioner', 'coach', 'director', 'officer', \\\n",
      "                 'manager']\n",
      "stereotypes_f = ['teacher', 'author', 'secretary', 'writer', \\\n",
      "                 'singer', 'teenager', 'editor', 'maid', \\\n",
      "                 'photographer']\n",
      "male = [\"male\", \"man\", \"boy\", \"boys\", \"brother\", \"he\", \"him\", \"son\", \"father\"]\n",
      "female = [\"female\", \"woman\", \"girl\", \"girls\", \"sister\", \"she\", \"her\", \\\n",
      "          \"daughter\", \"mother\"]\n",
      "def cos_sim(m, x, y):\n",
      "    wv1 = m.wv.get_vector(x)\n",
      "    wv2 = m.wv.get_vector(y)\n",
      "    sim = np.dot(wv1, wv2)/(np.linalg.norm(wv1)*np.linalg.norm(wv2))\n",
      "    return sim\n",
      "def s_word(m, w, A, B):\n",
      "    return np.mean([cos_sim(m, w, a) for a in A]) - np.mean([cos_sim(m, w, b) for b in B])\n",
      "def effect_size(m, X, Y, A, B):\n",
      "    num = np.mean([s_word(m, x, A, B) for x in X]) - np.mean([s_word(m, y, A, B) for y in Y])\n",
      "    return num/np.std([s_word(m, x, A, B) for x in X + Y])\n",
      "def wbias(m):\n",
      "    return effect_size(m, male, female, stereotypes_m, stereotypes_f)\n",
      "207/241:\n",
      "bookwbias = []\n",
      "for bm in booksmodels:\n",
      "    b = wbias(bm)\n",
      "    print(b)\n",
      "    bookwbias.append(b)\n",
      "207/242:\n",
      "pwbias = []\n",
      "for m in pmodels:\n",
      "    b = wbias(m)\n",
      "    print(b)\n",
      "    pwbias.append(b)\n",
      "207/243:\n",
      "stereotypes_m = ['captain', 'president', 'principal', 'dean', \\\n",
      "                 'coach', 'director', 'officer', 'manager']\n",
      "stereotypes_f = ['teacher', 'author', 'secretary', 'writer', \\\n",
      "                 'singer', 'teenager', 'editor', 'photographer']\n",
      "male = [\"male\", \"man\", \"boy\", \"boys\", \"brother\", \"he\", \"him\", \"son\", \"father\"]\n",
      "female = [\"female\", \"woman\", \"girl\", \"girls\", \"sister\", \"she\", \"her\", \\\n",
      "          \"daughter\", \"mother\"]\n",
      "def cos_sim(m, x, y):\n",
      "    wv1 = m.wv.get_vector(x)\n",
      "    wv2 = m.wv.get_vector(y)\n",
      "    sim = np.dot(wv1, wv2)/(np.linalg.norm(wv1)*np.linalg.norm(wv2))\n",
      "    return sim\n",
      "def s_word(m, w, A, B):\n",
      "    return np.mean([cos_sim(m, w, a) for a in A]) - np.mean([cos_sim(m, w, b) for b in B])\n",
      "def effect_size(m, X, Y, A, B):\n",
      "    num = np.mean([s_word(m, x, A, B) for x in X]) - np.mean([s_word(m, y, A, B) for y in Y])\n",
      "    return num/np.std([s_word(m, x, A, B) for x in X + Y])\n",
      "def wbias(m):\n",
      "    return effect_size(m, male, female, stereotypes_m, stereotypes_f)\n",
      "207/244:\n",
      "bookwbias = []\n",
      "for bm in booksmodels:\n",
      "    b = wbias(bm)\n",
      "    print(b)\n",
      "    bookwbias.append(b)\n",
      "207/245:\n",
      "pwbias = []\n",
      "for m in pmodels:\n",
      "    b = wbias(m)\n",
      "    print(b)\n",
      "    pwbias.append(b)\n",
      "207/246:\n",
      "plt.plot([i*500 for i in range(3,21)], bookwbias[2:], label=\"books\")\n",
      "plt.plot([i*500 for i in range(3,21)], pwbias[1:], label=\"politics\")\n",
      "plt.legend()\n",
      "plt.title(\"Projection Bias on Professions vs Number of Articles \\n in Training Set (1500-10000 articles)\")\n",
      "plt.show()\n",
      "207/247:\n",
      "plt.plot([i*500 for i in range(3,21)], bookwbias[2:], label=\"books\")\n",
      "plt.plot([i*500 for i in range(3,21)], pwbias[1:], label=\"politics\")\n",
      "plt.legend()\n",
      "plt.title(\"WEAT Bias on Professions vs Number of Articles \\n in Training Set (1500-10000 articles)\")\n",
      "plt.show()\n",
      "207/248:\n",
      "pwbiasshort = [wbias(m) for m in pmodelsshort]\n",
      "bwbiasshort = [wbias(m) for m in bmodelsshort]\n",
      "plt.plot([i*200 for i in range(1,10)], bwbiasshort, label=\"books\")\n",
      "plt.plot([i*200 for i in range(1,10)], pwbiasshort, label=\"politics\")\n",
      "plt.legend()\n",
      "plt.title(\"WEAT Bias on Professions vs Number of Articles \\n in Training Set (200-1800 articles)\")\n",
      "plt.show()\n",
      "207/249:\n",
      "stereotypes_m = ['captain', 'president', 'principal', 'dean', \\\n",
      "                 'coach', 'director', 'officer', 'manager']\n",
      "stereotypes_f = ['teacher', 'author', 'secretary', 'writer', \\\n",
      "                 'singer', 'teenager', 'editor', 'photographer']\n",
      "male = [\"man\", \"boy\", \"boys\", \"brother\", \"he\", \"him\", \"son\", \"father\"]\n",
      "female = [\"woman\", \"girl\", \"girls\", \"sister\", \"she\", \"her\", \\\n",
      "          \"daughter\", \"mother\"]\n",
      "def cos_sim(m, x, y):\n",
      "    wv1 = m.wv.get_vector(x)\n",
      "    wv2 = m.wv.get_vector(y)\n",
      "    sim = np.dot(wv1, wv2)/(np.linalg.norm(wv1)*np.linalg.norm(wv2))\n",
      "    return sim\n",
      "def s_word(m, w, A, B):\n",
      "    return np.mean([cos_sim(m, w, a) for a in A]) - np.mean([cos_sim(m, w, b) for b in B])\n",
      "def effect_size(m, X, Y, A, B):\n",
      "    num = np.mean([s_word(m, x, A, B) for x in X]) - np.mean([s_word(m, y, A, B) for y in Y])\n",
      "    return num/np.std([s_word(m, x, A, B) for x in X + Y])\n",
      "def wbias(m):\n",
      "    return effect_size(m, male, female, stereotypes_m, stereotypes_f)\n",
      "207/250:\n",
      "pwbiasshort = [wbias(m) for m in pmodelsshort]\n",
      "bwbiasshort = [wbias(m) for m in bmodelsshort]\n",
      "plt.plot([i*200 for i in range(1,10)], bwbiasshort, label=\"books\")\n",
      "plt.plot([i*200 for i in range(1,10)], pwbiasshort, label=\"politics\")\n",
      "plt.legend()\n",
      "plt.title(\"WEAT Bias on Professions vs Number of Articles \\n in Training Set (200-1800 articles)\")\n",
      "plt.show()\n",
      "207/251:\n",
      "stereotypes_m = ['captain', 'president', 'commissioner', 'dean', \\\n",
      "                 'coach', 'director', 'officer', 'manager']\n",
      "stereotypes_f = ['teacher', 'author', 'secretary', 'writer', \\\n",
      "                 'singer', 'teenager', 'editor', 'photographer']\n",
      "male = [\"man\", \"boy\", \"boys\", \"brother\", \"he\", \"him\", \"son\", \"father\"]\n",
      "female = [\"woman\", \"girl\", \"girls\", \"sister\", \"she\", \"her\", \\\n",
      "          \"daughter\", \"mother\"]\n",
      "def cos_sim(m, x, y):\n",
      "    wv1 = m.wv.get_vector(x)\n",
      "    wv2 = m.wv.get_vector(y)\n",
      "    sim = np.dot(wv1, wv2)/(np.linalg.norm(wv1)*np.linalg.norm(wv2))\n",
      "    return sim\n",
      "def s_word(m, w, A, B):\n",
      "    return np.mean([cos_sim(m, w, a) for a in A]) - np.mean([cos_sim(m, w, b) for b in B])\n",
      "def effect_size(m, X, Y, A, B):\n",
      "    num = np.mean([s_word(m, x, A, B) for x in X]) - np.mean([s_word(m, y, A, B) for y in Y])\n",
      "    return num/np.std([s_word(m, x, A, B) for x in X + Y])\n",
      "def wbias(m):\n",
      "    return effect_size(m, male, female, stereotypes_m, stereotypes_f)\n",
      "207/252:\n",
      "pwbiasshort = [wbias(m) for m in pmodelsshort]\n",
      "bwbiasshort = [wbias(m) for m in bmodelsshort]\n",
      "plt.plot([i*200 for i in range(1,10)], bwbiasshort, label=\"books\")\n",
      "plt.plot([i*200 for i in range(1,10)], pwbiasshort, label=\"politics\")\n",
      "plt.legend()\n",
      "plt.title(\"WEAT Bias on Professions vs Number of Articles \\n in Training Set (200-1800 articles)\")\n",
      "plt.show()\n",
      "207/253:\n",
      "stereotypes_m = ['captain', 'president', 'commissioner', 'dean', \\\n",
      "                 'director', 'officer', 'manager', 'analyst']\n",
      "stereotypes_f = ['teacher', 'author', 'secretary', 'writer', \\\n",
      "                 'singer', 'teenager', 'editor', 'photographer']\n",
      "male = [\"man\", \"boy\", \"boys\", \"brother\", \"he\", \"him\", \"son\", \"father\"]\n",
      "female = [\"woman\", \"girl\", \"girls\", \"sister\", \"she\", \"her\", \\\n",
      "          \"daughter\", \"mother\"]\n",
      "def cos_sim(m, x, y):\n",
      "    wv1 = m.wv.get_vector(x)\n",
      "    wv2 = m.wv.get_vector(y)\n",
      "    sim = np.dot(wv1, wv2)/(np.linalg.norm(wv1)*np.linalg.norm(wv2))\n",
      "    return sim\n",
      "def s_word(m, w, A, B):\n",
      "    return np.mean([cos_sim(m, w, a) for a in A]) - np.mean([cos_sim(m, w, b) for b in B])\n",
      "def effect_size(m, X, Y, A, B):\n",
      "    num = np.mean([s_word(m, x, A, B) for x in X]) - np.mean([s_word(m, y, A, B) for y in Y])\n",
      "    return num/np.std([s_word(m, x, A, B) for x in X + Y])\n",
      "def wbias(m):\n",
      "    return effect_size(m, male, female, stereotypes_m, stereotypes_f)\n",
      "207/254:\n",
      "pwbiasshort = [wbias(m) for m in pmodelsshort]\n",
      "bwbiasshort = [wbias(m) for m in bmodelsshort]\n",
      "plt.plot([i*200 for i in range(1,10)], bwbiasshort, label=\"books\")\n",
      "plt.plot([i*200 for i in range(1,10)], pwbiasshort, label=\"politics\")\n",
      "plt.legend()\n",
      "plt.title(\"WEAT Bias on Professions vs Number of Articles \\n in Training Set (200-1800 articles)\")\n",
      "plt.show()\n",
      "207/255:\n",
      "stereotypes_m = ['captain', 'president', 'commissioner', 'dean', \\\n",
      "                 'director', 'officer', 'manager']\n",
      "stereotypes_f = ['teacher', 'author', 'secretary', \\\n",
      "                 'singer', 'teenager', 'editor', 'photographer']\n",
      "male = [\"man\", \"boy\", \"boys\", \"brother\", \"he\", \"him\", \"son\", \"father\"]\n",
      "female = [\"woman\", \"girl\", \"girls\", \"sister\", \"she\", \"her\", \\\n",
      "          \"daughter\", \"mother\"]\n",
      "def cos_sim(m, x, y):\n",
      "    wv1 = m.wv.get_vector(x)\n",
      "    wv2 = m.wv.get_vector(y)\n",
      "    sim = np.dot(wv1, wv2)/(np.linalg.norm(wv1)*np.linalg.norm(wv2))\n",
      "    return sim\n",
      "def s_word(m, w, A, B):\n",
      "    return np.mean([cos_sim(m, w, a) for a in A]) - np.mean([cos_sim(m, w, b) for b in B])\n",
      "def effect_size(m, X, Y, A, B):\n",
      "    num = np.mean([s_word(m, x, A, B) for x in X]) - np.mean([s_word(m, y, A, B) for y in Y])\n",
      "    return num/np.std([s_word(m, x, A, B) for x in X + Y])\n",
      "def wbias(m):\n",
      "    return effect_size(m, male, female, stereotypes_m, stereotypes_f)\n",
      "207/256:\n",
      "pwbiasshort = [wbias(m) for m in pmodelsshort]\n",
      "bwbiasshort = [wbias(m) for m in bmodelsshort]\n",
      "plt.plot([i*200 for i in range(1,10)], bwbiasshort, label=\"books\")\n",
      "plt.plot([i*200 for i in range(1,10)], pwbiasshort, label=\"politics\")\n",
      "plt.legend()\n",
      "plt.title(\"WEAT Bias on Professions vs Number of Articles \\n in Training Set (200-1800 articles)\")\n",
      "plt.show()\n",
      "207/257:\n",
      "stereotypes_m = ['captain', 'president', 'commissioner', 'dean', \\\n",
      "                 'director', 'officer', 'manager']\n",
      "stereotypes_f = ['teacher', 'author', 'secretary', \\\n",
      "                 'teenager', 'editor', 'photographer']\n",
      "male = [\"man\", \"boy\", \"boys\", \"brother\", \"he\", \"him\", \"son\", \"father\"]\n",
      "female = [\"woman\", \"girl\", \"girls\", \"sister\", \"she\", \"her\", \\\n",
      "          \"daughter\", \"mother\"]\n",
      "def cos_sim(m, x, y):\n",
      "    wv1 = m.wv.get_vector(x)\n",
      "    wv2 = m.wv.get_vector(y)\n",
      "    sim = np.dot(wv1, wv2)/(np.linalg.norm(wv1)*np.linalg.norm(wv2))\n",
      "    return sim\n",
      "def s_word(m, w, A, B):\n",
      "    return np.mean([cos_sim(m, w, a) for a in A]) - np.mean([cos_sim(m, w, b) for b in B])\n",
      "def effect_size(m, X, Y, A, B):\n",
      "    num = np.mean([s_word(m, x, A, B) for x in X]) - np.mean([s_word(m, y, A, B) for y in Y])\n",
      "    return num/np.std([s_word(m, x, A, B) for x in X + Y])\n",
      "def wbias(m):\n",
      "    return effect_size(m, male, female, stereotypes_m, stereotypes_f)\n",
      "207/258:\n",
      "pwbiasshort = [wbias(m) for m in pmodelsshort]\n",
      "bwbiasshort = [wbias(m) for m in bmodelsshort]\n",
      "plt.plot([i*200 for i in range(1,10)], bwbiasshort, label=\"books\")\n",
      "plt.plot([i*200 for i in range(1,10)], pwbiasshort, label=\"politics\")\n",
      "plt.legend()\n",
      "plt.title(\"WEAT Bias on Professions vs Number of Articles \\n in Training Set (200-1800 articles)\")\n",
      "plt.show()\n",
      "207/259:\n",
      "stereotypes_m = ['captain', 'president', 'dean', 'officer', 'manager']\n",
      "stereotypes_f = ['teacher', 'author', 'secretary', 'editor', 'photographer']\n",
      "male = [\"man\", \"boy\", \"boys\", \"brother\", \"he\", \"him\", \"son\", \"father\"]\n",
      "female = [\"woman\", \"girl\", \"girls\", \"sister\", \"she\", \"her\", \\\n",
      "          \"daughter\", \"mother\"]\n",
      "def cos_sim(m, x, y):\n",
      "    wv1 = m.wv.get_vector(x)\n",
      "    wv2 = m.wv.get_vector(y)\n",
      "    sim = np.dot(wv1, wv2)/(np.linalg.norm(wv1)*np.linalg.norm(wv2))\n",
      "    return sim\n",
      "def s_word(m, w, A, B):\n",
      "    return np.mean([cos_sim(m, w, a) for a in A]) - np.mean([cos_sim(m, w, b) for b in B])\n",
      "def effect_size(m, X, Y, A, B):\n",
      "    num = np.mean([s_word(m, x, A, B) for x in X]) - np.mean([s_word(m, y, A, B) for y in Y])\n",
      "    return num/np.std([s_word(m, x, A, B) for x in X + Y])\n",
      "def wbias(m):\n",
      "    return effect_size(m, male, female, stereotypes_m, stereotypes_f)\n",
      "207/260:\n",
      "pwbiasshort = [wbias(m) for m in pmodelsshort]\n",
      "bwbiasshort = [wbias(m) for m in bmodelsshort]\n",
      "plt.plot([i*200 for i in range(1,10)], bwbiasshort, label=\"books\")\n",
      "plt.plot([i*200 for i in range(1,10)], pwbiasshort, label=\"politics\")\n",
      "plt.legend()\n",
      "plt.title(\"WEAT Bias on Professions vs Number of Articles \\n in Training Set (200-1800 articles)\")\n",
      "plt.show()\n",
      "207/261:\n",
      "stereotypes_m = ['captain', 'president', 'dean', 'officer', 'manager']\n",
      "stereotypes_f = ['teacher', 'author', 'secretary', 'editor']\n",
      "male = [\"man\", \"boy\", \"boys\", \"brother\", \"he\", \"him\", \"son\", \"father\"]\n",
      "female = [\"woman\", \"girl\", \"girls\", \"sister\", \"she\", \"her\", \\\n",
      "          \"daughter\", \"mother\"]\n",
      "def cos_sim(m, x, y):\n",
      "    wv1 = m.wv.get_vector(x)\n",
      "    wv2 = m.wv.get_vector(y)\n",
      "    sim = np.dot(wv1, wv2)/(np.linalg.norm(wv1)*np.linalg.norm(wv2))\n",
      "    return sim\n",
      "def s_word(m, w, A, B):\n",
      "    return np.mean([cos_sim(m, w, a) for a in A]) - np.mean([cos_sim(m, w, b) for b in B])\n",
      "def effect_size(m, X, Y, A, B):\n",
      "    num = np.mean([s_word(m, x, A, B) for x in X]) - np.mean([s_word(m, y, A, B) for y in Y])\n",
      "    return num/np.std([s_word(m, x, A, B) for x in X + Y])\n",
      "def wbias(m):\n",
      "    return effect_size(m, male, female, stereotypes_m, stereotypes_f)\n",
      "207/262:\n",
      "pwbiasshort = [wbias(m) for m in pmodelsshort]\n",
      "bwbiasshort = [wbias(m) for m in bmodelsshort]\n",
      "plt.plot([i*200 for i in range(1,10)], bwbiasshort, label=\"books\")\n",
      "plt.plot([i*200 for i in range(1,10)], pwbiasshort, label=\"politics\")\n",
      "plt.legend()\n",
      "plt.title(\"WEAT Bias on Professions vs Number of Articles \\n in Training Set (200-1800 articles)\")\n",
      "plt.show()\n",
      "207/263:\n",
      "stereotypes_m = ['captain', 'president', 'dean', 'officer', 'manager']\n",
      "stereotypes_f = ['teacher', 'author', 'secretary', 'editor']\n",
      "male = [\"man\", \"boys\", \"brother\", \"he\", \"him\", \"son\", \"father\"]\n",
      "female = [\"woman\", \"girls\", \"sister\", \"she\", \"her\", \\\n",
      "          \"daughter\", \"mother\"]\n",
      "def cos_sim(m, x, y):\n",
      "    wv1 = m.wv.get_vector(x)\n",
      "    wv2 = m.wv.get_vector(y)\n",
      "    sim = np.dot(wv1, wv2)/(np.linalg.norm(wv1)*np.linalg.norm(wv2))\n",
      "    return sim\n",
      "def s_word(m, w, A, B):\n",
      "    return np.mean([cos_sim(m, w, a) for a in A]) - np.mean([cos_sim(m, w, b) for b in B])\n",
      "def effect_size(m, X, Y, A, B):\n",
      "    num = np.mean([s_word(m, x, A, B) for x in X]) - np.mean([s_word(m, y, A, B) for y in Y])\n",
      "    return num/np.std([s_word(m, x, A, B) for x in X + Y])\n",
      "def wbias(m):\n",
      "    return effect_size(m, male, female, stereotypes_m, stereotypes_f)\n",
      "207/264:\n",
      "pwbiasshort = [wbias(m) for m in pmodelsshort]\n",
      "bwbiasshort = [wbias(m) for m in bmodelsshort]\n",
      "plt.plot([i*200 for i in range(1,10)], bwbiasshort, label=\"books\")\n",
      "plt.plot([i*200 for i in range(1,10)], pwbiasshort, label=\"politics\")\n",
      "plt.legend()\n",
      "plt.title(\"WEAT Bias on Professions vs Number of Articles \\n in Training Set (200-1800 articles)\")\n",
      "plt.show()\n",
      "207/265:\n",
      "stereotypes_m = ['captain', 'president', 'dean', 'officer', 'manager']\n",
      "stereotypes_f = ['teacher', 'author', 'secretary', 'editor']\n",
      "male = [\"man\", \"brother\", \"he\", \"him\", \"son\", \"father\"]\n",
      "female = [\"woman\", \"sister\", \"she\", \"her\", \\\n",
      "          \"daughter\", \"mother\"]\n",
      "def cos_sim(m, x, y):\n",
      "    wv1 = m.wv.get_vector(x)\n",
      "    wv2 = m.wv.get_vector(y)\n",
      "    sim = np.dot(wv1, wv2)/(np.linalg.norm(wv1)*np.linalg.norm(wv2))\n",
      "    return sim\n",
      "def s_word(m, w, A, B):\n",
      "    return np.mean([cos_sim(m, w, a) for a in A]) - np.mean([cos_sim(m, w, b) for b in B])\n",
      "def effect_size(m, X, Y, A, B):\n",
      "    num = np.mean([s_word(m, x, A, B) for x in X]) - np.mean([s_word(m, y, A, B) for y in Y])\n",
      "    return num/np.std([s_word(m, x, A, B) for x in X + Y])\n",
      "def wbias(m):\n",
      "    return effect_size(m, male, female, stereotypes_m, stereotypes_f)\n",
      "207/266:\n",
      "pwbiasshort = [wbias(m) for m in pmodelsshort]\n",
      "bwbiasshort = [wbias(m) for m in bmodelsshort]\n",
      "plt.plot([i*200 for i in range(1,10)], bwbiasshort, label=\"books\")\n",
      "plt.plot([i*200 for i in range(1,10)], pwbiasshort, label=\"politics\")\n",
      "plt.legend()\n",
      "plt.title(\"WEAT Bias on Professions vs Number of Articles \\n in Training Set (200-1800 articles)\")\n",
      "plt.show()\n",
      "207/267:\n",
      "stereotypes_m = ['captain', 'president', 'principal', 'dean', \\\n",
      "                 'coach', 'director', 'officer', 'manager']\n",
      "stereotypes_f = ['teacher', 'author', 'secretary', 'writer', \\\n",
      "                 'singer', 'teenager', 'editor', 'photographer']\n",
      "male = [\"male\", \"man\", \"boy\", \"boys\", \"brother\", \"he\", \"him\", \"son\", \"father\"]\n",
      "female = [\"female\", \"woman\", \"girl\", \"girls\", \"sister\", \"she\", \"her\", \\\n",
      "          \"daughter\", \"mother\"]\n",
      "def cos_sim(m, x, y):\n",
      "    wv1 = m.wv.get_vector(x)\n",
      "    wv2 = m.wv.get_vector(y)\n",
      "    sim = np.dot(wv1, wv2)/(np.linalg.norm(wv1)*np.linalg.norm(wv2))\n",
      "    return sim\n",
      "def s_word(m, w, A, B):\n",
      "    return np.mean([cos_sim(m, w, a) for a in A]) - np.mean([cos_sim(m, w, b) for b in B])\n",
      "def effect_size(m, X, Y, A, B):\n",
      "    num = np.mean([s_word(m, x, A, B) for x in X]) - np.mean([s_word(m, y, A, B) for y in Y])\n",
      "    return num/np.std([s_word(m, x, A, B) for x in X + Y])\n",
      "def wbias(m):\n",
      "    return effect_size(m, male, female, stereotypes_m, stereotypes_f)\n",
      "207/268:\n",
      "bookwbias = []\n",
      "for bm in booksmodels:\n",
      "    b = wbias(bm)\n",
      "    print(b)\n",
      "    bookwbias.append(b)\n",
      "207/269:\n",
      "pwbias = []\n",
      "for m in pmodels:\n",
      "    b = wbias(m)\n",
      "    print(b)\n",
      "    pwbias.append(b)\n",
      "207/270:\n",
      "plt.plot([i*500 for i in range(3,21)], bookwbias[2:], label=\"books\")\n",
      "plt.plot([i*500 for i in range(3,21)], pwbias[1:], label=\"politics\")\n",
      "plt.legend()\n",
      "plt.title(\"WEAT Bias on Professions vs Number of Articles \\n in Training Set (1500-10000 articles)\")\n",
      "plt.show()\n",
      "207/271: from nltk.corpus import wordnet as wn\n",
      "207/272: wn.synsets('person')\n",
      "207/273: wn.synsets('person'), wn.synsets('man'), wn.synsets('woman')\n",
      "207/274: person = wn.synset('person.n.01'), man = wn.synset('man.n.01'), woman = wn.synset('woman.n.01')\n",
      "207/275:\n",
      "person = wn.synset('person.n.01')\n",
      "man = wn.synset('man.n.01'), woman = wn.synset('woman.n.01')\n",
      "207/276:\n",
      "person = wn.synset('person.n.01')\n",
      "man = wn.synset('man.n.01')woman = wn.synset('woman.n.01')\n",
      "207/277:\n",
      "person = wn.synset('person.n.01')\n",
      "man = wn.synset('man.n.01')\n",
      "woman = wn.synset('woman.n.01')\n",
      "207/278: man.path_similarity(person)\n",
      "207/279: man.path_similarity(person), woman.path_similarity(person), man.path_similarity(woman)\n",
      "207/280: president = wn.synset('president.n.01')\n",
      "207/281: wn.synsets('person')\n",
      "207/282: man.path_similarity(president), woman.path_similarity(president)\n",
      "207/283: president = wn.synset('president.n.01'), nurse = wn.synset('nurse.n.01'), teacher = wn.synset('teacher.n.01')\n",
      "207/284:\n",
      "president = wn.synset('president.n.01')\n",
      "nurse = wn.synset('nurse.n.01')\n",
      "teacher = wn.synset('teacher.n.01')\n",
      "207/285:\n",
      "for x in [president, nurse, teacher]:\n",
      "    print(man.path_similarity(x), woman.path_similarity(x))\n",
      "207/286:\n",
      "president = wn.synset('president.n.01')\n",
      "nurse = wn.synset('nurse.n.01')\n",
      "teacher = wn.synset('teacher.n.01')\n",
      "mother = wn.synset('mother.n.01')\n",
      "207/287:\n",
      "for x in [president, nurse, teacher]:\n",
      "    print(man.path_similarity(x), woman.path_similarity(x))\n",
      "207/288:\n",
      "for x in [president, nurse, teacher, mother]:\n",
      "    print(man.path_similarity(x), woman.path_similarity(x))\n",
      "207/289:\n",
      "president = wn.synset('president.n.01')\n",
      "nurse = wn.synset('nurse.n.01')\n",
      "teacher = wn.synset('teacher.n.01')\n",
      "girl = wn.synset('girl.n.01')\n",
      "207/290:\n",
      "for x in [president, nurse, teacher, mother]:\n",
      "    print(man.path_similarity(x), woman.path_similarity(x))\n",
      "207/291:\n",
      "for x in [president, nurse, teacher, girl]:\n",
      "    print(man.path_similarity(x), woman.path_similarity(x))\n",
      "207/292:\n",
      "president = wn.synset('president.n.01')\n",
      "nurse = wn.synset('nurse.n.01')\n",
      "teacher = wn.synset('teacher.n.01')\n",
      "girl = wn.synset('girl.n.01')\n",
      "mother = wn.synset('mother.n.01')\n",
      "brother = wn.synset('brother.n.01')\n",
      "207/293:\n",
      "for x in [president, nurse, teacher, girl, mother, brother]:\n",
      "    print(man.path_similarity(x), woman.path_similarity(x))\n",
      "207/294:\n",
      "president = wn.synset('president.n.01')\n",
      "nurse = wn.synset('nurse.n.01')\n",
      "teacher = wn.synset('teacher.n.01')\n",
      "girl = wn.synset('girl.n.01')\n",
      "mother = wn.synset('mother.n.01')\n",
      "brother = wn.synset('brother.n.01')\n",
      "female = wn.synset('female.n.01')\n",
      "207/295:\n",
      "for x in [president, nurse, teacher, girl, mother, brother, female]:\n",
      "    print(man.path_similarity(x), woman.path_similarity(x))\n",
      "207/296:\n",
      "for x in [president, nurse, teacher, girl, mother, brother]:\n",
      "    print(man.path_similarity(x), woman.path_similarity(x), male.path_similarity(x), female.path_similarity(x))\n",
      "207/297: male.path_similarity(x)\n",
      "207/298: male\n",
      "207/299: person\n",
      "207/300: mother\n",
      "207/301: woman\n",
      "207/302: wn.synsets('male')\n",
      "207/303: wn.synsets('male')[0].path_similarity(girl)\n",
      "207/304:\n",
      "president = wn.synset('president.n.01')\n",
      "nurse = wn.synset('nurse.n.01')\n",
      "teacher = wn.synset('teacher.n.01')\n",
      "girl = wn.synset('girl.n.01')\n",
      "mother = wn.synset('mother.n.01')\n",
      "brother = wn.synset('brother.n.01')\n",
      "female = wn.synset('female.n.01')\n",
      "male = wn.synset('male.n.01')\n",
      "tall = wn.synset('tall.adj.01')\n",
      "207/305: wn.synsets('male')\n",
      "207/306:\n",
      "president = wn.synset('president.n.01')\n",
      "nurse = wn.synset('nurse.n.01')\n",
      "teacher = wn.synset('teacher.n.01')\n",
      "girl = wn.synset('girl.n.01')\n",
      "mother = wn.synset('mother.n.01')\n",
      "brother = wn.synset('brother.n.01')\n",
      "female = wn.synset('female.n.01')\n",
      "male = wn.synset('male.n.01')\n",
      "tall = wn.synset('tall.a.01')\n",
      "smart = wn.synset('smart.a.01')\n",
      "liberal = wn.synset('liberal.a.01')\n",
      "207/307: woman.path_similarity('liberal')\n",
      "207/308: woman.path_similarity(liberal)\n",
      "207/309: woman.path_similarity(tall)\n",
      "207/310: woman.path_similarity(president)\n",
      "207/311:\n",
      "president = wn.synset('president.n.01')\n",
      "nurse = wn.synset('nurse.n.01')\n",
      "teacher = wn.synset('teacher.n.01')\n",
      "girl = wn.synset('girl.n.01')\n",
      "mother = wn.synset('mother.n.01')\n",
      "brother = wn.synset('brother.n.01')\n",
      "female = wn.synset('female.n.01')\n",
      "male = wn.synset('male.n.01')\n",
      "tall = wn.synset('tall.a.01')\n",
      "smart = wn.synset('smart.a.01')\n",
      "liberal = wn.synset('liberal.a.01')\n",
      "home = wn.synset('home.a.01')\n",
      "207/312: woman.path_similarity(home)\n",
      "207/313:\n",
      "president = wn.synset('president.n.01')\n",
      "nurse = wn.synset('nurse.n.01')\n",
      "teacher = wn.synset('teacher.n.01')\n",
      "girl = wn.synset('girl.n.01')\n",
      "mother = wn.synset('mother.n.01')\n",
      "brother = wn.synset('brother.n.01')\n",
      "female = wn.synset('female.n.01')\n",
      "male = wn.synset('male.n.01')\n",
      "tall = wn.synset('tall.a.01')\n",
      "smart = wn.synset('smart.a.01')\n",
      "liberal = wn.synset('liberal.a.01')\n",
      "home = wn.synset('home.n.01')\n",
      "207/314: woman.path_similarity(home)\n",
      "207/315: wn.synset('tall.a.01')\n",
      "207/316: wn.synset('tall.a.01').path_similarity(liberal)\n",
      "207/317: wn.synset('tall.a.01').path_similarity(smart)\n",
      "207/318: mother.path_similarity(girl)\n",
      "207/319: mother.path_similarity(girl), mother.path_similarity(person)\n",
      "207/320: mother.path_similarity(girl), mother.path_similarity(person), mother.path_similarity(female)\n",
      "207/321: mother.path_similarity(girl), mother.path_similarity(person), mother.path_similarity(female), mother.path_similarity(male)\n",
      "207/322:\n",
      "for x in [president, nurse, teacher, girl, mother, brother]:\n",
      "    print(man.path_similarity(x), woman.path_similarity(x), male.path_similarity(x), female.path_similarity(x))\n",
      "207/323: wn.synset('smart.a.01')\n",
      "207/324: wn.synset('smart.a.01').path_similarity(woman)\n",
      "207/325: wn.synset('smart.a.01').path_similarity(tall)\n",
      "207/326: wn.synset('smart.a.01').path_similarity(female)\n",
      "207/327: smart.path_similarity(male)\n",
      "207/328: smart.path_similarity(wn.synset('male.a.01'))\n",
      "207/329:\n",
      "president = wn.synset('president.n.01')\n",
      "nurse = wn.synset('nurse.n.01')\n",
      "teacher = wn.synset('teacher.n.01')\n",
      "girl = wn.synset('girl.n.01')\n",
      "mother = wn.synset('mother.n.01')\n",
      "brother = wn.synset('brother.n.01')\n",
      "female = wn.synset('female.n.02')\n",
      "male = wn.synset('male.n.02')\n",
      "tall = wn.synset('tall.a.01')\n",
      "smart = wn.synset('smart.a.01')\n",
      "liberal = wn.synset('liberal.a.01')\n",
      "home = wn.synset('home.n.01')\n",
      "207/330: woman.path_similarity(home)\n",
      "207/331:\n",
      "woman.path_similarity(home)\n",
      "man.path_similarity(home)\n",
      "207/332: woman.path_similarity(home), man.path_similarity(home)\n",
      "207/333: mother.path_similarity(girl), mother.path_similarity(person), mother.path_similarity(female), mother.path_similarity(male)\n",
      "207/334:\n",
      "for x in [president, nurse, teacher, girl, mother, brother]:\n",
      "    print(man.path_similarity(x), woman.path_similarity(x), male.path_similarity(x), female.path_similarity(x))\n",
      "207/335: from nltk.corpus import sentiwordnet as swn\n",
      "207/336: gender_specific\n",
      "207/337:\n",
      "president = wn.synset('president.n.01')\n",
      "nurse = wn.synset('nurse.n.01')\n",
      "teacher = wn.synset('teacher.n.01')\n",
      "girl = wn.synset('girl.n.01')\n",
      "mother = wn.synset('mother.n.01')\n",
      "brother = wn.synset('brother.n.01')\n",
      "female = wn.synset('female.n.02')\n",
      "male = wn.synset('male.n.02')\n",
      "tall = wn.synset('tall.a.01')\n",
      "actress = wn.synset('actress.n.01')\n",
      "home = wn.synset('home.n.01')\n",
      "207/338:\n",
      "for x in [president, nurse, teacher, girl, mother, brother, actress]:\n",
      "    print(man.path_similarity(x), woman.path_similarity(x), male.path_similarity(x), female.path_similarity(x))\n",
      "207/339:\n",
      "president = wn.synset('president.n.01')\n",
      "nurse = wn.synset('nurse.n.01')\n",
      "teacher = wn.synset('teacher.n.01')\n",
      "girl = wn.synset('girl.n.01')\n",
      "mother = wn.synset('mother.n.01')\n",
      "brother = wn.synset('brother.n.01')\n",
      "female = wn.synset('female.n.02')\n",
      "male = wn.synset('male.n.02')\n",
      "tall = wn.synset('tall.a.01')\n",
      "maid = wn.synset('maid.n.01')\n",
      "home = wn.synset('home.n.01')\n",
      "207/340:\n",
      "for x in [president, nurse, teacher, girl, mother, brother, actress, maid]:\n",
      "    print(man.path_similarity(x), woman.path_similarity(x), male.path_similarity(x), female.path_similarity(x))\n",
      "207/341:\n",
      "president = wn.synset('president.n.01')\n",
      "nurse = wn.synset('nurse.n.01')\n",
      "teacher = wn.synset('teacher.n.01')\n",
      "girl = wn.synset('girl.n.01')\n",
      "mother = wn.synset('mother.n.01')\n",
      "brother = wn.synset('brother.n.01')\n",
      "female = wn.synset('female.n.02')\n",
      "male = wn.synset('male.n.02')\n",
      "tall = wn.synset('tall.a.01')\n",
      "maid = wn.synset('maid.n.01')\n",
      "home = wn.synset('home.n.01')\n",
      "schoolgirl = wn.synset('schoolgirl.n.01')\n",
      "207/342:\n",
      "for x in [president, nurse, teacher, girl, mother, brother, schoolgirl, maid]:\n",
      "    print(man.path_similarity(x), woman.path_similarity(x), male.path_similarity(x), female.path_similarity(x))\n",
      "207/343:\n",
      "for x in [president, nurse, teacher, girl, mother, brother, schoolgirl, maid, female]:\n",
      "    print(man.path_similarity(x), woman.path_similarity(x), male.path_similarity(x), female.path_similarity(x))\n",
      "207/344: swn.senti_synset('man.n.01'), swn.senti_synset('woman.n.01')\n",
      "207/345:\n",
      "nltk.download('sentiwordnet')\n",
      "from nltk.corpus import sentiwordnet as swn\n",
      "207/346: swn.senti_synset('man.n.01'), swn.senti_synset('woman.n.01')\n",
      "207/347:\n",
      "sman = swn.senti_synset('man.n.01')\n",
      "swoman = swn.senti_synset('woman.n.01')\n",
      "207/348: sman.pos_score(), swoman.pos_score\n",
      "207/349: sman.pos_score(), swoman.pos_score()\n",
      "207/350:\n",
      "sman = swn.senti_synset('man.n.01')\n",
      "swoman = swn.senti_synset('woman.n.01')\n",
      "tall = swn.senti_synset('tall.a.01')\n",
      "maid = swn.senti_synset('maid.n.01')\n",
      "home = swn.senti_synset('home.n.01')\n",
      "207/351: sman.pos_score(), swoman.pos_score(), tall.pos_score(), maid.pos_score(), home.pos_score()\n",
      "207/352:\n",
      "president = wn.synset('president.n.01')\n",
      "nurse = wn.synset('nurse.n.01')\n",
      "teacher = wn.synset('teacher.n.01')\n",
      "girl = wn.synset('girl.n.01')\n",
      "mother = wn.synset('mother.n.01')\n",
      "brother = wn.synset('brother.n.01')\n",
      "female = wn.synset('female.n.02')\n",
      "male = wn.synset('male.n.02')\n",
      "tall = wn.synset('tall.a.01')\n",
      "maid = wn.synset('maid.n.01')\n",
      "home = wn.synset('home.n.01')\n",
      "207/353:\n",
      "for x in [president, nurse, teacher, girl, mother, brother, schoolgirl, maid, female]:\n",
      "    print(man.path_similarity(x), woman.path_similarity(x), male.path_similarity(x), female.path_similarity(x))\n",
      "207/354:\n",
      "sman = swn.senti_synset('man.n.01')\n",
      "swoman = swn.senti_synset('woman.n.01')\n",
      "tall = swn.senti_synset('tall.a.01')\n",
      "maid = swn.senti_synset('maid.n.01')\n",
      "smart = swn.senti_synset('smart.n.01')\n",
      "207/355: sman.pos_score(), swoman.pos_score(), tall.pos_score(), maid.pos_score(), home.pos_score()\n",
      "207/356: sman.pos_score(), swoman.pos_score(), tall.pos_score(), maid.pos_score(), smart.pos_score()\n",
      "207/357:\n",
      "sman = swn.senti_synset('man.n.01')\n",
      "swoman = swn.senti_synset('woman.n.01')\n",
      "tall = swn.senti_synset('tall.a.01')\n",
      "maid = swn.senti_synset('maid.n.01')\n",
      "smart = swn.senti_synset('smart.a.01')\n",
      "207/358: sman.pos_score(), swoman.pos_score(), tall.pos_score(), maid.pos_score(), smart.pos_score()\n",
      "207/359:\n",
      "sman = swn.senti_synset('man.n.01')\n",
      "swoman = swn.senti_synset('woman.n.01')\n",
      "tall = swn.senti_synset('tall.a.01')\n",
      "maid = swn.senti_synset('maid.n.01')\n",
      "smart = swn.senti_synset('smart.a.01')\n",
      "awesome = swn.senti_synset('awesome.a.01')\n",
      "207/360: sman.pos_score(), swoman.pos_score(), tall.pos_score(), maid.pos_score(), smart.pos_score()\n",
      "207/361:\n",
      "for s in [sman, swoman, tall, maid, smart, awesome]:\n",
      "    print(s, s.pos_score(), s.neg_score())\n",
      "207/362:\n",
      "for s in [sman, swoman, tall, maid, smart, awesome]:\n",
      "    print(s)\n",
      "207/363:\n",
      "sman = swn.senti_synset('man.n.01')\n",
      "swoman = swn.senti_synset('woman.n.01')\n",
      "tall = swn.senti_synset('tall.a.01')\n",
      "maid = swn.senti_synset('maid.n.01')\n",
      "smart = swn.senti_synset('smart.a.01')\n",
      "awesome = swn.senti_synset('awesome.a.01')\n",
      "shy = swn.senti_synset('shy.a.01')\n",
      "207/364:\n",
      "for s in [sman, swoman, tall, maid, smart, awesome]:\n",
      "    print(s)\n",
      "207/365:\n",
      "for s in [sman, swoman, tall, maid, smart, awesome, shy]:\n",
      "    print(s)\n",
      "207/366:\n",
      "sman = swn.senti_synset('man.n.01')\n",
      "swoman = swn.senti_synset('woman.n.01')\n",
      "tall = swn.senti_synset('tall.a.01')\n",
      "maid = swn.senti_synset('maid.n.01')\n",
      "smart = swn.senti_synset('smart.a.01')\n",
      "awesome = swn.senti_synset('awesome.a.01')\n",
      "shy = swn.senti_synset('shy.a.01')\n",
      "liberal = swn.senti_synset('liberal.a.01')\n",
      "conservative = swn.senti_synset('conservative.a.01')\n",
      "207/367:\n",
      "for s in [sman, swoman, tall, maid, smart, awesome, shy, liberal, conservative]:\n",
      "    print(s)\n",
      "207/368:\n",
      "for s in [sman, swoman, tall, maid, smart, awesome, shy, liberal, conservative, moderate]:\n",
      "    print(s)\n",
      "207/369:\n",
      "sman = swn.senti_synset('man.n.01')\n",
      "swoman = swn.senti_synset('woman.n.01')\n",
      "tall = swn.senti_synset('tall.a.01')\n",
      "maid = swn.senti_synset('maid.n.01')\n",
      "smart = swn.senti_synset('smart.a.01')\n",
      "awesome = swn.senti_synset('awesome.a.01')\n",
      "shy = swn.senti_synset('shy.a.01')\n",
      "liberal = swn.senti_synset('liberal.a.01')\n",
      "conservative = swn.senti_synset('conservative.a.01')\n",
      "moderate = swn.senti_synset('moderate.a.01')\n",
      "207/370:\n",
      "for s in [sman, swoman, tall, maid, smart, awesome, shy, liberal, conservative, moderate]:\n",
      "    print(s)\n",
      "207/371:\n",
      "sman = swn.senti_synset('man.n.01')\n",
      "swoman = swn.senti_synset('woman.n.01')\n",
      "tall = swn.senti_synset('tall.a.01')\n",
      "maid = swn.senti_synset('maid.n.01')\n",
      "smart = swn.senti_synset('smart.a.01')\n",
      "awesome = swn.senti_synset('awesome.a.01')\n",
      "shy = swn.senti_synset('shy.a.01')\n",
      "liberal = swn.senti_synset('liberal.a.01')\n",
      "conservative = swn.senti_synset('conservative.a.01')\n",
      "moderate = swn.senti_synset('moderate.a.01')\n",
      "demanding = swn.senti_synset('demanding.a.01')\n",
      "207/372:\n",
      "for s in [sman, swoman, tall, maid, smart, awesome, shy, liberal, conservative, moderate, demanding]:\n",
      "    print(s)\n",
      "207/373:\n",
      "url = 'http://ideonomy.mit.edu/essays/traits.html'\n",
      "file = urllib2.urlopen(url)\n",
      "html = file.read()\n",
      "file.close()\n",
      "soup = BeautifulSoup(html)\n",
      "207/374: soup.find_all(attrs={\"class\": \"ol\"})\n",
      "207/375: soup.find_all('li')\n",
      "207/376: soup.find_all('li').contents[0]\n",
      "207/377: soup.find_all('li')[0].contents\n",
      "207/378: soup.find_all('li')[0].contents[0].trim()\n",
      "207/379: soup.find_all('li')[0].contents[0]\n",
      "207/380: trim(soup.find_all('li')[0].contents[0])\n",
      "207/381: soup.find_all('li')[0].contents[0].strip()\n",
      "207/382: adjs = [x.contents[0].strip() for x in soup.find_all('li')]\n",
      "207/383: adjs\n",
      "207/384: adjectives\n",
      "207/385: len([x for x in adjectives if x not in adjs])\n",
      "207/386: len(adjectives)\n",
      "207/387: adjectives\n",
      "207/388: adjs = [lower(x.contents[0].strip()) for x in soup.find_all('li')]\n",
      "207/389: adjs = [x.contents[0].strip().lower() for x in soup.find_all('li')]\n",
      "207/390: adjs\n",
      "207/391: len([x for x in adjectives if x not in adjs])\n",
      "207/392: [x for x in adjectives if x not in adjs]\n",
      "207/393: [x for x in adjectives if x in adjs]\n",
      "207/394: [x for x in adjectives if x not in adjs]\n",
      "207/395: adjectives[:5]\n",
      "207/396: adjectives[:10]\n",
      "207/397: [x for x in adjectives if x not in adjs][:10]\n",
      "207/398: adjs[:10]\n",
      "207/399:\n",
      "adjs_all = adjs + [x for x in adjectives if x not in adjs]\n",
      "len(adjs_all)\n",
      "207/400: len([x for x in adjs_all if x in vocab])\n",
      "207/401: # Sentiment Bias - weighted projection bias, each word multiplied by (pos_score - neg_score)\n",
      "207/402:\n",
      "def sbias(g, m):\n",
      "    b = 0\n",
      "    l = list(filter(lambda x: x in m.wv.vocab, adjs_all))\n",
      "    p, n, np, nn = 0, 0\n",
      "    for x in l:\n",
      "        proj = gproj(g, m, x)\n",
      "        b += abs(proj)\n",
      "        s = swn.senti_synset(x + \".a.01\")\n",
      "        score = s.pos_score() - s.neg_score()\n",
      "        if proj < 0:\n",
      "            n += abs(proj)*score\n",
      "            nn += 1\n",
      "        else:\n",
      "            p += abs(proj)*score\n",
      "            np += 1\n",
      "    return b/len(l), n/nn, p/np\n",
      "207/403: sbias(getg(mpol), mpol)\n",
      "207/404:\n",
      "def sbias(g, m):\n",
      "    b = 0\n",
      "    l = list(filter(lambda x: x in m.wv.vocab, adjs_all))\n",
      "    p, n, np, nn = 0, 0, 0, 0\n",
      "    for x in l:\n",
      "        proj = gproj(g, m, x)\n",
      "        b += abs(proj)\n",
      "        s = swn.senti_synset(x + \".a.01\")\n",
      "        score = s.pos_score() - s.neg_score()\n",
      "        if proj < 0:\n",
      "            n += abs(proj)*score\n",
      "            nn += 1\n",
      "        else:\n",
      "            p += abs(proj)*score\n",
      "            np += 1\n",
      "    return b/len(l), n/nn, p/np\n",
      "207/405: sbias(getg(mpol), mpol)\n",
      "207/406: swn.senti_synset('knowledge')\n",
      "207/407: swn.senti_synset.contains('knowledge')\n",
      "207/408: swn.senti_synset\n",
      "207/409: swn.senti_synset('knowledge.a.01')\n",
      "207/410:\n",
      "def sbias(g, m):\n",
      "    b = 0\n",
      "    l = list(filter(lambda x: x in m.wv.vocab, adjs_all))\n",
      "    p, n, np, nn = 0, 0, 0, 0\n",
      "    for x in l:\n",
      "        proj = gproj(g, m, x)\n",
      "        b += abs(proj)\n",
      "        try:\n",
      "            s = swn.senti_synset(x + \".a.01\")\n",
      "            score = s.pos_score() - s.neg_score()\n",
      "            if proj < 0:\n",
      "                n += abs(proj)*score\n",
      "                nn += 1\n",
      "            else:\n",
      "                p += abs(proj)*score\n",
      "                np += 1\n",
      "        except:\n",
      "            continue\n",
      "    return b/len(l), n/nn, p/np\n",
      "207/411: sbias(getg(mpol), mpol)\n",
      "207/412:\n",
      "def sbias(g, m):\n",
      "    b = 0\n",
      "    l = list(filter(lambda x: x in m.wv.vocab, adjs_all))\n",
      "    p, n, np, nn = 0, 0, 0, 0\n",
      "    for x in l:\n",
      "        proj = gproj(g, m, x)\n",
      "        b += abs(proj)\n",
      "        try:\n",
      "            s = swn.senti_synset(x + \".a.01\")\n",
      "            score = s.pos_score() - s.neg_score()\n",
      "            if proj < 0:\n",
      "                print(\"neg\", proj, score, x)\n",
      "                n += abs(proj)*score\n",
      "                nn += 1\n",
      "            else:\n",
      "                print(\"pos\", proj, score, x)\n",
      "                p += abs(proj)*score\n",
      "                np += 1\n",
      "        except:\n",
      "            continue\n",
      "    return b/len(l), n/nn, p/np\n",
      "207/413: sbias(getg(mpol), mpol)\n",
      "207/414:\n",
      "def sbias(g, m):\n",
      "    b = 0\n",
      "    l = list(filter(lambda x: x in m.wv.vocab, adjs_all))\n",
      "    p, n, np, nn = 0, 0, 0, 0\n",
      "    for x in l:\n",
      "        proj = gproj(g, m, x)\n",
      "        b += abs(proj)\n",
      "        try:\n",
      "            s = swn.senti_synset(x + \".a.01\")\n",
      "            score = s.pos_score() - s.neg_score()\n",
      "            if proj < 0:\n",
      "#                 print(\"neg\", proj, score, x)\n",
      "                n += score\n",
      "                nn += 1\n",
      "            else:\n",
      "#                 print(\"pos\", proj, score, x)\n",
      "                p += score\n",
      "                np += 1\n",
      "        except:\n",
      "            continue\n",
      "    return b/len(l), n/nn, p/np\n",
      "207/415: sbias(getg(mpol), mpol)\n",
      "207/416: gproj(getg(mpol), mpol, 'he')\n",
      "207/417:\n",
      "gproj(getg(mpol), mpol, 'he')\n",
      "gproj(getg(mpol), mpol, 'she')\n",
      "207/418: gproj(getg(mpol), mpol, 'he'), gproj(getg(mpol), mpol, 'she')\n",
      "207/419:\n",
      "def sbias(g, m):\n",
      "    b = 0\n",
      "    l = list(filter(lambda x: x in m.wv.vocab, adjs_all))\n",
      "    p, n, np, nn = 0, 0, 0, 0\n",
      "    for x in l:\n",
      "        proj = gproj(g, m, x)\n",
      "        b += abs(proj)\n",
      "        try:\n",
      "            s = swn.senti_synset(x + \".a.01\")\n",
      "            score = s.pos_score() - s.neg_score()\n",
      "            if proj < 0:\n",
      "#                 print(\"neg\", proj, score, x)\n",
      "                n += score\n",
      "                nn += 1\n",
      "            else:\n",
      "#                 print(\"pos\", proj, score, x)\n",
      "                p += score\n",
      "                np += 1\n",
      "        except:\n",
      "            continue\n",
      "    return b/len(l), p/np, n/nn\n",
      "207/420: gproj(getg(mpol), mpol, 'he'), gproj(getg(mpol), mpol, 'she')\n",
      "207/421: sbias(getg(mpol), mpol)\n",
      "207/422:\n",
      "def sbias(g, m):\n",
      "    b = 0\n",
      "    l = list(filter(lambda x: x in m.wv.vocab, adjs_all))\n",
      "    p, n, np, nn = 0, 0, 0, 0\n",
      "    for x in l:\n",
      "        proj = gproj(g, m, x)\n",
      "        b += abs(proj)\n",
      "        try:\n",
      "            s = swn.senti_synset(x + \".a.01\")\n",
      "            score = s.pos_score() - s.neg_score()\n",
      "            if proj < -0.1:\n",
      "#                 print(\"neg\", proj, score, x)\n",
      "                n += score\n",
      "                nn += 1\n",
      "            if proj > 0.1:\n",
      "#                 print(\"pos\", proj, score, x)\n",
      "                p += score\n",
      "                np += 1\n",
      "        except:\n",
      "            continue\n",
      "    return b/len(l), p/np, n/nn\n",
      "207/423: sbias(getg(mpol), mpol)\n",
      "207/424:\n",
      "def sbias(g, m, t):\n",
      "    b = 0\n",
      "    l = list(filter(lambda x: x in m.wv.vocab, adjs_all))\n",
      "    p, n, np, nn = 0, 0, 0, 0\n",
      "    for x in l:\n",
      "        proj = gproj(g, m, x)\n",
      "        b += abs(proj)\n",
      "        try:\n",
      "            s = swn.senti_synset(x + \".a.01\")\n",
      "            score = s.pos_score() - s.neg_score()\n",
      "            if proj < -t:\n",
      "#                 print(\"neg\", proj, score, x)\n",
      "                n += score\n",
      "                nn += 1\n",
      "            if proj > t:\n",
      "#                 print(\"pos\", proj, score, x)\n",
      "                p += score\n",
      "                np += 1\n",
      "        except:\n",
      "            continue\n",
      "    return b/len(l), p/np, n/nn\n",
      "207/425: sbias(getg(mpol), mpol, 0.1)\n",
      "207/426: sbias(getg(mpol), mpol, 0.2)\n",
      "207/427: sbias(getg(mpol), mpol, 0.5)\n",
      "207/428: sbias(getg(mpol), mpol, 0.05)\n",
      "207/429:\n",
      "def sbias(g, m):\n",
      "    b = 0\n",
      "    l = list(filter(lambda x: x in m.wv.vocab, adjs_all))\n",
      "    p, n, np, nn = 0, 0, 0, 0\n",
      "    for x in l:\n",
      "        proj = gproj(g, m, x)\n",
      "        b += abs(proj)\n",
      "        try:\n",
      "            s = swn.senti_synset(x + \".a.01\")\n",
      "            score = s.pos_score() - s.neg_score()\n",
      "            if proj < 0:\n",
      "#                 print(\"neg\", proj, score, x)\n",
      "                n += score\n",
      "                nn += 1\n",
      "            if proj > 0:\n",
      "#                 print(\"pos\", proj, score, x)\n",
      "                p += score\n",
      "                np += 1\n",
      "        except:\n",
      "            continue\n",
      "    return b/len(l), p/np, n/nn\n",
      "207/430: sbias(getg(mpol), mpol)\n",
      "207/431:\n",
      "def sbias(g, m):\n",
      "    b = 0\n",
      "    l = list(filter(lambda x: x in m.wv.vocab, adjs_all))\n",
      "    p, n, np, nn = 0, 0, 0, 0\n",
      "    for x in l:\n",
      "        proj = gproj(g, m, x)\n",
      "        b += abs(proj)\n",
      "        try:\n",
      "            s = swn.senti_synset(x + \".a.01\")\n",
      "            score = s.pos_score() - s.neg_score()\n",
      "            if proj < -0.2:\n",
      "                print(\"neg\", proj, score, x)\n",
      "                n += score\n",
      "                nn += 1\n",
      "            if proj > 0.2:\n",
      "                print(\"pos\", proj, score, x)\n",
      "                p += score\n",
      "                np += 1\n",
      "        except:\n",
      "            continue\n",
      "    return b/len(l), p/np, n/nn\n",
      "207/432: sbias(getg(mpol), mpol)\n",
      "207/433:\n",
      "def sbias(g, m):\n",
      "    b = 0\n",
      "    l = list(filter(lambda x: x in m.wv.vocab, adjs_all))\n",
      "    p, n, np, nn = 0, 0, 0, 0\n",
      "    for x in l:\n",
      "        proj = gproj(g, m, x)\n",
      "        b += abs(proj)\n",
      "        try:\n",
      "            s = swn.senti_synset(x + \".a.01\")\n",
      "            score = s.pos_score() - s.neg_score()\n",
      "            if proj < -0.2:\n",
      "                print(\"neg\", proj, score, x)\n",
      "                n += score\n",
      "                nn += 1\n",
      "            if proj > 0.1:\n",
      "                print(\"pos\", proj, score, x)\n",
      "                p += score\n",
      "                np += 1\n",
      "        except:\n",
      "            continue\n",
      "    return b/len(l), p/np, n/nn\n",
      "207/434: sbias(getg(mpol), mpol)\n",
      "207/435:\n",
      "def sbias(g, m):\n",
      "    b = 0\n",
      "    l = list(filter(lambda x: x in m.wv.vocab, adjs_all))\n",
      "    p, n, np, nn = 0, 0, 0, 0\n",
      "    for x in l:\n",
      "        proj = gproj(g, m, x)\n",
      "        b += abs(proj)\n",
      "        try:\n",
      "            s = swn.senti_synset(x + \".a.01\")\n",
      "            score = s.pos_score() - s.neg_score()\n",
      "            if proj < -0.2:\n",
      "                print(\"neg\", proj, score, x)\n",
      "                n += score\n",
      "                nn += 1\n",
      "            if proj > 0.15:\n",
      "                print(\"pos\", proj, score, x)\n",
      "                p += score\n",
      "                np += 1\n",
      "        except:\n",
      "            continue\n",
      "    return b/len(l), p/np, n/nn\n",
      "207/436: sbias(getg(mpol), mpol)\n",
      "207/437:\n",
      "def sbias(g, m):\n",
      "    b = 0\n",
      "    l = list(filter(lambda x: x in m.wv.vocab, adjs_all))\n",
      "    p, n, np, nn = 0, 0, 0, 0\n",
      "    for x in l:\n",
      "        proj = gproj(g, m, x)\n",
      "        b += abs(proj)\n",
      "        try:\n",
      "            s = swn.senti_synset(x + \".a.01\")\n",
      "            score = s.pos_score() - s.neg_score()\n",
      "            if proj < -0.15 and score is not 0:\n",
      "                print(\"neg\", proj, score, x)\n",
      "                n += score\n",
      "                nn += 1\n",
      "            if proj > 0.1 and score is not 0:\n",
      "                print(\"pos\", proj, score, x)\n",
      "                p += score\n",
      "                np += 1\n",
      "        except:\n",
      "            continue\n",
      "    return b/len(l), p/np, n/nn\n",
      "207/438: sbias(getg(mpol), mpol)\n",
      "207/439:\n",
      "def sbias(g, m):\n",
      "    b = 0\n",
      "    l = list(filter(lambda x: x in m.wv.vocab, adjs_all))\n",
      "    p, n, np, nn = 0, 0, 0, 0\n",
      "    for x in l:\n",
      "        proj = gproj(g, m, x)\n",
      "        b += abs(proj)\n",
      "        try:\n",
      "            s = swn.senti_synset(x + \".a.01\")\n",
      "            score = s.pos_score() - s.neg_score()\n",
      "            if proj < -0.15 and score != 0:\n",
      "                print(\"neg\", proj, score, x)\n",
      "                n += score\n",
      "                nn += 1\n",
      "            if proj > 0.1 and score != 0:\n",
      "                print(\"pos\", proj, score, x)\n",
      "                p += score\n",
      "                np += 1\n",
      "        except:\n",
      "            continue\n",
      "    return b/len(l), p/np, n/nn\n",
      "207/440: sbias(getg(mpol), mpol)\n",
      "207/441: sbias(getg(mbooks), mbooks)\n",
      "207/442:\n",
      "plt.plot([i*200 for i in range(1,10)] + [i*500 for i in range(3,21)], bbiasshort + bookbias[2:], label=\"books\")\n",
      "plt.plot([i*200 for i in range(1,10)] + [i*500 for i in range(3,21)], pbiasshort + pbias[1:], label=\"politics\")\n",
      "plt.legend()\n",
      "plt.title(\"Projection Bias on Professions vs Number of Articles \\n in Training Set (200-1800 articles)\")\n",
      "plt.show()\n",
      "207/443:\n",
      "slistshort = []\n",
      "for i in range(1, 10):\n",
      "    slistshort.append(sample(sports, i*200))\n",
      "smodelsshort = [getw2v(s) for s in slistshort]\n",
      "sbiasshort = [bias(getg(m), m, m.wv.vocab, proflist) for m in smodelsshort]\n",
      "207/444:\n",
      "plt.plot([i*200 for i in range(1,10)], sbiasshort, label=\"books\")\n",
      "plt.plot([i*200 for i in range(1,10)], sbiasshort, label=\"politics\")\n",
      "plt.legend()\n",
      "plt.title(\"Projection Bias on Professions vs Number of Articles \\n in Training Set (200-1800 articles)\")\n",
      "plt.show()\n",
      "207/445:\n",
      "plt.plot([i*200 for i in range(1,10)], bbiasshort, label=\"books\")\n",
      "plt.plot([i*200 for i in range(1,10)], pbiasshort, label=\"politics\")\n",
      "plt.plot([i*200 for i in range(1,10)], sbiasshort, label=\"sports\")\n",
      "\n",
      "plt.legend()\n",
      "plt.title(\"Projection Bias on Professions vs Number of Articles \\n in Training Set (200-1800 articles)\")\n",
      "plt.show()\n",
      "207/446:\n",
      "plt.plot([i*200 for i in range(1,10)], bbiasshort, label=\"books\")\n",
      "plt.plot([i*200 for i in range(1,10)], pbiasshort, label=\"politics\")\n",
      "plt.legend()\n",
      "plt.title(\"Projection Bias on Professions vs Number of Articles \\n in Training Set (200-1800 articles)\")\n",
      "plt.show()\n",
      "207/447: sports = getlist('Baseball') + getlist('Football')\n",
      "207/448:\n",
      "slistshort = []\n",
      "for i in range(1, 10):\n",
      "    slistshort.append(sample(sports, i*200))\n",
      "smodelsshort = [getw2v(s) for s in slistshort]\n",
      "sbiasshort = [bias(getg(m), m, m.wv.vocab, proflist) for m in smodelsshort]\n",
      "207/449:\n",
      "plt.plot([i*200 for i in range(1,10)], bbiasshort, label=\"books\")\n",
      "plt.plot([i*200 for i in range(1,10)], pbiasshort, label=\"politics\")\n",
      "plt.plot([i*200 for i in range(1,10)], sbiasshort, label=\"sports\")\n",
      "\n",
      "plt.legend()\n",
      "plt.title(\"Projection Bias on Professions vs Number of Articles \\n in Training Set (200-1800 articles)\")\n",
      "plt.show()\n",
      "207/450: len(sports)\n",
      "207/451:\n",
      "slist = []\n",
      "for i in range(2, 14):\n",
      "    slist.append(sample(sports, i*1000))\n",
      "smodels = [getw2v(s) for s in slist]\n",
      "sbias = [bias(getg(bm), bm, bm.wv.vocab, proflist) for m in smodels]\n",
      "207/452:\n",
      "plt.plot([i*500 for i in range(3,21)], bookbias[2:], label=\"books\")\n",
      "plt.plot([i*500 for i in range(3,21)], pbias[1:], label=\"politics\")\n",
      "plt.plot([i*500 for i in range(3,21)], sbias[1:], label=\"s\")\n",
      "\n",
      "plt.legend()\n",
      "plt.title(\"Projection Bias on Professions vs Number of Articles \\n in Training Set (1500-10000 articles)\")\n",
      "plt.show()\n",
      "207/453:\n",
      "plt.plot([i*500 for i in range(3,21)], bookbias[2:], label=\"books\")\n",
      "plt.plot([i*500 for i in range(3,21)], pbias[1:], label=\"politics\")\n",
      "plt.legend()\n",
      "plt.title(\"Projection Bias on Professions vs Number of Articles \\n in Training Set (1500-10000 articles)\")\n",
      "plt.show()\n",
      "207/454: plt.plot([i*1000 for i in range(1,14)], sbias[1:], label=\"s\")\n",
      "207/455: plt.plot([i*1000 for i in range(2,14)], sbias, label=\"s\")\n",
      "207/456:\n",
      "plt.plot([i*1000 for i in range(2,14)], sbias, label=\"s\")\n",
      "sbias\n",
      "207/457: sbias = [bias(getg(m), m, m.wv.vocab, proflist) for m in smodels]\n",
      "207/458:\n",
      "plt.plot([i*1000 for i in range(2,14)], sbias, label=\"s\")\n",
      "sbias\n",
      "207/459: top5('house')\n",
      "207/460:\n",
      "def top5(word):\n",
      "    for m in [mpol, mbooks, mmov, msports]:\n",
      "        print([x[0] for x in m.wv.most_similar(positive=[word], topn=5)])\n",
      "207/461: top5('house')\n",
      "207/462: top5('house')\n",
      "207/463:\n",
      "def top5(word):\n",
      "    for m in [mpol, mbooks, mmov, msports]:\n",
      "        print([x[0] for x in m.wv.most_similar(positive=[word], topn=6)])\n",
      "207/464: top5('house')\n",
      "207/465:\n",
      "def top5(word):\n",
      "    for m in [mpol, mbooks, mmov, msports]:\n",
      "        print([x[0] for x in m.wv.most_similar(positive=[word], topn=8)])\n",
      "207/466: top5('house')\n",
      "207/467: top5('monday')\n",
      "207/468: top5('seat')\n",
      "207/469:\n",
      "plt.plot([i*500 for i in range(3,21)], bookbias[2:], label=\"books\")\n",
      "plt.plot([i*500 for i in range(3,21)], pbias[1:], label=\"politics\")\n",
      "plt.legend()\n",
      "plt.title(\"Direct Bias on Professions vs Number of Articles \\n in Training Set (1500-10000 articles)\")\n",
      "plt.show()\n",
      "207/470:\n",
      "plt.plot([i*200 for i in range(1,10)], bbiasshort, label=\"books\")\n",
      "plt.plot([i*200 for i in range(1,10)], pbiasshort, label=\"politics\")\n",
      "plt.legend()\n",
      "plt.title(\"Direct Bias on Professions vs Number of Articles \\n in Training Set (200-1800 articles)\")\n",
      "plt.show()\n",
      "207/471:\n",
      "def coords(label):\n",
      "    i = labels.index(label)\n",
      "    return (heshes[i], biases[i])\n",
      "plt.plot(heshes, biases, \"o\")\n",
      "plt.title(\"Ratio of 'she' to 'he' vs Direct Bias on Professions\")\n",
      "plt.annotate(\"Automobiles\", coords(\"Automobiles\"), textcoords=\"offset points\", xytext=(5,0))\n",
      "plt.annotate(\"Weddings\", coords(\"Weddings and Engagements\"), textcoords=\"offset points\", xytext=(-52,0))\n",
      "plt.annotate(\"Baseball\", coords(\"Baseball\"), textcoords=\"offset points\", xytext=(5,0))\n",
      "plt.annotate(\"Football\", coords(\"Football\"), textcoords=\"offset points\", xytext=(5,-5))\n",
      "plt.annotate(\"Basketball\", coords(\"Basketball\"), textcoords=\"offset points\", xytext=(0,5))\n",
      "plt.annotate(\"Children\", coords(\"Children and Youth\"), textcoords=\"offset points\", xytext=(5,-7))\n",
      "plt.annotate(\"Education\", coords(\"Education and Schools\"), textcoords=\"offset points\", xytext=(2,5))\n",
      "plt.annotate(\"Computers\", coords(\"Computers and the Internet\"), textcoords=\"offset points\", xytext=(5,0))\n",
      "plt.annotate(\"Politics\", coords(\"Politics and Government\"), textcoords=\"offset points\", xytext=(22,0))\n",
      "plt.annotate(\"Travel\", coords(\"Travel and Vacations\"), textcoords=\"offset points\", xytext=(2,-10))\n",
      "plt.annotate(\"Art\", coords(\"Art\"), textcoords=\"offset points\", xytext=(-18,-5))\n",
      "plt.show()\n",
      "207/472:\n",
      "def sbias(g, m):\n",
      "    b = 0\n",
      "    l = list(filter(lambda x: x in m.wv.vocab, adjs_all))\n",
      "    p, n, np, nn = 0, 0, 0, 0\n",
      "    for x in l:\n",
      "        proj = gproj(g, m, x)\n",
      "        b += abs(proj)\n",
      "        try:\n",
      "            s = swn.senti_synset(x + \".a.01\")\n",
      "            score = s.pos_score() - s.neg_score()\n",
      "            if proj < -0.15 and score != 0:\n",
      "                print(\"neg\", proj, score, x)\n",
      "                n += score\n",
      "#                 nn += 1\n",
      "            if proj > 0.1 and score != 0:\n",
      "                print(\"pos\", proj, score, x)\n",
      "                p += score\n",
      "#                 np += 1\n",
      "        except:\n",
      "            continue\n",
      "    return b/len(l), p, n# p/np, n/nn\n",
      "207/473: gproj(getg(mpol), mpol, 'he'), gproj(getg(mpol), mpol, 'she')\n",
      "207/474: sbias(getg(mpol), mpol)\n",
      "207/475: sbias(getg(mbooks), mbooks)\n",
      "207/476:\n",
      "def sbias(g, m):\n",
      "    b = 0\n",
      "    l = list(filter(lambda x: x in m.wv.vocab, adjs_all))\n",
      "    p, n, np, nn = 0, 0, 0, 0\n",
      "    for x in l:\n",
      "        proj = gproj(g, m, x)\n",
      "        b += abs(proj)\n",
      "        try:\n",
      "            s = swn.senti_synset(x + \".a.01\")\n",
      "            score = s.pos_score() - s.neg_score()\n",
      "            if proj < -0.15 and score != 0:\n",
      "                print(\"neg\", proj, score, x)\n",
      "                n += score\n",
      "                nn += 1\n",
      "            if proj > 0.1 and score != 0:\n",
      "                print(\"pos\", proj, score, x)\n",
      "                p += score\n",
      "                np += 1\n",
      "        except:\n",
      "            continue\n",
      "    return b/len(l), p, n, np, nn# p/np, n/nn\n",
      "207/477: sbias(getg(mpol), mpol)\n",
      "207/478: sbias(getg(mbooks), mbooks)\n",
      "207/479:\n",
      "def sbias(g, m):\n",
      "    b = 0\n",
      "    l = list(filter(lambda x: x in m.wv.vocab, adjs_all))\n",
      "    p, n, np, nn = 0, 0, 0, 0\n",
      "    for x in l:\n",
      "        proj = gproj(g, m, x)\n",
      "        b += abs(proj)\n",
      "        try:\n",
      "            s = swn.senti_synset(x + \".a.01\")\n",
      "            score = s.pos_score() - s.neg_score()\n",
      "            if proj < -0.15 and score != 0:\n",
      "                print(\"neg\", proj, score, x)\n",
      "                n += score\n",
      "                nn += 1\n",
      "            if proj > 0.12 and score != 0:\n",
      "                print(\"pos\", proj, score, x)\n",
      "                p += score\n",
      "                np += 1\n",
      "        except:\n",
      "            continue\n",
      "    return b/len(l), p, n, np, nn# p/np, n/nn\n",
      "207/480: sbias(getg(mpol), mpol)\n",
      "207/481:\n",
      "def sbias(g, m):\n",
      "    b = 0\n",
      "    l = list(filter(lambda x: x in m.wv.vocab, adjs_all))\n",
      "    p, n, np, nn = 0, 0, 0, 0\n",
      "    for x in l:\n",
      "        proj = gproj(g, m, x)\n",
      "        b += abs(proj)\n",
      "        try:\n",
      "            s = swn.senti_synset(x + \".a.01\")\n",
      "            score = s.pos_score() - s.neg_score()\n",
      "            if proj < -0.15 and score != 0:\n",
      "                print(\"neg\", proj, score, x)\n",
      "                n += score\n",
      "                nn += 1\n",
      "            if proj > 0.11 and score != 0:\n",
      "                print(\"pos\", proj, score, x)\n",
      "                p += score\n",
      "                np += 1\n",
      "        except:\n",
      "            continue\n",
      "    return b/len(l), p, n, np, nn# p/np, n/nn\n",
      "207/482: sbias(getg(mpol), mpol)\n",
      "207/483: sbias(getg(mbooks), mbooks)\n",
      "207/484:\n",
      "wbiases = []\n",
      "i = 0\n",
      "for m in ms:\n",
      "    b = wbias(m)\n",
      "    print(heshestats(m, 'he', 'she'), b, labels[i])\n",
      "    i += 1\n",
      "    wbiases.append(b)\n",
      "207/485:\n",
      "ms = []\n",
      "for l in lists:\n",
      "    m = getw2v(sample(l, 2000))\n",
      "    heshestats(m, 'he', 'she')\n",
      "    ms.append(m)\n",
      "207/486:\n",
      "wbiases = []\n",
      "i = 0\n",
      "for m in ms:\n",
      "    b = wbias(m)\n",
      "    print(heshestats(m, 'he', 'she'), b, labels[i])\n",
      "    i += 1\n",
      "    wbiases.append(b)\n",
      "207/487:\n",
      "stereotypes_m = ['captain', 'president', 'principal', 'dean', \\\n",
      "                 'coach', 'director', 'officer', 'manager']\n",
      "stereotypes_f = ['teacher', 'author', 'secretary', 'writer', \\\n",
      "                 'singer', 'teenager', 'editor', 'photographer']\n",
      "male = [\"male\", \"man\", \"boy\", \"boys\", \"brother\", \"he\", \"him\", \"son\", \"father\"]\n",
      "female = [\"female\", \"woman\", \"girl\", \"girls\", \"sister\", \"she\", \"her\", \\\n",
      "          \"daughter\", \"mother\"]\n",
      "def cos_sim(m, x, y):\n",
      "    wv1 = m.wv.get_vector(x)\n",
      "    wv2 = m.wv.get_vector(y)\n",
      "    sim = np.dot(wv1, wv2)/(np.linalg.norm(wv1)*np.linalg.norm(wv2))\n",
      "    return sim\n",
      "def s_word(m, w, A, B):\n",
      "    return np.mean([cos_sim(m, w, a) for a in A]) - np.mean([cos_sim(m, w, b) for b in B])\n",
      "def effect_size(m, X, Y, A, B):\n",
      "    num = np.mean([s_word(m, x, A, B) for x in X]) - np.mean([s_word(m, y, A, B) for y in Y])\n",
      "    return num/np.std([s_word(m, x, A, B) for x in X + Y])\n",
      "def wbias(m):\n",
      "    return effect_size(m, male, female, stereotypes_m, stereotypes_f)\n",
      "207/488:\n",
      "wbiases = []\n",
      "i = 0\n",
      "for m in ms:\n",
      "    b = wbias(m)\n",
      "    print(heshestats(m, 'he', 'she'), b, labels[i])\n",
      "    i += 1\n",
      "    wbiases.append(b)\n",
      "207/489:\n",
      "stereotypes_m = ['captain', 'president', 'principal', 'dean', \\\n",
      "                 'coach', 'director', 'officer', 'manager']\n",
      "stereotypes_f = ['teacher', 'author', 'secretary', 'writer', \\\n",
      "                 'singer', 'teenager', 'editor']\n",
      "male = [\"male\", \"man\", \"boy\", \"boys\", \"brother\", \"he\", \"him\", \"son\", \"father\"]\n",
      "female = [\"female\", \"woman\", \"girl\", \"girls\", \"sister\", \"she\", \"her\", \\\n",
      "          \"daughter\", \"mother\"]\n",
      "def cos_sim(m, x, y):\n",
      "    wv1 = m.wv.get_vector(x)\n",
      "    wv2 = m.wv.get_vector(y)\n",
      "    sim = np.dot(wv1, wv2)/(np.linalg.norm(wv1)*np.linalg.norm(wv2))\n",
      "    return sim\n",
      "def s_word(m, w, A, B):\n",
      "    return np.mean([cos_sim(m, w, a) for a in A]) - np.mean([cos_sim(m, w, b) for b in B])\n",
      "def effect_size(m, X, Y, A, B):\n",
      "    num = np.mean([s_word(m, x, A, B) for x in X]) - np.mean([s_word(m, y, A, B) for y in Y])\n",
      "    return num/np.std([s_word(m, x, A, B) for x in X + Y])\n",
      "def wbias(m):\n",
      "    return effect_size(m, male, female, stereotypes_m, stereotypes_f)\n",
      "207/490:\n",
      "wbiases = []\n",
      "i = 0\n",
      "for m in ms:\n",
      "    b = wbias(m)\n",
      "    print(heshestats(m, 'he', 'she'), b, labels[i])\n",
      "    i += 1\n",
      "    wbiases.append(b)\n",
      "207/491:\n",
      "stereotypes_m = ['captain', 'president', 'principal', 'dean', \\\n",
      "                 'coach', 'director', 'officer', 'manager']\n",
      "stereotypes_f = ['teacher', 'author', 'secretary', 'writer', \\\n",
      "                 'singer', 'editor']\n",
      "male = [\"male\", \"man\", \"boy\", \"boys\", \"brother\", \"he\", \"him\", \"son\", \"father\"]\n",
      "female = [\"female\", \"woman\", \"girl\", \"girls\", \"sister\", \"she\", \"her\", \\\n",
      "          \"daughter\", \"mother\"]\n",
      "def cos_sim(m, x, y):\n",
      "    wv1 = m.wv.get_vector(x)\n",
      "    wv2 = m.wv.get_vector(y)\n",
      "    sim = np.dot(wv1, wv2)/(np.linalg.norm(wv1)*np.linalg.norm(wv2))\n",
      "    return sim\n",
      "def s_word(m, w, A, B):\n",
      "    return np.mean([cos_sim(m, w, a) for a in A]) - np.mean([cos_sim(m, w, b) for b in B])\n",
      "def effect_size(m, X, Y, A, B):\n",
      "    num = np.mean([s_word(m, x, A, B) for x in X]) - np.mean([s_word(m, y, A, B) for y in Y])\n",
      "    return num/np.std([s_word(m, x, A, B) for x in X + Y])\n",
      "def wbias(m):\n",
      "    return effect_size(m, male, female, stereotypes_m, stereotypes_f)\n",
      "207/492:\n",
      "wbiases = []\n",
      "i = 0\n",
      "for m in ms:\n",
      "    b = wbias(m)\n",
      "    print(heshestats(m, 'he', 'she'), b, labels[i])\n",
      "    i += 1\n",
      "    wbiases.append(b)\n",
      "207/493:\n",
      "stereotypes_m = ['captain', 'president', 'principal', 'dean', \\\n",
      "                 'director', 'officer', 'manager']\n",
      "stereotypes_f = ['teacher', 'author', 'secretary', 'writer', \\\n",
      "                 'singer', 'editor']\n",
      "male = [\"male\", \"man\", \"boy\", \"boys\", \"brother\", \"he\", \"him\", \"son\", \"father\"]\n",
      "female = [\"female\", \"woman\", \"girl\", \"girls\", \"sister\", \"she\", \"her\", \\\n",
      "          \"daughter\", \"mother\"]\n",
      "def cos_sim(m, x, y):\n",
      "    wv1 = m.wv.get_vector(x)\n",
      "    wv2 = m.wv.get_vector(y)\n",
      "    sim = np.dot(wv1, wv2)/(np.linalg.norm(wv1)*np.linalg.norm(wv2))\n",
      "    return sim\n",
      "def s_word(m, w, A, B):\n",
      "    return np.mean([cos_sim(m, w, a) for a in A]) - np.mean([cos_sim(m, w, b) for b in B])\n",
      "def effect_size(m, X, Y, A, B):\n",
      "    num = np.mean([s_word(m, x, A, B) for x in X]) - np.mean([s_word(m, y, A, B) for y in Y])\n",
      "    return num/np.std([s_word(m, x, A, B) for x in X + Y])\n",
      "def wbias(m):\n",
      "    return effect_size(m, male, female, stereotypes_m, stereotypes_f)\n",
      "207/494:\n",
      "wbiases = []\n",
      "i = 0\n",
      "for m in ms:\n",
      "    b = wbias(m)\n",
      "    print(heshestats(m, 'he', 'she'), b, labels[i])\n",
      "    i += 1\n",
      "    wbiases.append(b)\n",
      "207/495:\n",
      "stereotypes_m = ['captain', 'president', 'dean', \\\n",
      "                 'director', 'officer', 'manager']\n",
      "stereotypes_f = ['teacher', 'author', 'secretary', 'writer', \\\n",
      "                 'singer', 'editor']\n",
      "male = [\"male\", \"man\", \"boy\", \"boys\", \"brother\", \"he\", \"him\", \"son\", \"father\"]\n",
      "female = [\"female\", \"woman\", \"girl\", \"girls\", \"sister\", \"she\", \"her\", \\\n",
      "          \"daughter\", \"mother\"]\n",
      "def cos_sim(m, x, y):\n",
      "    wv1 = m.wv.get_vector(x)\n",
      "    wv2 = m.wv.get_vector(y)\n",
      "    sim = np.dot(wv1, wv2)/(np.linalg.norm(wv1)*np.linalg.norm(wv2))\n",
      "    return sim\n",
      "def s_word(m, w, A, B):\n",
      "    return np.mean([cos_sim(m, w, a) for a in A]) - np.mean([cos_sim(m, w, b) for b in B])\n",
      "def effect_size(m, X, Y, A, B):\n",
      "    num = np.mean([s_word(m, x, A, B) for x in X]) - np.mean([s_word(m, y, A, B) for y in Y])\n",
      "    return num/np.std([s_word(m, x, A, B) for x in X + Y])\n",
      "def wbias(m):\n",
      "    return effect_size(m, male, female, stereotypes_m, stereotypes_f)\n",
      "207/496:\n",
      "wbiases = []\n",
      "i = 0\n",
      "for m in ms:\n",
      "    b = wbias(m)\n",
      "    print(heshestats(m, 'he', 'she'), b, labels[i])\n",
      "    i += 1\n",
      "    wbiases.append(b)\n",
      "207/497:\n",
      "plt.plot(heshes, wbiases, \"o\")\n",
      "plt.title(\"Ratio of 'she' to 'he' vs Direct Bias on Professions\")\n",
      "plt.annotate(\"Automobiles\", coords(\"Automobiles\"), textcoords=\"offset points\", xytext=(5,0))\n",
      "plt.annotate(\"Weddings\", coords(\"Weddings and Engagements\"), textcoords=\"offset points\", xytext=(-52,0))\n",
      "plt.annotate(\"Baseball\", coords(\"Baseball\"), textcoords=\"offset points\", xytext=(5,0))\n",
      "plt.annotate(\"Football\", coords(\"Football\"), textcoords=\"offset points\", xytext=(5,-5))\n",
      "plt.annotate(\"Basketball\", coords(\"Basketball\"), textcoords=\"offset points\", xytext=(0,5))\n",
      "plt.annotate(\"Children\", coords(\"Children and Youth\"), textcoords=\"offset points\", xytext=(5,-7))\n",
      "plt.annotate(\"Education\", coords(\"Education and Schools\"), textcoords=\"offset points\", xytext=(2,5))\n",
      "plt.annotate(\"Computers\", coords(\"Computers and the Internet\"), textcoords=\"offset points\", xytext=(5,0))\n",
      "plt.annotate(\"Politics\", coords(\"Politics and Government\"), textcoords=\"offset points\", xytext=(22,0))\n",
      "plt.annotate(\"Travel\", coords(\"Travel and Vacations\"), textcoords=\"offset points\", xytext=(2,-10))\n",
      "plt.annotate(\"Art\", coords(\"Art\"), textcoords=\"offset points\", xytext=(-18,-5))\n",
      "plt.show()\n",
      "207/498:\n",
      "stereotypes_m = ['captain', 'president', 'dean', \\\n",
      "                 'director', 'officer', 'manager']\n",
      "stereotypes_f = ['teacher', 'author', 'secretary', 'writer', \\\n",
      "                 'singer', 'editor']\n",
      "male = [\"male\", \"man\", \"boy\", \"boys\", \"brother\", \"he\", \"him\", \"son\", \"father\"]\n",
      "female = [\"female\", \"woman\", \"girl\", \"girls\", \"sister\", \"she\", \"her\", \\\n",
      "          \"daughter\", \"mother\"]\n",
      "def cos_sim(m, x, y):\n",
      "    wv1 = m.wv.get_vector(x)\n",
      "    wv2 = m.wv.get_vector(y)\n",
      "    sim = np.dot(wv1, wv2)/(np.linalg.norm(wv1)*np.linalg.norm(wv2))\n",
      "    return sim\n",
      "def s_word(m, w, A, B):\n",
      "    return np.mean([cos_sim(m, w, a) for a in A]) - np.mean([cos_sim(m, w, b) for b in B])\n",
      "def effect_size(m, X, Y, A, B):\n",
      "    num = np.mean([s_word(m, x, A, B) for x in X]) - np.mean([s_word(m, y, A, B) for y in Y])\n",
      "    return num/np.std([s_word(m, x, A, B) for x in X + Y])\n",
      "def wbias(m):\n",
      "    return abs(effect_size(m, male, female, stereotypes_m, stereotypes_f))\n",
      "207/499:\n",
      "wbiases = []\n",
      "i = 0\n",
      "for m in ms:\n",
      "    b = wbias(m)\n",
      "    print(heshestats(m, 'he', 'she'), b, labels[i])\n",
      "    i += 1\n",
      "    wbiases.append(b)\n",
      "207/500:\n",
      "plt.plot(heshes, wbiases, \"o\")\n",
      "plt.title(\"Ratio of 'she' to 'he' vs Absolute WEAT Bias on Professions\")\n",
      "plt.annotate(\"Automobiles\", coords(\"Automobiles\"), textcoords=\"offset points\", xytext=(5,0))\n",
      "plt.annotate(\"Weddings\", coords(\"Weddings and Engagements\"), textcoords=\"offset points\", xytext=(-52,0))\n",
      "plt.annotate(\"Baseball\", coords(\"Baseball\"), textcoords=\"offset points\", xytext=(5,0))\n",
      "plt.annotate(\"Football\", coords(\"Football\"), textcoords=\"offset points\", xytext=(5,-5))\n",
      "plt.annotate(\"Basketball\", coords(\"Basketball\"), textcoords=\"offset points\", xytext=(0,5))\n",
      "plt.annotate(\"Children\", coords(\"Children and Youth\"), textcoords=\"offset points\", xytext=(5,-7))\n",
      "plt.annotate(\"Education\", coords(\"Education and Schools\"), textcoords=\"offset points\", xytext=(2,5))\n",
      "plt.annotate(\"Computers\", coords(\"Computers and the Internet\"), textcoords=\"offset points\", xytext=(5,0))\n",
      "plt.annotate(\"Politics\", coords(\"Politics and Government\"), textcoords=\"offset points\", xytext=(22,0))\n",
      "plt.annotate(\"Travel\", coords(\"Travel and Vacations\"), textcoords=\"offset points\", xytext=(2,-10))\n",
      "plt.annotate(\"Art\", coords(\"Art\"), textcoords=\"offset points\", xytext=(-18,-5))\n",
      "plt.show()\n",
      "207/501:\n",
      "plt.plot(heshes, wbiases, \"o\")\n",
      "plt.title(\"Ratio of 'she' to 'he' vs Absolute WEAT Bias on Professions\")\n",
      "plt.annotate(\"Automobiles\", coords(\"Automobiles\"), textcoords=\"offset points\", xytext=(0,0))\n",
      "plt.annotate(\"Weddings\", coords(\"Weddings and Engagements\"), textcoords=\"offset points\", xytext=(0,0))\n",
      "plt.annotate(\"Baseball\", coords(\"Baseball\"), textcoords=\"offset points\", xytext=(0,0))\n",
      "plt.annotate(\"Football\", coords(\"Football\"), textcoords=\"offset points\", xytext=(0,0))\n",
      "plt.annotate(\"Basketball\", coords(\"Basketball\"), textcoords=\"offset points\", xytext=(0,0))\n",
      "plt.annotate(\"Children\", coords(\"Children and Youth\"), textcoords=\"offset points\", xytext=(0,0))\n",
      "plt.annotate(\"Education\", coords(\"Education and Schools\"), textcoords=\"offset points\", xytext=(0,0))\n",
      "plt.annotate(\"Computers\", coords(\"Computers and the Internet\"), textcoords=\"offset points\", xytext=(0,0))\n",
      "plt.annotate(\"Politics\", coords(\"Politics and Government\"), textcoords=\"offset points\", xytext=(0,0))\n",
      "plt.annotate(\"Travel\", coords(\"Travel and Vacations\"), textcoords=\"offset points\", xytext=(2,0))\n",
      "plt.annotate(\"Art\", coords(\"Art\"), textcoords=\"offset points\", xytext=(0,0))\n",
      "plt.show()\n",
      "207/502:\n",
      "def coords(label):\n",
      "    i = labels.index(label)\n",
      "    return (heshes[i], wbiases[i])\n",
      "plt.plot(heshes, wbiases, \"o\")\n",
      "plt.title(\"Ratio of 'she' to 'he' vs Absolute WEAT Bias on Professions\")\n",
      "plt.annotate(\"Automobiles\", coords(\"Automobiles\"), textcoords=\"offset points\", xytext=(0,0))\n",
      "plt.annotate(\"Weddings\", coords(\"Weddings and Engagements\"), textcoords=\"offset points\", xytext=(0,0))\n",
      "plt.annotate(\"Baseball\", coords(\"Baseball\"), textcoords=\"offset points\", xytext=(0,0))\n",
      "plt.annotate(\"Football\", coords(\"Football\"), textcoords=\"offset points\", xytext=(0,0))\n",
      "plt.annotate(\"Basketball\", coords(\"Basketball\"), textcoords=\"offset points\", xytext=(0,0))\n",
      "plt.annotate(\"Children\", coords(\"Children and Youth\"), textcoords=\"offset points\", xytext=(0,0))\n",
      "plt.annotate(\"Education\", coords(\"Education and Schools\"), textcoords=\"offset points\", xytext=(0,0))\n",
      "plt.annotate(\"Computers\", coords(\"Computers and the Internet\"), textcoords=\"offset points\", xytext=(0,0))\n",
      "plt.annotate(\"Politics\", coords(\"Politics and Government\"), textcoords=\"offset points\", xytext=(0,0))\n",
      "plt.annotate(\"Travel\", coords(\"Travel and Vacations\"), textcoords=\"offset points\", xytext=(2,0))\n",
      "plt.annotate(\"Art\", coords(\"Art\"), textcoords=\"offset points\", xytext=(0,0))\n",
      "plt.show()\n",
      "207/503:\n",
      "def coords(label):\n",
      "    i = labels.index(label)\n",
      "    return (heshes[i], wbiases[i])\n",
      "plt.plot(heshes, wbiases, \"o\")\n",
      "plt.title(\"Ratio of 'she' to 'he' vs Absolute WEAT Bias on Professions\")\n",
      "plt.annotate(\"Automobiles\", coords(\"Automobiles\"), textcoords=\"offset points\", xytext=(0,0))\n",
      "plt.annotate(\"Weddings\", coords(\"Weddings and Engagements\"), textcoords=\"offset points\", xytext=(-20,0))\n",
      "plt.annotate(\"Baseball\", coords(\"Baseball\"), textcoords=\"offset points\", xytext=(0,0))\n",
      "plt.annotate(\"Football\", coords(\"Football\"), textcoords=\"offset points\", xytext=(0,0))\n",
      "plt.annotate(\"Basketball\", coords(\"Basketball\"), textcoords=\"offset points\", xytext=(0,0))\n",
      "plt.annotate(\"Children\", coords(\"Children and Youth\"), textcoords=\"offset points\", xytext=(2,0))\n",
      "plt.annotate(\"Education\", coords(\"Education and Schools\"), textcoords=\"offset points\", xytext=(2,0))\n",
      "plt.annotate(\"Computers\", coords(\"Computers and the Internet\"), textcoords=\"offset points\", xytext=(2,0))\n",
      "plt.annotate(\"Politics\", coords(\"Politics and Government\"), textcoords=\"offset points\", xytext=(2,0))\n",
      "plt.annotate(\"Travel\", coords(\"Travel and Vacations\"), textcoords=\"offset points\", xytext=(2,0))\n",
      "plt.annotate(\"Art\", coords(\"Art\"), textcoords=\"offset points\", xytext=(2,0))\n",
      "plt.show()\n",
      "207/504:\n",
      "def coords(label):\n",
      "    i = labels.index(label)\n",
      "    return (heshes[i], wbiases[i])\n",
      "plt.plot(heshes, wbiases, \"o\")\n",
      "plt.title(\"Ratio of 'she' to 'he' vs Absolute WEAT Bias on Professions\")\n",
      "plt.annotate(\"Automobiles\", coords(\"Automobiles\"), textcoords=\"offset points\", xytext=(0,0))\n",
      "plt.annotate(\"Weddings\", coords(\"Weddings and Engagements\"), textcoords=\"offset points\", xytext=(-20,0))\n",
      "plt.annotate(\"Baseball\", coords(\"Baseball\"), textcoords=\"offset points\", xytext=(3,0))\n",
      "plt.annotate(\"Football\", coords(\"Football\"), textcoords=\"offset points\", xytext=(3,0))\n",
      "plt.annotate(\"Basketball\", coords(\"Basketball\"), textcoords=\"offset points\", xytext=(3,0))\n",
      "plt.annotate(\"Children\", coords(\"Children and Youth\"), textcoords=\"offset points\", xytext=(2,0))\n",
      "plt.annotate(\"Education\", coords(\"Education and Schools\"), textcoords=\"offset points\", xytext=(2,0))\n",
      "plt.annotate(\"Computers\", coords(\"Computers and the Internet\"), textcoords=\"offset points\", xytext=(2,0))\n",
      "plt.annotate(\"Politics\", coords(\"Politics and Government\"), textcoords=\"offset points\", xytext=(2,0))\n",
      "plt.annotate(\"Travel\", coords(\"Travel and Vacations\"), textcoords=\"offset points\", xytext=(2,0))\n",
      "plt.annotate(\"Art\", coords(\"Art\"), textcoords=\"offset points\", xytext=(2,0))\n",
      "plt.show()\n",
      "207/505:\n",
      "def coords(label):\n",
      "    i = labels.index(label)\n",
      "    return (heshes[i], wbiases[i])\n",
      "plt.plot(heshes, wbiases, \"o\")\n",
      "plt.title(\"Ratio of 'she' to 'he' vs Absolute WEAT Bias on Professions\")\n",
      "plt.annotate(\"Automobiles\", coords(\"Automobiles\"), textcoords=\"offset points\", xytext=(0,0))\n",
      "plt.annotate(\"Weddings\", coords(\"Weddings and Engagements\"), textcoords=\"offset points\", xytext=(-20,0))\n",
      "plt.annotate(\"Baseball\", coords(\"Baseball\"), textcoords=\"offset points\", xytext=(3,0))\n",
      "plt.annotate(\"Football\", coords(\"Football\"), textcoords=\"offset points\", xytext=(2,10))\n",
      "plt.annotate(\"Basketball\", coords(\"Basketball\"), textcoords=\"offset points\", xytext=(3,-10))\n",
      "plt.annotate(\"Children\", coords(\"Children and Youth\"), textcoords=\"offset points\", xytext=(2,0))\n",
      "plt.annotate(\"Education\", coords(\"Education and Schools\"), textcoords=\"offset points\", xytext=(2,0))\n",
      "plt.annotate(\"Computers\", coords(\"Computers and the Internet\"), textcoords=\"offset points\", xytext=(2,0))\n",
      "plt.annotate(\"Politics\", coords(\"Politics and Government\"), textcoords=\"offset points\", xytext=(2,0))\n",
      "plt.annotate(\"Travel\", coords(\"Travel and Vacations\"), textcoords=\"offset points\", xytext=(2,0))\n",
      "plt.annotate(\"Art\", coords(\"Art\"), textcoords=\"offset points\", xytext=(2,0))\n",
      "plt.show()\n",
      "207/506:\n",
      "def coords(label):\n",
      "    i = labels.index(label)\n",
      "    return (heshes[i], wbiases[i])\n",
      "plt.plot(heshes, wbiases, \"o\")\n",
      "plt.title(\"Ratio of 'she' to 'he' vs Absolute WEAT Bias on Professions\")\n",
      "plt.annotate(\"Automobiles\", coords(\"Automobiles\"), textcoords=\"offset points\", xytext=(0,0))\n",
      "plt.annotate(\"Weddings\", coords(\"Weddings and Engagements\"), textcoords=\"offset points\", xytext=(-20,0))\n",
      "plt.annotate(\"Baseball\", coords(\"Baseball\"), textcoords=\"offset points\", xytext=(3,0))\n",
      "plt.annotate(\"Football\", coords(\"Football\"), textcoords=\"offset points\", xytext=(2,5))\n",
      "plt.annotate(\"Basketball\", coords(\"Basketball\"), textcoords=\"offset points\", xytext=(2,-15))\n",
      "plt.annotate(\"Children\", coords(\"Children and Youth\"), textcoords=\"offset points\", xytext=(2,0))\n",
      "plt.annotate(\"Education\", coords(\"Education and Schools\"), textcoords=\"offset points\", xytext=(2,0))\n",
      "plt.annotate(\"Computers\", coords(\"Computers and the Internet\"), textcoords=\"offset points\", xytext=(2,0))\n",
      "plt.annotate(\"Politics\", coords(\"Politics and Government\"), textcoords=\"offset points\", xytext=(2,0))\n",
      "plt.annotate(\"Travel\", coords(\"Travel and Vacations\"), textcoords=\"offset points\", xytext=(2,0))\n",
      "plt.annotate(\"Art\", coords(\"Art\"), textcoords=\"offset points\", xytext=(2,0))\n",
      "plt.show()\n",
      "207/507:\n",
      "def coords(label):\n",
      "    i = labels.index(label)\n",
      "    return (heshes[i], wbiases[i])\n",
      "plt.plot(heshes, wbiases, \"o\")\n",
      "plt.title(\"Ratio of 'she' to 'he' vs Absolute WEAT Bias on Professions\")\n",
      "plt.annotate(\"Automobiles\", coords(\"Automobiles\"), textcoords=\"offset points\", xytext=(0,0))\n",
      "plt.annotate(\"Weddings\", coords(\"Weddings and Engagements\"), textcoords=\"offset points\", xytext=(-20,0))\n",
      "plt.annotate(\"Baseball\", coords(\"Baseball\"), textcoords=\"offset points\", xytext=(3,0))\n",
      "plt.annotate(\"Football\", coords(\"Football\"), textcoords=\"offset points\", xytext=(2,5))\n",
      "plt.annotate(\"Basketball\", coords(\"Basketball\"), textcoords=\"offset points\", xytext=(1,-12))\n",
      "plt.annotate(\"Children\", coords(\"Children and Youth\"), textcoords=\"offset points\", xytext=(2,0))\n",
      "plt.annotate(\"Education\", coords(\"Education and Schools\"), textcoords=\"offset points\", xytext=(2,0))\n",
      "plt.annotate(\"Computers\", coords(\"Computers and the Internet\"), textcoords=\"offset points\", xytext=(2,0))\n",
      "plt.annotate(\"Politics\", coords(\"Politics and Government\"), textcoords=\"offset points\", xytext=(2,0))\n",
      "plt.annotate(\"Travel\", coords(\"Travel and Vacations\"), textcoords=\"offset points\", xytext=(2,0))\n",
      "plt.annotate(\"Art\", coords(\"Art\"), textcoords=\"offset points\", xytext=(2,0))\n",
      "plt.show()\n",
      "207/508:\n",
      "def coords(label):\n",
      "    i = labels.index(label)\n",
      "    return (heshes[i], wbiases[i])\n",
      "plt.plot(heshes, wbiases, \"o\")\n",
      "plt.title(\"Ratio of 'she' to 'he' vs Absolute WEAT Bias on Professions\")\n",
      "plt.annotate(\"Automobiles\", coords(\"Automobiles\"), textcoords=\"offset points\", xytext=(0,0))\n",
      "plt.annotate(\"Weddings\", coords(\"Weddings and Engagements\"), textcoords=\"offset points\", xytext=(-20,0))\n",
      "plt.annotate(\"Baseball\", coords(\"Baseball\"), textcoords=\"offset points\", xytext=(5,-2))\n",
      "plt.annotate(\"Football\", coords(\"Football\"), textcoords=\"offset points\", xytext=(2,5))\n",
      "plt.annotate(\"Basketball\", coords(\"Basketball\"), textcoords=\"offset points\", xytext=(1,-12))\n",
      "plt.annotate(\"Children\", coords(\"Children and Youth\"), textcoords=\"offset points\", xytext=(2,0))\n",
      "plt.annotate(\"Education\", coords(\"Education and Schools\"), textcoords=\"offset points\", xytext=(2,0))\n",
      "plt.annotate(\"Computers\", coords(\"Computers and the Internet\"), textcoords=\"offset points\", xytext=(2,0))\n",
      "plt.annotate(\"Politics\", coords(\"Politics and Government\"), textcoords=\"offset points\", xytext=(2,0))\n",
      "plt.annotate(\"Travel\", coords(\"Travel and Vacations\"), textcoords=\"offset points\", xytext=(2,0))\n",
      "plt.annotate(\"Art\", coords(\"Art\"), textcoords=\"offset points\", xytext=(2,0))\n",
      "plt.show()\n",
      "207/509:\n",
      "def coords(label):\n",
      "    i = labels.index(label)\n",
      "    return (heshes[i], wbiases[i])\n",
      "plt.plot(heshes, wbiases, \"o\")\n",
      "plt.title(\"Ratio of 'she' to 'he' vs Absolute WEAT Bias on Professions\")\n",
      "plt.annotate(\"Automobiles\", coords(\"Automobiles\"), textcoords=\"offset points\", xytext=(0,0))\n",
      "plt.annotate(\"Weddings\", coords(\"Weddings and Engagements\"), textcoords=\"offset points\", xytext=(-20,0))\n",
      "plt.annotate(\"Baseball\", coords(\"Baseball\"), textcoords=\"offset points\", xytext=(5,-2))\n",
      "plt.annotate(\"Football\", coords(\"Football\"), textcoords=\"offset points\", xytext=(2,5))\n",
      "plt.annotate(\"Basketball\", coords(\"Basketball\"), textcoords=\"offset points\", xytext=(1,-12))\n",
      "plt.annotate(\"Children\", coords(\"Children and Youth\"), textcoords=\"offset points\", xytext=(5,0))\n",
      "plt.annotate(\"Education\", coords(\"Education and Schools\"), textcoords=\"offset points\", xytext=(2,0))\n",
      "plt.annotate(\"Computers\", coords(\"Computers and the Internet\"), textcoords=\"offset points\", xytext=(2,0))\n",
      "plt.annotate(\"Politics\", coords(\"Politics and Government\"), textcoords=\"offset points\", xytext=(2,0))\n",
      "plt.annotate(\"Travel\", coords(\"Travel and Vacations\"), textcoords=\"offset points\", xytext=(2,0))\n",
      "plt.annotate(\"Art\", coords(\"Art\"), textcoords=\"offset points\", xytext=(2,0))\n",
      "plt.show()\n",
      "207/510:\n",
      "def coords(label):\n",
      "    i = labels.index(label)\n",
      "    return (heshes[i], wbiases[i])\n",
      "plt.plot(heshes, wbiases, \"o\")\n",
      "plt.title(\"Ratio of 'she' to 'he' vs Absolute WEAT Bias on Professions\")\n",
      "plt.annotate(\"Automobiles\", coords(\"Automobiles\"), textcoords=\"offset points\", xytext=(0,0))\n",
      "plt.annotate(\"Weddings\", coords(\"Weddings and Engagements\"), textcoords=\"offset points\", xytext=(-20,0))\n",
      "plt.annotate(\"Baseball\", coords(\"Baseball\"), textcoords=\"offset points\", xytext=(5,-2))\n",
      "plt.annotate(\"Football\", coords(\"Football\"), textcoords=\"offset points\", xytext=(2,5))\n",
      "plt.annotate(\"Basketball\", coords(\"Basketball\"), textcoords=\"offset points\", xytext=(1,-12))\n",
      "plt.annotate(\"Children\", coords(\"Children and Youth\"), textcoords=\"offset points\", xytext=(5,0))\n",
      "plt.annotate(\"Education\", coords(\"Education and Schools\"), textcoords=\"offset points\", xytext=(2,0))\n",
      "plt.annotate(\"Computers\", coords(\"Computers and the Internet\"), textcoords=\"offset points\", xytext=(3,2))\n",
      "plt.annotate(\"Politics\", coords(\"Politics and Government\"), textcoords=\"offset points\", xytext=(2,0))\n",
      "plt.annotate(\"Travel\", coords(\"Travel and Vacations\"), textcoords=\"offset points\", xytext=(2,0))\n",
      "plt.annotate(\"Art\", coords(\"Art\"), textcoords=\"offset points\", xytext=(2,0))\n",
      "plt.show()\n",
      "207/511:\n",
      "def coords(label):\n",
      "    i = labels.index(label)\n",
      "    return (heshes[i], wbiases[i])\n",
      "plt.plot(heshes, wbiases, \"o\")\n",
      "plt.title(\"Ratio of 'she' to 'he' vs Absolute WEAT Bias on Professions\")\n",
      "plt.annotate(\"Automobiles\", coords(\"Automobiles\"), textcoords=\"offset points\", xytext=(0,0))\n",
      "plt.annotate(\"Weddings\", coords(\"Weddings and Engagements\"), textcoords=\"offset points\", xytext=(-20,0))\n",
      "plt.annotate(\"Baseball\", coords(\"Baseball\"), textcoords=\"offset points\", xytext=(5,-2))\n",
      "plt.annotate(\"Football\", coords(\"Football\"), textcoords=\"offset points\", xytext=(2,5))\n",
      "plt.annotate(\"Basketball\", coords(\"Basketball\"), textcoords=\"offset points\", xytext=(1,-12))\n",
      "plt.annotate(\"Children\", coords(\"Children and Youth\"), textcoords=\"offset points\", xytext=(5,0))\n",
      "plt.annotate(\"Education\", coords(\"Education and Schools\"), textcoords=\"offset points\", xytext=(2,0))\n",
      "plt.annotate(\"Computers\", coords(\"Computers and the Internet\"), textcoords=\"offset points\", xytext=(-2,5))\n",
      "plt.annotate(\"Politics\", coords(\"Politics and Government\"), textcoords=\"offset points\", xytext=(2,0))\n",
      "plt.annotate(\"Travel\", coords(\"Travel and Vacations\"), textcoords=\"offset points\", xytext=(2,0))\n",
      "plt.annotate(\"Art\", coords(\"Art\"), textcoords=\"offset points\", xytext=(2,0))\n",
      "plt.show()\n",
      "207/512:\n",
      "def coords(label):\n",
      "    i = labels.index(label)\n",
      "    return (heshes[i], wbiases[i])\n",
      "plt.plot(heshes, wbiases, \"o\")\n",
      "plt.title(\"Ratio of 'she' to 'he' vs Absolute WEAT Bias on Professions\")\n",
      "plt.annotate(\"Automobiles\", coords(\"Automobiles\"), textcoords=\"offset points\", xytext=(0,0))\n",
      "plt.annotate(\"Weddings\", coords(\"Weddings and Engagements\"), textcoords=\"offset points\", xytext=(-20,0))\n",
      "plt.annotate(\"Baseball\", coords(\"Baseball\"), textcoords=\"offset points\", xytext=(5,-2))\n",
      "plt.annotate(\"Football\", coords(\"Football\"), textcoords=\"offset points\", xytext=(2,5))\n",
      "plt.annotate(\"Basketball\", coords(\"Basketball\"), textcoords=\"offset points\", xytext=(1,-12))\n",
      "plt.annotate(\"Children\", coords(\"Children and Youth\"), textcoords=\"offset points\", xytext=(5,0))\n",
      "plt.annotate(\"Education\", coords(\"Education and Schools\"), textcoords=\"offset points\", xytext=(2,0))\n",
      "plt.annotate(\"Computers\", coords(\"Computers and the Internet\"), textcoords=\"offset points\", xytext=(-1,5))\n",
      "plt.annotate(\"Politics\", coords(\"Politics and Government\"), textcoords=\"offset points\", xytext=(2,0))\n",
      "plt.annotate(\"Travel\", coords(\"Travel and Vacations\"), textcoords=\"offset points\", xytext=(2,0))\n",
      "plt.annotate(\"Art\", coords(\"Art\"), textcoords=\"offset points\", xytext=(2,0))\n",
      "plt.show()\n",
      "207/513:\n",
      "def coords(label):\n",
      "    i = labels.index(label)\n",
      "    return (heshes[i], wbiases[i])\n",
      "plt.plot(heshes, wbiases, \"o\")\n",
      "plt.title(\"Ratio of 'she' to 'he' vs Absolute WEAT Bias on Professions\")\n",
      "plt.annotate(\"Automobiles\", coords(\"Automobiles\"), textcoords=\"offset points\", xytext=(0,0))\n",
      "plt.annotate(\"Weddings\", coords(\"Weddings and Engagements\"), textcoords=\"offset points\", xytext=(-20,0))\n",
      "plt.annotate(\"Baseball\", coords(\"Baseball\"), textcoords=\"offset points\", xytext=(5,-2))\n",
      "plt.annotate(\"Football\", coords(\"Football\"), textcoords=\"offset points\", xytext=(2,5))\n",
      "plt.annotate(\"Basketball\", coords(\"Basketball\"), textcoords=\"offset points\", xytext=(1,-12))\n",
      "plt.annotate(\"Children\", coords(\"Children and Youth\"), textcoords=\"offset points\", xytext=(5,0))\n",
      "plt.annotate(\"Education\", coords(\"Education and Schools\"), textcoords=\"offset points\", xytext=(2,0))\n",
      "plt.annotate(\"Computers\", coords(\"Computers and the Internet\"), textcoords=\"offset points\", xytext=(-1,5))\n",
      "plt.annotate(\"Politics\", coords(\"Politics and Government\"), textcoords=\"offset points\", xytext=(-10,-5))\n",
      "plt.annotate(\"Travel\", coords(\"Travel and Vacations\"), textcoords=\"offset points\", xytext=(2,0))\n",
      "plt.annotate(\"Art\", coords(\"Art\"), textcoords=\"offset points\", xytext=(2,0))\n",
      "plt.show()\n",
      "207/514:\n",
      "def coords(label):\n",
      "    i = labels.index(label)\n",
      "    return (heshes[i], wbiases[i])\n",
      "plt.plot(heshes, wbiases, \"o\")\n",
      "plt.title(\"Ratio of 'she' to 'he' vs Absolute WEAT Bias on Professions\")\n",
      "plt.annotate(\"Automobiles\", coords(\"Automobiles\"), textcoords=\"offset points\", xytext=(0,0))\n",
      "plt.annotate(\"Weddings\", coords(\"Weddings and Engagements\"), textcoords=\"offset points\", xytext=(-20,0))\n",
      "plt.annotate(\"Baseball\", coords(\"Baseball\"), textcoords=\"offset points\", xytext=(5,-2))\n",
      "plt.annotate(\"Football\", coords(\"Football\"), textcoords=\"offset points\", xytext=(2,5))\n",
      "plt.annotate(\"Basketball\", coords(\"Basketball\"), textcoords=\"offset points\", xytext=(1,-12))\n",
      "plt.annotate(\"Children\", coords(\"Children and Youth\"), textcoords=\"offset points\", xytext=(5,0))\n",
      "plt.annotate(\"Education\", coords(\"Education and Schools\"), textcoords=\"offset points\", xytext=(2,0))\n",
      "plt.annotate(\"Computers\", coords(\"Computers and the Internet\"), textcoords=\"offset points\", xytext=(-1,5))\n",
      "plt.annotate(\"Politics\", coords(\"Politics and Government\"), textcoords=\"offset points\", xytext=(-10,-15))\n",
      "plt.annotate(\"Travel\", coords(\"Travel and Vacations\"), textcoords=\"offset points\", xytext=(2,0))\n",
      "plt.annotate(\"Art\", coords(\"Art\"), textcoords=\"offset points\", xytext=(2,0))\n",
      "plt.show()\n",
      "207/515:\n",
      "def coords(label):\n",
      "    i = labels.index(label)\n",
      "    return (heshes[i], wbiases[i])\n",
      "plt.plot(heshes, wbiases, \"o\")\n",
      "plt.title(\"Ratio of 'she' to 'he' vs Absolute WEAT Bias on Professions\")\n",
      "plt.annotate(\"Automobiles\", coords(\"Automobiles\"), textcoords=\"offset points\", xytext=(0,0))\n",
      "plt.annotate(\"Weddings\", coords(\"Weddings and Engagements\"), textcoords=\"offset points\", xytext=(-20,0))\n",
      "plt.annotate(\"Baseball\", coords(\"Baseball\"), textcoords=\"offset points\", xytext=(5,-2))\n",
      "plt.annotate(\"Football\", coords(\"Football\"), textcoords=\"offset points\", xytext=(2,5))\n",
      "plt.annotate(\"Basketball\", coords(\"Basketball\"), textcoords=\"offset points\", xytext=(1,-12))\n",
      "plt.annotate(\"Children\", coords(\"Children and Youth\"), textcoords=\"offset points\", xytext=(5,0))\n",
      "plt.annotate(\"Education\", coords(\"Education and Schools\"), textcoords=\"offset points\", xytext=(2,0))\n",
      "plt.annotate(\"Computers\", coords(\"Computers and the Internet\"), textcoords=\"offset points\", xytext=(-1,5))\n",
      "plt.annotate(\"Politics\", coords(\"Politics and Government\"), textcoords=\"offset points\", xytext=(-20,-10))\n",
      "plt.annotate(\"Travel\", coords(\"Travel and Vacations\"), textcoords=\"offset points\", xytext=(2,0))\n",
      "plt.annotate(\"Art\", coords(\"Art\"), textcoords=\"offset points\", xytext=(2,0))\n",
      "plt.show()\n",
      "207/516:\n",
      "def coords(label):\n",
      "    i = labels.index(label)\n",
      "    return (heshes[i], wbiases[i])\n",
      "plt.plot(heshes, wbiases, \"o\")\n",
      "plt.title(\"Ratio of 'she' to 'he' vs Absolute WEAT Bias on Professions\")\n",
      "plt.annotate(\"Automobiles\", coords(\"Automobiles\"), textcoords=\"offset points\", xytext=(0,0))\n",
      "plt.annotate(\"Weddings\", coords(\"Weddings and Engagements\"), textcoords=\"offset points\", xytext=(-20,0))\n",
      "plt.annotate(\"Baseball\", coords(\"Baseball\"), textcoords=\"offset points\", xytext=(5,-2))\n",
      "plt.annotate(\"Football\", coords(\"Football\"), textcoords=\"offset points\", xytext=(2,5))\n",
      "plt.annotate(\"Basketball\", coords(\"Basketball\"), textcoords=\"offset points\", xytext=(1,-12))\n",
      "plt.annotate(\"Children\", coords(\"Children and Youth\"), textcoords=\"offset points\", xytext=(5,0))\n",
      "plt.annotate(\"Education\", coords(\"Education and Schools\"), textcoords=\"offset points\", xytext=(2,0))\n",
      "plt.annotate(\"Computers\", coords(\"Computers and the Internet\"), textcoords=\"offset points\", xytext=(-1,5))\n",
      "plt.annotate(\"Politics\", coords(\"Politics and Government\"), textcoords=\"offset points\", xytext=(-25,-2))\n",
      "plt.annotate(\"Travel\", coords(\"Travel and Vacations\"), textcoords=\"offset points\", xytext=(2,0))\n",
      "plt.annotate(\"Art\", coords(\"Art\"), textcoords=\"offset points\", xytext=(2,0))\n",
      "plt.show()\n",
      "207/517:\n",
      "def coords(label):\n",
      "    i = labels.index(label)\n",
      "    return (heshes[i], wbiases[i])\n",
      "plt.plot(heshes, wbiases, \"o\")\n",
      "plt.title(\"Ratio of 'she' to 'he' vs Absolute WEAT Bias on Professions\")\n",
      "plt.annotate(\"Automobiles\", coords(\"Automobiles\"), textcoords=\"offset points\", xytext=(0,0))\n",
      "plt.annotate(\"Weddings\", coords(\"Weddings and Engagements\"), textcoords=\"offset points\", xytext=(-20,0))\n",
      "plt.annotate(\"Baseball\", coords(\"Baseball\"), textcoords=\"offset points\", xytext=(5,-2))\n",
      "plt.annotate(\"Football\", coords(\"Football\"), textcoords=\"offset points\", xytext=(2,5))\n",
      "plt.annotate(\"Basketball\", coords(\"Basketball\"), textcoords=\"offset points\", xytext=(1,-12))\n",
      "plt.annotate(\"Children\", coords(\"Children and Youth\"), textcoords=\"offset points\", xytext=(5,0))\n",
      "plt.annotate(\"Education\", coords(\"Education and Schools\"), textcoords=\"offset points\", xytext=(2,0))\n",
      "plt.annotate(\"Computers\", coords(\"Computers and the Internet\"), textcoords=\"offset points\", xytext=(-1,5))\n",
      "plt.annotate(\"Politics\", coords(\"Politics and Government\"), textcoords=\"offset points\", xytext=(-50,-2))\n",
      "plt.annotate(\"Travel\", coords(\"Travel and Vacations\"), textcoords=\"offset points\", xytext=(2,0))\n",
      "plt.annotate(\"Art\", coords(\"Art\"), textcoords=\"offset points\", xytext=(2,0))\n",
      "plt.show()\n",
      "207/518:\n",
      "def coords(label):\n",
      "    i = labels.index(label)\n",
      "    return (heshes[i], wbiases[i])\n",
      "plt.plot(heshes, wbiases, \"o\")\n",
      "plt.title(\"Ratio of 'she' to 'he' vs Absolute WEAT Bias on Professions\")\n",
      "plt.annotate(\"Automobiles\", coords(\"Automobiles\"), textcoords=\"offset points\", xytext=(0,0))\n",
      "plt.annotate(\"Weddings\", coords(\"Weddings and Engagements\"), textcoords=\"offset points\", xytext=(-20,0))\n",
      "plt.annotate(\"Baseball\", coords(\"Baseball\"), textcoords=\"offset points\", xytext=(5,-2))\n",
      "plt.annotate(\"Football\", coords(\"Football\"), textcoords=\"offset points\", xytext=(2,5))\n",
      "plt.annotate(\"Basketball\", coords(\"Basketball\"), textcoords=\"offset points\", xytext=(1,-12))\n",
      "plt.annotate(\"Children\", coords(\"Children and Youth\"), textcoords=\"offset points\", xytext=(5,0))\n",
      "plt.annotate(\"Education\", coords(\"Education and Schools\"), textcoords=\"offset points\", xytext=(2,0))\n",
      "plt.annotate(\"Computers\", coords(\"Computers and the Internet\"), textcoords=\"offset points\", xytext=(-1,5))\n",
      "plt.annotate(\"Politics\", coords(\"Politics and Government\"), textcoords=\"offset points\", xytext=(-30,-2))\n",
      "plt.annotate(\"Travel\", coords(\"Travel and Vacations\"), textcoords=\"offset points\", xytext=(2,0))\n",
      "plt.annotate(\"Art\", coords(\"Art\"), textcoords=\"offset points\", xytext=(2,0))\n",
      "plt.show()\n",
      "207/519:\n",
      "def coords(label):\n",
      "    i = labels.index(label)\n",
      "    return (heshes[i], wbiases[i])\n",
      "plt.plot(heshes, wbiases, \"o\")\n",
      "plt.title(\"Ratio of 'she' to 'he' vs Absolute WEAT Bias on Professions\")\n",
      "plt.annotate(\"Automobiles\", coords(\"Automobiles\"), textcoords=\"offset points\", xytext=(0,0))\n",
      "plt.annotate(\"Weddings\", coords(\"Weddings and Engagements\"), textcoords=\"offset points\", xytext=(-20,0))\n",
      "plt.annotate(\"Baseball\", coords(\"Baseball\"), textcoords=\"offset points\", xytext=(5,-2))\n",
      "plt.annotate(\"Football\", coords(\"Football\"), textcoords=\"offset points\", xytext=(2,5))\n",
      "plt.annotate(\"Basketball\", coords(\"Basketball\"), textcoords=\"offset points\", xytext=(1,-12))\n",
      "plt.annotate(\"Children\", coords(\"Children and Youth\"), textcoords=\"offset points\", xytext=(5,0))\n",
      "plt.annotate(\"Education\", coords(\"Education and Schools\"), textcoords=\"offset points\", xytext=(2,0))\n",
      "plt.annotate(\"Computers\", coords(\"Computers and the Internet\"), textcoords=\"offset points\", xytext=(-1,5))\n",
      "plt.annotate(\"Politics\", coords(\"Politics and Government\"), textcoords=\"offset points\", xytext=(-30,-1))\n",
      "plt.annotate(\"Travel\", coords(\"Travel and Vacations\"), textcoords=\"offset points\", xytext=(2,0))\n",
      "plt.annotate(\"Art\", coords(\"Art\"), textcoords=\"offset points\", xytext=(2,0))\n",
      "plt.show()\n",
      "207/520:\n",
      "def coords(label):\n",
      "    i = labels.index(label)\n",
      "    return (heshes[i], wbiases[i])\n",
      "plt.plot(heshes, wbiases, \"o\")\n",
      "plt.title(\"Ratio of 'she' to 'he' vs Absolute WEAT Bias on Professions\")\n",
      "plt.annotate(\"Automobiles\", coords(\"Automobiles\"), textcoords=\"offset points\", xytext=(0,0))\n",
      "plt.annotate(\"Weddings\", coords(\"Weddings and Engagements\"), textcoords=\"offset points\", xytext=(-20,0))\n",
      "plt.annotate(\"Baseball\", coords(\"Baseball\"), textcoords=\"offset points\", xytext=(5,-2))\n",
      "plt.annotate(\"Football\", coords(\"Football\"), textcoords=\"offset points\", xytext=(2,5))\n",
      "plt.annotate(\"Basketball\", coords(\"Basketball\"), textcoords=\"offset points\", xytext=(1,-12))\n",
      "plt.annotate(\"Children\", coords(\"Children and Youth\"), textcoords=\"offset points\", xytext=(5,0))\n",
      "plt.annotate(\"Education\", coords(\"Education and Schools\"), textcoords=\"offset points\", xytext=(2,0))\n",
      "plt.annotate(\"Computers\", coords(\"Computers and the Internet\"), textcoords=\"offset points\", xytext=(-1,5))\n",
      "plt.annotate(\"Politics\", coords(\"Politics and Government\"), textcoords=\"offset points\", xytext=(-35,-1))\n",
      "plt.annotate(\"Travel\", coords(\"Travel and Vacations\"), textcoords=\"offset points\", xytext=(2,0))\n",
      "plt.annotate(\"Art\", coords(\"Art\"), textcoords=\"offset points\", xytext=(2,0))\n",
      "plt.show()\n",
      "207/521:\n",
      "def coords(label):\n",
      "    i = labels.index(label)\n",
      "    return (heshes[i], wbiases[i])\n",
      "plt.plot(heshes, wbiases, \"o\")\n",
      "plt.title(\"Ratio of 'she' to 'he' vs Absolute WEAT Bias on Professions\")\n",
      "plt.annotate(\"Automobiles\", coords(\"Automobiles\"), textcoords=\"offset points\", xytext=(0,0))\n",
      "plt.annotate(\"Weddings\", coords(\"Weddings and Engagements\"), textcoords=\"offset points\", xytext=(-20,0))\n",
      "plt.annotate(\"Baseball\", coords(\"Baseball\"), textcoords=\"offset points\", xytext=(5,-2))\n",
      "plt.annotate(\"Football\", coords(\"Football\"), textcoords=\"offset points\", xytext=(2,5))\n",
      "plt.annotate(\"Basketball\", coords(\"Basketball\"), textcoords=\"offset points\", xytext=(1,-12))\n",
      "plt.annotate(\"Children\", coords(\"Children and Youth\"), textcoords=\"offset points\", xytext=(5,0))\n",
      "plt.annotate(\"Education\", coords(\"Education and Schools\"), textcoords=\"offset points\", xytext=(2,0))\n",
      "plt.annotate(\"Computers\", coords(\"Computers and the Internet\"), textcoords=\"offset points\", xytext=(-1,5))\n",
      "plt.annotate(\"Politics\", coords(\"Politics and Government\"), textcoords=\"offset points\", xytext=(-35,1))\n",
      "plt.annotate(\"Travel\", coords(\"Travel and Vacations\"), textcoords=\"offset points\", xytext=(2,0))\n",
      "plt.annotate(\"Art\", coords(\"Art\"), textcoords=\"offset points\", xytext=(2,0))\n",
      "plt.show()\n",
      "207/522:\n",
      "def coords(label):\n",
      "    i = labels.index(label)\n",
      "    return (heshes[i], wbiases[i])\n",
      "plt.plot(heshes, wbiases, \"o\")\n",
      "plt.title(\"Ratio of 'she' to 'he' vs Absolute WEAT Bias on Professions\")\n",
      "plt.annotate(\"Automobiles\", coords(\"Automobiles\"), textcoords=\"offset points\", xytext=(0,0))\n",
      "plt.annotate(\"Weddings\", coords(\"Weddings and Engagements\"), textcoords=\"offset points\", xytext=(-20,0))\n",
      "plt.annotate(\"Baseball\", coords(\"Baseball\"), textcoords=\"offset points\", xytext=(5,-2))\n",
      "plt.annotate(\"Football\", coords(\"Football\"), textcoords=\"offset points\", xytext=(2,5))\n",
      "plt.annotate(\"Basketball\", coords(\"Basketball\"), textcoords=\"offset points\", xytext=(1,-12))\n",
      "plt.annotate(\"Children\", coords(\"Children and Youth\"), textcoords=\"offset points\", xytext=(5,0))\n",
      "plt.annotate(\"Education\", coords(\"Education and Schools\"), textcoords=\"offset points\", xytext=(2,0))\n",
      "plt.annotate(\"Computers\", coords(\"Computers and the Internet\"), textcoords=\"offset points\", xytext=(-1,5))\n",
      "plt.annotate(\"Politics\", coords(\"Politics and Government\"), textcoords=\"offset points\", xytext=(-35,0))\n",
      "plt.annotate(\"Travel\", coords(\"Travel and Vacations\"), textcoords=\"offset points\", xytext=(2,0))\n",
      "plt.annotate(\"Art\", coords(\"Art\"), textcoords=\"offset points\", xytext=(2,0))\n",
      "plt.show()\n",
      "207/523:\n",
      "def coords(label):\n",
      "    i = labels.index(label)\n",
      "    return (heshes[i], wbiases[i])\n",
      "plt.plot(heshes, wbiases, \"o\")\n",
      "plt.title(\"Ratio of 'she' to 'he' vs Absolute WEAT Bias on Professions\")\n",
      "plt.annotate(\"Automobiles\", coords(\"Automobiles\"), textcoords=\"offset points\", xytext=(0,0))\n",
      "plt.annotate(\"Weddings\", coords(\"Weddings and Engagements\"), textcoords=\"offset points\", xytext=(-20,0))\n",
      "plt.annotate(\"Baseball\", coords(\"Baseball\"), textcoords=\"offset points\", xytext=(5,-2))\n",
      "plt.annotate(\"Football\", coords(\"Football\"), textcoords=\"offset points\", xytext=(2,5))\n",
      "plt.annotate(\"Basketball\", coords(\"Basketball\"), textcoords=\"offset points\", xytext=(1,-12))\n",
      "plt.annotate(\"Children\", coords(\"Children and Youth\"), textcoords=\"offset points\", xytext=(5,0))\n",
      "plt.annotate(\"Education\", coords(\"Education and Schools\"), textcoords=\"offset points\", xytext=(2,0))\n",
      "plt.annotate(\"Computers\", coords(\"Computers and the Internet\"), textcoords=\"offset points\", xytext=(-1,5))\n",
      "plt.annotate(\"Politics\", coords(\"Politics and Government\"), textcoords=\"offset points\", xytext=(-35,-5))\n",
      "plt.annotate(\"Travel\", coords(\"Travel and Vacations\"), textcoords=\"offset points\", xytext=(2,0))\n",
      "plt.annotate(\"Art\", coords(\"Art\"), textcoords=\"offset points\", xytext=(2,0))\n",
      "plt.show()\n",
      "207/524:\n",
      "def coords(label):\n",
      "    i = labels.index(label)\n",
      "    return (heshes[i], wbiases[i])\n",
      "plt.plot(heshes, wbiases, \"o\")\n",
      "plt.title(\"Ratio of 'she' to 'he' vs Absolute WEAT Bias on Professions\")\n",
      "plt.annotate(\"Automobiles\", coords(\"Automobiles\"), textcoords=\"offset points\", xytext=(0,0))\n",
      "plt.annotate(\"Weddings\", coords(\"Weddings and Engagements\"), textcoords=\"offset points\", xytext=(-20,0))\n",
      "plt.annotate(\"Baseball\", coords(\"Baseball\"), textcoords=\"offset points\", xytext=(5,-2))\n",
      "plt.annotate(\"Football\", coords(\"Football\"), textcoords=\"offset points\", xytext=(2,5))\n",
      "plt.annotate(\"Basketball\", coords(\"Basketball\"), textcoords=\"offset points\", xytext=(1,-12))\n",
      "plt.annotate(\"Children\", coords(\"Children and Youth\"), textcoords=\"offset points\", xytext=(5,0))\n",
      "plt.annotate(\"Education\", coords(\"Education and Schools\"), textcoords=\"offset points\", xytext=(2,0))\n",
      "plt.annotate(\"Computers\", coords(\"Computers and the Internet\"), textcoords=\"offset points\", xytext=(-1,5))\n",
      "plt.annotate(\"Politics\", coords(\"Politics and Government\"), textcoords=\"offset points\", xytext=(-36,-5))\n",
      "plt.annotate(\"Travel\", coords(\"Travel and Vacations\"), textcoords=\"offset points\", xytext=(2,0))\n",
      "plt.annotate(\"Art\", coords(\"Art\"), textcoords=\"offset points\", xytext=(2,0))\n",
      "plt.show()\n",
      "207/525:\n",
      "def coords(label):\n",
      "    i = labels.index(label)\n",
      "    return (heshes[i], wbiases[i])\n",
      "plt.plot(heshes, wbiases, \"o\")\n",
      "plt.title(\"Ratio of 'she' to 'he' vs Absolute WEAT Bias on Professions\")\n",
      "plt.annotate(\"Automobiles\", coords(\"Automobiles\"), textcoords=\"offset points\", xytext=(0,0))\n",
      "plt.annotate(\"Weddings\", coords(\"Weddings and Engagements\"), textcoords=\"offset points\", xytext=(-20,0))\n",
      "plt.annotate(\"Baseball\", coords(\"Baseball\"), textcoords=\"offset points\", xytext=(5,-2))\n",
      "plt.annotate(\"Football\", coords(\"Football\"), textcoords=\"offset points\", xytext=(2,5))\n",
      "plt.annotate(\"Basketball\", coords(\"Basketball\"), textcoords=\"offset points\", xytext=(1,-12))\n",
      "plt.annotate(\"Children\", coords(\"Children and Youth\"), textcoords=\"offset points\", xytext=(5,0))\n",
      "plt.annotate(\"Education\", coords(\"Education and Schools\"), textcoords=\"offset points\", xytext=(2,0))\n",
      "plt.annotate(\"Computers\", coords(\"Computers and the Internet\"), textcoords=\"offset points\", xytext=(-1,5))\n",
      "plt.annotate(\"Politics\", coords(\"Politics and Government\"), textcoords=\"offset points\", xytext=(-40,-5))\n",
      "plt.annotate(\"Travel\", coords(\"Travel and Vacations\"), textcoords=\"offset points\", xytext=(2,0))\n",
      "plt.annotate(\"Art\", coords(\"Art\"), textcoords=\"offset points\", xytext=(2,0))\n",
      "plt.show()\n",
      "207/526:\n",
      "def coords(label):\n",
      "    i = labels.index(label)\n",
      "    return (heshes[i], wbiases[i])\n",
      "plt.plot(heshes, wbiases, \"o\")\n",
      "plt.title(\"Ratio of 'she' to 'he' vs Absolute WEAT Bias on Professions\")\n",
      "plt.annotate(\"Automobiles\", coords(\"Automobiles\"), textcoords=\"offset points\", xytext=(0,0))\n",
      "plt.annotate(\"Weddings\", coords(\"Weddings and Engagements\"), textcoords=\"offset points\", xytext=(-20,0))\n",
      "plt.annotate(\"Baseball\", coords(\"Baseball\"), textcoords=\"offset points\", xytext=(5,-2))\n",
      "plt.annotate(\"Football\", coords(\"Football\"), textcoords=\"offset points\", xytext=(2,5))\n",
      "plt.annotate(\"Basketball\", coords(\"Basketball\"), textcoords=\"offset points\", xytext=(1,-12))\n",
      "plt.annotate(\"Children\", coords(\"Children and Youth\"), textcoords=\"offset points\", xytext=(5,0))\n",
      "plt.annotate(\"Education\", coords(\"Education and Schools\"), textcoords=\"offset points\", xytext=(2,0))\n",
      "plt.annotate(\"Computers\", coords(\"Computers and the Internet\"), textcoords=\"offset points\", xytext=(-1,5))\n",
      "plt.annotate(\"Politics\", coords(\"Politics and Government\"), textcoords=\"offset points\", xytext=(-40,-6))\n",
      "plt.annotate(\"Travel\", coords(\"Travel and Vacations\"), textcoords=\"offset points\", xytext=(2,0))\n",
      "plt.annotate(\"Art\", coords(\"Art\"), textcoords=\"offset points\", xytext=(2,0))\n",
      "plt.show()\n",
      "207/527:\n",
      "def coords(label):\n",
      "    i = labels.index(label)\n",
      "    return (heshes[i], wbiases[i])\n",
      "plt.plot(heshes, wbiases, \"o\")\n",
      "plt.title(\"Ratio of 'she' to 'he' vs Absolute WEAT Bias on Professions\")\n",
      "plt.annotate(\"Automobiles\", coords(\"Automobiles\"), textcoords=\"offset points\", xytext=(0,0))\n",
      "plt.annotate(\"Weddings\", coords(\"Weddings and Engagements\"), textcoords=\"offset points\", xytext=(-20,0))\n",
      "plt.annotate(\"Baseball\", coords(\"Baseball\"), textcoords=\"offset points\", xytext=(5,-2))\n",
      "plt.annotate(\"Football\", coords(\"Football\"), textcoords=\"offset points\", xytext=(2,5))\n",
      "plt.annotate(\"Basketball\", coords(\"Basketball\"), textcoords=\"offset points\", xytext=(1,-12))\n",
      "plt.annotate(\"Children\", coords(\"Children and Youth\"), textcoords=\"offset points\", xytext=(5,0))\n",
      "plt.annotate(\"Education\", coords(\"Education and Schools\"), textcoords=\"offset points\", xytext=(2,0))\n",
      "plt.annotate(\"Computers\", coords(\"Computers and the Internet\"), textcoords=\"offset points\", xytext=(-1,5))\n",
      "plt.annotate(\"Politics\", coords(\"Politics and Government\"), textcoords=\"offset points\", xytext=(-38,-6))\n",
      "plt.annotate(\"Travel\", coords(\"Travel and Vacations\"), textcoords=\"offset points\", xytext=(2,0))\n",
      "plt.annotate(\"Art\", coords(\"Art\"), textcoords=\"offset points\", xytext=(2,0))\n",
      "plt.show()\n",
      "207/528:\n",
      "def coords(label):\n",
      "    i = labels.index(label)\n",
      "    return (heshes[i], wbiases[i])\n",
      "plt.plot(heshes, wbiases, \"o\")\n",
      "plt.title(\"Ratio of 'she' to 'he' vs Absolute WEAT Bias on Professions\")\n",
      "plt.annotate(\"Automobiles\", coords(\"Automobiles\"), textcoords=\"offset points\", xytext=(0,0))\n",
      "plt.annotate(\"Weddings\", coords(\"Weddings and Engagements\"), textcoords=\"offset points\", xytext=(-20,0))\n",
      "plt.annotate(\"Baseball\", coords(\"Baseball\"), textcoords=\"offset points\", xytext=(5,-2))\n",
      "plt.annotate(\"Football\", coords(\"Football\"), textcoords=\"offset points\", xytext=(2,5))\n",
      "plt.annotate(\"Basketball\", coords(\"Basketball\"), textcoords=\"offset points\", xytext=(1,-12))\n",
      "plt.annotate(\"Children\", coords(\"Children and Youth\"), textcoords=\"offset points\", xytext=(5,0))\n",
      "plt.annotate(\"Education\", coords(\"Education and Schools\"), textcoords=\"offset points\", xytext=(2,0))\n",
      "plt.annotate(\"Computers\", coords(\"Computers and the Internet\"), textcoords=\"offset points\", xytext=(-1,5))\n",
      "plt.annotate(\"Politics\", coords(\"Politics and Government\"), textcoords=\"offset points\", xytext=(-38,-6))\n",
      "plt.annotate(\"Travel\", coords(\"Travel and Vacations\"), textcoords=\"offset points\", xytext=(-2,20))\n",
      "plt.annotate(\"Art\", coords(\"Art\"), textcoords=\"offset points\", xytext=(3,0))\n",
      "plt.show()\n",
      "207/529:\n",
      "def coords(label):\n",
      "    i = labels.index(label)\n",
      "    return (heshes[i], wbiases[i])\n",
      "plt.plot(heshes, wbiases, \"o\")\n",
      "plt.title(\"Ratio of 'she' to 'he' vs Absolute WEAT Bias on Professions\")\n",
      "plt.annotate(\"Automobiles\", coords(\"Automobiles\"), textcoords=\"offset points\", xytext=(0,0))\n",
      "plt.annotate(\"Weddings\", coords(\"Weddings and Engagements\"), textcoords=\"offset points\", xytext=(-20,0))\n",
      "plt.annotate(\"Baseball\", coords(\"Baseball\"), textcoords=\"offset points\", xytext=(5,-2))\n",
      "plt.annotate(\"Football\", coords(\"Football\"), textcoords=\"offset points\", xytext=(2,5))\n",
      "plt.annotate(\"Basketball\", coords(\"Basketball\"), textcoords=\"offset points\", xytext=(1,-12))\n",
      "plt.annotate(\"Children\", coords(\"Children and Youth\"), textcoords=\"offset points\", xytext=(5,0))\n",
      "plt.annotate(\"Education\", coords(\"Education and Schools\"), textcoords=\"offset points\", xytext=(2,0))\n",
      "plt.annotate(\"Computers\", coords(\"Computers and the Internet\"), textcoords=\"offset points\", xytext=(-1,5))\n",
      "plt.annotate(\"Politics\", coords(\"Politics and Government\"), textcoords=\"offset points\", xytext=(-38,-6))\n",
      "plt.annotate(\"Travel\", coords(\"Travel and Vacations\"), textcoords=\"offset points\", xytext=(-2, 10))\n",
      "plt.annotate(\"Art\", coords(\"Art\"), textcoords=\"offset points\", xytext=(3,0))\n",
      "plt.show()\n",
      "207/530:\n",
      "def coords(label):\n",
      "    i = labels.index(label)\n",
      "    return (heshes[i], wbiases[i])\n",
      "plt.plot(heshes, wbiases, \"o\")\n",
      "plt.title(\"Ratio of 'she' to 'he' vs Absolute WEAT Bias on Professions\")\n",
      "plt.annotate(\"Automobiles\", coords(\"Automobiles\"), textcoords=\"offset points\", xytext=(0,0))\n",
      "plt.annotate(\"Weddings\", coords(\"Weddings and Engagements\"), textcoords=\"offset points\", xytext=(-20,0))\n",
      "plt.annotate(\"Baseball\", coords(\"Baseball\"), textcoords=\"offset points\", xytext=(5,-2))\n",
      "plt.annotate(\"Football\", coords(\"Football\"), textcoords=\"offset points\", xytext=(2,5))\n",
      "plt.annotate(\"Basketball\", coords(\"Basketball\"), textcoords=\"offset points\", xytext=(1,-12))\n",
      "plt.annotate(\"Children\", coords(\"Children and Youth\"), textcoords=\"offset points\", xytext=(5,0))\n",
      "plt.annotate(\"Education\", coords(\"Education and Schools\"), textcoords=\"offset points\", xytext=(2,0))\n",
      "plt.annotate(\"Computers\", coords(\"Computers and the Internet\"), textcoords=\"offset points\", xytext=(-1,5))\n",
      "plt.annotate(\"Politics\", coords(\"Politics and Government\"), textcoords=\"offset points\", xytext=(-38,-6))\n",
      "plt.annotate(\"Travel\", coords(\"Travel and Vacations\"), textcoords=\"offset points\", xytext=(-2, 6))\n",
      "plt.annotate(\"Art\", coords(\"Art\"), textcoords=\"offset points\", xytext=(3,0))\n",
      "plt.show()\n",
      "207/531:\n",
      "def coords(label):\n",
      "    i = labels.index(label)\n",
      "    return (heshes[i], wbiases[i])\n",
      "plt.plot(heshes, wbiases, \"o\")\n",
      "plt.title(\"Ratio of 'she' to 'he' vs Absolute WEAT Bias on Professions\")\n",
      "plt.annotate(\"Automobiles\", coords(\"Automobiles\"), textcoords=\"offset points\", xytext=(0,0))\n",
      "plt.annotate(\"Weddings\", coords(\"Weddings and Engagements\"), textcoords=\"offset points\", xytext=(-20,0))\n",
      "plt.annotate(\"Baseball\", coords(\"Baseball\"), textcoords=\"offset points\", xytext=(5,-2))\n",
      "plt.annotate(\"Football\", coords(\"Football\"), textcoords=\"offset points\", xytext=(2,5))\n",
      "plt.annotate(\"Basketball\", coords(\"Basketball\"), textcoords=\"offset points\", xytext=(1,-12))\n",
      "plt.annotate(\"Children\", coords(\"Children and Youth\"), textcoords=\"offset points\", xytext=(5,0))\n",
      "plt.annotate(\"Education\", coords(\"Education and Schools\"), textcoords=\"offset points\", xytext=(2,0))\n",
      "plt.annotate(\"Computers\", coords(\"Computers and the Internet\"), textcoords=\"offset points\", xytext=(-1,5))\n",
      "plt.annotate(\"Politics\", coords(\"Politics and Government\"), textcoords=\"offset points\", xytext=(-38,-6))\n",
      "plt.annotate(\"Travel\", coords(\"Travel and Vacations\"), textcoords=\"offset points\", xytext=(-10, 6))\n",
      "plt.annotate(\"Art\", coords(\"Art\"), textcoords=\"offset points\", xytext=(3,0))\n",
      "plt.show()\n",
      "207/532:\n",
      "def coords(label):\n",
      "    i = labels.index(label)\n",
      "    return (heshes[i], wbiases[i])\n",
      "plt.plot(heshes, wbiases, \"o\")\n",
      "plt.title(\"Ratio of 'she' to 'he' vs Absolute WEAT Bias on Professions\")\n",
      "plt.annotate(\"Automobiles\", coords(\"Automobiles\"), textcoords=\"offset points\", xytext=(0,0))\n",
      "plt.annotate(\"Weddings\", coords(\"Weddings and Engagements\"), textcoords=\"offset points\", xytext=(-20,0))\n",
      "plt.annotate(\"Baseball\", coords(\"Baseball\"), textcoords=\"offset points\", xytext=(5,-2))\n",
      "plt.annotate(\"Football\", coords(\"Football\"), textcoords=\"offset points\", xytext=(2,5))\n",
      "plt.annotate(\"Basketball\", coords(\"Basketball\"), textcoords=\"offset points\", xytext=(1,-12))\n",
      "plt.annotate(\"Children\", coords(\"Children and Youth\"), textcoords=\"offset points\", xytext=(5,0))\n",
      "plt.annotate(\"Education\", coords(\"Education and Schools\"), textcoords=\"offset points\", xytext=(2,0))\n",
      "plt.annotate(\"Computers\", coords(\"Computers and the Internet\"), textcoords=\"offset points\", xytext=(-1,5))\n",
      "plt.annotate(\"Politics\", coords(\"Politics and Government\"), textcoords=\"offset points\", xytext=(-38,-6))\n",
      "plt.annotate(\"Travel\", coords(\"Travel and Vacations\"), textcoords=\"offset points\", xytext=(-20, 6))\n",
      "plt.annotate(\"Art\", coords(\"Art\"), textcoords=\"offset points\", xytext=(3,0))\n",
      "plt.show()\n",
      "207/533:\n",
      "def coords(label):\n",
      "    i = labels.index(label)\n",
      "    return (heshes[i], wbiases[i])\n",
      "plt.plot(heshes, wbiases, \"o\")\n",
      "plt.title(\"Ratio of 'she' to 'he' vs Absolute WEAT Bias on Professions\")\n",
      "plt.annotate(\"Automobiles\", coords(\"Automobiles\"), textcoords=\"offset points\", xytext=(0,0))\n",
      "plt.annotate(\"Weddings\", coords(\"Weddings and Engagements\"), textcoords=\"offset points\", xytext=(-20,0))\n",
      "plt.annotate(\"Baseball\", coords(\"Baseball\"), textcoords=\"offset points\", xytext=(5,-2))\n",
      "plt.annotate(\"Football\", coords(\"Football\"), textcoords=\"offset points\", xytext=(2,5))\n",
      "plt.annotate(\"Basketball\", coords(\"Basketball\"), textcoords=\"offset points\", xytext=(1,-12))\n",
      "plt.annotate(\"Children\", coords(\"Children and Youth\"), textcoords=\"offset points\", xytext=(5,0))\n",
      "plt.annotate(\"Education\", coords(\"Education and Schools\"), textcoords=\"offset points\", xytext=(2,0))\n",
      "plt.annotate(\"Computers\", coords(\"Computers and the Internet\"), textcoords=\"offset points\", xytext=(-1,5))\n",
      "plt.annotate(\"Politics\", coords(\"Politics and Government\"), textcoords=\"offset points\", xytext=(-38,-6))\n",
      "plt.annotate(\"Travel\", coords(\"Travel and Vacations\"), textcoords=\"offset points\", xytext=(-20, 6))\n",
      "plt.annotate(\"Art\", coords(\"Art\"), textcoords=\"offset points\", xytext=(-20,0))\n",
      "plt.show()\n",
      "207/534:\n",
      "def coords(label):\n",
      "    i = labels.index(label)\n",
      "    return (heshes[i], wbiases[i])\n",
      "plt.plot(heshes, wbiases, \"o\")\n",
      "plt.title(\"Ratio of 'she' to 'he' vs Absolute WEAT Bias on Professions\")\n",
      "plt.annotate(\"Automobiles\", coords(\"Automobiles\"), textcoords=\"offset points\", xytext=(0,0))\n",
      "plt.annotate(\"Weddings\", coords(\"Weddings and Engagements\"), textcoords=\"offset points\", xytext=(-20,0))\n",
      "plt.annotate(\"Baseball\", coords(\"Baseball\"), textcoords=\"offset points\", xytext=(5,-2))\n",
      "plt.annotate(\"Football\", coords(\"Football\"), textcoords=\"offset points\", xytext=(2,5))\n",
      "plt.annotate(\"Basketball\", coords(\"Basketball\"), textcoords=\"offset points\", xytext=(1,-12))\n",
      "plt.annotate(\"Children\", coords(\"Children and Youth\"), textcoords=\"offset points\", xytext=(5,0))\n",
      "plt.annotate(\"Education\", coords(\"Education and Schools\"), textcoords=\"offset points\", xytext=(2,0))\n",
      "plt.annotate(\"Computers\", coords(\"Computers and the Internet\"), textcoords=\"offset points\", xytext=(-1,5))\n",
      "plt.annotate(\"Politics\", coords(\"Politics and Government\"), textcoords=\"offset points\", xytext=(-38,-6))\n",
      "plt.annotate(\"Travel\", coords(\"Travel and Vacations\"), textcoords=\"offset points\", xytext=(-25, 6))\n",
      "plt.annotate(\"Art\", coords(\"Art\"), textcoords=\"offset points\", xytext=(-18,0))\n",
      "plt.show()\n",
      "207/535:\n",
      "def coords(label):\n",
      "    i = labels.index(label)\n",
      "    return (heshes[i], wbiases[i])\n",
      "plt.plot(heshes, wbiases, \"o\")\n",
      "plt.title(\"Ratio of 'she' to 'he' vs Absolute WEAT Bias on Professions\")\n",
      "plt.annotate(\"Automobiles\", coords(\"Automobiles\"), textcoords=\"offset points\", xytext=(0,0))\n",
      "plt.annotate(\"Weddings\", coords(\"Weddings and Engagements\"), textcoords=\"offset points\", xytext=(-20,0))\n",
      "plt.annotate(\"Baseball\", coords(\"Baseball\"), textcoords=\"offset points\", xytext=(5,-2))\n",
      "plt.annotate(\"Football\", coords(\"Football\"), textcoords=\"offset points\", xytext=(2,5))\n",
      "plt.annotate(\"Basketball\", coords(\"Basketball\"), textcoords=\"offset points\", xytext=(1,-12))\n",
      "plt.annotate(\"Children\", coords(\"Children and Youth\"), textcoords=\"offset points\", xytext=(5,0))\n",
      "plt.annotate(\"Education\", coords(\"Education and Schools\"), textcoords=\"offset points\", xytext=(2,0))\n",
      "plt.annotate(\"Computers\", coords(\"Computers and the Internet\"), textcoords=\"offset points\", xytext=(-1,5))\n",
      "plt.annotate(\"Politics\", coords(\"Politics and Government\"), textcoords=\"offset points\", xytext=(-38,-6))\n",
      "plt.annotate(\"Travel\", coords(\"Travel and Vacations\"), textcoords=\"offset points\", xytext=(-30, 6))\n",
      "plt.annotate(\"Art\", coords(\"Art\"), textcoords=\"offset points\", xytext=(-18,0))\n",
      "plt.show()\n",
      "207/536:\n",
      "def coords(label):\n",
      "    i = labels.index(label)\n",
      "    return (heshes[i], wbiases[i])\n",
      "plt.plot(heshes, wbiases, \"o\")\n",
      "plt.title(\"Ratio of 'she' to 'he' vs Absolute WEAT Bias on Professions\")\n",
      "plt.annotate(\"Automobiles\", coords(\"Automobiles\"), textcoords=\"offset points\", xytext=(0,0))\n",
      "plt.annotate(\"Weddings\", coords(\"Weddings and Engagements\"), textcoords=\"offset points\", xytext=(-20,0))\n",
      "plt.annotate(\"Baseball\", coords(\"Baseball\"), textcoords=\"offset points\", xytext=(5,-2))\n",
      "plt.annotate(\"Football\", coords(\"Football\"), textcoords=\"offset points\", xytext=(2,5))\n",
      "plt.annotate(\"Basketball\", coords(\"Basketball\"), textcoords=\"offset points\", xytext=(1,-12))\n",
      "plt.annotate(\"Children\", coords(\"Children and Youth\"), textcoords=\"offset points\", xytext=(5,0))\n",
      "plt.annotate(\"Education\", coords(\"Education and Schools\"), textcoords=\"offset points\", xytext=(2,0))\n",
      "plt.annotate(\"Computers\", coords(\"Computers and the Internet\"), textcoords=\"offset points\", xytext=(-1,5))\n",
      "plt.annotate(\"Politics\", coords(\"Politics and Government\"), textcoords=\"offset points\", xytext=(-38,-6))\n",
      "plt.annotate(\"Travel\", coords(\"Travel and Vacations\"), textcoords=\"offset points\", xytext=(-35, 4))\n",
      "plt.annotate(\"Art\", coords(\"Art\"), textcoords=\"offset points\", xytext=(-18,0))\n",
      "plt.show()\n",
      "207/537:\n",
      "def coords(label):\n",
      "    i = labels.index(label)\n",
      "    return (heshes[i], wbiases[i])\n",
      "plt.plot(heshes, wbiases, \"o\")\n",
      "plt.title(\"Ratio of 'she' to 'he' vs Absolute WEAT Bias on Professions\")\n",
      "plt.annotate(\"Automobiles\", coords(\"Automobiles\"), textcoords=\"offset points\", xytext=(0,0))\n",
      "plt.annotate(\"Weddings\", coords(\"Weddings and Engagements\"), textcoords=\"offset points\", xytext=(-20,0))\n",
      "plt.annotate(\"Baseball\", coords(\"Baseball\"), textcoords=\"offset points\", xytext=(5,-2))\n",
      "plt.annotate(\"Football\", coords(\"Football\"), textcoords=\"offset points\", xytext=(2,5))\n",
      "plt.annotate(\"Basketball\", coords(\"Basketball\"), textcoords=\"offset points\", xytext=(1,-12))\n",
      "plt.annotate(\"Children\", coords(\"Children and Youth\"), textcoords=\"offset points\", xytext=(5,0))\n",
      "plt.annotate(\"Education\", coords(\"Education and Schools\"), textcoords=\"offset points\", xytext=(2,0))\n",
      "plt.annotate(\"Computers\", coords(\"Computers and the Internet\"), textcoords=\"offset points\", xytext=(-1,5))\n",
      "plt.annotate(\"Politics\", coords(\"Politics and Government\"), textcoords=\"offset points\", xytext=(-38,-6))\n",
      "plt.annotate(\"Travel\", coords(\"Travel and Vacations\"), textcoords=\"offset points\", xytext=(-32, 4))\n",
      "plt.annotate(\"Art\", coords(\"Art\"), textcoords=\"offset points\", xytext=(-18,0))\n",
      "plt.show()\n",
      "207/538:\n",
      "def coords(label):\n",
      "    i = labels.index(label)\n",
      "    return (heshes[i], wbiases[i])\n",
      "plt.plot(heshes, wbiases, \"o\")\n",
      "plt.title(\"Ratio of 'she' to 'he' vs Absolute WEAT Bias on Professions\")\n",
      "plt.annotate(\"Automobiles\", coords(\"Automobiles\"), textcoords=\"offset points\", xytext=(0,0))\n",
      "plt.annotate(\"Weddings\", coords(\"Weddings and Engagements\"), textcoords=\"offset points\", xytext=(-20,0))\n",
      "plt.annotate(\"Baseball\", coords(\"Baseball\"), textcoords=\"offset points\", xytext=(5,-2))\n",
      "plt.annotate(\"Football\", coords(\"Football\"), textcoords=\"offset points\", xytext=(2,5))\n",
      "plt.annotate(\"Basketball\", coords(\"Basketball\"), textcoords=\"offset points\", xytext=(1,-12))\n",
      "plt.annotate(\"Children\", coords(\"Children and Youth\"), textcoords=\"offset points\", xytext=(6,0))\n",
      "plt.annotate(\"Education\", coords(\"Education and Schools\"), textcoords=\"offset points\", xytext=(2,0))\n",
      "plt.annotate(\"Computers\", coords(\"Computers and the Internet\"), textcoords=\"offset points\", xytext=(-1,5))\n",
      "plt.annotate(\"Politics\", coords(\"Politics and Government\"), textcoords=\"offset points\", xytext=(-38,-6))\n",
      "plt.annotate(\"Travel\", coords(\"Travel and Vacations\"), textcoords=\"offset points\", xytext=(-32, 4))\n",
      "plt.annotate(\"Art\", coords(\"Art\"), textcoords=\"offset points\", xytext=(-18,0))\n",
      "plt.show()\n",
      "207/539:\n",
      "def coords(label):\n",
      "    i = labels.index(label)\n",
      "    return (heshes[i], wbiases[i])\n",
      "plt.plot(heshes, wbiases, \"o\")\n",
      "plt.title(\"Ratio of 'she' to 'he' vs Absolute WEAT Bias on Professions\")\n",
      "plt.annotate(\"Automobiles\", coords(\"Automobiles\"), textcoords=\"offset points\", xytext=(0,0))\n",
      "plt.annotate(\"Weddings\", coords(\"Weddings and Engagements\"), textcoords=\"offset points\", xytext=(-30,5))\n",
      "plt.annotate(\"Baseball\", coords(\"Baseball\"), textcoords=\"offset points\", xytext=(5,-2))\n",
      "plt.annotate(\"Football\", coords(\"Football\"), textcoords=\"offset points\", xytext=(2,5))\n",
      "plt.annotate(\"Basketball\", coords(\"Basketball\"), textcoords=\"offset points\", xytext=(1,-12))\n",
      "plt.annotate(\"Children\", coords(\"Children and Youth\"), textcoords=\"offset points\", xytext=(8,0))\n",
      "plt.annotate(\"Education\", coords(\"Education and Schools\"), textcoords=\"offset points\", xytext=(2,0))\n",
      "plt.annotate(\"Computers\", coords(\"Computers and the Internet\"), textcoords=\"offset points\", xytext=(-1,5))\n",
      "plt.annotate(\"Politics\", coords(\"Politics and Government\"), textcoords=\"offset points\", xytext=(-38,-6))\n",
      "plt.annotate(\"Travel\", coords(\"Travel and Vacations\"), textcoords=\"offset points\", xytext=(-32, 4))\n",
      "plt.annotate(\"Art\", coords(\"Art\"), textcoords=\"offset points\", xytext=(-18,0))\n",
      "plt.show()\n",
      "207/540:\n",
      "def coords(label):\n",
      "    i = labels.index(label)\n",
      "    return (heshes[i], wbiases[i])\n",
      "plt.plot(heshes, wbiases, \"o\")\n",
      "plt.title(\"Ratio of 'she' to 'he' vs Absolute WEAT Bias on Professions\")\n",
      "plt.annotate(\"Automobiles\", coords(\"Automobiles\"), textcoords=\"offset points\", xytext=(0,0))\n",
      "plt.annotate(\"Weddings\", coords(\"Weddings and Engagements\"), textcoords=\"offset points\", xytext=(-35,5))\n",
      "plt.annotate(\"Baseball\", coords(\"Baseball\"), textcoords=\"offset points\", xytext=(5,-2))\n",
      "plt.annotate(\"Football\", coords(\"Football\"), textcoords=\"offset points\", xytext=(2,5))\n",
      "plt.annotate(\"Basketball\", coords(\"Basketball\"), textcoords=\"offset points\", xytext=(1,-12))\n",
      "plt.annotate(\"Children\", coords(\"Children and Youth\"), textcoords=\"offset points\", xytext=(8,0))\n",
      "plt.annotate(\"Education\", coords(\"Education and Schools\"), textcoords=\"offset points\", xytext=(2,0))\n",
      "plt.annotate(\"Computers\", coords(\"Computers and the Internet\"), textcoords=\"offset points\", xytext=(-1,5))\n",
      "plt.annotate(\"Politics\", coords(\"Politics and Government\"), textcoords=\"offset points\", xytext=(-38,-6))\n",
      "plt.annotate(\"Travel\", coords(\"Travel and Vacations\"), textcoords=\"offset points\", xytext=(-32, 4))\n",
      "plt.annotate(\"Art\", coords(\"Art\"), textcoords=\"offset points\", xytext=(-18,0))\n",
      "plt.show()\n",
      "207/541:\n",
      "def coords(label):\n",
      "    i = labels.index(label)\n",
      "    return (heshes[i], wbiases[i])\n",
      "plt.plot(heshes, wbiases, \"o\")\n",
      "plt.title(\"Ratio of 'she' to 'he' vs Absolute WEAT Bias on Professions\")\n",
      "plt.annotate(\"Automobiles\", coords(\"Automobiles\"), textcoords=\"offset points\", xytext=(0,0))\n",
      "plt.annotate(\"Weddings\", coords(\"Weddings and Engagements\"), textcoords=\"offset points\", xytext=(-35,5))\n",
      "plt.annotate(\"Baseball\", coords(\"Baseball\"), textcoords=\"offset points\", xytext=(5,-2))\n",
      "plt.annotate(\"Football\", coords(\"Football\"), textcoords=\"offset points\", xytext=(2,5))\n",
      "plt.annotate(\"Basketball\", coords(\"Basketball\"), textcoords=\"offset points\", xytext=(1,-12))\n",
      "plt.annotate(\"Children\", coords(\"Children and Youth\"), textcoords=\"offset points\", xytext=(6,0))\n",
      "plt.annotate(\"Education\", coords(\"Education and Schools\"), textcoords=\"offset points\", xytext=(2,0))\n",
      "plt.annotate(\"Computers\", coords(\"Computers and the Internet\"), textcoords=\"offset points\", xytext=(-1,5))\n",
      "plt.annotate(\"Politics\", coords(\"Politics and Government\"), textcoords=\"offset points\", xytext=(-38,-6))\n",
      "plt.annotate(\"Travel\", coords(\"Travel and Vacations\"), textcoords=\"offset points\", xytext=(-32, 4))\n",
      "plt.annotate(\"Art\", coords(\"Art\"), textcoords=\"offset points\", xytext=(-18,0))\n",
      "plt.show()\n",
      "207/542:\n",
      "def coords(label):\n",
      "    i = labels.index(label)\n",
      "    return (heshes[i], wbiases[i])\n",
      "plt.plot(heshes, wbiases, \"o\")\n",
      "plt.title(\"Ratio of 'she' to 'he' vs Absolute WEAT Bias on Professions\")\n",
      "plt.annotate(\"Automobiles\", coords(\"Automobiles\"), textcoords=\"offset points\", xytext=(0,0))\n",
      "plt.annotate(\"Weddings\", coords(\"Weddings and Engagements\"), textcoords=\"offset points\", xytext=(-35,5))\n",
      "plt.annotate(\"Baseball\", coords(\"Baseball\"), textcoords=\"offset points\", xytext=(5,-2))\n",
      "plt.annotate(\"Football\", coords(\"Football\"), textcoords=\"offset points\", xytext=(2,5))\n",
      "plt.annotate(\"Basketball\", coords(\"Basketball\"), textcoords=\"offset points\", xytext=(1,-12))\n",
      "plt.annotate(\"Children\", coords(\"Children and Youth\"), textcoords=\"offset points\", xytext=(5,0))\n",
      "plt.annotate(\"Education\", coords(\"Education and Schools\"), textcoords=\"offset points\", xytext=(5,0))\n",
      "plt.annotate(\"Computers\", coords(\"Computers and the Internet\"), textcoords=\"offset points\", xytext=(-1,5))\n",
      "plt.annotate(\"Politics\", coords(\"Politics and Government\"), textcoords=\"offset points\", xytext=(-38,-6))\n",
      "plt.annotate(\"Travel\", coords(\"Travel and Vacations\"), textcoords=\"offset points\", xytext=(-32, 4))\n",
      "plt.annotate(\"Art\", coords(\"Art\"), textcoords=\"offset points\", xytext=(-18,0))\n",
      "plt.show()\n",
      "207/543:\n",
      "def coords(label):\n",
      "    i = labels.index(label)\n",
      "    return (heshes[i], wbiases[i])\n",
      "plt.plot(heshes, wbiases, \"o\")\n",
      "plt.title(\"Ratio of 'she' to 'he' vs Absolute WEAT Bias on Professions\")\n",
      "plt.annotate(\"Automobiles\", coords(\"Automobiles\"), textcoords=\"offset points\", xytext=(0,0))\n",
      "plt.annotate(\"Weddings\", coords(\"Weddings and Engagements\"), textcoords=\"offset points\", xytext=(-35,5))\n",
      "plt.annotate(\"Baseball\", coords(\"Baseball\"), textcoords=\"offset points\", xytext=(5,-2))\n",
      "plt.annotate(\"Football\", coords(\"Football\"), textcoords=\"offset points\", xytext=(2,5))\n",
      "plt.annotate(\"Basketball\", coords(\"Basketball\"), textcoords=\"offset points\", xytext=(1,-1))\n",
      "plt.annotate(\"Children\", coords(\"Children and Youth\"), textcoords=\"offset points\", xytext=(5,0))\n",
      "plt.annotate(\"Education\", coords(\"Education and Schools\"), textcoords=\"offset points\", xytext=(5,0))\n",
      "plt.annotate(\"Computers\", coords(\"Computers and the Internet\"), textcoords=\"offset points\", xytext=(-1,5))\n",
      "plt.annotate(\"Politics\", coords(\"Politics and Government\"), textcoords=\"offset points\", xytext=(-38,-6))\n",
      "plt.annotate(\"Travel\", coords(\"Travel and Vacations\"), textcoords=\"offset points\", xytext=(-32, 4))\n",
      "plt.annotate(\"Art\", coords(\"Art\"), textcoords=\"offset points\", xytext=(-18,0))\n",
      "plt.show()\n",
      "207/544:\n",
      "def coords(label):\n",
      "    i = labels.index(label)\n",
      "    return (heshes[i], wbiases[i])\n",
      "plt.plot(heshes, wbiases, \"o\")\n",
      "plt.title(\"Ratio of 'she' to 'he' vs Absolute WEAT Bias on Professions\")\n",
      "plt.annotate(\"Automobiles\", coords(\"Automobiles\"), textcoords=\"offset points\", xytext=(0,0))\n",
      "plt.annotate(\"Weddings\", coords(\"Weddings and Engagements\"), textcoords=\"offset points\", xytext=(-35,5))\n",
      "plt.annotate(\"Baseball\", coords(\"Baseball\"), textcoords=\"offset points\", xytext=(5,-2))\n",
      "plt.annotate(\"Football\", coords(\"Football\"), textcoords=\"offset points\", xytext=(2,5))\n",
      "plt.annotate(\"Basketball\", coords(\"Basketball\"), textcoords=\"offset points\", xytext=(2,-2))\n",
      "plt.annotate(\"Children\", coords(\"Children and Youth\"), textcoords=\"offset points\", xytext=(5,0))\n",
      "plt.annotate(\"Education\", coords(\"Education and Schools\"), textcoords=\"offset points\", xytext=(5,0))\n",
      "plt.annotate(\"Computers\", coords(\"Computers and the Internet\"), textcoords=\"offset points\", xytext=(-1,5))\n",
      "plt.annotate(\"Politics\", coords(\"Politics and Government\"), textcoords=\"offset points\", xytext=(-38,-6))\n",
      "plt.annotate(\"Travel\", coords(\"Travel and Vacations\"), textcoords=\"offset points\", xytext=(-32, 4))\n",
      "plt.annotate(\"Art\", coords(\"Art\"), textcoords=\"offset points\", xytext=(-18,0))\n",
      "plt.show()\n",
      "207/545:\n",
      "def coords(label):\n",
      "    i = labels.index(label)\n",
      "    return (heshes[i], wbiases[i])\n",
      "plt.plot(heshes, wbiases, \"o\")\n",
      "plt.title(\"Ratio of 'she' to 'he' vs Absolute WEAT Bias on Professions\")\n",
      "plt.annotate(\"Automobiles\", coords(\"Automobiles\"), textcoords=\"offset points\", xytext=(0,0))\n",
      "plt.annotate(\"Weddings\", coords(\"Weddings and Engagements\"), textcoords=\"offset points\", xytext=(-35,5))\n",
      "plt.annotate(\"Baseball\", coords(\"Baseball\"), textcoords=\"offset points\", xytext=(5,-2))\n",
      "plt.annotate(\"Football\", coords(\"Football\"), textcoords=\"offset points\", xytext=(2,5))\n",
      "plt.annotate(\"Basketball\", coords(\"Basketball\"), textcoords=\"offset points\", xytext=(5,-2))\n",
      "plt.annotate(\"Children\", coords(\"Children and Youth\"), textcoords=\"offset points\", xytext=(5,0))\n",
      "plt.annotate(\"Education\", coords(\"Education and Schools\"), textcoords=\"offset points\", xytext=(5,0))\n",
      "plt.annotate(\"Computers\", coords(\"Computers and the Internet\"), textcoords=\"offset points\", xytext=(-1,5))\n",
      "plt.annotate(\"Politics\", coords(\"Politics and Government\"), textcoords=\"offset points\", xytext=(-38,-6))\n",
      "plt.annotate(\"Travel\", coords(\"Travel and Vacations\"), textcoords=\"offset points\", xytext=(-32, 4))\n",
      "plt.annotate(\"Art\", coords(\"Art\"), textcoords=\"offset points\", xytext=(-18,0))\n",
      "plt.show()\n",
      "207/546:\n",
      "def coords(label):\n",
      "    i = labels.index(label)\n",
      "    return (heshes[i], wbiases[i])\n",
      "plt.plot(heshes, wbiases, \"o\")\n",
      "plt.title(\"Ratio of 'she' to 'he' vs Absolute WEAT Bias on Professions\")\n",
      "plt.annotate(\"Automobiles\", coords(\"Automobiles\"), textcoords=\"offset points\", xytext=(-50,0))\n",
      "plt.annotate(\"Weddings\", coords(\"Weddings and Engagements\"), textcoords=\"offset points\", xytext=(-35,5))\n",
      "plt.annotate(\"Baseball\", coords(\"Baseball\"), textcoords=\"offset points\", xytext=(5,-2))\n",
      "plt.annotate(\"Football\", coords(\"Football\"), textcoords=\"offset points\", xytext=(2,5))\n",
      "plt.annotate(\"Basketball\", coords(\"Basketball\"), textcoords=\"offset points\", xytext=(5,-2))\n",
      "plt.annotate(\"Children\", coords(\"Children and Youth\"), textcoords=\"offset points\", xytext=(5,0))\n",
      "plt.annotate(\"Education\", coords(\"Education and Schools\"), textcoords=\"offset points\", xytext=(5,0))\n",
      "plt.annotate(\"Computers\", coords(\"Computers and the Internet\"), textcoords=\"offset points\", xytext=(-1,5))\n",
      "plt.annotate(\"Politics\", coords(\"Politics and Government\"), textcoords=\"offset points\", xytext=(-38,-6))\n",
      "plt.annotate(\"Travel\", coords(\"Travel and Vacations\"), textcoords=\"offset points\", xytext=(-32, 4))\n",
      "plt.annotate(\"Art\", coords(\"Art\"), textcoords=\"offset points\", xytext=(-18,0))\n",
      "plt.show()\n",
      "207/547:\n",
      "def coords(label):\n",
      "    i = labels.index(label)\n",
      "    return (heshes[i], wbiases[i])\n",
      "plt.plot(heshes, wbiases, \"o\")\n",
      "plt.title(\"Ratio of 'she' to 'he' vs Absolute WEAT Bias on Professions\")\n",
      "plt.annotate(\"Automobiles\", coords(\"Automobiles\"), textcoords=\"offset points\", xytext=(-55,0))\n",
      "plt.annotate(\"Weddings\", coords(\"Weddings and Engagements\"), textcoords=\"offset points\", xytext=(-35,5))\n",
      "plt.annotate(\"Baseball\", coords(\"Baseball\"), textcoords=\"offset points\", xytext=(5,-2))\n",
      "plt.annotate(\"Football\", coords(\"Football\"), textcoords=\"offset points\", xytext=(2,5))\n",
      "plt.annotate(\"Basketball\", coords(\"Basketball\"), textcoords=\"offset points\", xytext=(5,-2))\n",
      "plt.annotate(\"Children\", coords(\"Children and Youth\"), textcoords=\"offset points\", xytext=(5,0))\n",
      "plt.annotate(\"Education\", coords(\"Education and Schools\"), textcoords=\"offset points\", xytext=(5,0))\n",
      "plt.annotate(\"Computers\", coords(\"Computers and the Internet\"), textcoords=\"offset points\", xytext=(-1,5))\n",
      "plt.annotate(\"Politics\", coords(\"Politics and Government\"), textcoords=\"offset points\", xytext=(-38,-6))\n",
      "plt.annotate(\"Travel\", coords(\"Travel and Vacations\"), textcoords=\"offset points\", xytext=(-32, 4))\n",
      "plt.annotate(\"Art\", coords(\"Art\"), textcoords=\"offset points\", xytext=(-18,0))\n",
      "plt.show()\n",
      "207/548:\n",
      "def coords(label):\n",
      "    i = labels.index(label)\n",
      "    return (heshes[i], wbiases[i])\n",
      "plt.plot(heshes, wbiases, \"o\")\n",
      "plt.title(\"Ratio of 'she' to 'he' vs Absolute WEAT Bias on Professions\")\n",
      "plt.annotate(\"Automobiles\", coords(\"Automobiles\"), textcoords=\"offset points\", xytext=(-65,0))\n",
      "plt.annotate(\"Weddings\", coords(\"Weddings and Engagements\"), textcoords=\"offset points\", xytext=(-35,5))\n",
      "plt.annotate(\"Baseball\", coords(\"Baseball\"), textcoords=\"offset points\", xytext=(5,-2))\n",
      "plt.annotate(\"Football\", coords(\"Football\"), textcoords=\"offset points\", xytext=(2,5))\n",
      "plt.annotate(\"Basketball\", coords(\"Basketball\"), textcoords=\"offset points\", xytext=(5,-2))\n",
      "plt.annotate(\"Children\", coords(\"Children and Youth\"), textcoords=\"offset points\", xytext=(5,0))\n",
      "plt.annotate(\"Education\", coords(\"Education and Schools\"), textcoords=\"offset points\", xytext=(5,0))\n",
      "plt.annotate(\"Computers\", coords(\"Computers and the Internet\"), textcoords=\"offset points\", xytext=(-1,5))\n",
      "plt.annotate(\"Politics\", coords(\"Politics and Government\"), textcoords=\"offset points\", xytext=(-38,-6))\n",
      "plt.annotate(\"Travel\", coords(\"Travel and Vacations\"), textcoords=\"offset points\", xytext=(-32, 4))\n",
      "plt.annotate(\"Art\", coords(\"Art\"), textcoords=\"offset points\", xytext=(-18,0))\n",
      "plt.show()\n",
      "207/549:\n",
      "def coords(label):\n",
      "    i = labels.index(label)\n",
      "    return (heshes[i], wbiases[i])\n",
      "plt.plot(heshes, wbiases, \"o\")\n",
      "plt.title(\"Ratio of 'she' to 'he' vs Absolute WEAT Bias on Professions\")\n",
      "plt.annotate(\"Automobiles\", coords(\"Automobiles\"), textcoords=\"offset points\", xytext=(-64,1))\n",
      "plt.annotate(\"Weddings\", coords(\"Weddings and Engagements\"), textcoords=\"offset points\", xytext=(-35,5))\n",
      "plt.annotate(\"Baseball\", coords(\"Baseball\"), textcoords=\"offset points\", xytext=(5,-2))\n",
      "plt.annotate(\"Football\", coords(\"Football\"), textcoords=\"offset points\", xytext=(2,5))\n",
      "plt.annotate(\"Basketball\", coords(\"Basketball\"), textcoords=\"offset points\", xytext=(5,-2))\n",
      "plt.annotate(\"Children\", coords(\"Children and Youth\"), textcoords=\"offset points\", xytext=(5,0))\n",
      "plt.annotate(\"Education\", coords(\"Education and Schools\"), textcoords=\"offset points\", xytext=(5,0))\n",
      "plt.annotate(\"Computers\", coords(\"Computers and the Internet\"), textcoords=\"offset points\", xytext=(-1,5))\n",
      "plt.annotate(\"Politics\", coords(\"Politics and Government\"), textcoords=\"offset points\", xytext=(-38,-6))\n",
      "plt.annotate(\"Travel\", coords(\"Travel and Vacations\"), textcoords=\"offset points\", xytext=(-32, 4))\n",
      "plt.annotate(\"Art\", coords(\"Art\"), textcoords=\"offset points\", xytext=(-18,0))\n",
      "plt.show()\n",
      "207/550:\n",
      "def coords(label):\n",
      "    i = labels.index(label)\n",
      "    return (heshes[i], wbiases[i])\n",
      "plt.plot(heshes, wbiases, \"o\")\n",
      "plt.title(\"Ratio of 'she' to 'he' vs Absolute WEAT Bias on Professions\")\n",
      "plt.annotate(\"Automobiles\", coords(\"Automobiles\"), textcoords=\"offset points\", xytext=(-64,2))\n",
      "plt.annotate(\"Weddings\", coords(\"Weddings and Engagements\"), textcoords=\"offset points\", xytext=(-35,5))\n",
      "plt.annotate(\"Baseball\", coords(\"Baseball\"), textcoords=\"offset points\", xytext=(5,-2))\n",
      "plt.annotate(\"Football\", coords(\"Football\"), textcoords=\"offset points\", xytext=(2,5))\n",
      "plt.annotate(\"Basketball\", coords(\"Basketball\"), textcoords=\"offset points\", xytext=(5,-2))\n",
      "plt.annotate(\"Children\", coords(\"Children and Youth\"), textcoords=\"offset points\", xytext=(5,0))\n",
      "plt.annotate(\"Education\", coords(\"Education and Schools\"), textcoords=\"offset points\", xytext=(5,0))\n",
      "plt.annotate(\"Computers\", coords(\"Computers and the Internet\"), textcoords=\"offset points\", xytext=(-1,5))\n",
      "plt.annotate(\"Politics\", coords(\"Politics and Government\"), textcoords=\"offset points\", xytext=(-38,-6))\n",
      "plt.annotate(\"Travel\", coords(\"Travel and Vacations\"), textcoords=\"offset points\", xytext=(-32, 4))\n",
      "plt.annotate(\"Art\", coords(\"Art\"), textcoords=\"offset points\", xytext=(-18,0))\n",
      "plt.show()\n",
      "207/551:\n",
      "def coords(label):\n",
      "    i = labels.index(label)\n",
      "    return (heshes[i], wbiases[i])\n",
      "plt.plot(heshes, wbiases, \"o\")\n",
      "plt.title(\"Ratio of 'she' to 'he' vs Absolute WEAT Bias on Professions\")\n",
      "plt.annotate(\"Automobiles\", coords(\"Automobiles\"), textcoords=\"offset points\", xytext=(-62,4))\n",
      "plt.annotate(\"Weddings\", coords(\"Weddings and Engagements\"), textcoords=\"offset points\", xytext=(-35,5))\n",
      "plt.annotate(\"Baseball\", coords(\"Baseball\"), textcoords=\"offset points\", xytext=(5,-2))\n",
      "plt.annotate(\"Football\", coords(\"Football\"), textcoords=\"offset points\", xytext=(2,5))\n",
      "plt.annotate(\"Basketball\", coords(\"Basketball\"), textcoords=\"offset points\", xytext=(5,-2))\n",
      "plt.annotate(\"Children\", coords(\"Children and Youth\"), textcoords=\"offset points\", xytext=(5,0))\n",
      "plt.annotate(\"Education\", coords(\"Education and Schools\"), textcoords=\"offset points\", xytext=(5,0))\n",
      "plt.annotate(\"Computers\", coords(\"Computers and the Internet\"), textcoords=\"offset points\", xytext=(-1,5))\n",
      "plt.annotate(\"Politics\", coords(\"Politics and Government\"), textcoords=\"offset points\", xytext=(-38,-6))\n",
      "plt.annotate(\"Travel\", coords(\"Travel and Vacations\"), textcoords=\"offset points\", xytext=(-32, 4))\n",
      "plt.annotate(\"Art\", coords(\"Art\"), textcoords=\"offset points\", xytext=(-18,0))\n",
      "plt.show()\n",
      "207/552:\n",
      "plt.plot([i*200 for i in range(1,10)] + [i*500 for i in range(3,21)], bbiasshort + bookbias[2:], label=\"books\")\n",
      "plt.plot([i*200 for i in range(1,10)] + [i*500 for i in range(3,21)], pbiasshort + pbias[1:], label=\"politics\")\n",
      "plt.legend()\n",
      "plt.title(\"Projection Bias on Professions vs Number of Articles \\n in Training Set (200-10000 articles)\")\n",
      "plt.show()\n",
      "207/553: 1+1\n",
      "207/554:\n",
      "ms = []\n",
      "for l in lists:\n",
      "    m = getw2v(sample(l, 2000))\n",
      "    heshestats(m, 'he', 'she')\n",
      "    ms.append(m)\n",
      "207/555:\n",
      "ms = []\n",
      "for l in lists:\n",
      "    m = getw2v(sample(l, 2000))\n",
      "    heshestats(m, 'he', 'she')\n",
      "    ms.append(m)\n",
      "207/556:\n",
      "ms = []\n",
      "i = 0\n",
      "for l in lists:\n",
      "    m = getw2v(sample(l, 2000))\n",
      "    print(labels[i], heshestats(m, 'he', 'she'))\n",
      "    i += 1\n",
      "    ms.append(m)\n",
      "207/557: from scipy.stats import linregress\n",
      "207/558:\n",
      "# slope, intercept, r_value, p_value, std_err\n",
      "stats.linregress(heshes,wbiases)\n",
      "207/559:\n",
      "# slope, intercept, r_value, p_value, std_err\n",
      "stats.linregress(heshes,biases)\n",
      "207/560:\n",
      "wplistshort = []\n",
      "for i in range(1, 10):\n",
      "    wplistshort.append(sample(politics, i*200))\n",
      "wpmodelsshort = [getw2v(p) for p in wplistshort]\n",
      "wpbiasshort = [bias(getg(m), m, m.wv.vocab, proflist) for m in wpmodelsshort]\n",
      "207/561:\n",
      "wblistshort = []\n",
      "for i in range(1, 10):\n",
      "    wblistshort.append(sample(books, i*200))\n",
      "wbmodelsshort = [getw2v(b) for b in wblistshort]\n",
      "wbbiasshort = [bias(getg(m), m, m.wv.vocab, proflist) for m in wbmodelsshort]\n",
      "207/562: wpbiasshort = [wbias(m) for m in wpmodelsshort]\n",
      "207/563: wbbiasshort = [wbias(m) for m in wbmodelsshort]\n",
      "207/564:\n",
      "plt.plot([i*200 for i in range(1,10)], wbiasshort, label=\"books\")\n",
      "# plt.plot([i*200 for i in range(1,10)], wbiasshort, label=\"politics\")\n",
      "plt.legend()\n",
      "plt.title(\"Direct Bias on Professions vs Number of Articles \\n in Training Set (200-1800 articles)\")\n",
      "plt.show()\n",
      "207/565: wbbiasshort = [wbias(m) for m in wbmodelsshort]\n",
      "207/566:\n",
      "plt.plot([i*200 for i in range(1,10)], wbbiasshort, label=\"books\")\n",
      "# plt.plot([i*200 for i in range(1,10)], wbiasshort, label=\"politics\")\n",
      "plt.legend()\n",
      "plt.title(\"Direct Bias on Professions vs Number of Articles \\n in Training Set (200-1800 articles)\")\n",
      "plt.show()\n",
      "207/567:\n",
      "stereotypes_m = ['president', 'dean', \\\n",
      "                 'director', 'officer', 'manager']\n",
      "stereotypes_f = ['teacher', 'author', 'secretary', 'writer', \\\n",
      "                 'singer', 'editor']\n",
      "male = [\"male\", \"man\", \"boy\", \"boys\", \"brother\", \"he\", \"him\", \"son\", \"father\"]\n",
      "female = [\"female\", \"woman\", \"girl\", \"girls\", \"sister\", \"she\", \"her\", \\\n",
      "          \"daughter\", \"mother\"]\n",
      "def cos_sim(m, x, y):\n",
      "    wv1 = m.wv.get_vector(x)\n",
      "    wv2 = m.wv.get_vector(y)\n",
      "    sim = np.dot(wv1, wv2)/(np.linalg.norm(wv1)*np.linalg.norm(wv2))\n",
      "    return sim\n",
      "def s_word(m, w, A, B):\n",
      "    return np.mean([cos_sim(m, w, a) for a in A]) - np.mean([cos_sim(m, w, b) for b in B])\n",
      "def effect_size(m, X, Y, A, B):\n",
      "    num = np.mean([s_word(m, x, A, B) for x in X]) - np.mean([s_word(m, y, A, B) for y in Y])\n",
      "    return num/np.std([s_word(m, x, A, B) for x in X + Y])\n",
      "def wbias(m):\n",
      "    return abs(effect_size(m, male, female, stereotypes_m, stereotypes_f))\n",
      "207/568: wpbiasshort = [wbias(m) for m in wpmodelsshort]\n",
      "207/569:\n",
      "stereotypes_m = ['president', 'dean', \\\n",
      "                 'director', 'officer', 'manager']\n",
      "stereotypes_f = ['teacher', 'author', 'secretary', 'writer', \\\n",
      "                 'editor']\n",
      "male = [\"male\", \"man\", \"boy\", \"boys\", \"brother\", \"he\", \"him\", \"son\", \"father\"]\n",
      "female = [\"female\", \"woman\", \"girl\", \"girls\", \"sister\", \"she\", \"her\", \\\n",
      "          \"daughter\", \"mother\"]\n",
      "def cos_sim(m, x, y):\n",
      "    wv1 = m.wv.get_vector(x)\n",
      "    wv2 = m.wv.get_vector(y)\n",
      "    sim = np.dot(wv1, wv2)/(np.linalg.norm(wv1)*np.linalg.norm(wv2))\n",
      "    return sim\n",
      "def s_word(m, w, A, B):\n",
      "    return np.mean([cos_sim(m, w, a) for a in A]) - np.mean([cos_sim(m, w, b) for b in B])\n",
      "def effect_size(m, X, Y, A, B):\n",
      "    num = np.mean([s_word(m, x, A, B) for x in X]) - np.mean([s_word(m, y, A, B) for y in Y])\n",
      "    return num/np.std([s_word(m, x, A, B) for x in X + Y])\n",
      "def wbias(m):\n",
      "    return abs(effect_size(m, male, female, stereotypes_m, stereotypes_f))\n",
      "207/570: wpbiasshort = [wbias(m) for m in wpmodelsshort]\n",
      "207/571:\n",
      "stereotypes_m = ['president', 'dean', \\\n",
      "                 'director', 'officer', 'manager']\n",
      "stereotypes_f = ['teacher', 'author', 'secretary', 'writer', \\\n",
      "                 'editor']\n",
      "male = [\"male\", \"man\", \"boy\", \"brother\", \"he\", \"him\", \"son\", \"father\"]\n",
      "female = [\"female\", \"woman\", \"girl\", \"sister\", \"she\", \"her\", \\\n",
      "          \"daughter\", \"mother\"]\n",
      "def cos_sim(m, x, y):\n",
      "    wv1 = m.wv.get_vector(x)\n",
      "    wv2 = m.wv.get_vector(y)\n",
      "    sim = np.dot(wv1, wv2)/(np.linalg.norm(wv1)*np.linalg.norm(wv2))\n",
      "    return sim\n",
      "def s_word(m, w, A, B):\n",
      "    return np.mean([cos_sim(m, w, a) for a in A]) - np.mean([cos_sim(m, w, b) for b in B])\n",
      "def effect_size(m, X, Y, A, B):\n",
      "    num = np.mean([s_word(m, x, A, B) for x in X]) - np.mean([s_word(m, y, A, B) for y in Y])\n",
      "    return num/np.std([s_word(m, x, A, B) for x in X + Y])\n",
      "def wbias(m):\n",
      "    return abs(effect_size(m, male, female, stereotypes_m, stereotypes_f))\n",
      "207/572: wpbiasshort = [wbias(m) for m in wpmodelsshort]\n",
      "207/573:\n",
      "stereotypes_m = ['president', 'dean', \\\n",
      "                 'director', 'officer', 'manager']\n",
      "stereotypes_f = ['teacher', 'author', 'secretary', 'writer', \\\n",
      "                 'editor']\n",
      "male = [\"male\", \"man\", \"brother\", \"he\", \"him\", \"son\", \"father\"]\n",
      "female = [\"female\", \"woman\", \"sister\", \"she\", \"her\", \\\n",
      "          \"daughter\", \"mother\"]\n",
      "def cos_sim(m, x, y):\n",
      "    wv1 = m.wv.get_vector(x)\n",
      "    wv2 = m.wv.get_vector(y)\n",
      "    sim = np.dot(wv1, wv2)/(np.linalg.norm(wv1)*np.linalg.norm(wv2))\n",
      "    return sim\n",
      "def s_word(m, w, A, B):\n",
      "    return np.mean([cos_sim(m, w, a) for a in A]) - np.mean([cos_sim(m, w, b) for b in B])\n",
      "def effect_size(m, X, Y, A, B):\n",
      "    num = np.mean([s_word(m, x, A, B) for x in X]) - np.mean([s_word(m, y, A, B) for y in Y])\n",
      "    return num/np.std([s_word(m, x, A, B) for x in X + Y])\n",
      "def wbias(m):\n",
      "    return abs(effect_size(m, male, female, stereotypes_m, stereotypes_f))\n",
      "207/574: wpbiasshort = [wbias(m) for m in wpmodelsshort]\n",
      "207/575:\n",
      "plt.plot([i*200 for i in range(1,10)], wbbiasshort, label=\"books\")\n",
      "# plt.plot([i*200 for i in range(1,10)], wbiasshort, label=\"politics\")\n",
      "plt.legend()\n",
      "plt.title(\"Direct Bias on Professions vs Number of Articles \\n in Training Set (200-1800 articles)\")\n",
      "plt.show()\n",
      "207/576:\n",
      "plt.plot([i*200 for i in range(1,10)], wbbiasshort, label=\"books\")\n",
      "plt.plot([i*200 for i in range(1,10)], wpbiasshort, label=\"politics\")\n",
      "plt.legend()\n",
      "plt.title(\"Direct Bias on Professions vs Number of Articles \\n in Training Set (200-1800 articles)\")\n",
      "plt.show()\n",
      "207/577:\n",
      "plt.plot([i*200 for i in range(1,10)], wbbiasshort, label=\"books\")\n",
      "plt.plot([i*200 for i in range(1,10)], wpbiasshort, label=\"politics\")\n",
      "plt.legend()\n",
      "plt.title(\"WEAT Bias on Professions vs Number of Articles \\n in Training Set (200-1800 articles)\")\n",
      "plt.show()\n",
      "207/578: wbbiasshort = [wbias(m) for m in wbmodelsshort]\n",
      "207/579:\n",
      "plt.plot([i*200 for i in range(1,10)], wbbiasshort, label=\"books\")\n",
      "plt.plot([i*200 for i in range(1,10)], wpbiasshort, label=\"politics\")\n",
      "plt.legend()\n",
      "plt.title(\"WEAT Bias on Professions vs Number of Articles \\n in Training Set (200-1800 articles)\")\n",
      "plt.show()\n",
      "207/580: from sklearn.cluster import KMeans\n",
      "207/581: X = np.array(m.wv.get_vector(a) for a in adjs_all if a in mbooks.wv.vocab)\n",
      "207/582: X.shape\n",
      "207/583: X = np.array([m.wv.get_vector(a) for a in adjs_all if a in mbooks.wv.vocab])\n",
      "207/584: X = np.array([mbooks.wv.get_vector(a) for a in adjs_all if a in mbooks.wv.vocab])\n",
      "207/585: X.shape\n",
      "207/586: kmeans = KMeans(n_clusters=8).fit(X)\n",
      "207/587: kmeans.labels_\n",
      "207/588: for i in range(8):\n",
      "207/589:\n",
      "adjs_b = [a for a in adjs_all if a in mbooks.wv.vocab]\n",
      "X = np.array([mbooks.wv.get_vector(a) for a in adjs_b])\n",
      "207/590: X.shape\n",
      "207/591: kmeans = KMeans(n_clusters=8).fit(X)\n",
      "207/592: kmeans.labels_\n",
      "207/593: {x:[] for x in range(5)}\n",
      "207/594:\n",
      "labels = {x:[] for x in range(8)}\n",
      "for i in range(len(X)):\n",
      "    labels[kmeans.labels_[i]].append(adjs_b[i])\n",
      "207/595:\n",
      "for i in range(8):\n",
      "    print(i)\n",
      "    print(labels[i][:10])\n",
      "207/596: kmeans = KMeans(n_clusters=20).fit(X)\n",
      "207/597:\n",
      "labels = {x:[] for x in range(8)}\n",
      "for i in range(len(X)):\n",
      "    labels[kmeans.labels_[i]].append(adjs_b[i])\n",
      "207/598:\n",
      "labels = {x:[] for x in range(20)}\n",
      "for i in range(len(X)):\n",
      "    labels[kmeans.labels_[i]].append(adjs_b[i])\n",
      "207/599:\n",
      "for i in range(20):\n",
      "    print(i)\n",
      "    print(labels[i][:10])\n",
      "207/600: kmeans = KMeans(n_clusters=50).fit(X)\n",
      "207/601:\n",
      "labels = {x:[] for x in range(20)}\n",
      "for i in range(len(X)):\n",
      "    labels[kmeans.labels_[i]].append(adjs_b[i])\n",
      "207/602:\n",
      "labels = {x:[] for x in range(50)}\n",
      "for i in range(len(X)):\n",
      "    labels[kmeans.labels_[i]].append(adjs_b[i])\n",
      "207/603:\n",
      "for i in range(10):\n",
      "    print(i)\n",
      "    print(labels[i][:10])\n",
      "207/604:\n",
      "def sbias(g, m, adjs):\n",
      "    b = 0\n",
      "    l = list(filter(lambda x: x in m.wv.vocab, adjs))\n",
      "    p, n, np, nn = 0, 0, 0, 0\n",
      "    for x in l:\n",
      "        proj = gproj(g, m, x)\n",
      "        b += abs(proj)\n",
      "        try:\n",
      "            s = swn.senti_synset(x + \".a.01\")\n",
      "            score = s.pos_score() - s.neg_score()\n",
      "            if proj < 0 and score != 0:\n",
      "                print(\"neg\", proj, score, x)\n",
      "                n += score\n",
      "                nn += 1\n",
      "            if proj > 0 and score != 0:\n",
      "                print(\"pos\", proj, score, x)\n",
      "                p += score\n",
      "                np += 1\n",
      "        except:\n",
      "            continue\n",
      "    return b/len(l), p, n, np, nn# p/np, n/nn\n",
      "207/605:\n",
      "for i in range(50):\n",
      "    print(getg(mbooks), mbooks, sbias(adjs_b))\n",
      "207/606:\n",
      "for i in range(50):\n",
      "    print(sbias(getg(mbooks), mbooks, adjs_b))\n",
      "207/607:\n",
      "def sbias(g, m, adjs):\n",
      "    b = 0\n",
      "    l = list(filter(lambda x: x in m.wv.vocab, adjs))\n",
      "    p, n, np, nn = 0, 0, 0, 0\n",
      "    for x in l:\n",
      "        proj = gproj(g, m, x)\n",
      "        b += abs(proj)\n",
      "        try:\n",
      "            s = swn.senti_synset(x + \".a.01\")\n",
      "            score = s.pos_score() - s.neg_score()\n",
      "            if proj < 0 and score != 0:\n",
      "#                 print(\"neg\", proj, score, x)\n",
      "                n += score\n",
      "                nn += 1\n",
      "            if proj > 0 and score != 0:\n",
      "#                 print(\"pos\", proj, score, x)\n",
      "                p += score\n",
      "                np += 1\n",
      "        except:\n",
      "            continue\n",
      "    return b/len(l), p, n, np, nn# p/np, n/nn\n",
      "207/608:\n",
      "for i in range(50):\n",
      "    print(sbias(getg(mbooks), mbooks, adjs_b))\n",
      "207/609:\n",
      "for i in range(50):\n",
      "    print(sbias(getg(mbooks), mbooks, labels[i]))\n",
      "207/610: gproj(getg(mbooks), mbooks, 'he'), gproj(getg(mbooks), mbooks, 'she')\n",
      "207/611:\n",
      "for i in range(50):\n",
      "    print(i, sbias(getg(mbooks), mbooks, labels[i]))\n",
      "207/612:\n",
      "for i in range(50):\n",
      "    print(i)\n",
      "    print(labels[i][:10])\n",
      "207/613: clusters = 20\n",
      "207/614: kmeans = KMeans(n_clusters=clusters).fit(X)\n",
      "207/615:\n",
      "labels = {x:[] for x in range(clusters)}\n",
      "for i in range(len(X)):\n",
      "    labels[kmeans.labels_[i]].append(adjs_b[i])\n",
      "207/616: clusters = 25\n",
      "207/617: kmeans = KMeans(n_clusters=clusters).fit(X)\n",
      "207/618:\n",
      "labels = {x:[] for x in range(clusters)}\n",
      "for i in range(len(X)):\n",
      "    labels[kmeans.labels_[i]].append(adjs_b[i])\n",
      "207/619:\n",
      "def getscore(x):\n",
      "    s = swn.senti_synset(x + \".a.01\")\n",
      "    return s.pos_score() - s.neg_score()\n",
      "207/620:\n",
      "labels_split = {x:[] for x in range(2*clusters)}\n",
      "for i in range(clusters):\n",
      "    for w in range(labels[i]):\n",
      "        if getscore(w) >= 0:\n",
      "            labels_split[i].append(w)\n",
      "        else:\n",
      "            labels_split[i + clusters].append(w)\n",
      "207/621:\n",
      "labels_split = {x:[] for x in range(2*clusters)}\n",
      "for i in range(clusters):\n",
      "    for w in range(len(labels[i])):\n",
      "        if getscore(w) >= 0:\n",
      "            labels_split[i].append(w)\n",
      "        else:\n",
      "            labels_split[i + clusters].append(w)\n",
      "207/622:\n",
      "labels_split = {x:[] for x in range(2*clusters)}\n",
      "for i in range(clusters):\n",
      "    for w in labels[i]:\n",
      "        if getscore(w) >= 0:\n",
      "            labels_split[i].append(w)\n",
      "        else:\n",
      "            labels_split[i + clusters].append(w)\n",
      "207/623:\n",
      "adjs_b = []\n",
      "for a in adjs_all if a in mbooks.wv.vocab:\n",
      "    try:\n",
      "        getscore(a)\n",
      "        adjs_b.append(a)\n",
      "    except:\n",
      "        continue\n",
      "X = np.array([mbooks.wv.get_vector(a) for a in adjs_b])\n",
      "207/624:\n",
      "adjs_b = []\n",
      "for a in adjs_all:\n",
      "    if a not in mbooks.wv.vocab:\n",
      "        continue\n",
      "    try:\n",
      "        getscore(a)\n",
      "        adjs_b.append(a)\n",
      "    except:\n",
      "        continue\n",
      "X = np.array([mbooks.wv.get_vector(a) for a in adjs_b])\n",
      "207/625: X.shape\n",
      "207/626: kmeans = KMeans(n_clusters=clusters).fit(X)\n",
      "207/627:\n",
      "labels = {x:[] for x in range(clusters)}\n",
      "for i in range(len(X)):\n",
      "    labels[kmeans.labels_[i]].append(adjs_b[i])\n",
      "207/628:\n",
      "labels_split = {x:[] for x in range(2*clusters)}\n",
      "for i in range(clusters):\n",
      "    for w in labels[i]:\n",
      "        if getscore(w) >= 0:\n",
      "            labels_split[i].append(w)\n",
      "        else:\n",
      "            labels_split[i + clusters].append(w)\n",
      "207/629:\n",
      "for i in range(clusters*2):\n",
      "    print(i)\n",
      "    print(labels[i][:10])\n",
      "207/630:\n",
      "for i in range(clusters*2):\n",
      "    print(i)\n",
      "    print(labels_split[i][:10])\n",
      "207/631: clusters = 20\n",
      "207/632: kmeans = KMeans(n_clusters=clusters).fit(X)\n",
      "207/633:\n",
      "labels = {x:[] for x in range(clusters)}\n",
      "for i in range(len(X)):\n",
      "    labels[kmeans.labels_[i]].append(adjs_b[i])\n",
      "207/634:\n",
      "labels_split = {x:[] for x in range(2*clusters)}\n",
      "for i in range(clusters):\n",
      "    for w in labels[i]:\n",
      "        if getscore(w) >= 0:\n",
      "            labels_split[i].append(w)\n",
      "        else:\n",
      "            labels_split[i + clusters].append(w)\n",
      "207/635:\n",
      "for i in range(clusters*2):\n",
      "    print(i)\n",
      "    print(labels_split[i][:10])\n",
      "207/636:\n",
      "for i in range(50):\n",
      "    print(i, sbias(getg(mbooks), mbooks, labels[i]))\n",
      "207/637:\n",
      "for i in range(clusters*2):\n",
      "    print(i, sbias(getg(mbooks), mbooks, labels[i]))\n",
      "207/638:\n",
      "for i in range(clusters*2):\n",
      "    print(i, sbias(getg(mbooks), mbooks, labels_split[i]))\n",
      "207/639:\n",
      "for i in range(clusters*2):\n",
      "    if labels_split[i]:\n",
      "        print(i, sbias(getg(mbooks), mbooks, labels_split[i]))\n",
      "207/640: mbooks.wv.get_vector('rich') - mbooks.wv.get_vector('poor')\n",
      "207/641: rp = mbooks.wv.get_vector('rich') - mbooks.wv.get_vector('poor')\n",
      "207/642: cos_sim(rp, mbooks.wv.get_vector('smart'))\n",
      "207/643:\n",
      "t1 = mbooks.wv.get_vector('smart')\n",
      "t2 = mbooks.wv.get_vector('lazy')\n",
      "p.dot(rp, t1)/(np.linalg.norm(rp)*np.linalg.norm(t1))\n",
      "p.dot(rp, t2)/(np.linalg.norm(rp)*np.linalg.norm(t2))\n",
      "207/644:\n",
      "t1 = mbooks.wv.get_vector('smart')\n",
      "t2 = mbooks.wv.get_vector('lazy')\n",
      "np.dot(rp, t1)/(np.linalg.norm(rp)*np.linalg.norm(t1)), np.dot(rp, t2)/(np.linalg.norm(rp)*np.linalg.norm(t2))\n",
      "207/645: rp = mpol.wv.get_vector('rich') - mpol.wv.get_vector('poor')\n",
      "207/646:\n",
      "t1 = mpol.wv.get_vector('smart')\n",
      "t2 = mpol.wv.get_vector('lazy')\n",
      "np.dot(rp, t1)/(np.linalg.norm(rp)*np.linalg.norm(t1)), np.dot(rp, t2)/(np.linalg.norm(rp)*np.linalg.norm(t2))\n",
      "207/647:\n",
      "t1 = mpol.wv.get_vector('smart')\n",
      "t2 = mpol.wv.get_vector('business')\n",
      "np.dot(rp, t1)/(np.linalg.norm(rp)*np.linalg.norm(t1)), np.dot(rp, t2)/(np.linalg.norm(rp)*np.linalg.norm(t2))\n",
      "207/648:\n",
      "t1 = mpol.wv.get_vector('money')\n",
      "t2 = mpol.wv.get_vector('business')\n",
      "np.dot(rp, t1)/(np.linalg.norm(rp)*np.linalg.norm(t1)), np.dot(rp, t2)/(np.linalg.norm(rp)*np.linalg.norm(t2))\n",
      "207/649: rp = mpol.wv.get_vector('wealthy') - mpol.wv.get_vector('poor')\n",
      "207/650:\n",
      "t1 = mpol.wv.get_vector('money')\n",
      "t2 = mpol.wv.get_vector('business')\n",
      "np.dot(rp, t1)/(np.linalg.norm(rp)*np.linalg.norm(t1)), np.dot(rp, t2)/(np.linalg.norm(rp)*np.linalg.norm(t2))\n",
      "207/651: rp = mpol.wv.get_vector('old') - mpol.wv.get_vector('young')\n",
      "207/652:\n",
      "t1 = mpol.wv.get_vector('competent')\n",
      "t2 = mpol.wv.get_vector('weak')\n",
      "np.dot(rp, t1)/(np.linalg.norm(rp)*np.linalg.norm(t1)), np.dot(rp, t2)/(np.linalg.norm(rp)*np.linalg.norm(t2))\n",
      "207/653:\n",
      "t1 = mpol.wv.get_vector('strong')\n",
      "t2 = mpol.wv.get_vector('ambitious')\n",
      "np.dot(rp, t1)/(np.linalg.norm(rp)*np.linalg.norm(t1)), np.dot(rp, t2)/(np.linalg.norm(rp)*np.linalg.norm(t2))\n",
      "207/654:\n",
      "t1 = mpol.wv.get_vector('strong')\n",
      "t2 = mpol.wv.get_vector('powerful')\n",
      "np.dot(rp, t1)/(np.linalg.norm(rp)*np.linalg.norm(t1)), np.dot(rp, t2)/(np.linalg.norm(rp)*np.linalg.norm(t2))\n",
      "207/655:\n",
      "t1 = mpol.wv.get_vector('strong')\n",
      "t2 = mpol.wv.get_vector('smart')\n",
      "np.dot(rp, t1)/(np.linalg.norm(rp)*np.linalg.norm(t1)), np.dot(rp, t2)/(np.linalg.norm(rp)*np.linalg.norm(t2))\n",
      "207/656: # male: ['practical', 'responsible', 'serious', 'strong', 'obvious', 'careful', 'content', 'positive', 'quick', 'thinking']\n",
      "207/657:\n",
      "# male: ['practical', 'responsible', 'serious', 'strong', 'obvious', 'careful', 'content', 'positive', 'quick', 'thinking']\n",
      "# female: ['clean', 'decent', 'sensitive', 'sexy', 'sweet', 'warm', 'wise', 'busy', 'cute', 'proud']\n",
      "207/658:\n",
      "stereotypes_m = ['president', 'dean', \\\n",
      "                 'director', 'officer', 'manager']\n",
      "stereotypes_f = ['teacher', 'author', 'secretary', 'writer', \\\n",
      "                 'editor']\n",
      "male = [\"male\", \"man\", \"brother\", \"he\", \"him\", \"son\", \"father\"]\n",
      "female = [\"female\", \"woman\", \"sister\", \"she\", \"her\", \\\n",
      "          \"daughter\", \"mother\"]\n",
      "def cos_sim(m, x, y):\n",
      "    wv1 = m.wv.get_vector(x)\n",
      "    wv2 = m.wv.get_vector(y)\n",
      "    sim = np.dot(wv1, wv2)/(np.linalg.norm(wv1)*np.linalg.norm(wv2))\n",
      "    return sim\n",
      "def s_word(m, w, A, B):\n",
      "    return np.mean([cos_sim(m, w, a) for a in A]) - np.mean([cos_sim(m, w, b) for b in B])\n",
      "def effect_size(m, X, Y, A, B):\n",
      "    num = np.mean([s_word(m, x, A, B) for x in X]) - np.mean([s_word(m, y, A, B) for y in Y])\n",
      "    return num/np.std([s_word(m, x, A, B) for x in X + Y])\n",
      "def wbias(m, l):\n",
      "    return abs(effect_size(m, male, female, stereotypes_m[:l], stereotypes_f[:l]))\n",
      "207/659:\n",
      "stereotypes_m = ['president', 'dean', \\\n",
      "                 'director', 'officer', 'manager', 'captain', 'sergeant', 'principal', 'commissioner', 'coach']\n",
      "stereotypes_f = ['teacher', 'author', 'secretary', 'writer', \\\n",
      "                 'editor', 'maid', 'nurse', 'dancer', 'housekeeper', 'receptionist', 'photographer']\n",
      "male = [\"male\", \"man\", \"brother\", \"he\", \"him\", \"son\", \"father\"]\n",
      "female = [\"female\", \"woman\", \"sister\", \"she\", \"her\", \\\n",
      "          \"daughter\", \"mother\"]\n",
      "def cos_sim(m, x, y):\n",
      "    wv1 = m.wv.get_vector(x)\n",
      "    wv2 = m.wv.get_vector(y)\n",
      "    sim = np.dot(wv1, wv2)/(np.linalg.norm(wv1)*np.linalg.norm(wv2))\n",
      "    return sim\n",
      "def s_word(m, w, A, B):\n",
      "    return np.mean([cos_sim(m, w, a) for a in A]) - np.mean([cos_sim(m, w, b) for b in B])\n",
      "def effect_size(m, X, Y, A, B):\n",
      "    num = np.mean([s_word(m, x, A, B) for x in X]) - np.mean([s_word(m, y, A, B) for y in Y])\n",
      "    return num/np.std([s_word(m, x, A, B) for x in X + Y])\n",
      "def wbias(m, l):\n",
      "    return abs(effect_size(m, male, female, stereotypes_m[:l], stereotypes_f[:l]))\n",
      "207/660:\n",
      "stereotypes_m = ['president', 'dean', \\\n",
      "                 'director', 'officer', 'manager', 'captain', 'sergeant', 'principal', 'commissioner', 'coach']\n",
      "stereotypes_f = ['teacher', 'author', 'secretary', 'writer', \\\n",
      "                 'editor', 'maid', 'nurse', 'dancer', 'housekeeper', 'receptionist', 'photographer']\n",
      "male = [\"male\", \"man\", \"brother\", \"he\", \"him\", \"son\", \"father\"]\n",
      "female = [\"female\", \"woman\", \"sister\", \"she\", \"her\", \\\n",
      "          \"daughter\", \"mother\"]\n",
      "def cos_sim(m, x, y):\n",
      "    wv1 = m.wv.get_vector(x)\n",
      "    wv2 = m.wv.get_vector(y)\n",
      "    sim = np.dot(wv1, wv2)/(np.linalg.norm(wv1)*np.linalg.norm(wv2))\n",
      "    return sim\n",
      "def s_word(m, w, A, B):\n",
      "    return np.mean([cos_sim(m, w, a) for a in A]) - np.mean([cos_sim(m, w, b) for b in B])\n",
      "def effect_size(m, X, Y, A, B):\n",
      "    num = np.mean([s_word(m, x, A, B) for x in X]) - np.mean([s_word(m, y, A, B) for y in Y])\n",
      "    return num/np.std([s_word(m, x, A, B) for x in X + Y])\n",
      "def wbias(m, l):\n",
      "    return abs(effect_size(m, male, female, stereotypes_m[:l], stereotypes_f[:l]))\n",
      "207/661: wbbiases = [wbias(mbooks, l) for l in range(1, 11)]\n",
      "207/662:\n",
      "plt.plot(wbbiases, label=\"books\")\n",
      "# plt.plot([i*500 for i in range(3,21)], pwbias[1:], label=\"politics\")\n",
      "plt.legend()\n",
      "plt.title(\"WEAT Bias on Professions vs Number of Articles \\n in Training Set (1500-10000 articles)\")\n",
      "plt.show()\n",
      "207/663:\n",
      "stereotypes_m = ['president', 'dean', 'official'\\\n",
      "                 'director', 'officer', 'manager', 'captain', 'sergeant', 'principal', 'commissioner', 'coach']\n",
      "stereotypes_f = ['teacher', 'author', 'secretary', 'writer', \\\n",
      "                 'editor', 'maid', 'nurse', 'dancer', 'housekeeper', 'receptionist', 'photographer']\n",
      "male = [\"male\", \"man\", \"brother\", \"he\", \"him\", \"son\", \"father\"]\n",
      "female = [\"female\", \"woman\", \"sister\", \"she\", \"her\", \\\n",
      "          \"daughter\", \"mother\"]\n",
      "def cos_sim(m, x, y):\n",
      "    wv1 = m.wv.get_vector(x)\n",
      "    wv2 = m.wv.get_vector(y)\n",
      "    sim = np.dot(wv1, wv2)/(np.linalg.norm(wv1)*np.linalg.norm(wv2))\n",
      "    return sim\n",
      "def s_word(m, w, A, B):\n",
      "    return np.mean([cos_sim(m, w, a) for a in A]) - np.mean([cos_sim(m, w, b) for b in B])\n",
      "def effect_size(m, X, Y, A, B):\n",
      "    num = np.mean([s_word(m, x, A, B) for x in X]) - np.mean([s_word(m, y, A, B) for y in Y])\n",
      "    return num/np.std([s_word(m, x, A, B) for x in X + Y])\n",
      "def wbias(m, l):\n",
      "    return abs(effect_size(m, male, female, stereotypes_m[:l], stereotypes_f[:l]))\n",
      "207/664: wbbiases = [wbias(mbooks, l) for l in range(1, 11)]\n",
      "207/665:\n",
      "stereotypes_m = ['president', 'dean', 'official', \\\n",
      "                 'director', 'officer', 'manager', 'captain', 'sergeant', 'principal', 'commissioner', 'coach']\n",
      "stereotypes_f = ['teacher', 'author', 'secretary', 'writer', \\\n",
      "                 'editor', 'maid', 'nurse', 'dancer', 'housekeeper', 'receptionist', 'photographer']\n",
      "male = [\"male\", \"man\", \"brother\", \"he\", \"him\", \"son\", \"father\"]\n",
      "female = [\"female\", \"woman\", \"sister\", \"she\", \"her\", \\\n",
      "          \"daughter\", \"mother\"]\n",
      "def cos_sim(m, x, y):\n",
      "    wv1 = m.wv.get_vector(x)\n",
      "    wv2 = m.wv.get_vector(y)\n",
      "    sim = np.dot(wv1, wv2)/(np.linalg.norm(wv1)*np.linalg.norm(wv2))\n",
      "    return sim\n",
      "def s_word(m, w, A, B):\n",
      "    return np.mean([cos_sim(m, w, a) for a in A]) - np.mean([cos_sim(m, w, b) for b in B])\n",
      "def effect_size(m, X, Y, A, B):\n",
      "    num = np.mean([s_word(m, x, A, B) for x in X]) - np.mean([s_word(m, y, A, B) for y in Y])\n",
      "    return num/np.std([s_word(m, x, A, B) for x in X + Y])\n",
      "def wbias(m, l):\n",
      "    return abs(effect_size(m, male, female, stereotypes_m[:l], stereotypes_f[:l]))\n",
      "207/666: wbbiases = [wbias(mbooks, l) for l in range(1, 11)]\n",
      "207/667:\n",
      "plt.plot(wbbiases, label=\"books\")\n",
      "# plt.plot([i*500 for i in range(3,21)], pwbias[1:], label=\"politics\")\n",
      "plt.legend()\n",
      "plt.title(\"WEAT Bias on Professions vs Number of Articles \\n in Training Set (1500-10000 articles)\")\n",
      "plt.show()\n",
      "207/668:\n",
      "plt.plot([wbias(mbooks, l) for l in range(1, 11)], label=\"books 1\")\n",
      "plt.plot([wbias(mbooks, l) for l in range(1, 11)], label=\"books 2\")\n",
      "plt.legend()\n",
      "plt.title(\"WEAT Bias on Professions vs Number of Articles \\n in Training Set (1500-10000 articles)\")\n",
      "plt.show()\n",
      "207/669:\n",
      "plt.plot([wbias(mbooks, l) for l in range(1, 11)], label=\"books 1\")\n",
      "plt.plot([wbias(mbooks, l) for l in range(1, 11)][:-1], label=\"books 2\")\n",
      "plt.legend()\n",
      "plt.title(\"WEAT Bias on Professions vs Number of Articles \\n in Training Set (1500-10000 articles)\")\n",
      "plt.show()\n",
      "207/670: [1,2,3][:-1]\n",
      "207/671: [1,2,3][-1:]\n",
      "207/672: [1,2,3][::-1]\n",
      "207/673:\n",
      "plt.plot([wbias(mbooks, l) for l in range(1, 11)], label=\"books 1\")\n",
      "plt.plot([wbias(mbooks, l) for l in range(1, 11)][::-1], label=\"books 2\")\n",
      "plt.legend()\n",
      "plt.title(\"WEAT Bias on Professions vs Number of Articles \\n in Training Set (1500-10000 articles)\")\n",
      "plt.show()\n",
      "207/674:\n",
      "stereotypes_m = ['president', 'dean', 'official', \\\n",
      "                 'director', 'officer', 'manager', 'captain', 'sergeant', 'principal', 'commissioner', 'coach']\n",
      "stereotypes_f = ['teacher', 'author', 'secretary', 'writer', \\\n",
      "                 'editor', 'maid', 'nurse', 'dancer', 'housekeeper', 'receptionist', 'photographer']\n",
      "male = [\"male\", \"man\", \"brother\", \"he\", \"him\", \"son\", \"father\"]\n",
      "female = [\"female\", \"woman\", \"sister\", \"she\", \"her\", \\\n",
      "          \"daughter\", \"mother\"]\n",
      "def cos_sim(m, x, y):\n",
      "    wv1 = m.wv.get_vector(x)\n",
      "    wv2 = m.wv.get_vector(y)\n",
      "    sim = np.dot(wv1, wv2)/(np.linalg.norm(wv1)*np.linalg.norm(wv2))\n",
      "    return sim\n",
      "def s_word(m, w, A, B):\n",
      "    return np.mean([cos_sim(m, w, a) for a in A]) - np.mean([cos_sim(m, w, b) for b in B])\n",
      "def effect_size(m, X, Y, A, B):\n",
      "    num = np.mean([s_word(m, x, A, B) for x in X]) - np.mean([s_word(m, y, A, B) for y in Y])\n",
      "    return num/np.std([s_word(m, x, A, B) for x in X + Y])\n",
      "def wbias(m, l, shuf):\n",
      "    if shuf:\n",
      "        stereotypes_m.shuffle()\n",
      "        stereotypes_f.shuffle()\n",
      "    return abs(effect_size(m, male, female, stereotypes_m[:l], stereotypes_f[:l]))\n",
      "207/675:\n",
      "plt.plot([wbias(mbooks, l, False) for l in range(1, 11)], label=\"books\")\n",
      "plt.plot([wbias(mbooks, l, True) for l in range(1, 11)], label=\"books, shuffled\")\n",
      "plt.legend()\n",
      "plt.title(\"WEAT Bias on Professions vs Number of Words \\n in Target Sets\")\n",
      "plt.show()\n",
      "207/676:\n",
      "stereotypes_m = ['president', 'dean', 'official', \\\n",
      "                 'director', 'officer', 'manager', 'captain', 'sergeant', 'principal', 'commissioner', 'coach']\n",
      "stereotypes_f = ['teacher', 'author', 'secretary', 'writer', \\\n",
      "                 'editor', 'maid', 'nurse', 'dancer', 'housekeeper', 'receptionist', 'photographer']\n",
      "male = [\"male\", \"man\", \"brother\", \"he\", \"him\", \"son\", \"father\"]\n",
      "female = [\"female\", \"woman\", \"sister\", \"she\", \"her\", \\\n",
      "          \"daughter\", \"mother\"]\n",
      "def cos_sim(m, x, y):\n",
      "    wv1 = m.wv.get_vector(x)\n",
      "    wv2 = m.wv.get_vector(y)\n",
      "    sim = np.dot(wv1, wv2)/(np.linalg.norm(wv1)*np.linalg.norm(wv2))\n",
      "    return sim\n",
      "def s_word(m, w, A, B):\n",
      "    return np.mean([cos_sim(m, w, a) for a in A]) - np.mean([cos_sim(m, w, b) for b in B])\n",
      "def effect_size(m, X, Y, A, B):\n",
      "    num = np.mean([s_word(m, x, A, B) for x in X]) - np.mean([s_word(m, y, A, B) for y in Y])\n",
      "    return num/np.std([s_word(m, x, A, B) for x in X + Y])\n",
      "def wbias(m, l, shuf):\n",
      "    if shuf:\n",
      "        shuffle(stereotypes_m)\n",
      "        shuffle(stereotypes_f)\n",
      "    return abs(effect_size(m, male, female, stereotypes_m[:l], stereotypes_f[:l]))\n",
      "207/677:\n",
      "plt.plot([wbias(mbooks, l, False) for l in range(1, 11)], label=\"books\")\n",
      "plt.plot([wbias(mbooks, l, True) for l in range(1, 11)], label=\"books, shuffled\")\n",
      "plt.legend()\n",
      "plt.title(\"WEAT Bias on Professions vs Number of Words \\n in Target Sets\")\n",
      "plt.show()\n",
      "207/678:\n",
      "plt.plot([wbias(mbooks, l, False) for l in range(1, 11)], label=\"books\")\n",
      "plt.plot([wbias(mbooks, l, True) for l in range(1, 11)], label=\"books, shuffled\")\n",
      "plt.legend()\n",
      "plt.title(\"WEAT Bias on Professions vs Number of Words \\n in Target Sets\")\n",
      "plt.show()\n",
      "207/679:\n",
      "plt.plot([wbias(mbooks, l, False) for l in range(1, 11)], label=\"books\")\n",
      "plt.plot([wbias(mbooks, l, True) for l in range(1, 11)], label=\"books, shuffled\")\n",
      "plt.legend()\n",
      "plt.title(\"WEAT Bias on Professions vs Number of Words \\n in Target Sets\")\n",
      "plt.show()\n",
      "207/680:\n",
      "plt.plot([wbias(mbooks, l, False) for l in range(1, 11)], label=\"books\")\n",
      "plt.plot([wbias(mbooks, l, True) for l in range(1, 11)], label=\"books, shuffled\")\n",
      "plt.legend()\n",
      "plt.title(\"WEAT Bias on Professions vs Number of Words \\n in Target Sets\")\n",
      "plt.show()\n",
      "207/681:\n",
      "plt.plot([wbias(mbooks, l, False) for l in range(1, 11)], label=\"books\")\n",
      "plt.plot([wbias(mbooks, l, True) for l in range(1, 11)], label=\"books, shuffled\")\n",
      "plt.legend()\n",
      "plt.title(\"WEAT Bias on Professions vs Number of Words \\n in Target Sets\")\n",
      "plt.show()\n",
      "207/682:\n",
      "plt.plot([wbias(mbooks, l, False) for l in range(1, 11)], label=\"books\")\n",
      "plt.plot([wbias(mbooks, l, True) for l in range(1, 11)], label=\"books, shuffled\")\n",
      "plt.plot([wbias(mbooks, l, True) for l in range(1, 11)], label=\"books, shuffled again\")\n",
      "plt.legend()\n",
      "plt.title(\"WEAT Bias on Professions vs Number of Words \\n in Target Sets\")\n",
      "plt.show()\n",
      "207/683:\n",
      "plt.plot([wbias(mbooks, l, False) for l in range(1, 11)], label=\"books\")\n",
      "plt.plot([wbias(mbooks, l, True) for l in range(1, 11)], label=\"books, shuffled\")\n",
      "plt.plot([wbias(mbooks, l, True) for l in range(1, 11)], label=\"books, shuffled again\")\n",
      "plt.legend()\n",
      "plt.title(\"WEAT Bias on Professions vs Number of Words \\n in Target Sets\")\n",
      "plt.show()\n",
      "207/684:\n",
      "stereotypes_m = ['president', 'dean', 'official', \\\n",
      "                 'director', 'officer', 'manager', 'captain', 'sergeant', 'principal', 'commissioner', 'coach']\n",
      "stereotypes_f = ['teacher', 'author', 'secretary', 'writer', \\\n",
      "                 'editor', 'maid', 'nurse', 'dancer', 'housekeeper', 'receptionist', 'photographer']\n",
      "male = [\"male\", \"man\", \"brother\", \"he\", \"him\", \"son\", \"father\", \"mr\", \"john\"]\n",
      "female = [\"female\", \"woman\", \"sister\", \"she\", \"her\", \\\n",
      "          \"daughter\", \"mother\", \"ms\", \"mary\"]\n",
      "def cos_sim(m, x, y):\n",
      "    wv1 = m.wv.get_vector(x)\n",
      "    wv2 = m.wv.get_vector(y)\n",
      "    sim = np.dot(wv1, wv2)/(np.linalg.norm(wv1)*np.linalg.norm(wv2))\n",
      "    return sim\n",
      "def s_word(m, w, A, B):\n",
      "    return np.mean([cos_sim(m, w, a) for a in A]) - np.mean([cos_sim(m, w, b) for b in B])\n",
      "def effect_size(m, X, Y, A, B):\n",
      "    num = np.mean([s_word(m, x, A, B) for x in X]) - np.mean([s_word(m, y, A, B) for y in Y])\n",
      "    return num/np.std([s_word(m, x, A, B) for x in X + Y])\n",
      "def wbias(m, l, shuf, shu):\n",
      "#     if shuf:\n",
      "#         shuffle(stereotypes_m)\n",
      "#         shuffle(stereotypes_f)\n",
      "    if shuf:\n",
      "        shuffle(male)\n",
      "        shuffle(female)\n",
      "    return abs(effect_size(m, male, female, stereotypes_m[:l], stereotypes_f[:l]))\n",
      "207/685:\n",
      "plt.plot([wbias(mbooks, l, False) for l in range(1, 10)], label=\"books\")\n",
      "plt.plot([wbias(mbooks, l, True) for l in range(1, 10)], label=\"books, shuffled\")\n",
      "plt.plot([wbias(mbooks, l, True) for l in range(1, 10)], label=\"books, shuffled again\")\n",
      "plt.legend()\n",
      "plt.title(\"WEAT Bias on Professions vs Number of Words \\n in Attribute Sets\")\n",
      "plt.show()\n",
      "207/686:\n",
      "stereotypes_m = ['president', 'dean', 'official', \\\n",
      "                 'director', 'officer', 'manager', 'captain', 'sergeant', 'principal', 'commissioner', 'coach']\n",
      "stereotypes_f = ['teacher', 'author', 'secretary', 'writer', \\\n",
      "                 'editor', 'maid', 'nurse', 'dancer', 'housekeeper', 'receptionist', 'photographer']\n",
      "male = [\"male\", \"man\", \"brother\", \"he\", \"him\", \"son\", \"father\", \"mr\", \"john\"]\n",
      "female = [\"female\", \"woman\", \"sister\", \"she\", \"her\", \\\n",
      "          \"daughter\", \"mother\", \"ms\", \"mary\"]\n",
      "def cos_sim(m, x, y):\n",
      "    wv1 = m.wv.get_vector(x)\n",
      "    wv2 = m.wv.get_vector(y)\n",
      "    sim = np.dot(wv1, wv2)/(np.linalg.norm(wv1)*np.linalg.norm(wv2))\n",
      "    return sim\n",
      "def s_word(m, w, A, B):\n",
      "    return np.mean([cos_sim(m, w, a) for a in A]) - np.mean([cos_sim(m, w, b) for b in B])\n",
      "def effect_size(m, X, Y, A, B):\n",
      "    num = np.mean([s_word(m, x, A, B) for x in X]) - np.mean([s_word(m, y, A, B) for y in Y])\n",
      "    return num/np.std([s_word(m, x, A, B) for x in X + Y])\n",
      "def wbias(m, l, shuf):\n",
      "#     if shuf:\n",
      "#         shuffle(stereotypes_m)\n",
      "#         shuffle(stereotypes_f)\n",
      "    if shuf:\n",
      "        shuffle(male)\n",
      "        shuffle(female)\n",
      "    return abs(effect_size(m, male, female, stereotypes_m[:l], stereotypes_f[:l]))\n",
      "207/687:\n",
      "plt.plot([wbias(mbooks, l, False) for l in range(1, 10)], label=\"books\")\n",
      "plt.plot([wbias(mbooks, l, True) for l in range(1, 10)], label=\"books, shuffled\")\n",
      "plt.plot([wbias(mbooks, l, True) for l in range(1, 10)], label=\"books, shuffled again\")\n",
      "plt.legend()\n",
      "plt.title(\"WEAT Bias on Professions vs Number of Words \\n in Attribute Sets\")\n",
      "plt.show()\n",
      "207/688:\n",
      "plt.plot([wbias(mbooks, l, False) for l in range(1, 10)], label=\"books\")\n",
      "plt.plot([wbias(mbooks, l, True) for l in range(1, 10)], label=\"books, shuffled\")\n",
      "plt.plot([wbias(mbooks, l, True) for l in range(1, 10)], label=\"books, shuffled again\")\n",
      "plt.legend()\n",
      "plt.title(\"WEAT Bias on Professions vs Number of Words \\n in Attribute Sets\")\n",
      "plt.show()\n",
      "207/689:\n",
      "stereotypes_m = ['president', 'dean', 'official', \\\n",
      "                 'director', 'officer', 'manager', 'captain', 'sergeant', 'principal', 'commissioner', 'coach']\n",
      "stereotypes_f = ['teacher', 'author', 'secretary', 'writer', \\\n",
      "                 'editor', 'maid', 'nurse', 'dancer', 'housekeeper', 'receptionist', 'photographer']\n",
      "male = [\"male\", \"man\", \"brother\", \"he\", \"him\", \"son\", \"father\", \"mr\", \"john\"]\n",
      "female = [\"female\", \"woman\", \"sister\", \"she\", \"her\", \\\n",
      "          \"daughter\", \"mother\", \"ms\", \"mary\"]\n",
      "def cos_sim(m, x, y):\n",
      "    wv1 = m.wv.get_vector(x)\n",
      "    wv2 = m.wv.get_vector(y)\n",
      "    sim = np.dot(wv1, wv2)/(np.linalg.norm(wv1)*np.linalg.norm(wv2))\n",
      "    return sim\n",
      "def s_word(m, w, A, B):\n",
      "    return np.mean([cos_sim(m, w, a) for a in A]) - np.mean([cos_sim(m, w, b) for b in B])\n",
      "def effect_size(m, X, Y, A, B):\n",
      "    num = np.mean([s_word(m, x, A, B) for x in X]) - np.mean([s_word(m, y, A, B) for y in Y])\n",
      "    return num/np.std([s_word(m, x, A, B) for x in X + Y])\n",
      "def wbias(m, l, shuf):\n",
      "#     if shuf:\n",
      "#         shuffle(stereotypes_m)\n",
      "#         shuffle(stereotypes_f)\n",
      "    if shuf:\n",
      "        shuffle(male)\n",
      "        shuffle(female)\n",
      "    return abs(effect_size(m, male[:l], female[:l], stereotypes_m, stereotypes_f))\n",
      "207/690:\n",
      "plt.plot([wbias(mbooks, l, False) for l in range(1, 10)], label=\"books\")\n",
      "plt.plot([wbias(mbooks, l, True) for l in range(1, 10)], label=\"books, shuffled\")\n",
      "plt.plot([wbias(mbooks, l, True) for l in range(1, 10)], label=\"books, shuffled again\")\n",
      "plt.legend()\n",
      "plt.title(\"WEAT Bias on Professions vs Number of Words \\n in Attribute Sets\")\n",
      "plt.show()\n",
      "207/691:\n",
      "plt.plot([wbias(mbooks, l, False) for l in range(1, 10)], label=\"books\")\n",
      "plt.plot([wbias(mbooks, l, True) for l in range(1, 10)], label=\"books, shuffled\")\n",
      "plt.plot([wbias(mbooks, l, True) for l in range(1, 10)], label=\"books, shuffled again\")\n",
      "plt.legend()\n",
      "plt.title(\"WEAT Bias on Professions vs Number of Words \\n in Attribute Sets\")\n",
      "plt.show()\n",
      "207/692:\n",
      "plt.plot([wbias(mbooks, l, False) for l in range(1, 10)], label=\"books\")\n",
      "plt.plot([wbias(mbooks, l, True) for l in range(1, 10)], label=\"books, shuffled\")\n",
      "plt.plot([wbias(mbooks, l, True) for l in range(1, 10)], label=\"books, shuffled again\")\n",
      "plt.legend()\n",
      "plt.title(\"WEAT Bias on Professions vs Number of Words \\n in Attribute Sets\")\n",
      "plt.show()\n",
      "207/693:\n",
      "plt.plot([wbias(mbooks, l, False) for l in range(1, 10)], label=\"books\")\n",
      "plt.plot([wbias(mbooks, l, True) for l in range(1, 10)], label=\"books, shuffled\")\n",
      "plt.plot([wbias(mbooks, l, True) for l in range(1, 10)], label=\"books, shuffled again\")\n",
      "plt.legend()\n",
      "plt.title(\"WEAT Bias on Professions vs Number of Words \\n in Attribute Sets\")\n",
      "plt.show()\n",
      "207/694:\n",
      "plt.plot([wbias(mbooks, l, False) for l in range(1, 9)], label=\"books\")\n",
      "plt.plot([wbias(mbooks, l, True) for l in range(1, 9)], label=\"books, shuffled\")\n",
      "plt.plot([wbias(mbooks, l, True) for l in range(1, 9)], label=\"books, shuffled again\")\n",
      "plt.legend()\n",
      "plt.title(\"WEAT Bias on Professions vs Number of Words \\n in Attribute Sets\")\n",
      "plt.show()\n",
      "207/695:\n",
      "plt.plot([wbias(mbooks, l, False) for l in range(1, 9)], label=\"books\")\n",
      "plt.plot([wbias(mbooks, l, True) for l in range(1, 9)], label=\"books, shuffled\")\n",
      "plt.plot([wbias(mbooks, l, True) for l in range(1, 9)], label=\"books, shuffled again\")\n",
      "plt.legend()\n",
      "plt.title(\"WEAT Bias on Professions vs Number of Words in Attribute Sets\")\n",
      "plt.show()\n",
      "207/696:\n",
      "plt.plot([wbias(mbooks, l, False) for l in range(1, 9)], label=\"books\")\n",
      "plt.plot([wbias(mbooks, l, True) for l in range(1, 9)], label=\"books, shuffled\")\n",
      "plt.plot([wbias(mbooks, l, True) for l in range(1, 9)], label=\"books, shuffled again\")\n",
      "plt.legend()\n",
      "plt.title(\"WEAT Bias on Professions vs Number of Words in Attribute Sets\")\n",
      "plt.show()\n",
      "207/697:\n",
      "plt.plot([wbias(mbooks, l, False) for l in range(1, 9)], label=\"books\")\n",
      "plt.plot([wbias(mbooks, l, True) for l in range(1, 9)], label=\"books, shuffled\")\n",
      "plt.plot([wbias(mbooks, l, True) for l in range(1, 9)], label=\"books, shuffled again\")\n",
      "plt.legend()\n",
      "plt.title(\"WEAT Bias on Professions vs Number of Words \\n in Attribute Sets\")\n",
      "plt.show()\n",
      "207/698:\n",
      "plt.plot([wbias(mbooks, l, False) for l in range(1, 9)], label=\"books\")\n",
      "plt.plot([wbias(mbooks, l, True) for l in range(1, 9)], label=\"books, shuffled\")\n",
      "plt.plot([wbias(mbooks, l, True) for l in range(1, 9)], label=\"books, re-shuffled\")\n",
      "plt.legend()\n",
      "plt.title(\"WEAT Bias on Professions vs Number of Words \\n in Attribute Sets\")\n",
      "plt.show()\n",
      "207/699:\n",
      "plt.plot([wbias(mbooks, l, False) for l in range(1, 9)], label=\"books\")\n",
      "plt.plot([wbias(mbooks, l, True) for l in range(1, 9)], label=\"books, shuffled\")\n",
      "plt.plot([wbias(mbooks, l, True) for l in range(1, 9)], label=\"books, re-shuffled\")\n",
      "plt.legend()\n",
      "plt.title(\"WEAT Bias on Professions vs Number of Words \\n in Attribute Sets\")\n",
      "plt.show()\n",
      "207/700:\n",
      "plt.plot([wbias(mbooks, l, False) for l in range(1, 11)], label=\"books\")\n",
      "plt.plot([wbias(mbooks, l, True) for l in range(1, 11)], label=\"books, shuffled\")\n",
      "plt.plot([wbias(mbooks, l, True) for l in range(1, 11)], label=\"books, re-shuffled\")\n",
      "plt.legend()\n",
      "plt.title(\"WEAT Bias on Professions vs Number of Words \\n in Target Sets\")\n",
      "plt.show()\n",
      "207/701:\n",
      "stereotypes_m = ['president', 'dean', 'official', \\\n",
      "                 'director', 'officer', 'manager', 'captain', 'sergeant', 'principal', 'commissioner', 'coach']\n",
      "stereotypes_f = ['teacher', 'author', 'secretary', 'writer', \\\n",
      "                 'editor', 'maid', 'nurse', 'dancer', 'housekeeper', 'receptionist', 'photographer']\n",
      "male = [\"male\", \"man\", \"brother\", \"he\", \"him\", \"son\", \"father\", \"mr\", \"john\"]\n",
      "female = [\"female\", \"woman\", \"sister\", \"she\", \"her\", \\\n",
      "          \"daughter\", \"mother\", \"ms\", \"mary\"]\n",
      "def cos_sim(m, x, y):\n",
      "    wv1 = m.wv.get_vector(x)\n",
      "    wv2 = m.wv.get_vector(y)\n",
      "    sim = np.dot(wv1, wv2)/(np.linalg.norm(wv1)*np.linalg.norm(wv2))\n",
      "    return sim\n",
      "def s_word(m, w, A, B):\n",
      "    return np.mean([cos_sim(m, w, a) for a in A]) - np.mean([cos_sim(m, w, b) for b in B])\n",
      "def effect_size(m, X, Y, A, B):\n",
      "    num = np.mean([s_word(m, x, A, B) for x in X]) - np.mean([s_word(m, y, A, B) for y in Y])\n",
      "    return num/np.std([s_word(m, x, A, B) for x in X + Y])\n",
      "def wbias(m, l, shuf):\n",
      "    if shuf:\n",
      "        shuffle(stereotypes_m)\n",
      "        shuffle(stereotypes_f)\n",
      "#     if shuf:\n",
      "#         shuffle(male)\n",
      "#         shuffle(female)\n",
      "    return abs(effect_size(m, male, female, stereotypes_m[:l], stereotypes_f[:l]))\n",
      "207/702:\n",
      "plt.plot([wbias(mbooks, l, False) for l in range(1, 11)], label=\"books\")\n",
      "plt.plot([wbias(mbooks, l, True) for l in range(1, 11)], label=\"books, shuffled\")\n",
      "plt.plot([wbias(mbooks, l, True) for l in range(1, 11)], label=\"books, re-shuffled\")\n",
      "plt.legend()\n",
      "plt.title(\"WEAT Bias on Professions vs Number of Words \\n in Target Sets\")\n",
      "plt.show()\n",
      "207/703:\n",
      "long = politics + books + movies\n",
      "len(long)\n",
      "207/704: mlong = getw2v(long)\n",
      "212/1:\n",
      "import os\n",
      "import time\n",
      "import dill\n",
      "from typing import List\n",
      "from functools import reduce\n",
      "from calendar import monthrange\n",
      "212/2:\n",
      "import json\n",
      "import csv\n",
      "import requests\n",
      "import xml.etree.ElementTree as ET\n",
      "from bs4 import BeautifulSoup\n",
      "import urllib.request as urllib2\n",
      "212/3:\n",
      "import matplotlib.pyplot as plt\n",
      "import numpy as np\n",
      "from scipy import stats\n",
      "from random import sample, randint, shuffle\n",
      "from sklearn.decomposition import PCA\n",
      "212/4:\n",
      "import nltk\n",
      "import text_summarizer\n",
      "from gensim.utils import simple_preprocess\n",
      "from gensim.summarization.summarizer import summarize\n",
      "from gensim.models import Word2Vec, LsiModel\n",
      "from gensim.test.utils import common_dictionary, common_corpus\n",
      "212/5: from time import process_time\n",
      "212/6:\n",
      "t = process_time()\n",
      "dill.load_session('thesis_env.db')\n",
      "process_time() - t\n",
      "212/7: len(adjectives)\n",
      "212/8: len(all_)\n",
      "212/9: long = books + politics + movies\n",
      "212/10:\n",
      "t = time.process_time()\n",
      "alld = {}\n",
      "for doc in all_:\n",
      "    doc = Doc(doc.id, doc.gdescriptors, doc.descriptors, doc.text)\n",
      "    for d in doc.descriptors:\n",
      "        count, docs = alld.get(d, (0, set()))\n",
      "        docs.add(doc)\n",
      "        alld[d] = (count + 1, docs)\n",
      "time.process_time() - t\n",
      "212/11:\n",
      "alldlist = sorted([(d, alld[d][0], alld[d][1]) for d in alld], key=lambda x: x[1], reverse=True)\n",
      "for x in range(15):\n",
      "    print(alldlist[x][0:2])\n",
      "212/12:\n",
      "def exclude(e, x):\n",
      "    for d in x.descriptors:\n",
      "        if d == e:\n",
      "            continue\n",
      "        if d in [t[0] for t in alldlist[:30]]:\n",
      "            return False\n",
      "    return True\n",
      "212/13:\n",
      "def getlist(topic):\n",
      "    return list(filter(lambda x: exclude(topic, x), alld[topic][1]))\n",
      "212/14: politics = getlist('Politics and Government')\n",
      "212/15: movies = getlist('Motion Pictures')\n",
      "212/16: books = getlist('Books and Literature')\n",
      "212/17: sports = getlist('Baseball') + getlist('Football')\n",
      "212/18: len(politics), len(movies), len(books), len(sports)\n",
      "212/19: long = politics + movies + books\n",
      "212/20:\n",
      "long = politics + movies + books\n",
      "len(long)\n",
      "212/21:\n",
      "def exclude(e, x):\n",
      "    for d in x.descriptors:\n",
      "        if d == e:\n",
      "            continue\n",
      "        if d in [t[0] for t in ['Politics and Government', 'Motion Pictures', 'Books and Literature']]:\n",
      "            return False\n",
      "    return True\n",
      "212/22:\n",
      "def exclude(e, x):\n",
      "    for d in x.descriptors:\n",
      "        if d == e:\n",
      "            continue\n",
      "        if d in [t[0] for t in ['Politics and Government', 'Motion Pictures', 'Books and Literature']]:\n",
      "            return False\n",
      "    return True\n",
      "212/23: politics = getlist('Politics and Government')\n",
      "212/24: movies = getlist('Motion Pictures')\n",
      "212/25: books = getlist('Books and Literature')\n",
      "212/26: sports = getlist('Baseball') + getlist('Football')\n",
      "212/27: len(politics), len(movies), len(books), len(sports)\n",
      "212/28:\n",
      "long = politics + movies + books\n",
      "len(long)\n",
      "212/29: longlist = [sample(long, i*500) for i in range(1, 59)]\n",
      "212/30: len(long[57])\n",
      "212/31: len(longlist[57])\n",
      "212/32:\n",
      "t = time.process_time()\n",
      "longmodels = []\n",
      "for i in range(len(longlist)):\n",
      "    longmodels.append(getw2v(longlist[i]))\n",
      "    print(i)\n",
      "time.process_time() - t\n",
      "212/33:\n",
      "def getw2v(category):\n",
      "    sentences = []\n",
      "    for t in category:\n",
      "        sentences += nltk.sent_tokenize(t.text)\n",
      "    docs = [simple_preprocess(s) for s in sentences]\n",
      "    return Word2Vec(docs)\n",
      "212/34: longlist = [sample(long, i*500) for i in range(4, 59)]\n",
      "212/35:\n",
      "longlist = [sample(long, i*500) for i in range(4, 59)]\n",
      "len(longlist)\n",
      "212/36:\n",
      "t = time.process_time()\n",
      "longmodels = []\n",
      "for i in range(len(longlist)[:25]):\n",
      "    longmodels.append(getw2v(longlist[i]))\n",
      "    print(i)\n",
      "time.process_time() - t\n",
      "212/37:\n",
      "t = time.process_time()\n",
      "longmodels = []\n",
      "for i in range(len(longlist[:25])):\n",
      "    longmodels.append(getw2v(longlist[i]))\n",
      "    print(i)\n",
      "time.process_time() - t\n",
      "212/38:\n",
      "longbias = []\n",
      "for m in longmodels:\n",
      "    b = bias(getg(m), m, m.wv.vocab, proflist)\n",
      "    print(b)\n",
      "    longbias.append(b)\n",
      "212/39:\n",
      "def bias(g, m, vocab, wordlist):\n",
      "    b = 0\n",
      "    l = list(filter(lambda x: x in vocab, wordlist))\n",
      "    for x in l:\n",
      "        b += abs(gproj(g, m, x))\n",
      "    return b/len(l)\n",
      "212/40:\n",
      "longbias = []\n",
      "for m in longmodels:\n",
      "    b = bias(getg(m), m, m.wv.vocab, proflist)\n",
      "    print(b)\n",
      "    longbias.append(b)\n",
      "212/41:\n",
      "def getg(m):\n",
      "    matrix = []\n",
      "    for a, b in pairs:\n",
      "        if a not in m.wv.vocab or b not in m.wv.vocab:\n",
      "            continue\n",
      "        center = (m.wv.get_vector(a) + m.wv.get_vector(b))/2\n",
      "        matrix.append(m.wv.get_vector(a) - center)\n",
      "        matrix.append(m.wv.get_vector(b) - center)\n",
      "    matrix = np.array(matrix)\n",
      "    p = PCA(n_components = 10)\n",
      "    p.fit(matrix)\n",
      "    # print(p.explained_variance_ratio_[0])\n",
      "#         plt.bar(range(10), p.explained_variance_ratio_)\n",
      "#         plt.title(\"Explained Variance vs Component Number (\" + t + \")\")\n",
      "    return p.components_[0]\n",
      "212/42:\n",
      "longbias = []\n",
      "for m in longmodels:\n",
      "    b = bias(getg(m), m, m.wv.vocab, proflist)\n",
      "    print(b)\n",
      "    longbias.append(b)\n",
      "212/43:\n",
      "with open('debiaswe-master/data/equalize_pairs.json') as f:\n",
      "    pairs = json.loads(f.read())\n",
      "pairs = list(map(lambda x: [x[0].lower(), x[1].lower()], pairs))\n",
      "with open('debiaswe-master/data/professions.json') as f:\n",
      "    profs = json.loads(f.read())\n",
      "# proflist = list(filter(lambda x: x in mpol.wv.vocab, [p[0] for p in profs if abs(p[1]) < 0.6]))\n",
      "with open('debiaswe-master/data/gender_specific_seed.json') as f:\n",
      "    gender_specific = json.loads(f.read())\n",
      "with open('objects.csv') as csv_file:\n",
      "    objects = csv_file.read().split(\"\\n\")\n",
      "objects[0] = 'angle'\n",
      "with open('cities.csv') as csv_file:\n",
      "    cities = csv_file.read().split(\",\")\n",
      "cities = [x.lower() for x in cities]\n",
      "212/44: proflist = [p[0] for p in profs]\n",
      "212/45:\n",
      "longbias = []\n",
      "for m in longmodels:\n",
      "    b = bias(getg(m), m, m.wv.vocab, proflist)\n",
      "    print(b)\n",
      "    longbias.append(b)\n",
      "212/46:\n",
      "def gproj(g, m, w1):\n",
      "    wvec = m.wv.get_vector(w1)\n",
      "    p = np.dot(g, wvec)/(np.linalg.norm(g)*np.linalg.norm(wvec))\n",
      "    return p\n",
      "212/47:\n",
      "longbias = []\n",
      "for m in longmodels:\n",
      "    b = bias(getg(m), m, m.wv.vocab, proflist)\n",
      "    print(b)\n",
      "    longbias.append(b)\n",
      "212/48:\n",
      "plt.plot([i*500 for i in range(25), longbias, label=\"politics\")\n",
      "plt.legend()\n",
      "plt.title(\"Direct Bias on Professions\")\n",
      "plt.show()\n",
      "212/49:\n",
      "plt.plot([i*500 for i in range(25)], longbias, label=\"politics\")\n",
      "plt.legend()\n",
      "plt.title(\"Direct Bias on Professions\")\n",
      "plt.show()\n",
      "212/50:\n",
      "plt.plot([i*500 for i in range(3, 28)], longbias, label=\"politics\")\n",
      "plt.legend()\n",
      "plt.title(\"Direct Bias on Professions\")\n",
      "plt.show()\n",
      "212/51:\n",
      "plt.plot([i*500 for i in range(3, 29)], longbias, label=\"politics\")\n",
      "plt.legend()\n",
      "plt.title(\"Direct Bias on Professions\")\n",
      "plt.show()\n",
      "212/52:\n",
      "plt.plot([i*500 for i in range(3, 28)], longbias, label=\"politics\")\n",
      "plt.legend()\n",
      "plt.title(\"Direct Bias on Professions\")\n",
      "plt.show()\n",
      "212/53:\n",
      "t = time.process_time()\n",
      "for i in range(len(longlist[25:30])):\n",
      "    longmodels.append(getw2v(longlist[i]))\n",
      "    print(i)\n",
      "time.process_time() - t\n",
      "212/54:\n",
      "longbias = []\n",
      "for m in longmodels:\n",
      "    b = bias(getg(m), m, m.wv.vocab, proflist)\n",
      "    print(b)\n",
      "    longbias.append(b)\n",
      "212/55:\n",
      "plt.plot([i*500 for i in range(3, 28)], longbias, label=\"politics\")\n",
      "plt.legend()\n",
      "plt.title(\"Direct Bias on Professions\")\n",
      "plt.show()\n",
      "212/56:\n",
      "longbias = []\n",
      "for m in longmodels:\n",
      "    b = bias(getg(m), m, m.wv.vocab, proflist)\n",
      "    print(b)\n",
      "    longbias.append(b)\n",
      "212/57:\n",
      "plt.plot([i*500 for i in range(3, 28)], longbias, label=\"politics\")\n",
      "plt.legend()\n",
      "plt.title(\"Direct Bias on Professions\")\n",
      "plt.show()\n",
      "212/58:\n",
      "plt.plot([i*500 for i in range(3, 33)], longbias, label=\"politics\")\n",
      "plt.legend()\n",
      "plt.title(\"Direct Bias on Professions\")\n",
      "plt.show()\n",
      "212/59: longmodels = longmodels[:25]\n",
      "212/60:\n",
      "t = time.process_time()\n",
      "for i in range(len(longlist[25:30])):\n",
      "    longmodels.append(getw2v(longlist[25 + i]))\n",
      "    print(i)\n",
      "time.process_time() - t\n",
      "212/61:\n",
      "longbias = []\n",
      "for m in longmodels:\n",
      "    b = bias(getg(m), m, m.wv.vocab, proflist)\n",
      "    print(b)\n",
      "    longbias.append(b)\n",
      "212/62:\n",
      "plt.plot([i*500 for i in range(3, 33)], longbias, label=\"politics\")\n",
      "plt.legend()\n",
      "plt.title(\"Direct Bias on Professions\")\n",
      "plt.show()\n",
      "212/63:\n",
      "longbias = []\n",
      "for m in longmodels:\n",
      "    b = bias(getg(m), m, m.wv.vocab, proflist)\n",
      "    print(b)\n",
      "    longbias.append(b)\n",
      "212/64:\n",
      "plt.plot([i*500 for i in range(3, 33)], longbias, label=\"politics\")\n",
      "plt.legend()\n",
      "plt.title(\"Direct Bias on Professions\")\n",
      "plt.show()\n",
      "212/65:\n",
      "t = time.process_time()\n",
      "for i in range(len(longlist[30:40])):\n",
      "    longmodels.append(getw2v(longlist[30 + i]))\n",
      "    print(i)\n",
      "time.process_time() - t\n",
      "212/66:\n",
      "longbias = []\n",
      "for m in longmodels:\n",
      "    b = bias(getg(m), m, m.wv.vocab, proflist)\n",
      "    print(b)\n",
      "    longbias.append(b)\n",
      "212/67:\n",
      "plt.plot([i*500 for i in range(3, 43)], longbias, label=\"politics\")\n",
      "plt.legend()\n",
      "plt.title(\"Direct Bias on Professions\")\n",
      "plt.show()\n",
      "212/68:\n",
      "t = time.process_time()\n",
      "for i in range(len(longlist[40:50])):\n",
      "    longmodels.append(getw2v(longlist[40 + i]))\n",
      "    print(i)\n",
      "time.process_time() - t\n",
      "212/69:\n",
      "longbias = []\n",
      "for m in longmodels:\n",
      "    b = bias(getg(m), m, m.wv.vocab, proflist)\n",
      "    print(b)\n",
      "    longbias.append(b)\n",
      "212/70:\n",
      "plt.plot([i*500 for i in range(3, 53)], longbias, label=\"politics\")\n",
      "plt.legend()\n",
      "plt.title(\"Direct Bias on Professions\")\n",
      "plt.show()\n",
      "212/71:\n",
      "plt.plot([i*500 for i in range(3, 53)], longbias, label=\"politics\")\n",
      "plt.legend()\n",
      "plt.title(\"Direct Bias on Professions\")\n",
      "plt.show()\n",
      "212/72:\n",
      "plt.plot([i*500 for i in range(3, 52)], longbias[:52], label=\"politics\")\n",
      "plt.legend()\n",
      "plt.title(\"Direct Bias on Professions\")\n",
      "plt.show()\n",
      "212/73:\n",
      "plt.plot([i*500 for i in range(3, 52)], longbias[:50], label=\"politics\")\n",
      "plt.legend()\n",
      "plt.title(\"Direct Bias on Professions\")\n",
      "plt.show()\n",
      "212/74:\n",
      "plt.plot([i*500 for i in range(3, 52)], longbias[:49], label=\"politics\")\n",
      "plt.legend()\n",
      "plt.title(\"Direct Bias on Professions\")\n",
      "plt.show()\n",
      "212/75:\n",
      "plt.plot([i*500 for i in range(3, 52)], longbias[:49])\n",
      "plt.legend()\n",
      "plt.title(\"Direct Bias on Professions\")\n",
      "plt.show()\n",
      "212/76:\n",
      "plt.plot([i*500 for i in range(3, 52)], longbias[:49])\n",
      "plt.title(\"Direct Bias on Professions\")\n",
      "plt.show()\n",
      "212/77:\n",
      "plt.plot([i*500 for i in range(3, 52)], longbias[:49])\n",
      "plt.title(\"Direct Bias on Professions vs Number of Articles\")\n",
      "plt.show()\n",
      "212/78: mbooks\n",
      "212/79:\n",
      "t = time.process_time()\n",
      "mbooks = getw2v(books)\n",
      "time.process_time() - t\n",
      "212/80: mbooks\n",
      "212/81:\n",
      "stereotypes_m = ['president', 'dean', \\\n",
      "                 'director', 'officer', 'manager', 'captain', 'coach', 'commissioner', 'sergeant', \"principal\", \"programmer\"]\n",
      "stereotypes_f = ['teacher', 'author', 'secretary', 'writer', \\\n",
      "                 'editor', \"nurse\", \"housekeeper\", \"maid\", \"dancer\", \"artist\", \"homemaker\"]\n",
      "male = [\"male\", \"man\", \"brother\", \"he\", \"him\", \"son\", \"father\", \"boy\", \"masculine\", \"mr\", \"john\"]\n",
      "female = [\"female\", \"woman\", \"sister\", \"she\", \"her\", \\\n",
      "          \"daughter\", \"mother\", \"girl\", \"feminine\", \"mrs\", \"mary\"]\n",
      "def cos_sim(m, x, y):\n",
      "    wv1 = m.wv.get_vector(x)\n",
      "    wv2 = m.wv.get_vector(y)\n",
      "    sim = np.dot(wv1, wv2)/(np.linalg.norm(wv1)*np.linalg.norm(wv2))\n",
      "    return sim\n",
      "def s_word(m, w, A, B):\n",
      "    return np.mean([cos_sim(m, w, a) for a in A]) - np.mean([cos_sim(m, w, b) for b in B])\n",
      "def effect_size(m, X, Y, A, B):\n",
      "    num = np.mean([s_word(m, x, A, B) for x in X]) - np.mean([s_word(m, y, A, B) for y in Y])\n",
      "    return num/np.std([s_word(m, x, A, B) for x in X + Y])\n",
      "def wbias(m):\n",
      "    return abs(effect_size(m, male, female, stereotypes_m, stereotypes_f))\n",
      "212/82:\n",
      "bwbiases = []\n",
      "for i in range(11):\n",
      "    bwbiases.append(wbias(mbooks, i))\n",
      "212/83:\n",
      "stereotypes_m = ['president', 'dean', \\\n",
      "                 'director', 'officer', 'manager', 'captain', 'coach', 'commissioner', 'sergeant', \"principal\", \"programmer\"]\n",
      "stereotypes_f = ['teacher', 'author', 'secretary', 'writer', \\\n",
      "                 'editor', \"nurse\", \"housekeeper\", \"maid\", \"dancer\", \"artist\", \"homemaker\"]\n",
      "male = [\"male\", \"man\", \"brother\", \"he\", \"him\", \"son\", \"father\", \"boy\", \"masculine\", \"mr\", \"john\"]\n",
      "female = [\"female\", \"woman\", \"sister\", \"she\", \"her\", \\\n",
      "          \"daughter\", \"mother\", \"girl\", \"feminine\", \"mrs\", \"mary\"]\n",
      "def cos_sim(m, x, y):\n",
      "    wv1 = m.wv.get_vector(x)\n",
      "    wv2 = m.wv.get_vector(y)\n",
      "    sim = np.dot(wv1, wv2)/(np.linalg.norm(wv1)*np.linalg.norm(wv2))\n",
      "    return sim\n",
      "def s_word(m, w, A, B):\n",
      "    return np.mean([cos_sim(m, w, a) for a in A]) - np.mean([cos_sim(m, w, b) for b in B])\n",
      "def effect_size(m, X, Y, A, B):\n",
      "    num = np.mean([s_word(m, x, A, B) for x in X]) - np.mean([s_word(m, y, A, B) for y in Y])\n",
      "    return num/np.std([s_word(m, x, A, B) for x in X + Y])\n",
      "def wbias(m, l):\n",
      "    return abs(effect_size(m, male, female, stereotypes_m[:l], stereotypes_f[:l]))\n",
      "212/84:\n",
      "bwbiases = []\n",
      "for i in range(11):\n",
      "    bwbiases.append(wbias(mbooks, i))\n",
      "212/85: bwbiases\n",
      "212/86:\n",
      "bwbiases = []\n",
      "for i in range(1, 11):\n",
      "    bwbiases.append(wbias(mbooks, i))\n",
      "212/87: plt.plot(bwbiases)\n",
      "212/88:\n",
      "plt.plot(bwbiases)\n",
      "plt.title(\"WEAT Bias vs Number of Words in Target Sets\")\n",
      "plt.show()\n",
      "212/89:\n",
      "stereotypes_m = ['president', 'dean', \\\n",
      "                 'director', 'officer', 'manager', 'captain', 'coach', 'commissioner', 'sergeant', \"principal\", \"programmer\"]\n",
      "stereotypes_f = ['teacher', 'author', 'secretary', 'writer', \\\n",
      "                 'editor', \"nurse\", \"housekeeper\", \"maid\", \"dancer\", \"artist\", \"homemaker\"]\n",
      "male = [\"male\", \"man\", \"brother\", \"he\", \"him\", \"son\", \"father\", \"boy\", \"masculine\", \"mr\", \"john\"]\n",
      "female = [\"female\", \"woman\", \"sister\", \"she\", \"her\", \\\n",
      "          \"daughter\", \"mother\", \"girl\", \"feminine\", \"mrs\", \"mary\"]\n",
      "def cos_sim(m, x, y):\n",
      "    wv1 = m.wv.get_vector(x)\n",
      "    wv2 = m.wv.get_vector(y)\n",
      "    sim = np.dot(wv1, wv2)/(np.linalg.norm(wv1)*np.linalg.norm(wv2))\n",
      "    return sim\n",
      "def s_word(m, w, A, B):\n",
      "    return np.mean([cos_sim(m, w, a) for a in A]) - np.mean([cos_sim(m, w, b) for b in B])\n",
      "def effect_size(m, X, Y, A, B):\n",
      "    num = np.mean([s_word(m, x, A, B) for x in X]) - np.mean([s_word(m, y, A, B) for y in Y])\n",
      "    return num/np.std([s_word(m, x, A, B) for x in X + Y])\n",
      "def wbias(m, l):\n",
      "    shuffle(stereotypes_m)\n",
      "    shuffle(stereotypes_f)\n",
      "    return abs(effect_size(m, male, female, stereotypes_m[:l], stereotypes_f[:l]))\n",
      "212/90:\n",
      "bwbiases = []\n",
      "for i in range(1, 11):\n",
      "    bwbiases.append(wbias(mbooks, i))\n",
      "212/91:\n",
      "bwbiases = [wbias(mbooks, i, False)) for i in range(1,11)]\n",
      "bwbiases2 = [wbias(mbooks, i, True)) for i in range(1,11)]\n",
      "212/92:\n",
      "bwbiases = [wbias(mbooks, i, False) for i in range(1,11)]\n",
      "bwbiases2 = [wbias(mbooks, i, True) for i in range(1,11)]\n",
      "212/93:\n",
      "stereotypes_m = ['president', 'dean', \\\n",
      "                 'director', 'officer', 'manager', 'captain', 'coach', 'commissioner', 'sergeant', \"principal\", \"programmer\"]\n",
      "stereotypes_f = ['teacher', 'author', 'secretary', 'writer', \\\n",
      "                 'editor', \"nurse\", \"housekeeper\", \"maid\", \"dancer\", \"artist\", \"homemaker\"]\n",
      "male = [\"male\", \"man\", \"brother\", \"he\", \"him\", \"son\", \"father\", \"boy\", \"masculine\", \"mr\", \"john\"]\n",
      "female = [\"female\", \"woman\", \"sister\", \"she\", \"her\", \\\n",
      "          \"daughter\", \"mother\", \"girl\", \"feminine\", \"mrs\", \"mary\"]\n",
      "def cos_sim(m, x, y):\n",
      "    wv1 = m.wv.get_vector(x)\n",
      "    wv2 = m.wv.get_vector(y)\n",
      "    sim = np.dot(wv1, wv2)/(np.linalg.norm(wv1)*np.linalg.norm(wv2))\n",
      "    return sim\n",
      "def s_word(m, w, A, B):\n",
      "    return np.mean([cos_sim(m, w, a) for a in A]) - np.mean([cos_sim(m, w, b) for b in B])\n",
      "def effect_size(m, X, Y, A, B):\n",
      "    num = np.mean([s_word(m, x, A, B) for x in X]) - np.mean([s_word(m, y, A, B) for y in Y])\n",
      "    return num/np.std([s_word(m, x, A, B) for x in X + Y])\n",
      "def wbias(m, l, shuf):\n",
      "    if shuf:\n",
      "        shuffle(stereotypes_m)\n",
      "        shuffle(stereotypes_f)\n",
      "    return abs(effect_size(m, male, female, stereotypes_m[:l], stereotypes_f[:l]))\n",
      "212/94:\n",
      "bwbiases = [wbias(mbooks, i, False) for i in range(1,11)]\n",
      "bwbiases2 = [wbias(mbooks, i, True) for i in range(1,11)]\n",
      "212/95:\n",
      "plt.plot(bwbiases, label=\"books\")\n",
      "plt.plot(bwbiases2, label=\"books, shuffled\")\n",
      "plt.title(\"WEAT Bias vs Number of Words in Target Sets\")\n",
      "plt.legend()\n",
      "plt.show()\n",
      "212/96:\n",
      "plt.plot([i for i in range(1,11)], bwbiases, label=\"books\")\n",
      "plt.plot(i for i in range(1,11)], bwbiases2, label=\"books, shuffled\")\n",
      "plt.title(\"WEAT Bias vs Number of Words in Target Sets\")\n",
      "plt.legend()\n",
      "plt.show()\n",
      "212/97:\n",
      "plt.plot([i for i in range(1,11)], bwbiases, label=\"books\")\n",
      "plt.plot([i for i in range(1,11)], bwbiases2, label=\"books, shuffled\")\n",
      "plt.title(\"WEAT Bias vs Number of Words in Target Sets\")\n",
      "plt.legend()\n",
      "plt.show()\n",
      "212/98:\n",
      "plt.plot([i for i in range(1,11)], bwbiases, label=\"books\")\n",
      "plt.plot([i for i in range(1,11)], bwbiases2, label=\"books, shuffled\")\n",
      "plt.title(\"WEAT Bias vs Number of Words in Target Sets\")\n",
      "plt.legend()\n",
      "plt.show()\n",
      "212/99:\n",
      "bwbiases = [wbias(mbooks, i, False) for i in range(1,11)]\n",
      "bwbiases2 = [wbias(mbooks, i, True) for i in range(1,11)]\n",
      "212/100:\n",
      "plt.plot([i for i in range(1,11)], bwbiases, label=\"books\")\n",
      "plt.plot([i for i in range(1,11)], bwbiases2, label=\"books, shuffled\")\n",
      "plt.title(\"WEAT Bias vs Number of Words in Target Sets\")\n",
      "plt.legend()\n",
      "plt.show()\n",
      "212/101:\n",
      "bwbiases = [wbias(mbooks, i, False) for i in range(1,11)]\n",
      "bwbiases2 = [wbias(mbooks, i, True) for i in range(1,11)]\n",
      "212/102:\n",
      "plt.plot([i for i in range(1,11)], bwbiases, label=\"books\")\n",
      "plt.plot([i for i in range(1,11)], bwbiases2, label=\"books, shuffled\")\n",
      "plt.title(\"WEAT Bias vs Number of Words in Target Sets\")\n",
      "plt.legend()\n",
      "plt.show()\n",
      "212/103:\n",
      "stereotypes_m = ['president', 'dean', \\\n",
      "                 'director', 'officer', 'manager', 'captain', 'coach', 'commissioner', 'sergeant', \"principal\", \"programmer\"]\n",
      "stereotypes_f = ['teacher', 'author', 'secretary', 'writer', \\\n",
      "                 'editor', \"nurse\", \"housekeeper\", \"maid\", \"dancer\", \"artist\", \"homemaker\"]\n",
      "male = [\"male\", \"man\", \"brother\", \"he\", \"him\", \"son\", \"father\", \"boy\", \"masculine\", \"mr\", \"john\"]\n",
      "female = [\"female\", \"woman\", \"sister\", \"she\", \"her\", \\\n",
      "          \"daughter\", \"mother\", \"girl\", \"feminine\", \"mrs\", \"mary\"]\n",
      "def cos_sim(m, x, y):\n",
      "    wv1 = m.wv.get_vector(x)\n",
      "    wv2 = m.wv.get_vector(y)\n",
      "    sim = np.dot(wv1, wv2)/(np.linalg.norm(wv1)*np.linalg.norm(wv2))\n",
      "    return sim\n",
      "def s_word(m, w, A, B):\n",
      "    return np.mean([cos_sim(m, w, a) for a in A]) - np.mean([cos_sim(m, w, b) for b in B])\n",
      "def effect_size(m, X, Y, A, B):\n",
      "    num = np.mean([s_word(m, x, A, B) for x in X]) - np.mean([s_word(m, y, A, B) for y in Y])\n",
      "    print(num, np.std([s_word(m, x, A, B) for x in X + Y])\n",
      "    return num/np.std([s_word(m, x, A, B) for x in X + Y])\n",
      "def wbias(m, l, shuf):\n",
      "    if shuf:\n",
      "        shuffle(stereotypes_m)\n",
      "        shuffle(stereotypes_f)\n",
      "    return abs(effect_size(m, male, female, stereotypes_m[:l], stereotypes_f[:l]))\n",
      "212/104:\n",
      "stereotypes_m = ['president', 'dean', \\\n",
      "                 'director', 'officer', 'manager', 'captain', 'coach', 'commissioner', 'sergeant', \"principal\", \"programmer\"]\n",
      "stereotypes_f = ['teacher', 'author', 'secretary', 'writer', \\\n",
      "                 'editor', \"nurse\", \"housekeeper\", \"maid\", \"dancer\", \"artist\", \"homemaker\"]\n",
      "male = [\"male\", \"man\", \"brother\", \"he\", \"him\", \"son\", \"father\", \"boy\", \"masculine\", \"mr\", \"john\"]\n",
      "female = [\"female\", \"woman\", \"sister\", \"she\", \"her\", \\\n",
      "          \"daughter\", \"mother\", \"girl\", \"feminine\", \"mrs\", \"mary\"]\n",
      "def cos_sim(m, x, y):\n",
      "    wv1 = m.wv.get_vector(x)\n",
      "    wv2 = m.wv.get_vector(y)\n",
      "    sim = np.dot(wv1, wv2)/(np.linalg.norm(wv1)*np.linalg.norm(wv2))\n",
      "    return sim\n",
      "def s_word(m, w, A, B):\n",
      "    return np.mean([cos_sim(m, w, a) for a in A]) - np.mean([cos_sim(m, w, b) for b in B])\n",
      "def effect_size(m, X, Y, A, B):\n",
      "    num = np.mean([s_word(m, x, A, B) for x in X]) - np.mean([s_word(m, y, A, B) for y in Y])\n",
      "    print(num, np.std([s_word(m, x, A, B) for x in X + Y]))\n",
      "    return num/np.std([s_word(m, x, A, B) for x in X + Y])\n",
      "def wbias(m, l, shuf):\n",
      "    if shuf:\n",
      "        shuffle(stereotypes_m)\n",
      "        shuffle(stereotypes_f)\n",
      "    return abs(effect_size(m, male, female, stereotypes_m[:l], stereotypes_f[:l]))\n",
      "212/105:\n",
      "bwbiases = [wbias(mbooks, i, False) for i in range(1,11)]\n",
      "bwbiases2 = [wbias(mbooks, i, True) for i in range(1,11)]\n",
      "212/106: wbias(mbooks, len(stereotypes_m, False)\n",
      "212/107: wbias(mbooks, len(stereotypes_m, False))\n",
      "212/108: wbias(mbooks, len(stereotypes_m), False)\n",
      "212/109: wbias(mbooks, len(stereotypes_m), False)\n",
      "212/110: wbias(mbooks, 11, False)\n",
      "212/111: wbias(mbooks, 11, True)\n",
      "212/112: print(bwbiases)\n",
      "212/113:\n",
      "bwbiases = [wbias(mbooks, i, False) for i in range(1,12)]\n",
      "bwbiases2 = [wbias(mbooks, i, True) for i in range(1,12)]\n",
      "212/114:\n",
      "plt.plot([i for i in range(1,11)], bwbiases, label=\"books\")\n",
      "plt.plot([i for i in range(1,11)], bwbiases2, label=\"books, shuffled\")\n",
      "plt.title(\"WEAT Bias vs Number of Words in Target Sets\")\n",
      "plt.legend()\n",
      "plt.show()\n",
      "212/115:\n",
      "plt.plot([i for i in range(1,12)], bwbiases, label=\"books\")\n",
      "plt.plot([i for i in range(1,12)], bwbiases2, label=\"books, shuffled\")\n",
      "plt.title(\"WEAT Bias vs Number of Words in Target Sets\")\n",
      "plt.legend()\n",
      "plt.show()\n",
      "212/116:\n",
      "stereotypes_m = ['president', 'dean', \\\n",
      "                 'director', 'officer', 'manager', 'captain', 'coach', 'commissioner', 'sergeant', \"principal\", \"programmer\"]\n",
      "stereotypes_f = ['teacher', 'author', 'secretary', 'writer', \\\n",
      "                 'editor', \"nurse\", \"housekeeper\", \"maid\", \"dancer\", \"artist\", \"homemaker\"]\n",
      "male = [\"male\", \"man\", \"brother\", \"he\", \"him\", \"son\", \"father\", \"boy\", \"masculine\", \"mr\", \"john\"]\n",
      "female = [\"female\", \"woman\", \"sister\", \"she\", \"her\", \\\n",
      "          \"daughter\", \"mother\", \"girl\", \"feminine\", \"mrs\", \"mary\"]\n",
      "def cos_sim(m, x, y):\n",
      "    wv1 = m.wv.get_vector(x)\n",
      "    wv2 = m.wv.get_vector(y)\n",
      "    sim = np.dot(wv1, wv2)/(np.linalg.norm(wv1)*np.linalg.norm(wv2))\n",
      "    return sim\n",
      "def s_word(m, w, A, B):\n",
      "    return np.mean([cos_sim(m, w, a) for a in A]) - np.mean([cos_sim(m, w, b) for b in B])\n",
      "def effect_size(m, X, Y, A, B):\n",
      "    num = np.mean([s_word(m, x, A, B) for x in X]) - np.mean([s_word(m, y, A, B) for y in Y])\n",
      "    return num/np.std([s_word(m, x, A, B) for x in X + Y])\n",
      "def wbias(m, l, shuf):\n",
      "    if shuf:\n",
      "        shuffle(stereotypes_m)\n",
      "        shuffle(stereotypes_f)\n",
      "    return abs(effect_size(m, male, female, stereotypes_m[:l], stereotypes_f[:l]))\n",
      "212/117:\n",
      "bwbiases = [wbias(mbooks, i, False) for i in range(1,12)]\n",
      "bwbiases2 = [wbias(mbooks, i, True) for i in range(1,12)]\n",
      "212/118:\n",
      "plt.plot([i for i in range(1,12)], bwbiases, label=\"books\")\n",
      "plt.plot([i for i in range(1,12)], bwbiases2, label=\"books, shuffled\")\n",
      "plt.title(\"WEAT Bias vs Number of Words in Target Sets\")\n",
      "plt.legend()\n",
      "plt.show()\n",
      "212/119:\n",
      "stereotypes_m = ['president', 'dean', \\\n",
      "                 'director', 'officer', 'manager', 'captain', 'coach', 'commissioner', 'sergeant', \"principal\", \"programmer\"]\n",
      "stereotypes_f = ['teacher', 'author', 'secretary', 'writer', \\\n",
      "                 'editor', \"nurse\", \"housekeeper\", \"maid\", \"dancer\", \"artist\", \"homemaker\"]\n",
      "male = [\"male\", \"man\", \"brother\", \"he\", \"him\", \"son\", \"father\", \"boy\", \"masculine\", \"mr\", \"john\"]\n",
      "female = [\"female\", \"woman\", \"sister\", \"she\", \"her\", \\\n",
      "          \"daughter\", \"mother\", \"girl\", \"feminine\", \"mrs\", \"mary\"]\n",
      "def cos_sim(m, x, y):\n",
      "    wv1 = m.wv.get_vector(x)\n",
      "    wv2 = m.wv.get_vector(y)\n",
      "    sim = np.dot(wv1, wv2)/(np.linalg.norm(wv1)*np.linalg.norm(wv2))\n",
      "    return sim\n",
      "def s_word(m, w, A, B):\n",
      "    return np.mean([cos_sim(m, w, a) for a in A]) - np.mean([cos_sim(m, w, b) for b in B])\n",
      "def effect_size(m, X, Y, A, B):\n",
      "    num = np.mean([s_word(m, x, A, B) for x in X]) - np.mean([s_word(m, y, A, B) for y in Y])\n",
      "    return num/np.std([s_word(m, x, A, B) for x in X + Y])\n",
      "def wbias(m, l, shuf):\n",
      "    if shuf:\n",
      "#         shuffle(stereotypes_m)\n",
      "#         shuffle(stereotypes_f)\n",
      "        shuffle(male)\n",
      "        shuffle(female)\n",
      "    return abs(effect_size(m, male[:l], female[:l], stereotypes_m, stereotypes_f))\n",
      "212/120:\n",
      "bwbiases = [wbias(mbooks, i, False) for i in range(1,12)]\n",
      "bwbiases2 = [wbias(mbooks, i, True) for i in range(1,12)]\n",
      "212/121:\n",
      "plt.plot([i for i in range(1,12)], bwbiases, label=\"books\")\n",
      "plt.plot([i for i in range(1,12)], bwbiases2, label=\"books, shuffled\")\n",
      "plt.title(\"WEAT Bias vs Number of Words in Target Sets\")\n",
      "plt.legend()\n",
      "plt.show()\n",
      "212/122:\n",
      "bwbiases = [wbias(mbooks, i, False) for i in range(1,12)]\n",
      "bwbiases2 = [wbias(mbooks, i, True) for i in range(1,12)]\n",
      "212/123:\n",
      "plt.plot([i for i in range(1,12)], bwbiases, label=\"books\")\n",
      "plt.plot([i for i in range(1,12)], bwbiases2, label=\"books, shuffled\")\n",
      "plt.title(\"WEAT Bias vs Number of Words in Target Sets\")\n",
      "plt.legend()\n",
      "plt.show()\n",
      "212/124:\n",
      "bwbiases = [wbias(mbooks, i, False) for i in range(1,12)]\n",
      "bwbiases2 = [wbias(mbooks, i, True) for i in range(1,12)]\n",
      "212/125:\n",
      "plt.plot([i for i in range(1,12)], bwbiases, label=\"books\")\n",
      "plt.plot([i for i in range(1,12)], bwbiases2, label=\"books, shuffled\")\n",
      "plt.title(\"WEAT Bias vs Number of Words in Target Sets\")\n",
      "plt.legend()\n",
      "plt.show()\n",
      "212/126:\n",
      "plt.plot([i for i in range(1,12)], bwbiases, label=\"books\")\n",
      "plt.plot([i for i in range(1,12)], bwbiases2, label=\"books, shuffled\")\n",
      "plt.title(\"WEAT Bias vs Number of Words in Attribute Sets\")\n",
      "plt.legend()\n",
      "plt.show()\n",
      "212/127: len(longlist[51])\n",
      "212/128: len(longlist[49])\n",
      "212/129: len(longlist[51])\n",
      "212/130:\n",
      "plt.plot([i*500 for i in range(3, 52)], longbias[:49])\n",
      "plt.title(\"Direct Bias on Professions vs Number of Articles (2000-25500)\")\n",
      "plt.show()\n",
      "212/131: len(politics), len(movies), len(books), len(sports)\n",
      "212/132: politics = sample(politics, 7370)\n",
      "212/133: movies = sample(movies, 7370)\n",
      "212/134: books = sample(books, 7370)\n",
      "212/135: sports = sample(sports, 7370)\n",
      "212/136:\n",
      "t = time.process_time()\n",
      "mbooks = getw2v(books)\n",
      "time.process_time() - t\n",
      "212/137:\n",
      "t = time.process_time()\n",
      "mpol = getw2v(politics)\n",
      "time.process_time() - t\n",
      "212/138:\n",
      "t = time.process_time()\n",
      "msports = getw2v(sports)\n",
      "time.process_time() - t\n",
      "212/139:\n",
      "t = time.process_time()\n",
      "mmov = getw2v(movies)\n",
      "time.process_time() - t\n",
      "212/140:\n",
      "nltk.download('sentiwordnet')\n",
      "from nltk.corpus import sentiwordnet as swn\n",
      "212/141:\n",
      "sman = swn.senti_synset('man.n.01')\n",
      "swoman = swn.senti_synset('woman.n.01')\n",
      "tall = swn.senti_synset('tall.a.01')\n",
      "maid = swn.senti_synset('maid.n.01')\n",
      "smart = swn.senti_synset('smart.a.01')\n",
      "awesome = swn.senti_synset('awesome.a.01')\n",
      "shy = swn.senti_synset('shy.a.01')\n",
      "liberal = swn.senti_synset('liberal.a.01')\n",
      "conservative = swn.senti_synset('conservative.a.01')\n",
      "moderate = swn.senti_synset('moderate.a.01')\n",
      "demanding = swn.senti_synset('demanding.a.01')\n",
      "212/142:\n",
      "for s in [sman, swoman, tall, maid, smart, awesome, shy, liberal, conservative, moderate, demanding]:\n",
      "    print(s)\n",
      "212/143: gproj(getg(mpol), mpol, 'he'), gproj(getg(mpol), mpol, 'she')\n",
      "212/144: gproj(getg(mpol), mpol, 'man'), gproj(getg(mpol), mpol, 'woman')\n",
      "212/145: gproj(getg(mpol), mpol, 'he'), gproj(getg(mpol), mpol, 'she')\n",
      "212/146: gproj(getg(mpol), mpol, 'male'), gproj(getg(mpol), mpol, 'female')\n",
      "212/147: gproj(getg(mpol), mpol, 'boy'), gproj(getg(mpol), mpol, 'girl')\n",
      "212/148: gproj(getg(mlong), mlong, 'man'), gproj(getg(mlong), mlong, 'woman')\n",
      "212/149: long\n",
      "212/150: longmodels\n",
      "212/151: len(longmodels)\n",
      "212/152: len(longmodels), len(long[49].text)\n",
      "212/153: len(longmodels), len(longlist[49].text)\n",
      "212/154: len(longmodels), len(longlist[49])\n",
      "212/155: mlong = longmodels[49]\n",
      "212/156: gproj(getg(mlong), mlong, 'man'), gproj(getg(mlong), mlong, 'woman')\n",
      "212/157: gproj(getg(mlong), mlong, 'he'), gproj(getg(mlong), mlong, 'she')\n",
      "212/158: gproj(getg(mlong), mlong, 'strong'), gproj(getg(mlong), mlong, 'beautiful')\n",
      "212/159:\n",
      "# distributions of male and female pair words projections on gender vector\n",
      "pairs\n",
      "212/160:\n",
      "# distributions of male and female pair words projections on gender vector\n",
      "pairs = [(x,y) for x,y in pairs if x in mbooks.wv.vocab and y in mbooks.wv.vocab]\n",
      "212/161: pairs\n",
      "212/162:\n",
      "# distributions of male and female pair words projections on gender vector\n",
      "pairs = [(x,y) for x,y in pairs if x in mbooks.wv.vocab and y in mbooks.wv.vocab and x in mmov.wv.vocab and y in mmov.wv.vocab]\n",
      "212/163: pairs\n",
      "212/164: len(pairs)\n",
      "212/165: pair_ones = [x for x,y in pairs] + [y for x,y in pairs]\n",
      "212/166: len(pair_ones)\n",
      "212/167:\n",
      "pairm = [x for x,y in pairs if ]\n",
      "pairf = [y for x,y in pairs]\n",
      "pair_ones = pairm  + pairf\n",
      "212/168:\n",
      "pairm = [x for x,y in pairs]\n",
      "pairf = [y for x,y in pairs]\n",
      "pair_ones = pairm  + pairf\n",
      "212/169: len(pair_ones)\n",
      "212/170: plt.hist([gproj(getg(mbooks), mbooks, x) for x in pair_ones])\n",
      "212/171: plt.hist([gproj(getg(mbooks), mbooks, x) for x in pairm])\n",
      "212/172: plt.hist([gproj(getg(mbooks), mbooks, x) for x in pairf])\n",
      "212/173:\n",
      "fprojs = [gproj(getg(mbooks), mbooks, x) for x in pairf]\n",
      "plt.hist(fprojs)\n",
      "np.mean(fprojs)\n",
      "212/174:\n",
      "fprojs = [gproj(getg(mbooks), mbooks, x) for x in pairf]\n",
      "plt.hist(fprojs)\n",
      "np.mean(fprojs), np.std(fprojs), np.median(fprojs)\n",
      "212/175:\n",
      "fprojs = [gproj(getg(msports), msports, x) for x in pairf]\n",
      "plt.hist(fprojs)\n",
      "np.mean(fprojs), np.std(fprojs), np.median(fprojs)\n",
      "212/176:\n",
      "fprojs = [gproj(getg(msports), msports, x) for x in pairf if x in msports.wv.vocab]\n",
      "plt.hist(fprojs)\n",
      "np.mean(fprojs), np.std(fprojs), np.median(fprojs)\n",
      "212/177:\n",
      "fprojs = [gproj(getg(msports), msports, x) for x in pairm if x in msports.wv.vocab]\n",
      "plt.hist(fprojs)\n",
      "np.mean(fprojs), np.std(fprojs), np.median(fprojs)\n",
      "212/178:\n",
      "mprojs = [gproj(getg(mbooks), mbooks, x) for x in pairm]\n",
      "plt.hist(mprojs)\n",
      "np.mean(mprojs), np.std(mprojs), np.median(mprojs)\n",
      "212/179: pairm\n",
      "212/180: pairs\n",
      "212/181:\n",
      "mwords = ['dad', 'men', 'grandpa', 'uncle', 'he', 'boy', 'brother', 'father', 'he', 'his', 'king', 'himself', 'man']\n",
      "fwords = ['mom', 'women', 'grandma', 'aunt', 'she', 'girl', 'sister', 'mother', 'she', 'her', 'queen', 'herself', 'woman']\n",
      "212/182:\n",
      "fprojs = [gproj(getg(mbooks), mbooks, x) for x in pairf + pairm]\n",
      "plt.hist(fprojs)\n",
      "np.mean(fprojs), np.std(fprojs), np.median(fprojs)\n",
      "212/183: gproj(getg(mpol), mpol, 'rational'), gproj(getg(mpol), mpol, 'romantic')\n",
      "212/184: gproj(getg(mlong), mlong, 'rational'), gproj(getg(mlong), mlong, 'romantic')\n",
      "212/185: len(politics)\n",
      "212/186: gproj(getg(mbooks), mbooks, 'rational'), gproj(getg(mbooks), mbooks, 'romantic')\n",
      "212/187: gproj(getg(mmov), mmov, 'rational'), gproj(getg(mmov), mmov, 'romantic')\n",
      "212/188: gproj(getg(msports), msports, 'rational'), gproj(getg(msports), msports, 'romantic')\n",
      "212/189: gproj(getg(mbooks), mbooks, 'macho'), gproj(getg(mbooks), mbooks, 'sensitive')\n",
      "212/190: gproj(getg(mbooks), mbooks, 'boyish'), gproj(getg(mbooks), mbooks, 'sensitive')\n",
      "212/191: gproj(getg(mmov), mmov, 'he'), gproj(getg(mmov), mmov, 'she')\n",
      "212/192: gproj(getg(mbooks), mbooks, 'he'), gproj(getg(mbooks), mbooks, 'she')\n",
      "212/193: %history\n",
      "212/194: len(adjs_all)\n",
      "212/195:\n",
      "url = 'http://ideonomy.mit.edu/essays/traits.html'\n",
      "file = urllib2.urlopen(url)\n",
      "html = file.read()\n",
      "file.close()\n",
      "soup = BeautifulSoup(html)\n",
      "212/196: soup.find_all('li')[0].contents[0].strip()\n",
      "212/197: adjs = [x.contents[0].strip().lower() for x in soup.find_all('li')]\n",
      "212/198: len(adjectives)\n",
      "212/199: len(adjectives), len(adjs)\n",
      "212/200: len(adjectives), len(adjs), adjs[0], adjectives[9]\n",
      "212/201: len(adjectives), len(adjs), adjs[0], adjectives[0]\n",
      "212/202:\n",
      "adjs_all = adjs + [x for x in adjectives if x not in adjs]\n",
      "len(adjs_all)\n",
      "212/203: list(filter(lambda x: x > 2, [1,2,3]))\n",
      "212/204:\n",
      "adjs_bias = []\n",
      "for x in adjs_all:\n",
      "    try:\n",
      "        s = swn.senti_synset(x + \".a.01\")\n",
      "        if s.pos_score() - s.neg_score() != 0:\n",
      "            adjs_bias.append(x)\n",
      "    except:\n",
      "        continue\n",
      "212/205: len(adjs_bias)\n",
      "212/206: plt.hist([gproj(getg(mpol), mpol, x) for x in adjs_bias])\n",
      "212/207: plt.hist([gproj(getg(mpol), mpol, x) for x in adjs_bias if x in mpol.wv.vocab])\n",
      "212/208: np.sum([gproj(getg(mpol), mpol, x) for x in adjs_bias if x in mpol.wv.vocab])\n",
      "212/209: temp = [gproj(getg(mpol), mpol, x) for x in adjs_bias if x in mpol.wv.vocab]\n",
      "212/210: len(list(filter(lambda x: x > 0, temp)))\n",
      "212/211: len(list(filter(lambda x: x < 0, temp)))\n",
      "212/212: len(list(filter(lambda x: x > 0, temp)))\n",
      "212/213:\n",
      "def get_adj_lists(m):\n",
      "    temp = sorted([(x, gproj(getg(m), m, x)) for x in adjs_bias], key=lambda x: x[1])\n",
      "    return temp[:300], temp[len(temp)-300:]\n",
      "212/214:\n",
      "apol1, apol2 = get_adj_lists(mpol)\n",
      "len(apol1), len(apol2)\n",
      "212/215:\n",
      "def get_adj_lists(m):\n",
      "    temp = sorted([(x, gproj(getg(m), m, x)) for x in adjs_bias if x in m.wv.vocab], key=lambda x: x[1])\n",
      "    return temp[:300], temp[len(temp)-300:]\n",
      "212/216:\n",
      "apol1, apol2 = get_adj_lists(mpol)\n",
      "len(apol1), len(apol2)\n",
      "212/217: abooks1, abooks2 = get_adj_lists(mbooks)\n",
      "212/218: amov1, amov2 = get_adj_lists(mmov)\n",
      "212/219:\n",
      "def get_gdir(m):\n",
      "    print(gproj(getg(m), m, 'he'), gproj(getg(m), m, 'she'))\n",
      "212/220: get_gdir(mpol), get_gdir(mbooks)\n",
      "212/221: get_gdir(mpol), get_gdir(mbooks), get_gdir(mmov)\n",
      "212/222:\n",
      "def get_gdir(m):\n",
      "    h, s = gproj(getg(m), m, 'he'), gproj(getg(m), m, 'she')\n",
      "    print(h, s)\n",
      "    return h, s, (h+s)/2\n",
      "212/223:\n",
      "hpol, spol, avgpol = get_gdir(mpol)\n",
      "hbooks, sbooks, avgbooks = get_gdir(mbooks)\n",
      "hmov, smov, avgmov = get_gdir(mmov)\n",
      "212/224:\n",
      "def swn_score(x):\n",
      "    try:\n",
      "        s = swn.senti_synset(x + \".a.01\")\n",
      "        return s.pos_score() - s.neg_score()\n",
      "    except:\n",
      "        return\n",
      "212/225:\n",
      "def swn_score(x):\n",
      "    try:\n",
      "        s = swn.senti_synset(x + \".a.01\")\n",
      "        return s.pos_score() - s.neg_score()\n",
      "    except:\n",
      "        return 0\n",
      "212/226:\n",
      "def swn_bias(words):\n",
      "    return sum([swn_score(x) for x in words])\n",
      "212/227:\n",
      "def swn_bias(m):\n",
      "    a1, a2 = get_adj_lists(m)\n",
      "    return sum([swn_score(x) for x in a1]), sum([swn_score(x) for x in a2])\n",
      "212/228: apol1[0]\n",
      "212/229: apol1[0], apol2[0]\n",
      "212/230: apol1[0], apol2[100]\n",
      "212/231: apol1[100], apol2[100]\n",
      "212/232: apol1[100], apol2[200]\n",
      "212/233: apol1[200], apol2[200]\n",
      "212/234:\n",
      "def get_adj_lists(m):\n",
      "    temp = sorted([(x, gproj(getg(m), m, x)) for x in adjs_bias if x in m.wv.vocab], key=lambda x: x[1])\n",
      "    return [x[0] for x in temp[:300]], [x[0] for x in temp[len(temp)-300:]]\n",
      "212/235:\n",
      "apol1, apol2 = get_adj_lists(mpol)\n",
      "len(apol1), len(apol2)\n",
      "212/236: abooks1, abooks2 = get_adj_lists(mbooks)\n",
      "212/237: amov1, amov2 = get_adj_lists(mmov)\n",
      "212/238: swn_bias(mpol)\n",
      "212/239: swn_bias(mpol), swn_bias(mbooks), swn_bias(mbooks)\n",
      "212/240: swn_bias(mpol), swn_bias(mbooks), swn_bias(mmov)\n",
      "212/241: sum([swn_score(x) for x in apol1])\n",
      "212/242: len([swn_score(x) for x in apol1])\n",
      "212/243: len([swn_score(x) for x in apol2])\n",
      "212/244: sum([swn_score(x) for x in apol2])\n",
      "212/245: sorted([(x, swn_score(x)) for x in apol2], lambda x: x[1])[\"5\"]\n",
      "212/246: sorted([(x, swn_score(x)) for x in apol2], lambda x: x[1])[:5]\n",
      "212/247: sorted([(x, swn_score(x)) for x in apol2], lambda x: x[1])\n",
      "212/248: sorted([(x, swn_score(x)) for x in apol2], key=lambda x: x[1])[:5]\n",
      "212/249: sorted([(x, gproj(getg(mpol), mpol, x)) for x in apol2], key=lambda x: x[1])[:5]\n",
      "212/250: sorted([(x, gproj(getg(mpol), mpol, x)) for x in apol2], key=lambda x: x[1])[:10]\n",
      "212/251: sorted([(x, gproj(getg(mbooks), mbooks, x)) for x in abooks2], key=lambda x: x[1])[:10]\n",
      "212/252: sorted([(x, gproj(getg(mpol), mpol, x)) for x in apol2], key=lambda x: x[1], reverse=True)[:10]\n",
      "212/253: sorted([(x, gproj(getg(mbooks), mbooks, x)) for x in abooks2], key=lambda x: x[1], reverse=True)[:10]\n",
      "212/254:\n",
      "def weighted_sb(m):\n",
      "    a1, a2 = get_adj_lists(m)\n",
      "    return sum([abs(gproj(getg(m), m, x))*swn_score(x) for x in a1]), \\\n",
      "           sum([abs(gproj(getg(m), m, x))*swn_score(x) for x in a2])\n",
      "212/255: weighted_sb(mpol)\n",
      "212/256: weighted_sb(mbooks)\n",
      "212/257: weighted_sb(mmov)\n",
      "212/258:\n",
      "def norm_sb(m):\n",
      "    a1, a2 = get_adj_lists(m)\n",
      "    h, s, hs = get_gdir(m)\n",
      "    return sum([abs(gproj(getg(m), m, x)/s)*swn_score(x) for x in a1]), \\\n",
      "           sum([abs(gproj(getg(m), m, x)/h)*swn_score(x) for x in a2])\n",
      "212/259: norm_sb(mpol), norm_sb(mbooks), norm_sb(mmov)\n",
      "212/260:\n",
      "def scale_sb(m):\n",
      "    a1, a2 = get_adj_lists(m)\n",
      "    h, s, hs = get_gdir(m)\n",
      "    return sum([abs(gproj(getg(m), m, x) - hs)*swn_score(x) for x in a1]), \\\n",
      "           sum([abs(gproj(getg(m), m, x) - hs)*swn_score(x) for x in a2])\n",
      "212/261: scale_sb(mpol), scale_sb(mbooks), scale_sb(mmov)\n",
      "212/262:\n",
      "abooks1, abooks2 = get_adj_lists(mbooks)\n",
      "len(abooks1), len(abooks2)\n",
      "212/263:\n",
      "amov1, amov2 = get_adj_lists(mmov)\n",
      "len(amov1), len(amov2)\n",
      "212/264:\n",
      "def get_adj_lists(m):\n",
      "    temp = sorted([(x, gproj(getg(m), m, x)) for x in adjs_bias if x in m.wv.vocab], key=lambda x: x[1])\n",
      "    return [x[0] for x in temp[:200]], [x[0] for x in temp[len(temp)-200:]]\n",
      "212/265:\n",
      "def weighted_sb(m):\n",
      "    a1, a2 = get_adj_lists(m)\n",
      "    return sum([abs(gproj(getg(m), m, x))*swn_score(x) for x in a1]), \\\n",
      "           sum([abs(gproj(getg(m), m, x))*swn_score(x) for x in a2])\n",
      "212/266: weighted_sb(mpol), weighted_sb(mbooks), weighted_sb(mmov)\n",
      "212/267:\n",
      "def get_adj_lists(m):\n",
      "    temp = sorted([(x, gproj(getg(m), m, x)) for x in adjs_bias if x in m.wv.vocab], key=lambda x: x[1])\n",
      "    return [x[0] for x in temp[:100]], [x[0] for x in temp[len(temp)-100:]]\n",
      "212/268:\n",
      "def weighted_sb(m):\n",
      "    a1, a2 = get_adj_lists(m)\n",
      "    return sum([abs(gproj(getg(m), m, x))*swn_score(x) for x in a1]), \\\n",
      "           sum([abs(gproj(getg(m), m, x))*swn_score(x) for x in a2])\n",
      "212/269: weighted_sb(mpol), weighted_sb(mbooks), weighted_sb(mmov)\n",
      "212/270:\n",
      "def norm_sb(m):\n",
      "    a1, a2 = get_adj_lists(m)\n",
      "    h, s, hs = get_gdir(m)\n",
      "    return sum([abs(gproj(getg(m), m, x)/s)*swn_score(x) for x in a1]), \\\n",
      "           sum([abs(gproj(getg(m), m, x)/h)*swn_score(x) for x in a2])\n",
      "212/271: norm_sb(mpol), norm_sb(mbooks), norm_sb(mmov)\n",
      "212/272:\n",
      "def scale_sb(m):\n",
      "    a1, a2 = get_adj_lists(m)\n",
      "    h, s, hs = get_gdir(m)\n",
      "    return sum([abs(gproj(getg(m), m, x) - hs)*swn_score(x) for x in a1]), \\\n",
      "           sum([abs(gproj(getg(m), m, x) - hs)*swn_score(x) for x in a2])\n",
      "212/273: scale_sb(mpol), scale_sb(mbooks), scale_sb(mmov)\n",
      "212/274: len(temp)\n",
      "212/275: temp = [gproj(getg(mpol), mpol, x) for x in adjs_bias if x in mpol.wv.vocab]\n",
      "212/276: len(temp)\n",
      "212/277: np.std(temp)\n",
      "212/278: np.min(temp), np.max(temp)\n",
      "212/279: len(filter(lambda x: x > 0, temp))\n",
      "212/280: len(list(filter(lambda x: x > 0, temp)))\n",
      "212/281:\n",
      "tmin, tmax = np.min(temp), np.max(temp)\n",
      "tmin, tmax\n",
      "212/282: tstd = np.std(temp)\n",
      "212/283:\n",
      "tstd = np.std(temp)\n",
      "tstd\n",
      "212/284: len(list(filter(lambda x: x > tmin + tstd)))\n",
      "212/285: len(list(filter(lambda x: x > tmin + tstd, temp)))\n",
      "212/286: len(list(filter(lambda x: x < tmin + tstd, temp)))\n",
      "212/287: len(list(filter(lambda x: x < tmin + 3*tstd, temp)))\n",
      "212/288: len(list(filter(lambda x: x > tmax - 3*tstd, temp)))\n",
      "212/289: plt.hist(temp)\n",
      "212/290:\n",
      "tmean, tstd = np.mean(temp), np.std(temp)\n",
      "tstd\n",
      "212/291:\n",
      "tmean, tstd = np.mean(temp), np.std(temp)\n",
      "tmean, tstd\n",
      "212/292: len(list(filter(lambda x: x < tmean - 3*tstd, temp)))\n",
      "212/293: len(list(filter(lambda x: x > tmean + 3*tstd, temp)))\n",
      "212/294: len(list(filter(lambda x: x > tmean + 2*tstd, temp)))\n",
      "212/295: len(list(filter(lambda x: x < tmean - 2*tstd, temp)))\n",
      "212/296: len(list(filter(lambda x: x < tmean - tstd, temp)))\n",
      "212/297: len(list(filter(lambda x: x > tmean + tstd, temp)))\n",
      "212/298:\n",
      "def get_adj_lists(m):\n",
      "    temp = sorted([(x, gproj(getg(m), m, x)) for x in adjs_bias \\\n",
      "                   if x in m.wv.vocab], key=lambda x: x[1])\n",
      "    return [x[0] for x in temp[:75]], \\\n",
      "           [x[0] for x in temp[len(temp)-75:]]\n",
      "212/299: weighted_sb(mpol), weighted_sb(mbooks), weighted_sb(mmov)\n",
      "212/300: norm_sb(mpol), norm_sb(mbooks), norm_sb(mmov)\n",
      "212/301: scale_sb(mpol), scale_sb(mbooks), scale_sb(mmov)\n",
      "212/302: swn_bias(mpol), swn_bias(mbooks), swn_bias(mmov)\n",
      "212/303: grams = [gproj(getg(mpol), mpol, x) for x in grammar if x in mpol.wv.vocab]\n",
      "212/304: [gproj(getg(mpol), mpol, x) for x in ['a', 'an', 'the'] if x in mpol.wv.vocab]\n",
      "212/305: [gproj(getg(mpol), mpol, x) for x in ['if', 'a', 'the', 'are', 'or'] if x in mpol.wv.vocab]\n",
      "212/306: [gproj(getg(mpol), mpol, x) for x in ['above', 'about', 'ago', 'although'] if x in mpol.wv.vocab]\n",
      "212/307: [gproj(getg(mpol), mpol, x) for x in ['above', 'about', 'ago', 'although', 'and', 'an', 'as'] if x in mpol.wv.vocab]\n",
      "212/308: [gproj(getg(mpol), mpol, x) for x in ['above', 'about', 'ago', 'although'] if x in mpol.wv.vocab]\n",
      "212/309:\n",
      "with open('grammar.csv') as csv_file:\n",
      "    grammar = csv_file.read().split(\",\")\n",
      "212/310: grammar[:10]\n",
      "212/311: grams = [gproj(getg(mpol), mpol, x) for x in grammar if x in mpol.wv.vocab]\n",
      "212/312: np.mean(grams), np.std(grams), np.min(grams), np.max(grams)\n",
      "212/313: plt.hist(grams)\n",
      "212/314:\n",
      "bins = np.linspace(-0.3, 0.3, 50)\n",
      "plt.hist(grams, bins, label=\"grammar\")\n",
      "plt.hist(temp, bins, label=\"adjectives\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "212/315:\n",
      "bins = np.linspace(-0.3, 0.3, 50)\n",
      "plt.hist(grams, bins, alpha=0.5, label=\"grammar\")\n",
      "plt.hist(temp, bins, alpha=0.5, label=\"adjectives\")\n",
      "212/316:\n",
      "bins = np.linspace(-0.3, 0.3, 50)\n",
      "plt.hist(grams, bins, alpha=0.5, label=\"grammar\")\n",
      "plt.hist(temp, bins, alpha=0.5, label=\"adjectives\")\n",
      "plt.legend()\n",
      "plt.show()\n",
      "212/317:\n",
      "bins = np.linspace(-0.3, 0.3, 50)\n",
      "plt.hist(grams, bins, alpha=0.75, label=\"grammar\")\n",
      "plt.hist(temp, bins, alpha=0.5, label=\"adjectives\")\n",
      "plt.legend()\n",
      "plt.show()\n",
      "212/318:\n",
      "bins = np.linspace(-0.3, 0.3, 50)\n",
      "plt.hist(temp, bins, alpha=0.5, label=\"adjectives\")\n",
      "plt.hist(grams, bins, alpha=0.5, label=\"grammar\")\n",
      "plt.legend()\n",
      "plt.show()\n",
      "212/319:\n",
      "bins = np.linspace(-0.3, 0.3, 20)\n",
      "plt.hist(temp, bins, alpha=0.5, label=\"adjectives\")\n",
      "plt.hist(grams, bins, alpha=0.5, label=\"grammar\")\n",
      "plt.legend()\n",
      "plt.show()\n",
      "212/320:\n",
      "bins = np.linspace(-0.3, 0.3, 10)\n",
      "plt.hist(temp, bins, alpha=0.5, label=\"adjectives\")\n",
      "plt.hist(grams, bins, alpha=0.5, label=\"grammar\")\n",
      "plt.legend()\n",
      "plt.show()\n",
      "212/321:\n",
      "bins = np.linspace(-0.3, 0.3, 15)\n",
      "plt.hist(temp, bins, alpha=0.5, label=\"adjectives\")\n",
      "plt.hist(grams, bins, alpha=0.5, label=\"grammar\")\n",
      "plt.legend()\n",
      "plt.show()\n",
      "212/322:\n",
      "bins = np.linspace(-0.3, 0.3, 20)\n",
      "plt.hist(temp, bins, alpha=0.5, label=\"adjectives\")\n",
      "plt.hist(grams, bins, alpha=0.5, label=\"grammar\")\n",
      "plt.legend()\n",
      "plt.show()\n",
      "212/323:\n",
      "bins = np.linspace(-0.3, 0.3, 20)\n",
      "plt.hist(temp, bins, alpha=0.5, label=\"adjectives\")\n",
      "plt.hist(grams, bins, alpha=0.5, label=\"grammar\", color=\"green\")\n",
      "plt.legend()\n",
      "plt.show()\n",
      "212/324:\n",
      "bins = np.linspace(-0.3, 0.3, 20)\n",
      "plt.hist(temp, bins, alpha=0.5, label=\"adjectives\")\n",
      "plt.hist(grams, bins, alpha=0.75, label=\"grammar\", color=\"green\")\n",
      "plt.legend()\n",
      "plt.show()\n",
      "212/325:\n",
      "bins = np.linspace(-0.3, 0.3, 20)\n",
      "plt.hist(temp, bins, alpha=0.5, label=\"adjectives\")\n",
      "plt.hist(grams, bins, alpha=0.7, label=\"grammar\", color=\"green\")\n",
      "plt.legend()\n",
      "plt.show()\n",
      "212/326: np.mean(grams), np.std(grams), np.min(grams), np.max(grams)\n",
      "212/327: num_adjs = 70\n",
      "212/328:\n",
      "def get_adj_lists(m):\n",
      "    temp = sorted([(x, gproj(getg(m), m, x)) for x in adjs_bias \\\n",
      "                   if x in m.wv.vocab], key=lambda x: x[1])\n",
      "    return [x[0] for x in temp[:num_adjs]], \\\n",
      "           [x[0] for x in temp[len(temp)-75:]]\n",
      "212/329:\n",
      "def get_adj_lists(m):\n",
      "    temp = sorted([(x, gproj(getg(m), m, x)) for x in adjs_bias \\\n",
      "                   if x in m.wv.vocab], key=lambda x: x[1])\n",
      "    return [x[0] for x in temp[:num_adjs]], \\\n",
      "           [x[0] for x in temp[len(temp)-num_adjs:]]\n",
      "212/330: norm_sb(mpol), norm_sb(mbooks), norm_sb(mmov)\n",
      "212/331: scale_sb(mpol), scale_sb(mbooks), scale_sb(mmov)\n",
      "212/332: swn_bias(mpol), swn_bias(mbooks), swn_bias(mmov)\n",
      "212/333: weighted_sb(mpol), weighted_sb(mbooks), weighted_sb(mmov)\n",
      "212/334: num_adjs = 80\n",
      "212/335: weighted_sb(mpol), weighted_sb(mbooks), weighted_sb(mmov)\n",
      "212/336: norm_sb(mpol), norm_sb(mbooks), norm_sb(mmov)\n",
      "212/337: scale_sb(mpol), scale_sb(mbooks), scale_sb(mmov)\n",
      "212/338: swn_bias(mpol), swn_bias(mbooks), swn_bias(mmov)\n",
      "212/339: num_adjs = 75\n",
      "212/340: weighted_sb(mpol), weighted_sb(mbooks), weighted_sb(mmov)\n",
      "212/341: norm_sb(mpol), norm_sb(mbooks), norm_sb(mmov)\n",
      "212/342: scale_sb(mpol), scale_sb(mbooks), scale_sb(mmov)\n",
      "212/343: swn_bias(mpol), swn_bias(mbooks), swn_bias(mmov)\n",
      "212/344:\n",
      "def get_gdir(m, n):\n",
      "    if n == 1:\n",
      "        return hpol, spol, avgpol\n",
      "    elif n == 2:\n",
      "        return hbooks, sbooks, avgbooks\n",
      "    return hmov, smov, avgmov\n",
      "212/345:\n",
      "def get_gdir(m):\n",
      "    h, s = gproj(getg(m), m, 'he'), gproj(getg(m), m, 'she')\n",
      "    print(h, s)\n",
      "    return h, s, (h+s)/2\n",
      "212/346:\n",
      "#run\n",
      "num_adjs = 74\n",
      "212/347: weighted_sb(mpol), weighted_sb(mbooks), weighted_sb(mmov)\n",
      "212/348: norm_sb(mpol), norm_sb(mbooks), norm_sb(mmov)\n",
      "212/349: scale_sb(mpol), scale_sb(mbooks), scale_sb(mmov)\n",
      "212/350: swn_bias(mpol), swn_bias(mbooks), swn_bias(mmov)\n",
      "212/351:\n",
      "#run\n",
      "num_adjs = 70\n",
      "212/352: weighted_sb(mpol), weighted_sb(mbooks), weighted_sb(mmov)\n",
      "212/353: norm_sb(mpol), norm_sb(mbooks), norm_sb(mmov)\n",
      "212/354: scale_sb(mpol), scale_sb(mbooks), scale_sb(mmov)\n",
      "212/355: swn_bias(mpol), swn_bias(mbooks), swn_bias(mmov)\n",
      "212/356: len(list(filter(lambda x: x < tmean - 2*.056, temp)))\n",
      "212/357: len(list(filter(lambda x: x > tmean + 2*.056, temp)))\n",
      "212/358:\n",
      "#run\n",
      "num_adjs = 32\n",
      "212/359: weighted_sb(mpol), weighted_sb(mbooks), weighted_sb(mmov)\n",
      "212/360: norm_sb(mpol), norm_sb(mbooks), norm_sb(mmov)\n",
      "212/361: scale_sb(mpol), scale_sb(mbooks), scale_sb(mmov)\n",
      "212/362: swn_bias(mpol), swn_bias(mbooks), swn_bias(mmov)\n",
      "212/363: len(list(filter(lambda x: x < tmean - 2*.05, temp)))\n",
      "212/364: len(list(filter(lambda x: x > tmean + 2*.05, temp)))\n",
      "212/365:\n",
      "#run\n",
      "num_adjs = 45\n",
      "212/366: weighted_sb(mpol), weighted_sb(mbooks), weighted_sb(mmov)\n",
      "212/367: norm_sb(mpol), norm_sb(mbooks), norm_sb(mmov)\n",
      "212/368: scale_sb(mpol), scale_sb(mbooks), scale_sb(mmov)\n",
      "212/369: swn_bias(mpol), swn_bias(mbooks), swn_bias(mmov)\n",
      "212/370: weighted_sb(mpol), weighted_sb(mbooks), weighted_sb(mmov)\n",
      "212/371:\n",
      "#run\n",
      "num_adjs = 50\n",
      "212/372: weighted_sb(mpol), weighted_sb(mbooks), weighted_sb(mmov)\n",
      "212/373: norm_sb(mpol), norm_sb(mbooks), norm_sb(mmov)\n",
      "212/374: scale_sb(mpol), scale_sb(mbooks), scale_sb(mmov)\n",
      "212/375: swn_bias(mpol), swn_bias(mbooks), swn_bias(mmov)\n",
      "212/376: weighted_sb(mpol), weighted_sb(mbooks), weighted_sb(mmov)\n",
      "212/377:\n",
      "#run\n",
      "num_adjs = 40\n",
      "212/378: weighted_sb(mpol), weighted_sb(mbooks), weighted_sb(mmov)\n",
      "212/379: norm_sb(mpol), norm_sb(mbooks), norm_sb(mmov)\n",
      "212/380: scale_sb(mpol), scale_sb(mbooks), scale_sb(mmov)\n",
      "212/381: swn_bias(mpol), swn_bias(mbooks), swn_bias(mmov)\n",
      "212/382:\n",
      "# cluster all adjectives + profs, and then look at average M/F projections\n",
      "# OR\n",
      "# separate by male and female, and then cluster all adjs + profs...\n",
      "212/383: # 1. cluster all adjectives and profs, and then look at average M/F projs\n",
      "212/384:\n",
      "# cluster all adjectives + profs, and then look at average M/F projs\n",
      "# OR\n",
      "# separate by male and female, and then cluster all adjs + profs...\n",
      "212/385: # 2. separate by M/F, and then cluster for all adjs + profs\n",
      "212/386:\n",
      "from sklearn.cluster import KMeans\n",
      "from sklearn.mixture import GaussianMixture\n",
      "212/387: len(profs)\n",
      "212/388: pplwords = adjs_all + profs\n",
      "212/389:\n",
      "def getwv(m, x):\n",
      "    return m.wv.get_vector(x)\n",
      "212/390:\n",
      "Xpol = np.array([getwv(mpol, x) for x in pplwords])\n",
      "Xpol.shape\n",
      "212/391:\n",
      "Xpol = np.array([getwv(mpol, x) for x in pplwords if x in mpol.wv.vocab])\n",
      "Xpol.shape\n",
      "212/392:\n",
      "Xpol = [getwv(mpol, x) for x in pplwords if x in mpol.wv.vocab]\n",
      "Xpol.shape\n",
      "212/393: getwv(mpol, \"hi\")\n",
      "212/394: np.array([getwv(mpol, \"hi\"),getwv(mpol, \"hi\")]).shape\n",
      "212/395:\n",
      "pplwords = adjs_all + profs\n",
      "len(pplwords)\n",
      "212/396:\n",
      "Xpol = [getwv(mpol, x) for x in pplwords[:2] if x in mpol.wv.vocab]\n",
      "Xpol.shape\n",
      "212/397:\n",
      "Xpol = [getwv(mpol, x) for x in pplwords if x in mpol.wv.vocab]\n",
      "Xpol.shape\n",
      "212/398: pplwords[1200:]\n",
      "212/399: proflist\n",
      "212/400:\n",
      "pplwords = adjs_all + proflist\n",
      "len(pplwords)\n",
      "212/401:\n",
      "Xpol = [getwv(mpol, x) for x in pplwords if x in mpol.wv.vocab]\n",
      "Xpol.shape\n",
      "212/402:\n",
      "Xpol = np.array([getwv(mpol, x) for x in pplwords if x in mpol.wv.vocab])\n",
      "Xpol.shape\n",
      "212/403:\n",
      "kmeanspol = []\n",
      "inertias = []\n",
      "for i in range(1, 11):\n",
      "    temp = KMeans(n_clusters=5*i, random_state=0).fit(X)\n",
      "    kmeanspol.append(temp)\n",
      "    inertias.append(temp.inertia_)\n",
      "212/404:\n",
      "kmeanspol = []\n",
      "inertias = []\n",
      "for i in range(1, 11):\n",
      "    temp = KMeans(n_clusters=5*i, random_state=0).fit(Xpol)\n",
      "    kmeanspol.append(temp)\n",
      "    inertias.append(temp.inertia_)\n",
      "212/405: plt.plot(inertias)\n",
      "212/406:\n",
      "kmeanspol = []\n",
      "inertias = []\n",
      "for i in range(1, 16):\n",
      "    temp = KMeans(n_clusters=5*i, random_state=0).fit(Xpol)\n",
      "    kmeanspol.append(temp)\n",
      "    inertias.append(temp.inertia_)\n",
      "212/407: plt.plot(inertias)\n",
      "212/408:\n",
      "kmeanspol = []\n",
      "inertias = []\n",
      "for i in range(1, 20):\n",
      "    temp = KMeans(n_clusters=5*i, random_state=0).fit(Xpol)\n",
      "    kmeanspol.append(temp)\n",
      "    inertias.append(temp.inertia_)\n",
      "212/409: plt.plot(inertias)\n",
      "212/410:\n",
      "kmeanspol = []\n",
      "inertias = []\n",
      "for i in range(1, 30):\n",
      "    temp = KMeans(n_clusters=5*i, random_state=0).fit(Xpol)\n",
      "    kmeanspol.append(temp)\n",
      "    inertias.append(temp.inertia_)\n",
      "212/411: plt.plot(inertias)\n",
      "212/412:\n",
      "Xbooks = np.array([getwv(mbooks, x) for x in pplwords if x in mbooks.wv.vocab])\n",
      "Xbooks.shape\n",
      "212/413:\n",
      "kmeansbooks = []\n",
      "inertias = []\n",
      "for i in range(1, 30):\n",
      "    temp = KMeans(n_clusters=5*i, random_state=0).fit(Xbooks)\n",
      "    kmeansbooks.append(temp)\n",
      "    inertias.append(temp.inertia_)\n",
      "212/414:\n",
      "kmeanspol = []\n",
      "inertiaspol = []\n",
      "for i in range(1, 30):\n",
      "    temp = KMeans(n_clusters=5*i, random_state=0).fit(Xpol)\n",
      "    kmeanspol.append(temp)\n",
      "    inertiaspol.append(temp.inertia_)\n",
      "212/415: plt.plot(inertiaspol)\n",
      "212/416:\n",
      "kmeansbooks = []\n",
      "inertiasbooks = []\n",
      "for i in range(1, 30):\n",
      "    temp = KMeans(n_clusters=5*i, random_state=0).fit(Xbooks)\n",
      "    kmeansbooks.append(temp)\n",
      "    inertiasbooks.append(temp.inertia_)\n",
      "212/417:\n",
      "Xmov = np.array([getwv(mmov, x) for x in pplwords if x in mmov.wv.vocab])\n",
      "Xmov.shape\n",
      "212/418:\n",
      "kmeansmov = []\n",
      "inertiasmov = []\n",
      "for i in range(1, 30):\n",
      "    temp = KMeans(n_clusters=5*i, random_state=0).fit(Xmov)\n",
      "    kmeansmov.append(temp)\n",
      "    inertiasmov.append(temp.inertia_)\n",
      "212/419: plt.plot([5*i for i in range(1,30)], inertiasbooks)\n",
      "212/420: plt.plot([5*i for i in range(1,20)], inertiasbooks)\n",
      "212/421:\n",
      "kmeansbooks = []\n",
      "inertiasbooks = []\n",
      "for i in range(1, 20):\n",
      "    temp = KMeans(n_clusters=5*i, random_state=0).fit(Xbooks)\n",
      "    kmeansbooks.append(temp)\n",
      "    inertiasbooks.append(temp.inertia_)\n",
      "212/422: plt.plot([5*i for i in range(1,20)], inertiasbooks)\n",
      "212/423:\n",
      "kmeansmov = []\n",
      "inertiasmov = []\n",
      "for i in range(1, 20):\n",
      "    temp = KMeans(n_clusters=5*i, random_state=0).fit(Xmov)\n",
      "    kmeansmov.append(temp)\n",
      "    inertiasmov.append(temp.inertia_)\n",
      "212/424: plt.plot([5*i for i in range(1,20)], inertiasmov)\n",
      "212/425:\n",
      "kmeanspol = []\n",
      "inertiaspol = []\n",
      "for i in range(1, 20):\n",
      "    temp = KMeans(n_clusters=5*i, random_state=0).fit(Xpol)\n",
      "    kmeanspol.append(temp)\n",
      "    inertiaspol.append(temp.inertia_)\n",
      "212/426: plt.plot([5*i for i in range(1,20)], inertiaspol)\n",
      "212/427: kmeanspol[5]\n",
      "212/428: kmeanspol[5].labels_\n",
      "212/429: len(Xbooks)\n",
      "212/430: len(kmeanspol[5].labels_)\n",
      "212/431:\n",
      "\n",
      "for x in range(len(Xbooks)):\n",
      "    kmeansbooks[5].labels_\n",
      "212/432: len(kmeansbooks[5].labels_)\n",
      "212/433:\n",
      "wordsb = list(filter(lamda x: x in mbooks.wv.vocab, pplwords))\n",
      "dbooks = {k:[] for k in range(30)}\n",
      "for i in range(len(Xbooks)):\n",
      "    dbooks[kmeans[5].labels_[i]].append(wordsb[i])\n",
      "212/434:\n",
      "wordsb = list(filter(lambda x: x in mbooks.wv.vocab, pplwords))\n",
      "dbooks = {k:[] for k in range(30)}\n",
      "for i in range(len(Xbooks)):\n",
      "    dbooks[kmeans[5].labels_[i]].append(wordsb[i])\n",
      "212/435:\n",
      "wordsb = list(filter(lambda x: x in mbooks.wv.vocab, pplwords))\n",
      "dbooks = {k:[] for k in range(30)}\n",
      "for i in range(len(Xbooks)):\n",
      "    dbooks[kmeansbooks[5].labels_[i]].append(wordsb[i])\n",
      "212/436: dbooks\n",
      "212/437:\n",
      "wordsb = list(filter(lambda x: x in mbooks.wv.vocab, pplwords))\n",
      "dbooks = {k:[] for k in range(40)}\n",
      "for i in range(len(Xbooks)):\n",
      "    dbooks[kmeansbooks[7].labels_[i]].append(wordsb[i])\n",
      "212/438: dbooks\n",
      "212/439:\n",
      "wordsb = list(filter(lambda x: x in mbooks.wv.vocab, pplwords))\n",
      "dbooks = {k:[] for k in range(50)}\n",
      "for i in range(len(Xbooks)):\n",
      "    dbooks[kmeansbooks[9].labels_[i]].append(wordsb[i])\n",
      "212/440: dbooks\n",
      "212/441:\n",
      "part1 = np.array([getwv(mbooks, x) for x in adjs_all if x in mbooks.wv.vocab])\n",
      "part1 = np.array([getwv(mbooks, x) for x in proflist if x in mbooks.wv.vocab])\n",
      "212/442:\n",
      "part1 = np.array([getwv(mbooks, x) for x in adjs_all if x in mbooks.wv.vocab])\n",
      "part2 = np.array([getwv(mbooks, x) for x in proflist if x in mbooks.wv.vocab])\n",
      "212/443: np.mean(part1)\n",
      "212/444: np.mean(part1, axis=1)\n",
      "212/445: len(np.mean(part1, axis=1))\n",
      "212/446: len(np.mean(part1, axis=0))\n",
      "212/447: part1\n",
      "212/448: part1 - np.mean(part1, axis=0)\n",
      "212/449: np.concatenate(part1 - np.mean(part1, axis=0), part2 - np.mean(part2, axis=0), axis=0)\n",
      "212/450: np.concatenate((part1 - np.mean(part1, axis=0), part2 - np.mean(part2, axis=0)), axis=0)\n",
      "212/451: np.concatenate((part1 - np.mean(part1, axis=0), part2 - np.mean(part2, axis=0)), axis=0).shape\n",
      "212/452: Xbooks2 = np.concatenate((part1 - np.mean(part1, axis=0), part2 - np.mean(part2, axis=0)), axis=0).shape\n",
      "212/453: temp = KMeans(n_clusters=5*i, random_state=0).fit(Xbooks2)\n",
      "212/454: Xbooks2.shape\n",
      "212/455: Xbooks2 = np.concatenate((part1 - np.mean(part1, axis=0), part2 - np.mean(part2, axis=0)), axis=0).shape\n",
      "212/456: Xbooks2.shape\n",
      "212/457: Xbooks2 = np.concatenate((part1 - np.mean(part1, axis=0), part2 - np.mean(part2, axis=0)), axis=0)\n",
      "212/458: Xbooks2.shape\n",
      "212/459: temp = KMeans(n_clusters=5*i, random_state=0).fit(Xbooks2)\n",
      "212/460: temp = KMeans(n_clusters=30, random_state=0).fit(Xbooks2)\n",
      "212/461:\n",
      "wordsb = list(filter(lambda x: x in mbooks.wv.vocab, pplwords))\n",
      "dbooks = {k:[] for k in range(50)}\n",
      "for i in range(len(Xbooks)):\n",
      "    dbooks[temp.labels_[i]].append(wordsb[i])\n",
      "212/462: dbooks\n",
      "212/463: temp = KMeans(n_clusters=50, random_state=0).fit(Xbooks2)\n",
      "212/464:\n",
      "wordsb = list(filter(lambda x: x in mbooks.wv.vocab, pplwords))\n",
      "dbooks = {k:[] for k in range(50)}\n",
      "for i in range(len(Xbooks)):\n",
      "    dbooks[temp.labels_[i]].append(wordsb[i])\n",
      "212/465: dbooks\n",
      "212/466: temp = KMeans(n_clusters=50, random_state=0).fit(Xbooks2)\n",
      "212/467:\n",
      "wordsb = list(filter(lambda x: x in mbooks.wv.vocab, pplwords))\n",
      "dbooks = {k:[] for k in range(50)}\n",
      "for i in range(len(Xbooks)):\n",
      "    dbooks[temp.labels_[i]].append(wordsb[i])\n",
      "212/468: dbooks\n",
      "212/469:\n",
      "wordsb = list(filter(lambda x: x in mbooks.wv.vocab, pplwords))\n",
      "dbooks = {k:[] for k in range(70)}\n",
      "for i in range(len(Xbooks)):\n",
      "    dbooks[temp.labels_[i]].append(wordsb[i])\n",
      "212/470: dbooks\n",
      "212/471: temp = KMeans(n_clusters=70, random_state=0).fit(Xbooks2)\n",
      "212/472:\n",
      "wordsb = list(filter(lambda x: x in mbooks.wv.vocab, pplwords))\n",
      "dbooks = {k:[] for k in range(70)}\n",
      "for i in range(len(Xbooks)):\n",
      "    dbooks[temp.labels_[i]].append(wordsb[i])\n",
      "212/473: dbooks\n",
      "212/474:\n",
      "sorted([(x, gproj(getg(mpol), mpol, x), swn_score(x)) for x in apol2], \\\n",
      "       key=lambda x: x[1], reverse=True)[:10]\n",
      "212/475:\n",
      "sorted([(x, gproj(getg(mbooks), mbooks, x), swn_score(x)) for x in abooks2], \\\n",
      "       key=lambda x: x[1], reverse=True)[:10]\n",
      "212/476:\n",
      "sorted([(x, gproj(getg(mmov), mmov, x), swn_score(x)) for x in amov2], \\\n",
      "       key=lambda x: x[1], reverse=True)[:10]\n",
      "212/477:\n",
      "sorted([(x, gproj(getg(mbooks), mbooks, x), swn_score(x)) for x in abooks2], \\\n",
      "       key=lambda x: x[1], reverse=True)[:10]\n",
      "212/478:\n",
      "sorted([(x, gproj(getg(mmov), mmov, x), swn_score(x)) for x in amov2], \\\n",
      "       key=lambda x: x[1], reverse=True)[:10]\n",
      "212/479:\n",
      "sorted([(x, gproj(getg(mmov), mmov, x), swn_score(x)) for x in amov2], \\\n",
      "       key=lambda x: x[1])[:10]\n",
      "212/480:\n",
      "sorted([(x, gproj(getg(mmov), mmov, x), swn_score(x)) for x in amov1], \\\n",
      "       key=lambda x: x[1])[:10]\n",
      "212/481:\n",
      "sorted([(x, gproj(getg(mpol), mpol, x), swn_score(x)) for x in apol1], \\\n",
      "       key=lambda x: x[1])[:10]\n",
      "212/482:\n",
      "sorted([(x, gproj(getg(mpol), mpol, x), swn_score(x)) for x in apol2], \\\n",
      "       key=lambda x: x[1], reverse=True)[:5]\n",
      "212/483:\n",
      "sorted([(x, gproj(getg(mmov), mmov, x), swn_score(x)) for x in amov2], \\\n",
      "       key=lambda x: x[1], reverse=True)[:5]\n",
      "212/484:\n",
      "sorted([(x, gproj(getg(mmov), mmov, x), swn_score(x)) for x in amov1], \\\n",
      "       key=lambda x: x[1])[:5]\n",
      "212/485:\n",
      "sorted([(x, gproj(getg(mpol), mpol, x), swn_score(x)) for x in apol1], \\\n",
      "       key=lambda x: x[1])[:5]\n",
      "212/486:\n",
      "sorted([(x, gproj(getg(mbooks), mbooks, x), swn_score(x)) for x in abooks2], \\\n",
      "       key=lambda x: x[1], reverse=True)[:5]\n",
      "212/487:\n",
      "sorted([(x, gproj(getg(mbooks), mbooks, x), swn_score(x)) for x in abooks2], \\\n",
      "       key=lambda x: x[1], reverse=True)[:5]\n",
      "212/488:\n",
      "sorted([(x, gproj(getg(mbooks), mbooks, x), swn_score(x)) for x in abooks2], \\\n",
      "       key=lambda x: x[1])[:5]\n",
      "212/489:\n",
      "sorted([(x, gproj(getg(mbooks), mbooks, x), swn_score(x)) for x in abooks1], \\\n",
      "       key=lambda x: x[1])[:5]\n",
      "212/490: plt.hist(temp)\n",
      "212/491: temp = [gproj(getg(mpol), mpol, x) for x in adjs_bias if x in mpol.wv.vocab]\n",
      "212/492:\n",
      "plt.hist(temp)\n",
      "plt.title(Projections of Adjectives on Gender v=Vector)\n",
      "plt.show()\n",
      "212/493:\n",
      "plt.hist(temp)\n",
      "plt.title(\"Projections of Adjectives on Gender Vector - Politics\")\n",
      "plt.show()\n",
      "212/494:\n",
      "#run\n",
      "num_adjs = 45\n",
      "212/495: weighted_sb(mpol), weighted_sb(mbooks), weighted_sb(mmov)\n",
      "212/496: norm_sb(mpol), norm_sb(mbooks), norm_sb(mmov)\n",
      "212/497: scale_sb(mpol), scale_sb(mbooks), scale_sb(mmov)\n",
      "212/498: swn_bias(mpol), swn_bias(mbooks), swn_bias(mmov)\n",
      "212/499: len(list(filter(lambda x: x < tmean - tstd, temp)))\n",
      "212/500: len(list(filter(lambda x: x > tmean + tstd, temp)))\n",
      "212/501:\n",
      "#run\n",
      "num_adjs = 75\n",
      "212/502: weighted_sb(mpol), weighted_sb(mbooks), weighted_sb(mmov)\n",
      "212/503: norm_sb(mpol), norm_sb(mbooks), norm_sb(mmov)\n",
      "212/504: scale_sb(mpol), scale_sb(mbooks), scale_sb(mmov)\n",
      "212/505: swn_bias(mpol), swn_bias(mbooks), swn_bias(mmov)\n",
      "212/506:\n",
      "#run\n",
      "num_adjs = 80\n",
      "212/507: weighted_sb(mpol), weighted_sb(mbooks), weighted_sb(mmov)\n",
      "212/508: norm_sb(mpol), norm_sb(mbooks), norm_sb(mmov)\n",
      "212/509: scale_sb(mpol), scale_sb(mbooks), scale_sb(mmov)\n",
      "212/510: swn_bias(mpol), swn_bias(mbooks), swn_bias(mmov)\n",
      "212/511:\n",
      "#run\n",
      "num_adjs = 70\n",
      "212/512: weighted_sb(mpol), weighted_sb(mbooks), weighted_sb(mmov)\n",
      "212/513: norm_sb(mpol), norm_sb(mbooks), norm_sb(mmov)\n",
      "212/514: scale_sb(mpol), scale_sb(mbooks), scale_sb(mmov)\n",
      "212/515: swn_bias(mpol), swn_bias(mbooks), swn_bias(mmov)\n",
      "212/516:\n",
      "#run\n",
      "num_adjs = 45\n",
      "212/517: weighted_sb(mpol), weighted_sb(mbooks), weighted_sb(mmov)\n",
      "212/518: norm_sb(mpol), norm_sb(mbooks), norm_sb(mmov)\n",
      "212/519: scale_sb(mpol), scale_sb(mbooks), scale_sb(mmov)\n",
      "212/520: swn_bias(mpol), swn_bias(mbooks), swn_bias(mmov)\n",
      "212/521: len(list(filter(lambda x: x in mpol.wv.vocab, adjs_all)))\n",
      "212/522:\n",
      "#run\n",
      "num_adjs = 310\n",
      "212/523:\n",
      "#run\n",
      "num_adjs = 314\n",
      "212/524: weighted_sb(mpol), weighted_sb(mbooks), weighted_sb(mmov)\n",
      "212/525: norm_sb(mpol), norm_sb(mbooks), norm_sb(mmov)\n",
      "212/526: scale_sb(mpol), scale_sb(mbooks), scale_sb(mmov)\n",
      "212/527: swn_bias(mpol), swn_bias(mbooks), swn_bias(mmov)\n",
      "212/528: len(list(filter(lambda x: x > 0, temp)))\n",
      "212/529:\n",
      "#run\n",
      "num_adjs = 209\n",
      "212/530:\n",
      "#run\n",
      "num_adjs = 200\n",
      "212/531: weighted_sb(mpol), weighted_sb(mbooks), weighted_sb(mmov)\n",
      "212/532: norm_sb(mpol), norm_sb(mbooks), norm_sb(mmov)\n",
      "212/533: scale_sb(mpol), scale_sb(mbooks), scale_sb(mmov)\n",
      "212/534: swn_bias(mpol), swn_bias(mbooks), swn_bias(mmov)\n",
      "212/535:\n",
      "#run\n",
      "num_adjs = 209\n",
      "212/536: weighted_sb(mpol), weighted_sb(mbooks), weighted_sb(mmov)\n",
      "212/537: norm_sb(mpol), norm_sb(mbooks), norm_sb(mmov)\n",
      "212/538: scale_sb(mpol), scale_sb(mbooks), scale_sb(mmov)\n",
      "212/539: swn_bias(mpol), swn_bias(mbooks), swn_bias(mmov)\n",
      "212/540:\n",
      "#run\n",
      "num_adjs = 250\n",
      "212/541: swn_bias(mpol), swn_bias(mbooks), swn_bias(mmov)\n",
      "212/542:\n",
      "temp = GaussianMixture(n_clusters=70, random_state=0).fit(Xbooks2)\n",
      "wordsb = list(filter(lambda x: x in mbooks.wv.vocab, pplwords))\n",
      "dbooks = {k:[] for k in range(70)}\n",
      "for i in range(len(Xbooks)):\n",
      "    dgbooks[temp.labels_[i]].append(wordsb[i])\n",
      "212/543:\n",
      "temp = GaussianMixture(n_components=50, random_state=0).fit(Xbooks2)\n",
      "wordsb = list(filter(lambda x: x in mbooks.wv.vocab, pplwords))\n",
      "dbooks = {k:[] for k in range(70)}\n",
      "for i in range(len(Xbooks)):\n",
      "    dgbooks[temp.labels_[i]].append(wordsb[i])\n",
      "212/544:\n",
      "temp = GaussianMixture(n_components=50, random_state=0).fit(Xbooks2)\n",
      "wordsb = list(filter(lambda x: x in mbooks.wv.vocab, pplwords))\n",
      "dgbooks = {k:[] for k in range(50)}\n",
      "for i in range(len(Xbooks)):\n",
      "    dgbooks[temp.labels_[i]].append(wordsb[i])\n",
      "212/545:\n",
      "temp = GaussianMixture(n_components=50, random_state=0).fit(Xbooks2)\n",
      "wordsb = list(filter(lambda x: x in mbooks.wv.vocab, pplwords))\n",
      "dgbooks = {k:[] for k in range(50)}\n",
      "for i in range(len(Xbooks2)):\n",
      "    dgbooks[temp.predict(Xbooks2[i]].append(wordsb[i])\n",
      "212/546:\n",
      "temp = GaussianMixture(n_components=50, random_state=0).fit(Xbooks2)\n",
      "wordsb = list(filter(lambda x: x in mbooks.wv.vocab, pplwords))\n",
      "dgbooks = {k:[] for k in range(50)}\n",
      "for i in range(len(Xbooks2)):\n",
      "    dgbooks[temp.predict(Xbooks2)[i]].append(wordsb[i])\n",
      "212/547: dgbooks\n",
      "212/548:\n",
      "# cluster all adjectives, and then look at average M/F projs\n",
      "# OR\n",
      "# separate by male and female, and then cluster all adjs...\n",
      "212/549:\n",
      "Xpol = np.array([getwv(mpol, x) for x in adjs_all if x in mpol.wv.vocab])\n",
      "Xpol.shape\n",
      "212/550:\n",
      "kmeanspol = []\n",
      "inertiaspol = []\n",
      "for i in range(1, 20):\n",
      "    temp = KMeans(n_clusters=5*i, random_state=0).fit(Xpol)\n",
      "    kmeanspol.append(temp)\n",
      "    inertiaspol.append(temp.inertia_)\n",
      "212/551: plt.plot([5*i for i in range(1,20)], inertiaspol)\n",
      "212/552:\n",
      "temp = KMeans(n_clusters=40, random_state=0).fit(Xbooks)\n",
      "wordsb = list(filter(lambda x: x in mbooks.wv.vocab, pplwords))\n",
      "dbooks = {k:[] for k in range(40)}\n",
      "for i in range(len(Xbooks)):\n",
      "    dbooks[temp.labels_[i]].append(wordsb[i])\n",
      "212/553: dbooks\n",
      "212/554:\n",
      "temp = KMeans(n_clusters=40, random_state=0).fit(Xpol)\n",
      "wordsp = list(filter(lambda x: x in mpol.wv.vocab, adjs_all))\n",
      "dpol = {k:[] for k in range(40)}\n",
      "for i in range(len(Xpol)):\n",
      "    dpol[temp.labels_[i]].append(wordsp[i])\n",
      "212/555: dpol\n",
      "212/556:\n",
      "kmeanspol = []\n",
      "inertiaspol = []\n",
      "for i in range(1, 25):\n",
      "    temp = KMeans(n_clusters=5*i, random_state=0).fit(Xpol)\n",
      "    kmeanspol.append(temp)\n",
      "    inertiaspol.append(temp.inertia_)\n",
      "212/557: plt.plot([5*i for i in range(1,25)], inertiaspol)\n",
      "212/558:\n",
      "temp = KMeans(n_clusters=60, random_state=0).fit(Xpol)\n",
      "wordsp = list(filter(lambda x: x in mpol.wv.vocab, adjs_all))\n",
      "dpol = {k:[] for k in range(60)}\n",
      "for i in range(len(Xpol)):\n",
      "    dpol[temp.labels_[i]].append(wordsp[i])\n",
      "212/559: dpol\n",
      "212/560:\n",
      "temp = GaussianMixture(n_components=50, random_state=0).fit(Xpol)\n",
      "wordsb = list(filter(lambda x: x in mpol.wv.vocab, adjs_all))\n",
      "dgpol = {k:[] for k in range(50)}\n",
      "for i in range(len(Xpol)):\n",
      "    dgpol[temp.predict(Xpol)[i]].append(wordsb[i])\n",
      "212/561: dgbooks\n",
      "212/562: dgpol\n",
      "212/563:\n",
      "def getwv(m, w):\n",
      "    return m.wv.get_vector(w)\n",
      "212/564: m.wv.most_similar(positive=[\"hi\"])\n",
      "212/565: mpol.wv.most_similar(positive=[\"hi\"])\n",
      "212/566: mpol.wv.most_similar(positive=[\"man\"])\n",
      "212/567: mpol.wv.most_similar(positive=[getwv(\"man\") + getwv(\"bad\")])\n",
      "212/568: mpol.wv.most_similar(positive=[getwv(mpol, \"man\") + getwv(mpol, \"bad\")])\n",
      "212/569: mpol.wv.most_similar(positive=[getwv(mpol, \"man\") + getwv(mpol, \"bad\")], topn=20)\n",
      "212/570: mpol.wv.most_similar(positive=[\"man\", \"bad\"], topn=20)\n",
      "212/571: mpol.wv.most_similar(positive=[\"man\", \"bad\", \"politician\"], topn=20)\n",
      "212/572: mpol.wv.most_similar(positive=[\"man\", \"bad\"], negative=[\"woman\"], topn=20)\n",
      "212/573: t = getwv(mpol, \"man\") + getwv(mpol, \"king\") - getwv(mpol, \"woman\")\n",
      "212/574: mpol.wv.most_similar(positive=[t], topn=20)\n",
      "212/575: t = getwv(mpol, \"woman\") + getwv(mpol, \"king\") - getwv(mpol, \"man\")\n",
      "212/576: mpol.wv.most_similar(positive=[t], topn=20)\n",
      "212/577: mpol.wv.most_similar(positive=[t], topn=10)\n",
      "212/578: t = getwv(mpol, \"woman\") + getwv(mpol, \"doctor\") - getwv(mpol, \"man\")\n",
      "212/579: mpol.wv.most_similar(positive=[t], topn=10)\n",
      "212/580: t = getwv(mpol, \"woman\") + getwv(mpol, \"programmer\") - getwv(mpol, \"man\")\n",
      "212/581: mpol.wv.most_similar(positive=[t], topn=10)\n",
      "212/582: t = getwv(mpol, \"woman\") + getwv(mpol, \"black\") - getwv(mpol, \"man\")\n",
      "212/583: mpol.wv.most_similar(positive=[t], topn=10)\n",
      "212/584: gpol\n",
      "212/585: gpol = getg(mpol)\n",
      "212/586: t = getwv(mpol, \"asian\") + gpol\n",
      "212/587:\n",
      "stereotypes_m = ['president', 'dean', \\\n",
      "                 'director', 'officer', 'manager', 'captain', 'coach', 'commissioner', 'sergeant', \"principal\", \"programmer\"]\n",
      "stereotypes_f = ['teacher', 'author', 'secretary', 'writer', \\\n",
      "                 'editor', \"nurse\", \"housekeeper\", \"maid\", \"dancer\", \"artist\", \"homemaker\"]\n",
      "male = [\"male\", \"man\", \"brother\", \"he\", \"him\", \"son\", \"father\", \"boy\", \"masculine\", \"mr\", \"john\"]\n",
      "female = [\"female\", \"woman\", \"sister\", \"she\", \"her\", \\\n",
      "          \"daughter\", \"mother\", \"girl\", \"feminine\", \"mrs\", \"mary\"]\n",
      "def cos_sim(m, x, y):\n",
      "    wv1 = m.wv.get_vector(x)\n",
      "    wv2 = m.wv.get_vector(y)\n",
      "    sim = np.dot(wv1, wv2)/(np.linalg.norm(wv1)*np.linalg.norm(wv2))\n",
      "    return sim\n",
      "def cos_sim2(wv1, wv2):\n",
      "    return np.dot(wv1, wv2)/(np.linalg.norm(wv1)*np.linalg.norm(wv2))\n",
      "def s_word(m, w, A, B):\n",
      "    return np.mean([cos_sim(m, w, a) for a in A]) - np.mean([cos_sim(m, w, b) for b in B])\n",
      "def effect_size(m, X, Y, A, B):\n",
      "    num = np.mean([s_word(m, x, A, B) for x in X]) - np.mean([s_word(m, y, A, B) for y in Y])\n",
      "    return num/np.std([s_word(m, x, A, B) for x in X + Y])\n",
      "def wbias(m, l, shuf):\n",
      "    if shuf:\n",
      "#         shuffle(stereotypes_m)\n",
      "#         shuffle(stereotypes_f)\n",
      "        shuffle(male)\n",
      "        shuffle(female)\n",
      "    return abs(effect_size(m, male[:l], female[:l], stereotypes_m, stereotypes_f))\n",
      "212/588: cos_sim2(t, getwv(mpol, \"submissive\"))\n",
      "212/589: cos_sim2(t, getwv(mpol, \"weak\"))\n",
      "212/590: cos_sim(mpol, \"asian\", \"weak\")\n",
      "212/591: cos_sim2(t, getwv(mpol, \"weak\")), cos_sim(mpol, \"asian\", \"weak\")\n",
      "212/592: cos_sim2(getwv(mpol, \"asian\") + gpol, getwv(mpol, \"weak\")), cos_sim(mpol, \"asian\", \"weak\")\n",
      "212/593: cos_sim2(getwv(mpol, \"white\") + gpol, getwv(mpol, \"weak\")), cos_sim(mpol, \"white\", \"weak\")\n",
      "212/594: cos_sim2(getwv(mpol, \"black\") + gpol, getwv(mpol, \"weak\")), cos_sim(mpol, \"white\", \"weak\")\n",
      "212/595: cos_sim2(getwv(mpol, \"hispanic\") + gpol, getwv(mpol, \"weak\")), cos_sim(mpol, \"white\", \"weak\")\n",
      "212/596: cos_sim2(getwv(mpol, \"hispanic\") + gpol, getwv(mpol, \"weak\")), cos_sim(mpol, \"hispanic\", \"weak\")\n",
      "212/597: cos_sim2(getwv(mpol, \"black\") + gpol, getwv(mpol, \"weak\")), cos_sim(mpol, \"black\", \"weak\")\n",
      "212/598: cos_sim2(getwv(mpol, \"black\") + gpol, getwv(mpol, \"weak\")) - cos_sim(mpol, \"black\", \"weak\")\n",
      "212/599: cos_sim2(getwv(mpol, \"asian\") + gpol, getwv(mpol, \"weak\")) - cos_sim(mpol, \"asian\", \"weak\")\n",
      "212/600: cos_sim2(getwv(mpol, \"black\") + gpol, getwv(mpol, \"feminine\")) - cos_sim(mpol, \"black\", \"feminine\")\n",
      "212/601: cos_sim2(getwv(mpol, \"asian\") + gpol, getwv(mpol, \"feminine\")) - cos_sim(mpol, \"asian\", \"feminine\")\n",
      "212/602: cos_sim2(getwv(mpol, \"black\") + gpol, getwv(mpol, \"strong\")) - cos_sim(mpol, \"black\", \"strong\")\n",
      "212/603: cos_sim2(getwv(mpol, \"asian\") + gpol, getwv(mpol, \"strong\")) - cos_sim(mpol, \"asian\", \"strong\")\n",
      "212/604: gpol = getwv(mpol, \"man\") - getwv(mpol, \"woman\")\n",
      "212/605: cos_sim2(getwv(mpol, \"asian\") + gpol, getwv(mpol, \"strong\")) - cos_sim(mpol, \"asian\", \"strong\")\n",
      "212/606: cos_sim2(getwv(mpol, \"black\") + gpol, getwv(mpol, \"strong\")) - cos_sim(mpol, \"black\", \"strong\")\n",
      "212/607: gpol = getg(mpol)\n",
      "212/608: cos_sim2(getwv(mpol, \"asian\") + gpol, getwv(mpol, \"strong\")) - cos_sim(mpol, \"asian\", \"strong\")\n",
      "212/609: cos_sim2(getwv(mpol, \"black\") + gpol, getwv(mpol, \"strong\")) - cos_sim(mpol, \"black\", \"strong\")\n",
      "212/610: cos_sim2(getwv(mpol, \"asian\") + gpol, getwv(mpol, \"criminal\")) - cos_sim(mpol, \"asian\", \"criminal\")\n",
      "212/611: cos_sim2(getwv(mpol, \"black\") + gpol, getwv(mpol, \"criminal\")) - cos_sim(mpol, \"black\", \"criminal\")\n",
      "212/612: gpol = getwv(mpol, \"man\") - getwv(mpol, \"woman\")\n",
      "212/613: cos_sim2(getwv(mpol, \"asian\") + gpol, getwv(mpol, \"criminal\")) - cos_sim(mpol, \"asian\", \"criminal\")\n",
      "212/614: cos_sim2(getwv(mpol, \"black\") + gpol, getwv(mpol, \"criminal\")) - cos_sim(mpol, \"black\", \"criminal\")\n",
      "212/615: gpol = -getwv(mpol, \"man\") + getwv(mpol, \"woman\")\n",
      "212/616: cos_sim2(getwv(mpol, \"asian\") + gpol, getwv(mpol, \"criminal\")) - cos_sim(mpol, \"asian\", \"criminal\")\n",
      "212/617: cos_sim2(getwv(mpol, \"black\") + gpol, getwv(mpol, \"criminal\")) - cos_sim(mpol, \"black\", \"criminal\")\n",
      "212/618: cos_sim2(getwv(mpol, \"asian\") - gpol, getwv(mpol, \"criminal\")) - cos_sim(mpol, \"asian\", \"criminal\")\n",
      "212/619: cos_sim2(getwv(mpol, \"black\") - gpol, getwv(mpol, \"criminal\")) - cos_sim(mpol, \"black\", \"criminal\")\n",
      "212/620: gpol = getg(mpol)\n",
      "212/621: cos_sim2(getwv(mpol, \"asian\") - gpol, getwv(mpol, \"criminal\")) - cos_sim(mpol, \"asian\", \"criminal\")\n",
      "212/622: cos_sim2(getwv(mpol, \"black\") - gpol, getwv(mpol, \"criminal\")) - cos_sim(mpol, \"black\", \"criminal\")\n",
      "212/623: cos_sim2(getwv(mpol, \"hispanic\") - gpol, getwv(mpol, \"criminal\")) - cos_sim(mpol, \"hispanic\", \"criminal\")\n",
      "212/624: cos_sim2(getwv(mpol, \"caucasian\") - gpol, getwv(mpol, \"criminal\")) - cos_sim(mpol, \"caucasian\", \"criminal\")\n",
      "212/625: cos_sim2(getwv(mpol, \"white\") - gpol, getwv(mpol, \"criminal\")) - cos_sim(mpol, \"white\", \"criminal\")\n",
      "212/626:\n",
      "cos_sim2(getwv(mpol, \"asian\") - gpol, getwv(mpol, \"lazy\")) - cos_sim(mpol, \"asian\", \"lazy\"), \\\n",
      "cos_sim2(getwv(mpol, \"black\") - gpol, getwv(mpol, \"lazy\")) - cos_sim(mpol, \"black\", \"lazy\"), \\\n",
      "cos_sim2(getwv(mpol, \"hispanic\") - gpol, getwv(mpol, \"lazy\")) - cos_sim(mpol, \"hispanic\", \"lazy\"), \\\n",
      "cos_sim2(getwv(mpol, \"white\") - gpol, getwv(mpol, \"lazy\")) - cos_sim(mpol, \"white\", \"lazy\")\n",
      "212/627:\n",
      "cos_sim2(getwv(mpol, \"asian\") - gpol, getwv(mpol, \"lazy\")) - cos_sim(mpol, \"asian\", \"lazy\"), \\\n",
      "cos_sim2(getwv(mpol, \"black\") - gpol, getwv(mpol, \"lazy\")) - cos_sim(mpol, \"black\", \"lazy\"), \\\n",
      "cos_sim2(getwv(mpol, \"african\") - gpol, getwv(mpol, \"lazy\")) - cos_sim(mpol, \"african\", \"lazy\"), \\\n",
      "cos_sim2(getwv(mpol, \"european\") - gpol, getwv(mpol, \"lazy\")) - cos_sim(mpol, \"european\", \"lazy\")\n",
      "cos_sim2(getwv(mpol, \"hispanic\") - gpol, getwv(mpol, \"lazy\")) - cos_sim(mpol, \"hispanic\", \"lazy\"), \\\n",
      "cos_sim2(getwv(mpol, \"white\") - gpol, getwv(mpol, \"lazy\")) - cos_sim(mpol, \"white\", \"lazy\")\n",
      "212/628:\n",
      "cos_sim2(getwv(mpol, \"asian\") - gpol, getwv(mpol, \"lazy\")) - cos_sim(mpol, \"asian\", \"lazy\"), \\\n",
      "cos_sim2(getwv(mpol, \"black\") - gpol, getwv(mpol, \"lazy\")) - cos_sim(mpol, \"black\", \"lazy\"), \\\n",
      "cos_sim2(getwv(mpol, \"african\") - gpol, getwv(mpol, \"lazy\")) - cos_sim(mpol, \"african\", \"lazy\"), \\\n",
      "cos_sim2(getwv(mpol, \"european\") - gpol, getwv(mpol, \"lazy\")) - cos_sim(mpol, \"european\", \"lazy\"), \\\n",
      "cos_sim2(getwv(mpol, \"hispanic\") - gpol, getwv(mpol, \"lazy\")) - cos_sim(mpol, \"hispanic\", \"lazy\"), \\\n",
      "cos_sim2(getwv(mpol, \"white\") - gpol, getwv(mpol, \"lazy\")) - cos_sim(mpol, \"white\", \"lazy\")\n",
      "212/629: gpol\n",
      "212/630: np.min(gpol)\n",
      "212/631: np.min(gpol), np.max(gpol)\n",
      "212/632: rando = [random.uniform]\n",
      "212/633: from random import random\n",
      "212/634: rando = [random.uniform(-.23, 0.33) for x in range(100)]\n",
      "212/635: import random\n",
      "212/636: rando = [random.uniform(-.23, 0.33) for x in range(100)]\n",
      "212/637: rando\n",
      "212/638:\n",
      "cos_sim2(getwv(mpol, \"asian\") - rando, getwv(mpol, \"lazy\")) - cos_sim(mpol, \"asian\", \"lazy\"), \\\n",
      "cos_sim2(getwv(mpol, \"black\") - rando, getwv(mpol, \"lazy\")) - cos_sim(mpol, \"black\", \"lazy\"), \\\n",
      "cos_sim2(getwv(mpol, \"african\") - rando, getwv(mpol, \"lazy\")) - cos_sim(mpol, \"african\", \"lazy\"), \\\n",
      "cos_sim2(getwv(mpol, \"european\") - rando, getwv(mpol, \"lazy\")) - cos_sim(mpol, \"european\", \"lazy\"), \\\n",
      "cos_sim2(getwv(mpol, \"hispanic\") - rando, getwv(mpol, \"lazy\")) - cos_sim(mpol, \"hispanic\", \"lazy\"), \\\n",
      "cos_sim2(getwv(mpol, \"white\") - rando, getwv(mpol, \"lazy\")) - cos_sim(mpol, \"white\", \"lazy\")\n",
      "212/639: rando = [random.uniform(-.23, 0.33) for x in range(100)]\n",
      "212/640:\n",
      "cos_sim2(getwv(mpol, \"asian\") - rando, getwv(mpol, \"lazy\")) - cos_sim(mpol, \"asian\", \"lazy\"), \\\n",
      "cos_sim2(getwv(mpol, \"black\") - rando, getwv(mpol, \"lazy\")) - cos_sim(mpol, \"black\", \"lazy\"), \\\n",
      "cos_sim2(getwv(mpol, \"african\") - rando, getwv(mpol, \"lazy\")) - cos_sim(mpol, \"african\", \"lazy\"), \\\n",
      "cos_sim2(getwv(mpol, \"european\") - rando, getwv(mpol, \"lazy\")) - cos_sim(mpol, \"european\", \"lazy\"), \\\n",
      "cos_sim2(getwv(mpol, \"hispanic\") - rando, getwv(mpol, \"lazy\")) - cos_sim(mpol, \"hispanic\", \"lazy\"), \\\n",
      "cos_sim2(getwv(mpol, \"white\") - rando, getwv(mpol, \"lazy\")) - cos_sim(mpol, \"white\", \"lazy\")\n",
      "212/641: rando = [random.uniform(-.23, 0.33) for x in range(100)]\n",
      "212/642:\n",
      "cos_sim2(getwv(mpol, \"asian\") - rando, getwv(mpol, \"lazy\")) - cos_sim(mpol, \"asian\", \"lazy\"), \\\n",
      "cos_sim2(getwv(mpol, \"black\") - rando, getwv(mpol, \"lazy\")) - cos_sim(mpol, \"black\", \"lazy\"), \\\n",
      "cos_sim2(getwv(mpol, \"african\") - rando, getwv(mpol, \"lazy\")) - cos_sim(mpol, \"african\", \"lazy\"), \\\n",
      "cos_sim2(getwv(mpol, \"european\") - rando, getwv(mpol, \"lazy\")) - cos_sim(mpol, \"european\", \"lazy\"), \\\n",
      "cos_sim2(getwv(mpol, \"hispanic\") - rando, getwv(mpol, \"lazy\")) - cos_sim(mpol, \"hispanic\", \"lazy\"), \\\n",
      "cos_sim2(getwv(mpol, \"white\") - rando, getwv(mpol, \"lazy\")) - cos_sim(mpol, \"white\", \"lazy\")\n",
      "212/643: rando = [random.uniform(-.23, 0.33) for x in range(100)]\n",
      "212/644:\n",
      "cos_sim2(getwv(mpol, \"asian\") - rando, getwv(mpol, \"lazy\")) - cos_sim(mpol, \"asian\", \"lazy\"), \\\n",
      "cos_sim2(getwv(mpol, \"black\") - rando, getwv(mpol, \"lazy\")) - cos_sim(mpol, \"black\", \"lazy\"), \\\n",
      "cos_sim2(getwv(mpol, \"african\") - rando, getwv(mpol, \"lazy\")) - cos_sim(mpol, \"african\", \"lazy\"), \\\n",
      "cos_sim2(getwv(mpol, \"european\") - rando, getwv(mpol, \"lazy\")) - cos_sim(mpol, \"european\", \"lazy\"), \\\n",
      "cos_sim2(getwv(mpol, \"hispanic\") - rando, getwv(mpol, \"lazy\")) - cos_sim(mpol, \"hispanic\", \"lazy\"), \\\n",
      "cos_sim2(getwv(mpol, \"white\") - rando, getwv(mpol, \"lazy\")) - cos_sim(mpol, \"white\", \"lazy\")\n",
      "212/645: rando = [random.uniform(-.23, 0.33) for x in range(100)]\n",
      "212/646:\n",
      "cos_sim2(getwv(mpol, \"asian\") - rando, getwv(mpol, \"lazy\")) - cos_sim(mpol, \"asian\", \"lazy\"), \\\n",
      "cos_sim2(getwv(mpol, \"black\") - rando, getwv(mpol, \"lazy\")) - cos_sim(mpol, \"black\", \"lazy\"), \\\n",
      "cos_sim2(getwv(mpol, \"african\") - rando, getwv(mpol, \"lazy\")) - cos_sim(mpol, \"african\", \"lazy\"), \\\n",
      "cos_sim2(getwv(mpol, \"european\") - rando, getwv(mpol, \"lazy\")) - cos_sim(mpol, \"european\", \"lazy\"), \\\n",
      "cos_sim2(getwv(mpol, \"hispanic\") - rando, getwv(mpol, \"lazy\")) - cos_sim(mpol, \"hispanic\", \"lazy\"), \\\n",
      "cos_sim2(getwv(mpol, \"white\") - rando, getwv(mpol, \"lazy\")) - cos_sim(mpol, \"white\", \"lazy\")\n",
      "212/647:\n",
      "cos_sim2(getwv(mpol, \"asian\") - gpol, getwv(mpol, \"loud\")) - cos_sim(mpol, \"asian\", \"loud\"), \\\n",
      "cos_sim2(getwv(mpol, \"black\") - gpol, getwv(mpol, \"loud\")) - cos_sim(mpol, \"black\", \"loud\"), \\\n",
      "cos_sim2(getwv(mpol, \"african\") - gpol, getwv(mpol, \"loud\")) - cos_sim(mpol, \"african\", \"loud\"), \\\n",
      "cos_sim2(getwv(mpol, \"european\") - gpol, getwv(mpol, \"loud\")) - cos_sim(mpol, \"european\", \"loud\"), \\\n",
      "cos_sim2(getwv(mpol, \"hispanic\") - gpol, getwv(mpol, \"loud\")) - cos_sim(mpol, \"hispanic\", \"loud\"), \\\n",
      "cos_sim2(getwv(mpol, \"white\") - gpol, getwv(mpol, \"loud\")) - cos_sim(mpol, \"white\", \"loud\")\n",
      "212/648:\n",
      "cos_sim2(getwv(mpol, \"asian\") - rando, getwv(mpol, \"angry\")) - cos_sim(mpol, \"asian\", \"angry\"), \\\n",
      "cos_sim2(getwv(mpol, \"black\") - rando, getwv(mpol, \"angry\")) - cos_sim(mpol, \"black\", \"angry\"), \\\n",
      "cos_sim2(getwv(mpol, \"african\") - rando, getwv(mpol, \"angry\")) - cos_sim(mpol, \"african\", \"angry\"), \\\n",
      "cos_sim2(getwv(mpol, \"european\") - rando, getwv(mpol, \"angry\")) - cos_sim(mpol, \"european\", \"angry\"), \\\n",
      "cos_sim2(getwv(mpol, \"hispanic\") - rando, getwv(mpol, \"angry\")) - cos_sim(mpol, \"hispanic\", \"angry\"), \\\n",
      "cos_sim2(getwv(mpol, \"white\") - rando, getwv(mpol, \"angry\")) - cos_sim(mpol, \"white\", \"angry\")\n",
      "212/649: rando = [random.uniform(-.23, 0.33) for x in range(100)]\n",
      "212/650:\n",
      "cos_sim2(getwv(mpol, \"asian\") - rando, getwv(mpol, \"angry\")) - cos_sim(mpol, \"asian\", \"angry\"), \\\n",
      "cos_sim2(getwv(mpol, \"black\") - rando, getwv(mpol, \"angry\")) - cos_sim(mpol, \"black\", \"angry\"), \\\n",
      "cos_sim2(getwv(mpol, \"african\") - rando, getwv(mpol, \"angry\")) - cos_sim(mpol, \"african\", \"angry\"), \\\n",
      "cos_sim2(getwv(mpol, \"european\") - rando, getwv(mpol, \"angry\")) - cos_sim(mpol, \"european\", \"angry\"), \\\n",
      "cos_sim2(getwv(mpol, \"hispanic\") - rando, getwv(mpol, \"angry\")) - cos_sim(mpol, \"hispanic\", \"angry\"), \\\n",
      "cos_sim2(getwv(mpol, \"white\") - rando, getwv(mpol, \"angry\")) - cos_sim(mpol, \"white\", \"angry\")\n",
      "212/651:\n",
      "cos_sim2(getwv(mpol, \"asian\") - gpol, getwv(mpol, \"angry\")) - cos_sim(mpol, \"asian\", \"angry\"), \\\n",
      "cos_sim2(getwv(mpol, \"black\") - gpol, getwv(mpol, \"angry\")) - cos_sim(mpol, \"black\", \"angry\"), \\\n",
      "cos_sim2(getwv(mpol, \"african\") - gpol, getwv(mpol, \"angry\")) - cos_sim(mpol, \"african\", \"angry\"), \\\n",
      "cos_sim2(getwv(mpol, \"european\") - gpol, getwv(mpol, \"angry\")) - cos_sim(mpol, \"european\", \"angry\"), \\\n",
      "cos_sim2(getwv(mpol, \"hispanic\") - gpol, getwv(mpol, \"angry\")) - cos_sim(mpol, \"hispanic\", \"angry\"), \\\n",
      "cos_sim2(getwv(mpol, \"white\") - gpol, getwv(mpol, \"angry\")) - cos_sim(mpol, \"white\", \"angry\")\n",
      "212/652:\n",
      "cos_sim2(getwv(mpol, \"asian\") - gpol, getwv(mpol, \"angry\")) - cos_sim(-1*gpol, \"asian\", \"angry\"), \\\n",
      "cos_sim2(getwv(mpol, \"black\") - gpol, getwv(mpol, \"angry\")) - cos_sim(-1*gpol, \"black\", \"angry\"), \\\n",
      "cos_sim2(getwv(mpol, \"african\") - gpol, getwv(mpol, \"angry\")) - cos_sim(-1*gpol, \"african\", \"angry\"), \\\n",
      "cos_sim2(getwv(mpol, \"european\") - gpol, getwv(mpol, \"angry\")) - cos_sim(-1*gpol, \"european\", \"angry\"), \\\n",
      "cos_sim2(getwv(mpol, \"hispanic\") - gpol, getwv(mpol, \"angry\")) - cos_sim(-1*gpol, \"hispanic\", \"angry\"), \\\n",
      "cos_sim2(getwv(mpol, \"white\") - gpol, getwv(mpol, \"angry\")) - cos_sim(-1*gpol, \"white\", \"angry\")\n",
      "212/653:\n",
      "cos_sim2(getwv(mpol, \"asian\") - gpol, getwv(mpol, \"angry\")) - cos_sim2(-1(gpol, getwv(mpol, \"angry\")), \\\n",
      "cos_sim2(getwv(mpol, \"black\") - gpol, getwv(mpol, \"angry\")) - cos_sim2(-1(gpol, getwv(mpol, \"angry\")), \\\n",
      "cos_sim2(getwv(mpol, \"african\") - gpol, getwv(mpol, \"angry\")) - cos_sim2(-1(gpol, getwv(mpol, \"angry\")), \\\n",
      "cos_sim2(getwv(mpol, \"european\") - gpol, getwv(mpol, \"angry\")) - cos_sim2(-1(gpol, getwv(mpol, \"angry\")), \\\n",
      "cos_sim2(getwv(mpol, \"hispanic\") - gpol, getwv(mpol, \"angry\")) - cos_sim2(-1(gpol, getwv(mpol, \"angry\")), \\\n",
      "cos_sim2(getwv(mpol, \"white\") - gpol, getwv(mpol, \"angry\")) - cos_sim2(-1(gpol, getwv(mpol, \"angry\"))\n",
      "212/654:\n",
      "cos_sim2(getwv(mpol, \"asian\") - gpol, getwv(mpol, \"angry\")) - cos_sim2(-1(gpol, getwv(mpol, \"angry\"), \\\n",
      "cos_sim2(getwv(mpol, \"black\") - gpol, getwv(mpol, \"angry\")) - cos_sim2(-1(gpol, getwv(mpol, \"angry\"), \\\n",
      "cos_sim2(getwv(mpol, \"african\") - gpol, getwv(mpol, \"angry\")) - cos_sim2(-1(gpol, getwv(mpol, \"angry\"), \\\n",
      "cos_sim2(getwv(mpol, \"european\") - gpol, getwv(mpol, \"angry\")) - cos_sim2(-1(gpol, getwv(mpol, \"angry\"), \\\n",
      "cos_sim2(getwv(mpol, \"hispanic\") - gpol, getwv(mpol, \"angry\")) - cos_sim2(-1(gpol, getwv(mpol, \"angry\"), \\\n",
      "cos_sim2(getwv(mpol, \"white\") - gpol, getwv(mpol, \"angry\")) - cos_sim2(-1(gpol, getwv(mpol, \"angry\")\n",
      "212/655:\n",
      "cos_sim2(getwv(mpol, \"asian\") - gpol, getwv(mpol, \"angry\")) - cos_sim2(-1*gpol, getwv(mpol, \"angry\")), \\\n",
      "cos_sim2(getwv(mpol, \"black\") - gpol, getwv(mpol, \"angry\")) - cos_sim2(-1*gpol, getwv(mpol, \"angry\")), \\\n",
      "cos_sim2(getwv(mpol, \"african\") - gpol, getwv(mpol, \"angry\")) - cos_sim2(-1*gpol, getwv(mpol, \"angry\")), \\\n",
      "cos_sim2(getwv(mpol, \"european\") - gpol, getwv(mpol, \"angry\")) - cos_sim2(-1*gpol, getwv(mpol, \"angry\")), \\\n",
      "cos_sim2(getwv(mpol, \"hispanic\") - gpol, getwv(mpol, \"angry\")) - cos_sim2(-1*gpol, getwv(mpol, \"angry\")), \\\n",
      "cos_sim2(getwv(mpol, \"white\") - gpol, getwv(mpol, \"angry\")) - cos_sim2(-1*gpol, getwv(mpol, \"angry\"))\n",
      "212/656:\n",
      "a = getwv(mpol, \"white\")\n",
      "np.min(a), np.max(a)\n",
      "212/657:\n",
      "a = getwv(mpol, \"asian\")\n",
      "np.min(a), np.max(a)\n",
      "212/658:\n",
      "a = getwv(mpol, \"black\")\n",
      "np.min(a), np.max(a)\n",
      "212/659:\n",
      "a, b, c, d = getwv(mpol, \"asian\"), getwv(mpol, \"black\"), getwv(mpol, \"hispanic\"), getwv(mpol, \"white\")\n",
      "for x in [a,b,c,d]:\n",
      "    print(np.min(x), np.max(x), np.mean(x))\n",
      "212/660:\n",
      "cos_sim2(getwv(mpol, \"asian\") - gpol, getwv(mpol, \"docile\")) - cos_sim2(-1*gpol, getwv(mpol, \"docile\")), \\\n",
      "cos_sim2(getwv(mpol, \"black\") - gpol, getwv(mpol, \"docile\")) - cos_sim2(-1*gpol, getwv(mpol, \"docile\")), \\\n",
      "cos_sim2(getwv(mpol, \"african\") - gpol, getwv(mpol, \"docile\")) - cos_sim2(-1*gpol, getwv(mpol, \"docile\")), \\\n",
      "cos_sim2(getwv(mpol, \"european\") - gpol, getwv(mpol, \"docile\")) - cos_sim2(-1*gpol, getwv(mpol, \"docile\")), \\\n",
      "cos_sim2(getwv(mpol, \"hispanic\") - gpol, getwv(mpol, \"docile\")) - cos_sim2(-1*gpol, getwv(mpol, \"docile\")), \\\n",
      "cos_sim2(getwv(mpol, \"white\") - gpol, getwv(mpol, \"docile\")) - cos_sim2(-1*gpol, getwv(mpol, \"docile\"))\n",
      "212/661:\n",
      "cos_sim2(getwv(mpol, \"asian\") - gpol, getwv(mpol, \"quiet\")) - cos_sim2(-1*gpol, getwv(mpol, \"quiet\")), \\\n",
      "cos_sim2(getwv(mpol, \"black\") - gpol, getwv(mpol, \"quiet\")) - cos_sim2(-1*gpol, getwv(mpol, \"quiet\")), \\\n",
      "cos_sim2(getwv(mpol, \"african\") - gpol, getwv(mpol, \"quiet\")) - cos_sim2(-1*gpol, getwv(mpol, \"quiet\")), \\\n",
      "cos_sim2(getwv(mpol, \"european\") - gpol, getwv(mpol, \"quiet\")) - cos_sim2(-1*gpol, getwv(mpol, \"quiet\")), \\\n",
      "cos_sim2(getwv(mpol, \"hispanic\") - gpol, getwv(mpol, \"quiet\")) - cos_sim2(-1*gpol, getwv(mpol, \"quiet\")), \\\n",
      "cos_sim2(getwv(mpol, \"white\") - gpol, getwv(mpol, \"quiet\")) - cos_sim2(-1*gpol, getwv(mpol, \"quiet\"))\n",
      "212/662: \"feminine\" in mpol.wv.vocab\n",
      "212/663:\n",
      "cos_sim2(getwv(mpol, \"asian\") - gpol, getwv(mpol, \"feminine\")) - cos_sim2(-1*gpol, getwv(mpol, \"feminine\")), \\\n",
      "cos_sim2(getwv(mpol, \"black\") - gpol, getwv(mpol, \"feminine\")) - cos_sim2(-1*gpol, getwv(mpol, \"feminine\")), \\\n",
      "cos_sim2(getwv(mpol, \"african\") - gpol, getwv(mpol, \"feminine\")) - cos_sim2(-1*gpol, getwv(mpol, \"feminine\")), \\\n",
      "cos_sim2(getwv(mpol, \"european\") - gpol, getwv(mpol, \"feminine\")) - cos_sim2(-1*gpol, getwv(mpol, \"feminine\")), \\\n",
      "cos_sim2(getwv(mpol, \"hispanic\") - gpol, getwv(mpol, \"feminine\")) - cos_sim2(-1*gpol, getwv(mpol, \"feminine\")), \\\n",
      "cos_sim2(getwv(mpol, \"white\") - gpol, getwv(mpol, \"feminine\")) - cos_sim2(-1*gpol, getwv(mpol, \"feminine\"))\n",
      "212/664:\n",
      "cos_sim2(getwv(mpol, \"asian\") - gpol, getwv(mpol, \"quiet\")) - cos_sim2(-1*gpol, getwv(mpol, \"quiet\")), \\\n",
      "cos_sim2(getwv(mpol, \"black\") - gpol, getwv(mpol, \"quiet\")) - cos_sim2(-1*gpol, getwv(mpol, \"quiet\")), \\\n",
      "cos_sim2(getwv(mpol, \"african\") - gpol, getwv(mpol, \"quiet\")) - cos_sim2(-1*gpol, getwv(mpol, \"quiet\")), \\\n",
      "cos_sim2(getwv(mpol, \"european\") - gpol, getwv(mpol, \"quiet\")) - cos_sim2(-1*gpol, getwv(mpol, \"quiet\")), \\\n",
      "cos_sim2(getwv(mpol, \"hispanic\") - gpol, getwv(mpol, \"quiet\")) - cos_sim2(-1*gpol, getwv(mpol, \"quiet\")), \\\n",
      "cos_sim2(getwv(mpol, \"white\") - gpol, getwv(mpol, \"quiet\")) - cos_sim2(-1*gpol, getwv(mpol, \"quiet\"))\n",
      "212/665:\n",
      "cos_sim2(getwv(mpol, \"asian\") - gpol, getwv(mpol, \"criminal\")) - cos_sim2(-1*gpol, getwv(mpol, \"criminal\")), \\\n",
      "cos_sim2(getwv(mpol, \"black\") - gpol, getwv(mpol, \"criminal\")) - cos_sim2(-1*gpol, getwv(mpol, \"criminal\")), \\\n",
      "cos_sim2(getwv(mpol, \"african\") - gpol, getwv(mpol, \"criminal\")) - cos_sim2(-1*gpol, getwv(mpol, \"criminal\")), \\\n",
      "cos_sim2(getwv(mpol, \"european\") - gpol, getwv(mpol, \"criminal\")) - cos_sim2(-1*gpol, getwv(mpol, \"criminal\")), \\\n",
      "cos_sim2(getwv(mpol, \"hispanic\") - gpol, getwv(mpol, \"criminal\")) - cos_sim2(-1*gpol, getwv(mpol, \"criminal\")), \\\n",
      "cos_sim2(getwv(mpol, \"white\") - gpol, getwv(mpol, \"criminal\")) - cos_sim2(-1*gpol, getwv(mpol, \"criminal\"))\n",
      "212/666: rando = [random.uniform(-3, 3) for x in range(100)]\n",
      "212/667:\n",
      "cos_sim2(getwv(mpol, \"asian\") - rando, getwv(mpol, \"angry\")) - cos_sim(mpol, \"asian\", \"angry\"), \\\n",
      "cos_sim2(getwv(mpol, \"black\") - rando, getwv(mpol, \"angry\")) - cos_sim(mpol, \"black\", \"angry\"), \\\n",
      "cos_sim2(getwv(mpol, \"african\") - rando, getwv(mpol, \"angry\")) - cos_sim(mpol, \"african\", \"angry\"), \\\n",
      "cos_sim2(getwv(mpol, \"european\") - rando, getwv(mpol, \"angry\")) - cos_sim(mpol, \"european\", \"angry\"), \\\n",
      "cos_sim2(getwv(mpol, \"hispanic\") - rando, getwv(mpol, \"angry\")) - cos_sim(mpol, \"hispanic\", \"angry\"), \\\n",
      "cos_sim2(getwv(mpol, \"white\") - rando, getwv(mpol, \"angry\")) - cos_sim(mpol, \"white\", \"angry\")\n",
      "212/668: rando = [random.uniform(-3, 3) for x in range(100)]\n",
      "212/669:\n",
      "cos_sim2(getwv(mpol, \"asian\") - rando, getwv(mpol, \"angry\")) - cos_sim(mpol, \"asian\", \"angry\"), \\\n",
      "cos_sim2(getwv(mpol, \"black\") - rando, getwv(mpol, \"angry\")) - cos_sim(mpol, \"black\", \"angry\"), \\\n",
      "cos_sim2(getwv(mpol, \"african\") - rando, getwv(mpol, \"angry\")) - cos_sim(mpol, \"african\", \"angry\"), \\\n",
      "cos_sim2(getwv(mpol, \"european\") - rando, getwv(mpol, \"angry\")) - cos_sim(mpol, \"european\", \"angry\"), \\\n",
      "cos_sim2(getwv(mpol, \"hispanic\") - rando, getwv(mpol, \"angry\")) - cos_sim(mpol, \"hispanic\", \"angry\"), \\\n",
      "cos_sim2(getwv(mpol, \"white\") - rando, getwv(mpol, \"angry\")) - cos_sim(mpol, \"white\", \"angry\")\n",
      "212/670: rando = [random.uniform(-3, 3) for x in range(100)]\n",
      "212/671:\n",
      "cos_sim2(getwv(mpol, \"asian\") - rando, getwv(mpol, \"angry\")) - cos_sim(mpol, \"asian\", \"angry\"), \\\n",
      "cos_sim2(getwv(mpol, \"black\") - rando, getwv(mpol, \"angry\")) - cos_sim(mpol, \"black\", \"angry\"), \\\n",
      "cos_sim2(getwv(mpol, \"african\") - rando, getwv(mpol, \"angry\")) - cos_sim(mpol, \"african\", \"angry\"), \\\n",
      "cos_sim2(getwv(mpol, \"european\") - rando, getwv(mpol, \"angry\")) - cos_sim(mpol, \"european\", \"angry\"), \\\n",
      "cos_sim2(getwv(mpol, \"hispanic\") - rando, getwv(mpol, \"angry\")) - cos_sim(mpol, \"hispanic\", \"angry\"), \\\n",
      "cos_sim2(getwv(mpol, \"white\") - rando, getwv(mpol, \"angry\")) - cos_sim(mpol, \"white\", \"angry\")\n",
      "212/672:\n",
      "cos_sim2(rando - gpol, getwv(mpol, \"criminal\")) - cos_sim2(-1*gpol, getwv(mpol, \"criminal\")), \\\n",
      "cos_sim2(rando - gpol, getwv(mpol, \"criminal\")) - cos_sim2(-1*gpol, getwv(mpol, \"criminal\")), \\\n",
      "cos_sim2(rando - gpol, getwv(mpol, \"criminal\")) - cos_sim2(-1*gpol, getwv(mpol, \"criminal\")), \\\n",
      "cos_sim2(rando - gpol, getwv(mpol, \"criminal\")) - cos_sim2(-1*gpol, getwv(mpol, \"criminal\")), \\\n",
      "cos_sim2(rando - gpol, getwv(mpol, \"criminal\")) - cos_sim2(-1*gpol, getwv(mpol, \"criminal\")), \\\n",
      "cos_sim2(rando - gpol, getwv(mpol, \"criminal\")) - cos_sim2(-1*gpol, getwv(mpol, \"criminal\"))\n",
      "212/673: cos_sim2(getwv(mpol, \"blue\") - gpol, getwv(mpol, \"criminal\")) - cos_sim2(-1*gpol, getwv(mpol, \"criminal\"))\n",
      "212/674:\n",
      "cos_sim2(getwv(mpol, \"blue\") - gpol, getwv(mpol, \"criminal\")) - cos_sim2(-1*gpol, getwv(mpol, \"criminal\")), \\\n",
      "cos_sim2(getwv(mpol, \"cat\") - gpol, getwv(mpol, \"criminal\")) - cos_sim2(-1*gpol, getwv(mpol, \"criminal\")), \\\n",
      "cos_sim2(getwv(mpol, \"math\") - gpol, getwv(mpol, \"criminal\")) - cos_sim2(-1*gpol, getwv(mpol, \"criminal\"))\n",
      "212/675: 2 -= 1\n",
      "212/676: 2 += 1\n",
      "212/677:\n",
      "a = 2\n",
      "a -= 1\n",
      "212/678:\n",
      "a = 2\n",
      "a -= 1\n",
      "a\n",
      "212/679:\n",
      "asian = getwv(mpol, \"asian\")\n",
      "african = getwv(mpol, \"african\")\n",
      "european = getwv(mpol, \"european\")\n",
      "hispanic = getwv(mpol, \"hispanic\")\n",
      "avg = (asian + african + european + hispanic)/4\n",
      "asian -= avg\n",
      "african -= avg\n",
      "european -= avg\n",
      "hispanic -= avg\n",
      "212/680:\n",
      "asian = getwv(mpol, \"asian\")\n",
      "african = getwv(mpol, \"african\")\n",
      "european = getwv(mpol, \"european\")\n",
      "hispanic = getwv(mpol, \"hispanic\")\n",
      "avg = (asian + african + european + hispanic)/4\n",
      "asian = asian - avg\n",
      "african -= avg\n",
      "european -= avg\n",
      "hispanic -= avg\n",
      "212/681:\n",
      "asian = getwv(mpol, \"asian\")\n",
      "african = getwv(mpol, \"african\")\n",
      "european = getwv(mpol, \"european\")\n",
      "hispanic = getwv(mpol, \"hispanic\")\n",
      "avg = (asian + african + european + hispanic)/4\n",
      "asian = asian - avg\n",
      "african = african - avg\n",
      "european = european - avg\n",
      "hispanic = hispanic - avg\n",
      "212/682:\n",
      "cos_sim2(asian - gpol, getwv(mpol, \"criminal\")) - cos_sim2(-1*gpol, getwv(mpol, \"criminal\")), \\\n",
      "cos_sim2(african - gpol, getwv(mpol, \"criminal\")) - cos_sim2(-1*gpol, getwv(mpol, \"criminal\")), \\\n",
      "cos_sim2(european - gpol, getwv(mpol, \"criminal\")) - cos_sim2(-1*gpol, getwv(mpol, \"criminal\")), \\\n",
      "cos_sim2(hispanic - gpol, getwv(mpol, \"criminal\")) - cos_sim2(-1*gpol, getwv(mpol, \"criminal\"))\n",
      "212/683:\n",
      "cos_sim2(asian + gpol, getwv(mpol, \"criminal\")) - cos_sim2(gpol, getwv(mpol, \"criminal\")), \\\n",
      "cos_sim2(african + gpol, getwv(mpol, \"criminal\")) - cos_sim2(gpol, getwv(mpol, \"criminal\")), \\\n",
      "cos_sim2(european + gpol, getwv(mpol, \"criminal\")) - cos_sim2(gpol, getwv(mpol, \"criminal\")), \\\n",
      "cos_sim2(hispanic + gpol, getwv(mpol, \"criminal\")) - cos_sim2(gpol, getwv(mpol, \"criminal\"))\n",
      "212/684:\n",
      "cos_sim2(asian + gpol, getwv(mpol, \"angry\")) - cos_sim2(gpol, getwv(mpol, \"angry\")), \\\n",
      "cos_sim2(african + gpol, getwv(mpol, \"angry\")) - cos_sim2(gpol, getwv(mpol, \"angry\")), \\\n",
      "cos_sim2(european + gpol, getwv(mpol, \"angry\")) - cos_sim2(gpol, getwv(mpol, \"angry\")), \\\n",
      "cos_sim2(hispanic + gpol, getwv(mpol, \"angry\")) - cos_sim2(gpol, getwv(mpol, \"angry\"))\n",
      "212/685: shuffle(gpol)\n",
      "212/686:\n",
      "cos_sim2(asian + gpol, getwv(mpol, \"angry\")) - cos_sim2(gpol, getwv(mpol, \"angry\")), \\\n",
      "cos_sim2(african + gpol, getwv(mpol, \"angry\")) - cos_sim2(gpol, getwv(mpol, \"angry\")), \\\n",
      "cos_sim2(european + gpol, getwv(mpol, \"angry\")) - cos_sim2(gpol, getwv(mpol, \"angry\")), \\\n",
      "cos_sim2(hispanic + gpol, getwv(mpol, \"angry\")) - cos_sim2(gpol, getwv(mpol, \"angry\"))\n",
      "212/687: shuffle(gpol)\n",
      "212/688:\n",
      "cos_sim2(asian + gpol, getwv(mpol, \"angry\")) - cos_sim2(gpol, getwv(mpol, \"angry\")), \\\n",
      "cos_sim2(african + gpol, getwv(mpol, \"angry\")) - cos_sim2(gpol, getwv(mpol, \"angry\")), \\\n",
      "cos_sim2(european + gpol, getwv(mpol, \"angry\")) - cos_sim2(gpol, getwv(mpol, \"angry\")), \\\n",
      "cos_sim2(hispanic + gpol, getwv(mpol, \"angry\")) - cos_sim2(gpol, getwv(mpol, \"angry\"))\n",
      "212/689: shuffle(gpol)\n",
      "212/690:\n",
      "cos_sim2(asian + gpol, getwv(mpol, \"angry\")) - cos_sim2(gpol, getwv(mpol, \"angry\")), \\\n",
      "cos_sim2(african + gpol, getwv(mpol, \"angry\")) - cos_sim2(gpol, getwv(mpol, \"angry\")), \\\n",
      "cos_sim2(european + gpol, getwv(mpol, \"angry\")) - cos_sim2(gpol, getwv(mpol, \"angry\")), \\\n",
      "cos_sim2(hispanic + gpol, getwv(mpol, \"angry\")) - cos_sim2(gpol, getwv(mpol, \"angry\"))\n",
      "212/691: x = [random.uniform(-5,5) for i in range(100)]\n",
      "212/692:\n",
      "cos_sim2(asian + x, getwv(mpol, \"angry\")) - cos_sim2(x, getwv(mpol, \"angry\")), \\\n",
      "cos_sim2(african + x, getwv(mpol, \"angry\")) - cos_sim2(x, getwv(mpol, \"angry\")), \\\n",
      "cos_sim2(european + x, getwv(mpol, \"angry\")) - cos_sim2(x, getwv(mpol, \"angry\")), \\\n",
      "cos_sim2(hispanic + x, getwv(mpol, \"angry\")) - cos_sim2(x, getwv(mpol, \"angry\"))\n",
      "212/693: x = [random.uniform(-4,4) for i in range(100)]\n",
      "212/694:\n",
      "cos_sim2(asian + x, getwv(mpol, \"angry\")) - cos_sim2(x, getwv(mpol, \"angry\")), \\\n",
      "cos_sim2(african + x, getwv(mpol, \"angry\")) - cos_sim2(x, getwv(mpol, \"angry\")), \\\n",
      "cos_sim2(european + x, getwv(mpol, \"angry\")) - cos_sim2(x, getwv(mpol, \"angry\")), \\\n",
      "cos_sim2(hispanic + x, getwv(mpol, \"angry\")) - cos_sim2(x, getwv(mpol, \"angry\"))\n",
      "212/695: x = [random.uniform(-4,4) for i in range(100)]\n",
      "212/696:\n",
      "cos_sim2(asian + x, getwv(mpol, \"angry\")) - cos_sim2(x, getwv(mpol, \"angry\")), \\\n",
      "cos_sim2(african + x, getwv(mpol, \"angry\")) - cos_sim2(x, getwv(mpol, \"angry\")), \\\n",
      "cos_sim2(european + x, getwv(mpol, \"angry\")) - cos_sim2(x, getwv(mpol, \"angry\")), \\\n",
      "cos_sim2(hispanic + x, getwv(mpol, \"angry\")) - cos_sim2(x, getwv(mpol, \"angry\"))\n",
      "212/697: x = [random.uniform(-3,3) for i in range(100)]\n",
      "212/698:\n",
      "cos_sim2(asian + x, getwv(mpol, \"angry\")) - cos_sim2(x, getwv(mpol, \"angry\")), \\\n",
      "cos_sim2(african + x, getwv(mpol, \"angry\")) - cos_sim2(x, getwv(mpol, \"angry\")), \\\n",
      "cos_sim2(european + x, getwv(mpol, \"angry\")) - cos_sim2(x, getwv(mpol, \"angry\")), \\\n",
      "cos_sim2(hispanic + x, getwv(mpol, \"angry\")) - cos_sim2(x, getwv(mpol, \"angry\"))\n",
      "212/699: x = getwv(mpol, \"kid\")\n",
      "212/700:\n",
      "cos_sim2(asian + x, getwv(mpol, \"angry\")) - cos_sim2(x, getwv(mpol, \"angry\")), \\\n",
      "cos_sim2(african + x, getwv(mpol, \"angry\")) - cos_sim2(x, getwv(mpol, \"angry\")), \\\n",
      "cos_sim2(european + x, getwv(mpol, \"angry\")) - cos_sim2(x, getwv(mpol, \"angry\")), \\\n",
      "cos_sim2(hispanic + x, getwv(mpol, \"angry\")) - cos_sim2(x, getwv(mpol, \"angry\"))\n",
      "212/701: x = getwv(mpol, \"cat\")\n",
      "212/702:\n",
      "cos_sim2(asian + x, getwv(mpol, \"angry\")) - cos_sim2(x, getwv(mpol, \"angry\")), \\\n",
      "cos_sim2(african + x, getwv(mpol, \"angry\")) - cos_sim2(x, getwv(mpol, \"angry\")), \\\n",
      "cos_sim2(european + x, getwv(mpol, \"angry\")) - cos_sim2(x, getwv(mpol, \"angry\")), \\\n",
      "cos_sim2(hispanic + x, getwv(mpol, \"angry\")) - cos_sim2(x, getwv(mpol, \"angry\"))\n",
      "212/703: x = getwv(mpol, \"sky\")\n",
      "212/704:\n",
      "cos_sim2(asian + x, getwv(mpol, \"angry\")) - cos_sim2(x, getwv(mpol, \"angry\")), \\\n",
      "cos_sim2(african + x, getwv(mpol, \"angry\")) - cos_sim2(x, getwv(mpol, \"angry\")), \\\n",
      "cos_sim2(european + x, getwv(mpol, \"angry\")) - cos_sim2(x, getwv(mpol, \"angry\")), \\\n",
      "cos_sim2(hispanic + x, getwv(mpol, \"angry\")) - cos_sim2(x, getwv(mpol, \"angry\"))\n",
      "   1:\n",
      "import os\n",
      "import time\n",
      "import dill\n",
      "from typing import List\n",
      "from functools import reduce\n",
      "from calendar import monthrange\n",
      "   2:\n",
      "import json\n",
      "import csv\n",
      "import requests\n",
      "import xml.etree.ElementTree as ET\n",
      "from bs4 import BeautifulSoup\n",
      "import urllib.request as urllib2\n",
      "   3:\n",
      "import matplotlib.pyplot as plt\n",
      "import numpy as np\n",
      "from scipy import stats\n",
      "from random import sample, randint, shuffle\n",
      "from sklearn.decomposition import PCA\n",
      "   4:\n",
      "import nltk\n",
      "import text_summarizer\n",
      "from gensim.utils import simple_preprocess\n",
      "from gensim.summarization.summarizer import summarize\n",
      "from gensim.models import Word2Vec, LsiModel\n",
      "from gensim.test.utils import common_dictionary, common_corpus\n",
      "   5: from time import process_time\n",
      "   6: %history\n",
      "   7: %history -g -f\n",
      "   8: %history -g -f Thesis-Sentiment.ipynb\n",
      "   9: %history -g\n"
     ]
    }
   ],
   "source": [
    "%history -g"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "99.361834"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "t = process_time()\n",
    "dill.load_session('thesis_env.db')\n",
    "process_time() - t"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(adjectives)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(all_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Stereotype Clustering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 161,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.cluster import KMeans\n",
    "from sklearn.mixture import GaussianMixture\n",
    "from sklearn.preprocessing import StandardScaler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 456,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.stats import norm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getwv(m, x):\n",
    "    return m.wv.get_vector(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 201,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(677, 100)"
      ]
     },
     "execution_count": 201,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Xpol = np.array([getwv(mpol, x) for x in adjs_all if x in mpol.wv.vocab])\n",
    "Xpol = StandardScaler().fit_transform(Xpol)\n",
    "Xpol.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 401,
   "metadata": {},
   "outputs": [],
   "source": [
    "kmeanspollist = []\n",
    "inertias2 = []\n",
    "for i in range(1, 40):\n",
    "    temp = KMeans(n_clusters=5*i, random_state=0).fit(Xpol)\n",
    "    kmeanspollist.append(temp)\n",
    "    inertias2.append(temp.inertia_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 402,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Text(0.5, 1.0, 'Within-Cluster Sum-of-Squares (Inertia) vs \\n Number of Clusters for KMeans')"
      ]
     },
     "execution_count": 402,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.plot([5*i for i in range(1,40)], inertias2)\n",
    "plt.title(\"Within-Cluster Sum-of-Squares (Inertia) vs \\n Number of Clusters for KMeans\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 213,
   "metadata": {},
   "outputs": [],
   "source": [
    "kmeanspollist = []\n",
    "inertias = []\n",
    "for i in range(1, 40):\n",
    "    temp = KMeans(n_clusters=5*i, random_state=0).fit(Xpol_s)\n",
    "    kmeanspollist.append(temp)\n",
    "    inertias.append(temp.inertia_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 400,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Text(0.5, 1.0, 'Within-Cluster Sum-of-Squares (Inertia) vs \\n Number of Clusters for KMeans')"
      ]
     },
     "execution_count": 400,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYUAAAEXCAYAAABCjVgAAAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADh0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uMy4xLjIsIGh0dHA6Ly9tYXRwbG90bGliLm9yZy8li6FKAAAgAElEQVR4nO3dd3xV9fnA8c+TCRlkM8NOQIYgGBER3AMn2KrVWqVqtVbtsrbVn7bW/vRXrW2ts1brxFbFjdaFA0UQJEyZEmYIBDJIAmGGPL8/zjfxck1Cxk1OxvN+vfLKvd+znnPuuee55/s953tEVTHGGGMAwvwOwBhjTOthScEYY0w1SwrGGGOqWVIwxhhTzZKCMcaYapYUjDHGVLOk4DMR2SUiA+oYvkFETqtl2AQRWd2Msf1BRJ5vrvm3RyLSWUTeEpFSEXnZ73hakojMFpFRPi273t8FETlPRF5q7pjaKksKISQit4rIu0Fla2opuwRAVeNUdZ0rf0ZE7qrv8lR1lqoObmLM3xeRbJectorIuyIyvinzDJp/PxFREYkI1TzdfK8WkVUislNEtonIOyISH8plNNKFQDcgRVUvqmkEEZkkIotFpExECkXkYxHp37JhhpaInAfsVNVF7n2z/qBw+1RG1fuGfBdU9S1gmIiMaK742jJLCqH1GTBORMIBRKQHEAmMCirLcOP6SkRuAv4O/B/egawP8Cgwyc+4AtWUTETkRLyYL1XVeGAI0Fp++fUFvlbVipoGugPZc8CvgASgP/AIcLDFIvTiEBEJ5ff/OmBqCOdXoxD+uHgBuDZE82pfVNX+QvQHRAG7gaPd+4uBp4FPg8pyAqZRvCRxLXAA2A/sAt5ywzcANwNLgVK8g18nN+wkYHPAvGodt4ZYE9xyLqpjff4APF/TsgKWd5p7PQbIBsqAbcDfXPkmt4673N9xrvwqYCWwA3gf6Bu0TW4A1gDra4jrZuCNOuKeCfwo4P0Pgc+D5n+9m/9O4H+BgcAcF/80IKqO+Q9xyygBlgPnu/I73ed3wK3r1TVMeyGwuI55dwaecdtlBfDroM9YgYyA988Ad7nXScDbQIGb/m0gPWi73A3MBvbg7XcJwJPAViAPuAsId+Nn4O27pUAh8FId+/2eoGVV7zsBcV/ntnkJXiKUgOH13h/wflApUO628/f49nfhFmCt+3xXABcExXx8TfuWG/Zb4JWgsgeABwP2p3Vu3uuBy/w65jTHn+8BtLc/4BPgl+71w25nvzuo7KmA8au/5IFf8IDhG4AvgZ5AsvviXOeGBX8Rah23hjgnAhVARB3rUv3FDl5WwPKqksIXwOXudRww1r3u59YxImC6SUAO3sE1ArgdmBO0TWa4dehcQ1wT8A5Cd7ovd3TQ8JkcPim8CXQBhgH7gI+AAXgHyRXAlFq2SaSL/X/wDoanuIPD4OBtVsv0A4C9wP3AyUBc0PB7gFlu3XsDy6h/UkgBvgvEAPHAywQkT7ddNrl1jnDr8jrwTyAW6Or2nx+78V8AbsOrUegEjK9lnYYB5bXtOwFxvw0k4p2RFgATG7s/1LAdTgraThfhfQ/C8JJGOdAjYHiym0eXGtanL96Pu3j3PhwvaY5126ks4PPuAQzz+7gTyj+rPgq9T4ET3OsJeF/wWUFlnzZwng+q6hZVLQbeAo4KwbgpQKHWUs3RCAeADBFJVdVdqjq3jnGvA/6kqivd8v8POEpE+gaM8ydVLVbVPcETq+os4DvAaOC/QJGI/K2qiq6e/qyqZaq6HO/A+4GqrlPVUuBdoLYG07F4Se8eVd2vqh/jHewurc9C1Ws/OgnohXdGUujakuLcKBcDd7t1zwUerO8KqWqRqr6qqrtVdSfej5ETg0Z7RlWXu+2eDJwN/EJVy1V1O16yusSNewDvANlTVfeq6ue1LDoRLzEezj2qWqKqm/B+PFXtm03aH2qiqi+770Glqr6Ed5YxJmCUqngTa5h2I7AQuMAVnQLsDtinK4HhItJZVbe6fajdsKQQep8B40UkGUhT1TV41RLjXNlwGt6ekB/wejfeQalB47oG5F3u7zKgCEgNYR3t1cAgYJWIzBeRc+sYty/wgIiUiEgJUAwI3oGySm5dC1PVd1X1PLwD2yS8s4EfNSDebQGv99Twvrbt1hPIVdXKgPE3BsVeTUSWB0w/wcU+V1UvVtU0vB8JJ+D9Iqdq/kHzrhcRiRGRf4rIRhEpw9vPEoOSZeC8++KdLWwN+Cz+iXfGAPAbvM/lS7ceV9Wy6B14ZyaHU9t+3OT9IZiIXOEa86vmORxIDRilKt6SWmbxH75J9N9371HVcrwzj+vwttt/ReSIhsTW2oX0ihADeNUoCcA1eHW3qGqZiGxxZVtUdX0t0zZbl7WqelbgexFJwKs2mQy8Uo9ZlONVS1RNHw6kBcx/DXCpa7z8DvCKiKRQ8zrl4v0a/nddIdcjJtzB+SMR+Rjvi/+tWIHu9ZlXLfMP3m4TgN4iEhaQGPoAX9cy/bDDzH++iLzGN7Fvxas2qvr12Sdokt18e902u9e/AgYDx6pqvogcBSzCO8BWLzLgdS7ePpBa0xmjqubj7bO4K9I+FJHPVDUnaNQcbxTppap5da1vLUK2P7hY+wJPAKcCX6jqQRFZzKHbYQiwQVXLapnNy8BfRSQd74zhuOpAVN8H3heRznhtME/gJfd2wc4UQsyd3mYDN+FVG1X53JXVdZawDa/Oudm5apLfA4+IyGT3KzNSRM4SkT/XMMnXQCcROUdEIvHqfaOrBorID0QkzR0oq359VeLVHVdy6Ho9BtwqIsPctAkiUuPlmzVxl3ReIiJJ7iqaMXjVJFWn94uB77h1ysA7iwmVeXgH5t+47XUScB7wYj1jHy8i14hIV/f+COD8gNin4W2bJHdA+mnQLBYD3xeRcBGZyKHVQ/F4Zzkl7qz0jrpiUdWtwAd4B78uIhImIgPd1V2IyEUuBvDOBhTvswyez37gQ75dVVVfjdkf6vquxLpYC9z8ruSbpFvlRLxqwhqpagFeG8zTeA3SK928urn9LxYvoe6ihm3SlllSaB6f4p2CB9bBznJldSWFJ4Gh7pT3jWaMDwBV/Steorod7wuUC9wIfGvZLolcD/wL7yqVcr75hQpew/VyEdmFd6XGJaq6R1V34654ces1VlVfB+4FXnTVHMuAQ36RH8YOvF+wa/Aa/Z4H7gv4pXk/3lVA24Bngbp+gTaIOwCe5+ItxLuE9wpVXVXPWZTgJYGv3LZ6D6+xtyoR34lXZbQe74AdfJnnz93yS4DLOPSz+jve1UuFeEnmvXrEcwVeg/kKvO36Cl7jKcAxwDwX53Tg565NpCb/BC6vx/K+pZH7wx+AZ90+dXHQ/FYAf8U7a98GHIk7aw9wqYu5Lv8BTnP/q4ThfWe24FVznQj85DDzaVNEtdlqLIwxTeTORJ5X1fTDjes3EZkN3KjuBrbWSrwb7S5X1YsPO3IHZEnBmFasLSUF0z5Y9ZExxphqdqZgjDGmmp0pGGOMqWZJwTQLEZkpIg25mSyUy25S99XSTD27NjcRuUu8XlfzDz+2MTWzpNBBuIPcVxLQM6Y7iDzjY1jNpT7dVw8SkZfdQbRURJaKyE3SsK4y6tSSiVFE+uDdvDZUVRt9s17QPA/pnlpEbhave/VhInKSG/560DQjXfnMUMRgWp4lhY6lJ9/0a9MmuJvTGrqf9qXu7qsH4t2ElgscqaoJeB2oZVG/7hpaRAMTVB+gyPVf1NDlHPaMSERuB34BnBjQ108BcJy7c73KFGq5u9u0DZYUOpY/A3fWdBBwv/w2B5VVP/VNvIemvCwiz4v3YJuv3K/tW0Vku4jkisgZQbMdKCJfivcwmTfdXbZV8x4rInPczUdL3KWXVcNmisjd7rr33dRw56qIDHHjlYjXL8/5rvxOvDu1vydef0M13c18J14vnDe5u3pR1dWq+n1V/VZfOBL09DsJeICMiHRy26TIxTLf3fV6N17XBw+7OB524x8hIjNEpFhEVgfeeCVex3j/EO+BQeXAySJytoiscNs8T0RuriG+0/B6Ee3plvWMKz/fbZsSt62GBK3Tb0VkKVBeV2IQ78FPPwJOUNXAA/5+vJvnLnHjheP1C/TvoOnrWudzRGSR20dyReQPAcOqqvGmiMgmd1Z3W8DwMeI9IKpMvAct/a22dTANEIquVu2v9f/h3fafCSzAdSuN12/LM+71SdTdNfYf8Lp8PhOvz6zn8O66vQ2vU7VrCOifHq+LgDy87gVigVf5phvuXngd8p2N98PkdPc+LWDaQ7p4Doqrqd1X5wNX1jG8HwHdfQduh+D5Az/G6402Bq+L5aNx3THz7S68Y/HOTq506zUK7+7joW74M3jPLjieb7qr3gpMcMOTgNG1xHzI54fXOWG527aReJ3b5eCeE+HWaTFeP0vf6p48YJ95Be/O8T41LQ8YB8xzZWfjPQvhR8DMeq7zSXh3HIcBI/DuQJ4c9Dk8gXen9ki8riWGuOE1dtduf037szOFjkWB3wG/E5GoRkw/S1XfV69a5mW8DvHuUdUDeH3/9BORwK6Ip6rqMvV6lvwdcLH7NfkD4B1VfUe9ro1n4PUXdXbAtM+o6+LZzT9Qk7qvxus2fGvDVr1WB9z8MlT1oKou0No7WTsXrxO2p916LcJLloHtHm+q6my3Xfa6+Q8VkS6qukNVF9Yzru8B/1XVGW77/QXvwDouYJwHVTVX6+6O+gzgPfW6u/4WVZ0DJIvIYLwuM55ryDqr6kxV/cqt71K8ZzgE96F0p3pdpiwBluAlB2hYd+2mniwpdDCq+g7eL7wfN2Ly4O6lC1X1YMB7OLRb7+AuoCPxui/uC1zkqjWqujYezzd97gRPG6xB3VfXoChoWU0xFe/X8YsiskVE/ixeh4E16QscG7Tel3FoL67B6/1dvGS5UUQ+FZHjqJ+eBHS77bZVLg3vjvoS4EJXLVebqXh9Zp2M149ToDrXWUSOFZFPRKRARErxuqRODZpHbV1uN6S7dlNPbeqSOxMyt+H9InshoKzOrrEbqXfA6z54v+wK8Q5GU1X1mjqmreuuyi00oPvqGnyId7B9up7j19oVt/sVfideW00/4B1gNV7nhsHrkAt8qqqn17GsQ6ZR1fnAJJdobsTrRbV3TRMG2YJXLQN4DfZuusCuretz5+rXeJ3CzRSRPap6Tw3jTMWrmnpOVXd7i6p2uHX+D97TCM9S1b0i8ne+nRRqpLV01+7OTE0j2ZlCB6SqM/F6opwSUFxn19iN9AMRGSoiMcAf8Z57exCvV9PzRORM8bqA7iReQ3d9+/dpUvfVeF1KjxOR+0Sk6hdrhmsw/taTuPDq3i9xy8rCu+QVN93JInKkS6JleImvKlEFd+/8NjBIRC5384oUkWMCG4ADiUiUiFwmIgku+ZRR/26apwHniMip7vP8FV59/Jx6Tl9NvauNTgN+LSK/qGH4erwqn9uCh3H4dY4Hil1CGIP3QJt6kdq7azdNYEmh47od76llQL26xm6MqXiNp/l4jaY/c8vKxXta2v/wTZfdv6ae+6M2sftqVV2L99CUfnjdfZfi1XNnU/NjJX8HDMTrWvpODu1KuTteY2wZ3jOxP+Wb7q4fwKt62SEiD6r3iMwz8KpktuBtl3upO/leDmwQr0vp6/CqXuqzjqvx2m4ewttG5wHnuW3XYK4+/0zgDhG5robhn6vqlhrKD7fO1wN/FJGdeFeNTWtAWDV2196A6U0NrO8jY4wx1exMwRhjTDVLCsYYY6pZUjDGGFPNkoIxxphqbfY+hdTUVO3Xr5/fYRhjTJuyYMGCQlWt9R6kNpsU+vXrR3Z2tt9hGGNMmyIiG+sabtVHxhhjqllSMMYYU82SgjHGmGqWFIwxxlSzpGCMMaaaJQVjjDHVLCkYY4yp1qGSgqoy9YsNvL30Wz38GmOMoR5JQUSeEpHtIrIsoOw+EVklIktF5PXAB5OIyK0ikiMiq0XkzIDyia4sR0RuCSjvLyLzXPlLjXx2cL2ICC8v2Mwzszc01yKMMaZNq8+ZwjN4D7MINAMYrqoj8J7YdSuAiAzFe5jGMDfNo+7JWuHAI3gPRRmK9wi9oW5e9wL3q2oG3kNMrm7SGh3GCZlpLMotoWxv8LPgjTHGHDYpqOpnQHFQ2QeqWuHezgWqHqM4CXhRVfe5R/TlAGPcX46qrnNPfnoR77mzApyC9+QqgGeByU1cpzpNyEzlYKUyJ6eoORdjjDFtUijaFK4C3nWve+E9WrHKZldWW3kKUBKQYKrKayQi14pItohkFxQUNCrY0X2TiI0KZ9aaxk1vjDHtWZOSgojcBlQA/w5NOHVT1cdVNUtVs9LSau3kr06R4WEcNzCVz9YUYI8iNcaYQzU6KYjID4Fzgcv0m6NrHtA7YLR0V1ZbeRGQKCIRQeXN6sRBqeQW72Fj0e7mXpQxxrQpjUoKIjIR+A1wvqoGHlmnA5eISLSI9AcygS+B+UCmu9IoCq8xerpLJp8AF7rppwBvNm5V6m9CpneW8ZlVIRljzCHqc0nqC8AXwGAR2SwiVwMPA/HADBFZLCKPAajqcmAasAJ4D7hBVQ+6NoMbgfeBlcA0Ny7Ab4GbRCQHr43hyZCuYQ36pcbSJzmGz74ubO5FGWNMm3LYh+yo6qU1FNd64FbVu4G7ayh/B3inhvJ1eFcntagJmam8sSiP/RWVREV0qHv4jDGmVh32aDghM43y/QdZtGmH36EYY0yr0WGTwriMFMLDxNoVjDEmQIdNCl06RTKqdyKz1li7gjHGVOmwSQHghEFpfJVXSnH5fr9DMcaYVqFDJ4UJmamowuc5drZgjDHQwZPCiPREEjpHMutra1cwxhjo4EkhPEwYn2FdXhhjTJUOnRQAThiUyrayfazZvsvvUIwxxncdPilUd3lhVUjGGGNJoWdiZzK6xvGZXZpqjDGWFMC7CmneuiL2HjjodyjGGOMrSwp49yvsq6hk/obiw49sjDHtmCUF4Nj+yUSFh1m7gjGmw7OkAMRERXBM/yTr8sIY0+FZUnAmZKaxKn8n28r2+h2KMcb4xpKCMyEzFcDOFowxHZolBWdI9y6kxkVbu4IxpkOzpOCEhQkTMlP5PKeQykrr8sIY0zFZUghwwqBUisv3s3xLmd+hGGOMLywpBBif4bq8sKexGWM6KEsKAdLioxmZnsCrCzdz0KqQjDEdkCWFINedOJB1BeVMX5LndyjGGNPiLCkEOXNYd4b06MIDH66h4mCl3+EYY0yLsqQQJCxM+OVpmWwo2s3ri+xswRjTsVhSqMHpQ7sxvFcXHvx4DQfsbMEY04FYUqiBiHDT6YPILd7Dqws2+x2OMca0GEsKtTh5cFdG9k7koY9z2F9hZwvGmI7hsElBRJ4Ske0isiygLFlEZojIGvc/yZWLiDwoIjkislRERgdMM8WNv0ZEpgSUHy0iX7lpHhQRCfVKNkbV2UJeyR6mZef6HY4xxrSI+pwpPANMDCq7BfhIVTOBj9x7gLOATPd3LfAP8JIIcAdwLDAGuKMqkbhxrgmYLnhZvjkhM5Wj+ybx8Mc59lQ2Y0yHcNikoKqfAcGPJJsEPOtePwtMDih/Tj1zgUQR6QGcCcxQ1WJV3QHMACa6YV1Uda6qKvBcwLx8V3W2kF+2lxe/3OR3OMYY0+wa26bQTVW3utf5QDf3uhcQWNey2ZXVVb65hvIaici1IpItItkFBS3TFcW4gSmM6Z/MIzPX2tmCMabda3JDs/uF3yJ9Qqjq46qapapZaWlpLbHI6rOFgp37eH7uxhZZpjHG+KWxSWGbq/rB/d/uyvOA3gHjpbuyusrTayhvVcYOSOH4jBQe+3Qtu/dX+B2OMcY0m8YmhelA1RVEU4A3A8qvcFchjQVKXTXT+8AZIpLkGpjPAN53w8pEZKy76uiKgHm1Kr88bRCFu/Yz9Qs7WzDGtF/1uST1BeALYLCIbBaRq4F7gNNFZA1wmnsP8A6wDsgBngCuB1DVYuB/gfnu74+uDDfOv9w0a4F3Q7NqoZXVL5kTBqXx2Kdr2bXPzhaMMe2TeE0CbU9WVpZmZ2e36DIXbdrBBY/O4eYzBnHjKZktumxjjAkFEVmgqlm1Dbc7mhtgVJ8kTh/ajUdnriWvZI/f4RhjTMhZUmig3587lEpV7py+3O9QjDEm5CwpNFDv5Bh+fuogPlixjRkrtvkdjjHGhJQlhUb40YT+DOoWxx+mL7dLVI0x7YolhUaIDA/jrslHkleyhwc+WuN3OMYYEzKWFBppTP9kLs5K58lZ61mVX+Z3OMYYExKWFJrglrOGEN8pgttfX0ZlZdu8tNcYYwJZUmiC5Ngobj17CNkbd/DyAnvmgjGm7bOk0EQXjk5nTL9k/vTuKop27fM7HGOMaRJLCk0UFibcdcFwdu2t4E/vrvI7HGOMaRJLCiEwqFs815wwgFcWbGbuuiK/wzHGmEazpBAiPzslk/Skztz+xjL2V1T6HY4xxjSKJYUQ6RwVzh8nDSNn+y6emLXO73CMMaZRLCmE0ClHdOPMYd14+OMctliHecaYNsiSQojdfo7XYd7d76z0OxRjjGkwSwoh1js5hp+cNJD/Lt3KnLWFfodjjDENYkmhGVx34kDSkzpz5/QVVBy0RmdjTNthSaEZdIoM5/ZzhrJ6206mzrVnOhtj2g5LCs3kzGHdmJCZyt9mfE2h3elsjGkjLCk0ExHhjvOGsWf/Qe57b7Xf4RhjTL1YUmhGGV3juGp8f17KzmVxbonf4RhjzGFZUmhmPz0lg7T4aO5407rXNsa0fpYUmll8p0huPesIlmwu5ZUFm/0Oxxhj6mRJoQVcMKoXR/dN4t73VlG654Df4RhjTK0sKbQAEeHO84dRvHs/f//wa7/DMcaYWllSaCHDeyXw/TF9eO6LjazO3+l3OMYYUyNLCi3o5jMGE98pgptfXsLu/RV+h2OMMd/SpKQgIr8UkeUiskxEXhCRTiLSX0TmiUiOiLwkIlFu3Gj3PscN7xcwn1td+WoRObNpq9R6JcVG8ZcLR7J8Syk//c8i6wLDGNPqNDopiEgv4GdAlqoOB8KBS4B7gftVNQPYAVztJrka2OHK73fjISJD3XTDgInAoyIS3ti4WrvThnbjzknD+WjVdn4/fTmqdpmqMab1aGr1UQTQWUQigBhgK3AK8Iob/iww2b2e5N7jhp8qIuLKX1TVfaq6HsgBxjQxrlbt8rF9+clJA/nPvE08OnOt3+EYY0y1RicFVc0D/gJswksGpcACoERVqyrMNwO93OteQK6btsKNnxJYXsM0hxCRa0UkW0SyCwoKGht6q/DrMwYz6aie3Pf+al5fZPcvGGNah6ZUHyXh/crvD/QEYvGqf5qNqj6uqlmqmpWWltaci2p2YWHCny8cwXEDUvjNK0uZnWPPXjDG+K8p1UenAetVtUBVDwCvAccDia46CSAdyHOv84DeAG54AlAUWF7DNO1adEQ4j11+NANS47hu6gJW5Zf5HZIxpoNrSlLYBIwVkRjXNnAqsAL4BLjQjTMFeNO9nu7e44Z/rF4r63TgEnd1Un8gE/iyCXG1KQmdI3n6ymOIjY7gh0/NZ2upPdvZGOOfprQpzMNrMF4IfOXm9TjwW+AmEcnBazN40k3yJJDiym8CbnHzWQ5Mw0so7wE3qOrBxsbVFvVM7MzTVx5D+b4KfvjUfMr2WlcYxhh/SFu9JDIrK0uzs7P9DiOkZucUMuWpLzluYArPXDmG8DDxOyRjTDsjIgtUNau24XZHcytyfEYqd18wnFlrCrl/hvWRZIxpeZYUWpnvHdOHS8f05uFPcvhgeb7f4RhjOhhLCq3QHecNY0R6Ar+atoT1heV+h2OM6UAsKbRCnSLDefSy0USEC9dNXWCd5xljWowlhVYqPSmGhy4dzZrtO7nl1a+sjyRjTIuwpNCKjc9M5VdnDGb6ki08M2eD3+EYYzoASwqt3E9OHMjpQ7tx939XMn9Dsd/hGGPaOUsKrVxYmPDXi0fSOzmG6/+9kO1le/0OyRjTjllSaAO6dIrksR8cza69Fdzwn4UcsIfzGGOaiSWFNmJw93juvXAE8zfs4E/vrPI7HGNMO2VJoQ05f2RPfjiuH0/NXs/7dmObMaYZWFJoY249+wiO7JXAr19ewuYdu/0OxxjTzlhSaGOiI8J5+PujqFT46QuLrH3BGBNSlhTaoL4psdzz3SNZtKmEv3yw2u9wjDHtiCWFNurcET257Ng+/PPTdXyyervf4Rhj2glLCm3Y784dyhHd4/nVtCXkl9r9C8aYprOk0IZ1igzn4e+PZu+Bg/zsxUVUWPuCMaaJLCm0cRld47hr8nC+XF/Mgx/n+B2OMaaNs6TQDnxndDoXHp3OQx+vYXZOod/hGGPaMEsK7cQfJw1jQGosP39xMQU79/kdjjGmjbKk0E7EREXwyGWj2bn3AD96LtsSgzGmUSwptCNHdO/Cg5eOYnV+GZMfmc3KrWV+h2SMaWMsKbQzZw7rzss/HkdFZSXf/cccZqzY5ndIxpg2xJJCO3RkegLTbxxPRtc4rp2azT8/XWuP8zTG1IslhXaqW5dOvHTtcZx9ZA/+9O4qfv3KUvZVHPQ7LGNMKxfhdwCm+XSOCufhS0eRkRbHAx+tYWNROY/94GhS4qL9Ds0Y00o16UxBRBJF5BURWSUiK0XkOBFJFpEZIrLG/U9y44qIPCgiOSKyVERGB8xniht/jYhMaepKmW+ICL88fRAPXTqKpZtLmfTIbFbn7/Q7LGNMK9XU6qMHgPdU9QhgJLASuAX4SFUzgY/ce4CzgEz3dy3wDwARSQbuAI4FxgB3VCUSEzrnjezJtB8fx/6KSi56bA4LNhb7HZIxphVqdFIQkQTgBOBJAFXdr6olwCTgWTfas8Bk93oS8Jx65gKJItIDOBOYoarFqroDmAFMbGxcpnYjeyfy2vXjSImL5rJ/zePTrwv8DskY08o05UyhP1AAPC0ii0TkXyISC3RT1a1unHygm3vdC8gNmH6zK6ut/FtE5FoRyRaR7IICO6A1RnpSDNN+fBwDUuP40bPzeXvpFr9DMsa0Ik1JChHAaOAfqjoKKOebqiIA1LsOMmTXQqrq46qapapZaWlpoZpth5MWH80L1wCRW+gAABZ5SURBVI7lqN6J/PSFRbzw5Sa/QzLGtBJNSQqbgc2qOs+9fwUvSWxz1UK4/1VPgMkDegdMn+7Kais3zSihcyTPXXUsJw5K49bXvuKxT9f6HZIxphVodFJQ1XwgV0QGu6JTgRXAdKDqCqIpwJvu9XTgCncV0lig1FUzvQ+cISJJroH5DFdmmlnnqHAevzyLc0f04J53V3HPu6vsJjdjOrim3qfwU+DfIhIFrAOuxEs000TkamAjcLEb9x3gbCAH2O3GRVWLReR/gfluvD+qql0a00KiIsJ44JJRJHSO5LFP11K65wB3TR5OeJj4HZoxxgfSVn8ZZmVlaXZ2tt9htBuqyn3vr+bRmWs5bUg3/nLRCBJjovwOyxgTYiKyQFWzahtu3VwYwLvJ7TcTj+CO84Yyc/V2znpgFvPWFfkdljGmhVlSMIe48vj+vHb9OKIjwrj0ibn87YPV9uxnYzoQSwrmW0akJ/L2zyZwwah0Hvw4h+89Ppfc4t1+h2WMaQGWFEyN4qIj+OvFI3ngkqNYnb+Tsx+cZTe6GdMBWFIwdZp0VC/e+dkEBqbFceN/FvHbV5aye3+F32EZY5qJJQVzWH1SYnj5uuO44eSBTFuQy6SHZ7OpyKqTjGmPLCmYeokMD+PXZx7B1KuOZfvOfUx65HPm2tVJxrQ7lhRMg4zPTOXNG44nOTaKH/xrnvWbZEw7Y0nBNFi/1Fheu/54xmWkcutrX3HnW8vtslVj2glLCqZREjpH8tSULK48vh9Pz97AVc9mU7b3gN9hGWOayJKCabSI8DDuOG8Yf/rOkczJKeSCR2azobDc77CMMU1gScE02aVj+vD8j46luHw/kx6ZzeycQr9DMsY0kiUFExJjB6Tw5g3j6dYlmh88Oc+6xzCmjbKkYEKmT0oMr19/PN8d7XWPcekTc8kr2eN3WMaYBrCkYEIqNjqCv1zkdY+xcutOzn5gFu8ty/c7LGNMPVlSMM1i0lG9ePun4+mTHMN1zy/gd28sY++Bg36HZYw5DEsKptn0S43l1Z+M45oJ/Zk6dyOTH5lNzvadfodljKmDJQXTrKIiwrjtnKE8feUxFOzcx7kPfc4LX26yZ0Eb00pZUjAt4uTBXXn35xM4um8St772FT96NpvtO/f6HZYxJoglBdNiunbpxNSrjuX35w7l85xCzrz/M95bttXvsIwxASwpmBYVFiZcNb4///3ZeNKTYrju+YXc9NJi6yLDmFbCkoLxRUbXeF67fhw/OzWTN5dsYeL9nzHH7oQ2xneWFIxvIsPDuOn0Qbz6k3F0igzn+/+ax51vLbdLV43xkbTVq0CysrI0Ozvb7zBMiOzZf5B731vFM3M2kBYfzXkjejLpqJ6MSE9ARPwOz5h2Q0QWqGpWrcMtKZjW5Iu1RTwzZz2frCpg/8FK+qfGcv5IL0EMSIvzOzxj2jxLCqZNKt19gPeWb+WNRVuYu74IVRiRnuASRC/S4qP9DtGYNsmSgmnz8kv38taSLby5JI9leWXERIVzw8kZXD2+P50iw/0Oz5g25XBJockNzSISLiKLRORt976/iMwTkRwReUlEolx5tHuf44b3C5jHra58tYic2dSYTPvSPaET15wwgLd/OoEZvzyB8Rmp3Pf+as64/zNmrNhmd0cbE0KhuPro58DKgPf3AveragawA7jalV8N7HDl97vxEJGhwCXAMGAi8KiI2M8/U6PMbvE8fkUWU68eQ1REGNc8l80VT31pfSoZEyJNSgoikg6cA/zLvRfgFOAVN8qzwGT3epJ7jxt+qht/EvCiqu5T1fVADjCmKXGZ9m9CZhrv/nwCvz93KItzS5j491n88a0VlO6xm+CMaYqmnin8HfgNUPWIrRSgRFUr3PvNQC/3uheQC+CGl7rxq8trmOYQInKtiGSLSHZBQUETQzdtXWR4GFeN78/Mm0/ioqzePD1nPaf8ZSbPz93Ivgq718GYxmh0UhCRc4HtqroghPHUSVUfV9UsVc1KS0trqcWaVi4lLpo/fedI3rpxPAPT4rj9jWWcdN9Mnp2zwW6EM6aBmnKmcDxwvohsAF7EqzZ6AEgUkQg3TjqQ517nAb0B3PAEoCiwvIZpjKm34b0SeOnHY5l69Rh6J8Vwx/TlTPjzJzzx2Tp27684/AyMMY1PCqp6q6qmq2o/vIbij1X1MuAT4EI32hTgTfd6unuPG/6xepeNTAcucVcn9QcygS8bG5fp2ESECZlpTLvuOF68diyZXeO4+52VjL/3Ex75JIed1vGeMXWKOPwoDfZb4EURuQtYBDzpyp8EpopIDlCMl0hQ1eUiMg1YAVQAN6iqnfObJhs7IIWxA1JYsLGYhz7O4b73V/P4Z+u4fGxfvndMb3onx/gdojGtjt28ZjqMpZtLeOjjHD5cuQ2A8RmpXHJMH04f2o2oCOsb0nQMdkezMUHySvbwcnYu0+bnsqV0L8mxUXx3dC++d0xvMrrG+x2eMc3KkoIxtThYqcxaU8CLX+by4cptVFQqWX2TuGJcP849sgdhYdY7q2l/LCkYUw8FO/fx2sLNvDg/l/WF5Qzr2YVbzxrC+MxUv0MzJqQsKRjTAJWVyvQlW7jv/dXklexhQmYqt5x1BMN6JvgdmjEh0ewd4hnTnoSFCZNH9eLjm0/k9nOG8FVeKec+9Dm/fGkxm3fs9js8Y5qdnSkYU4fSPQf4x8y1PD17PaowZVxfrj8pg6TYKL9DM6ZRrPrImBDYUrKHv834mlcXbiYyLIzxmalMHN6d04d0swRh2hRLCsaE0Or8nbycncu7y/LJK9lDeJhw3IAUJg7vzhnDutE1vpPfIRpTJ0sKxjQDVWVZXhnvLtvKe8vyWVdYjggc0zeZ80b2YPKoXsR3ivQ7TGO+xZKCMc1MVfl6267qBLEqfyexUeFcMLoXVxzXj0Hd7IY403pYUjCmhS3JLWHq3I1MX7KF/RWVHNs/mcuP68uZw7oTGW4X/Bl/WVIwxic7yvczLTuX5+dtJLd4D13jo7l0TB8uHdOH7gnW9mD8YUnBGJ8drFQ+/Xo7z32xkU+/LkCAcQNTmTyqFxOHdycuujk6KzamZpYUjGlFNhaV8+rCPN5YlMem4t10igzj9KHduWBUTyZkpln1kml2lhSMaYVUlYWbSnhjUR5vLd1Cye4DJMdGcd4I78qlo3onImId8pnQs6RgTCu3v6KSz74u4PXFecxYsY39FZUMSI3lO6N7MXlUL9KT7GFAJnQsKRjThpTtPcB7X+Xz6sLNzFtfDMDYAcl8Z3Q6Zw3vbvc+mCazpGBMG5VbvJs3FuXx2qI81heW0ykyjDOHdef8kT0ZOyCFWGugNo1gScGYNk5VWZRbwmsLN/PWkq2U7jlAZLgwqk8SEzJSOT4zlRG9EoiwRmpTD5YUjGlH9lUcJHvDDmatKeTznAKWbylDFeI7RTBuYArjM9M4aVAavZOtHcLU7HBJwc4/jWlDoiPCOT4jleMzUoEjKC7fz+ycQj5fU8jnOYW8v3wbAKP6JHL+yJ6cM6KHddJnGsTOFIxpJ1SVDUW7eX95Pm8u3sLKrWWEiXej3PlH9WTi8O50sYbqDs+qj4zpoNZs28n0JVuYvmQLG4t2ExURxsmD0zhreA+O7ptEelJnuxeiA7KkYEwHp6os2VzKm4vzeHvpVgp27gMgNS6a0X0SGdUnidF9EhmRnkjnqHCfozXNzZKCMabawUplVX4ZCzeVsGjjDhbllrC+sByA8DBhSI94jumXzEmDu3Js/2Q6RVqSaG8sKRhj6lRcvp9Fm3awaFMJCzftYMHGHeyrqKRTZBjjBqZy8uA0Thrc1a5oaifs6iNjTJ2SY6M4dUg3Th3SDYC9Bw7yxboiPl1dwMertvPxqu3AcjK6xnHSIC9BZPVLsrOIdqrRZwoi0ht4DugGKPC4qj4gIsnAS0A/YANwsaruEK9F6wHgbGA38ENVXejmNQW43c36LlV99nDLtzMFY5qfqrK+sJxPVhcwc/V25q0rZv/BSqIiwji6TxLjBqYwLiOVEekJ1sNrG9Fs1Uci0gPooaoLRSQeWABMBn4IFKvqPSJyC5Ckqr8VkbOBn+IlhWOBB1T1WJdEsoEsvOSyADhaVXfUtXxLCsa0vPJ9FXy5oZg5OYXMzilixdYyAOKiIxjTP9lLEgNTOaJ7PGFhdmVTa9Rs1UequhXY6l7vFJGVQC9gEnCSG+1ZYCbwW1f+nHpZaK6IJLrEchIwQ1WLXcAzgInAC42NzRjTPGKjIzh5cFdOHtwV8Noj5q4rYnZOIXPWFrmqJkiMieTY/smMHZDCcQNTGNTVkkRbEZI2BRHpB4wC5gHdXMIAyMerXgIvYeQGTLbZldVWXtNyrgWuBejTp08oQjfGNEFybBRnH9mDs4/sAcCWkj18sbaIueuKmLu+qPoO6+TYqEOSRGbXOLtHopVqclIQkTjgVeAXqloW+EGrqopIyC5vUtXHgcfBqz4K1XyNMaHRM7Ez3z06ne8enQ54Pb3OXVfE3HXFzF1XxLvL8gHo1iWa4zNSmZCZyvEDU+naxbriaC2alBREJBIvIfxbVV9zxdtEpIeqbnXVQ9tdeR7QO2DydFeWxzfVTVXlM5sSlzGmdeidHEPv5BguyuqNqpJbvIc5awuZlVPIJ6u289rCPAAGd4tnfGYq4zNTObZ/MjFRdmGkX5rS0Cx4bQbFqvqLgPL7gKKAhuZkVf2NiJwD3Mg3Dc0PquoY19C8ABjtZrEQr6G5uK7lW0OzMW1bZaWyfEsZn+d4Pb7O37CD/RWVhIcJg7vFc1SfRI7qncio3okMTIuzNokQac6rj8YDs4CvgEpX/D947QrTgD7ARrxLUotdEnkYrxF5N3Clqma7eV3lpgW4W1WfPtzyLSkY077s2X+Q+RuK+XJ9MYtzS1iSW8LOfRWAd3XTiPQEjuqdyIj0BAamxdEnJYboCLtXoqHsjmZjTJtUWamsK9zF4txSFufuYHFuCau27qSi0jtmhYlXPTUgNZaBaXEMSItjQFosmV3jSImL9jn61svuaDbGtElhYUJG13gyusZzoWu43nvgIF9v28m6gnLWFexibWE5a7fvYs7aIvZVVFZP2yuxMyPSExiRnsjI9ASGpydYt+H1ZEnBGNNmdIoMZ0S616NroMpKZUvpHtYVlLM6fydLNpewdHNp9dVOAAPSYhmZnsjwXglkdo1jYNc4eiZ0sktjg1hSMMa0eWFhQnpSDOlJMZwwKK26fEf5fpbmlbIkt4Slm0v4PKeQ1xflVQ+PiQpnYFocA9NiyegaR0bXOAamxdE7OabD9u1kScEY024lxUZx4qA0TnSJQlUp3LWftQW7yNnu/a0t2MWX64t5Y/GW6ulEoEeXTvRLjaVvSiz9UmK8/6kx9E2ObdfPnbCkYIzpMESEtPho0uKjGTsg5ZBh5fsqWFuwi3UF5Wws2s2GonI2FJXz/vJ8isv3V48XJjAgLY7hPbswrGcCw3p1YViPBBJi2kebhSUFY4zB69eppvYKgNI9B9jkEsWa7btYsaWUeUFnF+lJnRneM4GhPbuQ2TWOzG5x9E2JbXO9x1pSMMaYw0joHMmR6QkcmZ5wSHnRrn0s31LGsi2lLN9SxootZby3/JvG7YgwoV+qd5lsVeP2wLQ4uid0IjkmqlXekGdJwRhjGiklLpoTBqUd0ri9e38F6wrKWbN9J2u27WLN9l2szt/J+8vzqQy4LSwqPIyuXaLpkdCJbl06Vf9PT+rMkB5d6JMc48uVUZYUjDEmhGKiIhjeK4HhvQ49q9h74KDXTlFYTn7pXraW7WVb6V62lu5lWV4pH67cxt4D39xrER8dwZCeXRjaowvDenZx1VLxREU0b3WUJQVjjGkBnSLDOaJ7F47o3qXG4apK6Z4DbCzazYqtXlXU8i2lvDQ/lz0HDgIQGS5kdo3nhWvGNlvDtiUFY4xpBUSExJgoEmOiGNn7m8bug5XKhqJylyTKWFewiy6dm+/QbUnBGGNasfAwcTfYxXHeyJ7Nvry2da2UMcaYZmVJwRhjTDVLCsYYY6pZUjDGGFPNkoIxxphqlhSMMcZUs6RgjDGmmiUFY4wx1URVDz9WKyQiBcDGWganAoUtGE5DWXxNY/E1jcXXNG09vr6qmlbbwDabFOoiItmqmuV3HLWx+JrG4msai69p2nt8Vn1kjDGmmiUFY4wx1dprUnjc7wAOw+JrGouvaSy+pmnX8bXLNgVjjDGN017PFIwxxjSCJQVjjDHV2lVSEJGJIrJaRHJE5JZWEE9vEflERFaIyHIR+bkr/4OI5InIYvd3ts9xbhCRr1ws2a4sWURmiMga9z/Jh7gGB2yjxSJSJiK/8Hv7ichTIrJdRJYFlNW4vcTzoNsnl4rIaJ/iu09EVrkYXheRRFfeT0T2BGzLx3yKr9bPVERuddtvtYic6VN8LwXEtkFEFrtyP7ZfbceV0OyDqtou/oBwYC0wAIgClgBDfY6pBzDavY4HvgaGAn8AbvZ7mwXEuQFIDSr7M3CLe30LcG8r+Hzzgb5+bz/gBGA0sOxw2ws4G3gXEGAsMM+n+M4AItzrewPi6xc4no/br8bP1H1flgDRQH/3HQ9v6fiChv8V+L2P26+240pI9sH2dKYwBshR1XWquh94EZjkZ0CqulVVF7rXO4GVQC8/Y2qAScCz7vWzwGQfYwE4FVirqrXdxd5iVPUzoDiouLbtNQl4Tj1zgUQR6dHS8anqB6pa4d7OBdKbM4a61LL9ajMJeFFV96nqeiAH77vebOqKT0QEuBh4oTljqEsdx5WQ7IPtKSn0AnID3m+mFR2ARaQfMAqY54pudKdyT/lRNRNEgQ9EZIGIXOvKuqnqVvc6H+jmT2jVLuHQL2Jr2n5Q+/ZqjfvlVXi/HKv0F5FFIvKpiEzwKyhq/kxb2/abAGxT1TUBZb5tv6DjSkj2wfaUFFotEYkDXgV+oaplwD+AgcBRwFa801E/jVfV0cBZwA0ickLgQPXOQX27dllEooDzgZddUWvbfofwe3vVRURuAyqAf7uirUAfVR0F3AT8R0S6+BBaq/5MA1zKoT9OfNt+NRxXqjVlH2xPSSEP6B3wPt2V+UpEIvE+uH+r6msAqrpNVQ+qaiXwBM18Onw4qprn/m8HXnfxbKs6xXT/t/sXIWcBC1V1G7S+7efUtr1azX4pIj8EzgUucwcNXLVMkXu9AK/OflBLx1bHZ9qatl8E8B3gpaoyv7ZfTccVQrQPtqekMB/IFJH+7pflJcB0PwNy9Y9PAitV9W8B5YH1eRcAy4KnbSkiEisi8VWv8Rokl+FtuylutCnAm/5ECAT9OmtN2y9AbdtrOnCFuwJkLFAacIrfYkRkIvAb4HxV3R1QniYi4e71ACATWOdDfLV9ptOBS0QkWkT6u/i+bOn4nNOAVaq6uarAj+1X23GFUO2DLdlq3tx/eK3sX+Nl69taQTzj8U7hlgKL3d/ZwFTgK1c+HejhY4wD8K7uWAIsr9puQArwEbAG+BBI9im+WKAISAgo83X74SWorcABvPrZq2vbXnhXfDzi9smvgCyf4svBq1eu2g8fc+N+133ui4GFwHk+xVfrZwrc5rbfauAsP+Jz5c8A1wWN68f2q+24EpJ90Lq5MMYYU609VR8ZY4xpIksKxhhjqllSMMYYU82SgjHGmGqWFIwxxlSzpGCMMaaaJQVjjDHV/h9UQOGJ+plMwAAAAABJRU5ErkJggg==\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.plot([5*i for i in range(1,40)], inertias)\n",
    "plt.title(\"Within-Cluster Sum-of-Squares (Inertia) vs \\n Number of Clusters for KMeans\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 195,
   "metadata": {},
   "outputs": [],
   "source": [
    "K = 80"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 233,
   "metadata": {},
   "outputs": [],
   "source": [
    "kmeanspol = KMeans(n_clusters=K, random_state=0).fit(Xpol)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 235,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(0, 57), (1, 4), (4, 91), (7, 4), (8, 7), (10, 21), (14, 16), (32, 7), (37, 3), (39, 16), (41, 335), (48, 16), (51, 10), (58, 3), (64, 16), (69, 4)]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(13,\n",
       " {1: ['narrow', 'steady', 'weak', 'solid'],\n",
       "  7: ['complex', 'sensitive', 'intense', 'delicate'],\n",
       "  8: ['negative',\n",
       "   'positive',\n",
       "   'genuine',\n",
       "   'conventional',\n",
       "   'practical',\n",
       "   'dramatic',\n",
       "   'profound'],\n",
       "  10: ['shy',\n",
       "   'sincere',\n",
       "   'cynical',\n",
       "   'thoughtful',\n",
       "   'passionate',\n",
       "   'provocative',\n",
       "   'conciliatory',\n",
       "   'confidential',\n",
       "   'false',\n",
       "   'vague'],\n",
       "  14: ['attractive',\n",
       "   'educated',\n",
       "   'responsive',\n",
       "   'extreme',\n",
       "   'assertive',\n",
       "   'capable',\n",
       "   'sympathetic',\n",
       "   'experienced',\n",
       "   'devoted',\n",
       "   'patriotic'],\n",
       "  32: ['respected',\n",
       "   'revered',\n",
       "   'ardent',\n",
       "   'outspoken',\n",
       "   'charismatic',\n",
       "   'loyal',\n",
       "   'unpopular'],\n",
       "  37: ['ambitious', 'efficient', 'aggressive'],\n",
       "  39: ['helpful',\n",
       "   'clever',\n",
       "   'simple',\n",
       "   'brilliant',\n",
       "   'plain',\n",
       "   'stupid',\n",
       "   'wonderful',\n",
       "   'pure',\n",
       "   'beautiful',\n",
       "   'decent'],\n",
       "  48: ['honorable',\n",
       "   'unstable',\n",
       "   'awful',\n",
       "   'odd',\n",
       "   'honest',\n",
       "   'emotional',\n",
       "   'excellent',\n",
       "   'ugly',\n",
       "   'intelligent',\n",
       "   'extraordinary'],\n",
       "  51: ['cautious',\n",
       "   'enthusiastic',\n",
       "   'angry',\n",
       "   'tired',\n",
       "   'anxious',\n",
       "   'confused',\n",
       "   'wary',\n",
       "   'skeptical',\n",
       "   'optimistic',\n",
       "   'nervous'],\n",
       "  58: ['calm', 'quiet', 'busy'],\n",
       "  64: ['transparent',\n",
       "   'normal',\n",
       "   'stable',\n",
       "   'healthy',\n",
       "   'generous',\n",
       "   'balanced',\n",
       "   'strict',\n",
       "   'clean',\n",
       "   'physical',\n",
       "   'reliable'],\n",
       "  69: ['surprising', 'uncertain', 'interesting', 'obvious']})"
      ]
     },
     "execution_count": 235,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wordsp = list(filter(lambda x: x in mpol.wv.vocab, adjs_all))\n",
    "dpols = {k:[] for k in range(K)}\n",
    "for i in range(len(Xpol)):\n",
    "    dpols[kmeanspol.labels_[i]].append(wordsp[i])\n",
    "print([(x, len(dpols[x])) for x in range(K) if len(dpols[x]) > 2])\n",
    "pstereos = {k:v[:10] for k,v in dpols.items() if shuffle(v) is None and len(v) > 2 and len(v) < 50}\n",
    "len(pstereos), pstereos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 266,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.436043827856252"
      ]
     },
     "execution_count": 266,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.std(polpointsy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 486,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0.75, -0.625)"
      ]
     },
     "execution_count": 486,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "swn_score(\"good\"), swn_score(\"bad\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 306,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0.765625, 0.015625)"
      ]
     },
     "execution_count": 306,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "d1 = swn_score(\"nice\") - swn_score(\"mean\")\n",
    "d2 = swn_score(\"nice\") - swn_score(\"good\")\n",
    "d1**2, d2**2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 487,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(82.70683637814363, 137.34023008169606)"
      ]
     },
     "execution_count": 487,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "diff = mpol.wv.get_vector(\"good\") - mpol.wv.get_vector(\"bad\")\n",
    "diff2 = mpol.wv.get_vector(\"nice\") - mpol.wv.get_vector(\"good\")\n",
    "sum(diff**2), sum(diff2**2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 284,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(0, 15), (2, 63), (3, 21), (4, 46), (5, 8), (6, 11), (7, 72), (20, 111), (22, 10), (25, 6), (27, 4), (33, 51), (35, 9), (38, 21), (55, 16), (60, 4), (64, 5), (65, 8), (68, 3), (70, 71), (75, 10), (79, 49)]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(17,\n",
       " {0: ['bright',\n",
       "   'amazing',\n",
       "   'musical',\n",
       "   'warm',\n",
       "   'exciting',\n",
       "   'pure',\n",
       "   'smooth',\n",
       "   'eloquent',\n",
       "   'odd',\n",
       "   'inexperienced'],\n",
       "  3: ['content',\n",
       "   'creative',\n",
       "   'grave',\n",
       "   'conciliatory',\n",
       "   'dynamic',\n",
       "   'sweet',\n",
       "   'tense',\n",
       "   'sharp',\n",
       "   'passionate',\n",
       "   'conventional'],\n",
       "  4: ['innovative',\n",
       "   'lively',\n",
       "   'tranquil',\n",
       "   'zealous',\n",
       "   'sentimental',\n",
       "   'trendy',\n",
       "   'alert',\n",
       "   'irresponsible',\n",
       "   'flamboyant',\n",
       "   'funny'],\n",
       "  5: ['genuine',\n",
       "   'simple',\n",
       "   'lame',\n",
       "   'curious',\n",
       "   'slight',\n",
       "   'vague',\n",
       "   'challenging',\n",
       "   'bizarre'],\n",
       "  6: ['uncertain',\n",
       "   'objective',\n",
       "   'decisive',\n",
       "   'capable',\n",
       "   'balanced',\n",
       "   'practical',\n",
       "   'impressive',\n",
       "   'neutral',\n",
       "   'slow',\n",
       "   'realistic'],\n",
       "  22: ['outspoken',\n",
       "   'directed',\n",
       "   'ardent',\n",
       "   'blunt',\n",
       "   'skeptical',\n",
       "   'friendly',\n",
       "   'dedicated',\n",
       "   'sympathetic',\n",
       "   'motivated',\n",
       "   'vulnerable'],\n",
       "  25: ['revered',\n",
       "   'loyal',\n",
       "   'charismatic',\n",
       "   'experienced',\n",
       "   'educated',\n",
       "   'progressive'],\n",
       "  27: ['generous', 'competitive', 'efficient', 'modest'],\n",
       "  35: ['obvious',\n",
       "   'profound',\n",
       "   'smart',\n",
       "   'emotional',\n",
       "   'interesting',\n",
       "   'honest',\n",
       "   'extraordinary',\n",
       "   'positive',\n",
       "   'careful'],\n",
       "  38: ['grim',\n",
       "   'artful',\n",
       "   'petty',\n",
       "   'false',\n",
       "   'disturbing',\n",
       "   'humble',\n",
       "   'messy',\n",
       "   'awful',\n",
       "   'dirty',\n",
       "   'negative'],\n",
       "  55: ['wonderful',\n",
       "   'competent',\n",
       "   'attractive',\n",
       "   'healthy',\n",
       "   'decent',\n",
       "   'charming',\n",
       "   'jocular',\n",
       "   'precise',\n",
       "   'discerning',\n",
       "   'solid'],\n",
       "  60: ['intense', 'complex', 'sensitive', 'delicate'],\n",
       "  64: ['stable', 'active', 'authoritarian', 'extreme', 'unpopular'],\n",
       "  65: ['optimistic',\n",
       "   'nervous',\n",
       "   'thinking',\n",
       "   'striking',\n",
       "   'angry',\n",
       "   'proud',\n",
       "   'anxious',\n",
       "   'tired'],\n",
       "  68: ['cautious', 'wary', 'enthusiastic'],\n",
       "  75: ['dramatic',\n",
       "   'confident',\n",
       "   'tolerant',\n",
       "   'helpful',\n",
       "   'sophisticated',\n",
       "   'ambitious',\n",
       "   'subtle',\n",
       "   'reliable',\n",
       "   'flexible',\n",
       "   'assertive'],\n",
       "  79: ['mercurial',\n",
       "   'conscientious',\n",
       "   'airy',\n",
       "   'abrupt',\n",
       "   'insincere',\n",
       "   'tasteless',\n",
       "   'petulant',\n",
       "   'reclusive',\n",
       "   'lean',\n",
       "   'fearful']})"
      ]
     },
     "execution_count": 284,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Xpol_s = np.hstack((Xpol, 10*np.array([polpointsy]).T))\n",
    "kmeanspol_s = KMeans(n_clusters=K, random_state=0).fit(Xpol_s)\n",
    "wordsp = list(filter(lambda x: x in mpol.wv.vocab, adjs_all))\n",
    "dpols2 = {k:[] for k in range(K)}\n",
    "for i in range(len(Xpol)):\n",
    "    dpols2[kmeanspol_s.labels_[i]].append(wordsp[i])\n",
    "print([(x, len(dpols2[x])) for x in range(K) if len(dpols2[x]) > 2])\n",
    "pstereos2 = {k:v[:10] for k,v in dpols2.items() if shuffle(v) is None and len(v) > 2 and len(v) < 50}\n",
    "len(pstereos2), pstereos2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 308,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[-0.125, 0.0, 0.0, 0.0, -0.25, 0.125, 0.125, 0.0, 0.0, 0.125]"
      ]
     },
     "execution_count": 308,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[swn_score(x) for x in pstereos2[22]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 466,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(-0.18152553, 0.16618495)"
      ]
     },
     "execution_count": 466,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gproj(mpol, \"he\"), gproj(mbooks, \"he\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 472,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "0.037\t p: \t0.19085\n",
      "3\n",
      "0.036\t p: \t0.21293\n",
      "4\n",
      "0.037\t p: \t0.20268\n",
      "5\n",
      "0.017\t p: \t0.39857\n",
      "6\n",
      "0.016\t p: \t0.39886\n",
      "22\n",
      "-0.043\t p: \t0.00120\n",
      "25\n",
      "-0.100\t p: \t0.00000\n",
      "27\n",
      "0.087\t p: \t0.00012\n",
      "35\n",
      "0.086\t p: \t0.00015\n",
      "38\n",
      "0.050\t p: \t0.05864\n",
      "55\n",
      "0.088\t p: \t0.00009\n",
      "60\n",
      "0.111\t p: \t0.00000\n",
      "64\n",
      "-0.045\t p: \t0.00081\n",
      "65\n",
      "0.049\t p: \t0.06875\n",
      "68\n",
      "-0.050\t p: \t0.00030\n",
      "75\n",
      "0.036\t p: \t0.20445\n",
      "79\n",
      "-0.044\t p: \t0.00097\n"
     ]
    }
   ],
   "source": [
    "for k,v in pstereos2.items():\n",
    "    mval = np.mean([gproj(mpol, x) for x in v])\n",
    "    zval = (mval - np.mean(means))/(np.std(means)/np.sqrt(3))\n",
    "    pval = norm.pdf(zval)\n",
    "#     if pval > 0.07:\n",
    "#         continue\n",
    "    print(k)\n",
    "    print(f\"{mval:.3f}\\t\", f\"p: \\t{pval:.5f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 392,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0.016298609, 0.03022331, 0.07674522884190083, -0.04414801113307476)"
      ]
     },
     "execution_count": 392,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "means = [np.mean([gproj(mpol, x) for x in sample(wordsp, randint(6, 11))]) for i in range(1000)]\n",
    "np.mean(means), np.std(means), np.mean(means) + 2*np.std(means), np.mean(means) - 2*np.std(means)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 393,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Text(0.5, 1.0, 'Histogram of Mean Gender Projection for Random Subsets of Adjectives')"
      ]
     },
     "execution_count": 393,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAb8AAAEICAYAAADP3Pq/AAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADh0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uMy4xLjIsIGh0dHA6Ly9tYXRwbG90bGliLm9yZy8li6FKAAAesUlEQVR4nO3de7xcVX338c8XEsItkMSENCTIAYxWYhU1Co9FjQLK1WBVhCoEhCda8Wm1tDVeixZstNWipYp4KSAKRBSlglVMQbwhBgQkXEqAYAi5EQwGQRT4PX+sNbDPMDPnJDmTPZP1fb9e8zr7uvZvr732/u3bzFFEYGZmVpKt6g7AzMxsc3PyMzOz4jj5mZlZcZz8zMysOE5+ZmZWHCc/MzMrzogkP0mLJc0aibL6laTXS1om6SFJL6w7nl4kaamkA+uOo5VuteEuljtZ0tWS1kv65EiX322SrpJ0Ut1xdCLpeEk/rjuO4RrpNiFpQFJIGpX7vytpzqZH+rTldKXcoQyZ/FodsJobRUTMiIirhihnUEVugf4VeFdE7BgRv2wemdd9dXX9JY3Ow2r5sqWkKZK+IOm+nLTvknSOpD+tI54NIWmWpCdy3Osl3S7phI0tbzhteBgxnSPptJEut425wP3AThFxyqYWlvfpx3N9/lbSjZIO3/Qw6yVpnKQvS1qZ28n/SppXc0wh6VldKHpYbULSqTmGfTek8Ig4JCLO3ZQA87LPH+lyN8YWc9uzB5Lq7sDiIab5DXBIpf+QPGyzk/QM4KfA9sDLgbHAi4AfAgfVEVM7HbbtfRGxI7AT8F7gC5L23oD5+9nuwC2xEb9S0aE+fpbrcxzwWeBCSeM2IcZe8G/AjsBzgZ2B1wFLao2oe4ZsE5IEHAc8kP+WKyI6foClwIFNw44HftxqGuClwCLgt8Aq4FN5+K+BAB7Kn/9DSr4fBO4BVgPnATtXyj0uj1sLfKhpOacCFwPn52WdlJf9M2AdsAI4E9imUl4A7wTuANYD/wTsRUoCvwUWVKdvWueWsQJj8voE8DvgzjbzR57/65VhFwMfSJvhyWE7A1/K8S8HTgO2zuP2Av4n18f9wFeBcU3b4e+Am4AHgYuAbdvEcxpwI7DVENt/v1w/6/L0syrjrsp1+JNcn98HJlbGH1vZfh9o2n5bAfOAO/P4BcCEPG4g19eJud1c3SKuWcC9TcPWAG9sNz/pwLc4r8tVwHPbtOG2seXx+1fqZBlpf5gL/BH4Q24P/9Wi3DHAGcB9+XMGMKa6PsAppPa1AjihzTY5p2lZBw6z7PcCK4GvtCjzeAbv09vnOnxJZdjX8/wPAlcDM5pi+g/gstwWfg7sVRl/EHBbnvdM0knWSZ32raa2cEKu698A7wBeQmrn64AzO7Tfm4Ej24xrlD2qqU2fVKmTn+R4H8zxH9BUZ3fl9b0beEtl3NuAW3O83wN2z8Ov5qljxUPAm4GJwHfyujwA/Ig2+yXwMuAXOZ5fAC9r1ybazP8K4BHgLaS2XT0+bk26g3V/Xq+Tq/VTrZtO65jHzQCuyOuzCng/cHCO7485xhur5ZLa8DrgeZVyJuV4d8n9hwM35Ol+Cjy/Mu17ScfM9cDt1W3Vsi46jWzeeTvsKE9OQ0o+x+buHYH9OjS0t5HOwvbM036TvGMCe+cK2h/YJm+UPzI4+f0ROJK082wHvJh0sB6Vl3cr8O7K8gL4NulKYQbwKLAwL39n4BZgTpt6aBtrpexndajHAJ6XG8I4YHzufh6Dk98lwOeBHYBdgGuBt+dxzyIdRMbkRnE1cEbTdrgW2BWYkNf/HW3iuQY4dYhtP5W0gxya6/ig3D+p0mjvBJ6d6/8qYH7T9ntFjvdTwGOV7fc3OYZpefzngQua2sp5uR62axHbLHLyy7G9PreH57SaP8f4u7wOo4F/yNtzmxZtuFNsu5N2rmNyOc8A9qkcgE5rt/8AH83l7pK330+Bf6qsz2N5mtG5zh8GxrfZNoOWNcyyP57Xp1V9Hk/ep0kHwZNJB6pdmvaBsTyVaG9oimct6QR0FOnE7MI8bmKuszfmdXtPjuekSrntjgONbXkWsC3wGuD3wLfyuk4lJcxXtqmnL5JOeE4ApjeNa5TdKfk9luMdTUpUD5L2rR1IJ8zPydNOIZ8MALPz+jw318UHgZ+2O1YA/5zXb3T+vBxQi3WZQEo0x+Zyj8n9z2jX/lqU8SXSydzovL3eUBn3DlKC3y0v60raJL9O65jbyArSidy2uX/fynH7/KaYquV+GTi9Mu5k4L9z9wvztt6X1EbnkPavMaT9fhmwa2Xb7tWxLjqNrOy8D5EybePzMO2T39XAR6hcAXRoaAuBd1b6n0M6gI0CPkw+4ORx25N2xmrye9oVQdMy3w1c0tTo/rzSfx3w3kr/J6kkk6ay2sbaqkG3mD9IyeuLwNtzQ/tCHhZ5msmkhLxdZb5jgCvblHkk8Mum7fDWSv8ngLPazLuESmIkXRWtI1/B5WHvpekqgXSGN6fSaD9YGffOSkP9MPngl/t3aNp+tzL4LHpKZds32sqeHepzFvAET50t3wAc3dTW9qxM/yFgQaV/K9JZ4qwWbbhTbO+rtqmmmM6hc/K7Ezi0Mu61wNLK+jzC4P1jNfnkcahlDaPsP9DmLkCe5njSgX5dXtdHgKM6TD8u1/HOlXi+WBl/KHBb7j4OuKYyTqQr0cYBr9NxoLEtp1bGrwXeXOn/BpWT3KY4tyNddVyXy1wCHNLUTjolv/uoJCLSyeWxpPa8DngDTScTwHeBE5va2sM8dfXXnPw+Sjopb3v8yNMdC1zbNOxnwPHt2l/TtNuTEvaRuf/zwLcr4/+HwceE19A++bVdR9Ix65dtYjiVzsnvQCp3z0hX3sfl7s+RT+gq428HXkk6jq7O84/uVI+Nz3Cf+R0ZEeMaH9JBrp0TSWfZt0n6xRAPzXcl3epouIfU4CfnccsaIyLiYVKjr1pW7ZH0bEnfyQ+3fwt8jHTWWbWq0v1Ii/4dNyLWDXEe6WBwXO6u2p10RrZC0jpJ60gNdBd48m2uCyUtz+t3Pk9fv5WV7oc7rM9a0kEdgIi4NG/b95CutBvxvKkRS45n/+p8HZbXvP1+x+DttztwSaXcW4HHGVyfg7ZvC/flNjkhIvaJiAubxlfnH7T9IuKJPH5qi3I7xbYbKdFsjFZtaNdK/9qIeKzS32n7bWjZayLi90OUcU1uA+OBS0lXIABI2lrSfEl35ra3NI+qtr/htoWgw7ah9b61UfttRDwSER+LiBeTrtIXAF+XNKHV9C0sz/FWY9s1t+c3k05iV0i6rPKi2O7Apyvt5wFSwm/V1gD+hZSUv59fOmv3Qk5zPTXiaVdus9eTTnAuz/1fBQ6RNKlSfnW7NC+rqtM6bso+ciWwvaR9JQ0A+5DuhjWWeUrT8Wg30vZYQrrYORVYnY+Tuz6t9IoRf+ElIu6IiGNIB+yPAxdL2oF0BtHsPtIKNTyTtHFWkS6bpzVGSNqO1HgHLa6p/3Oky/bpEbET6YxPG782w451Q/yIlDwmA82vUS8jXflNrJxs7BQRM/L4j5HW+c/y+r2VjV+/hcCRkjq1gWWkK79xlc8OETF/GOWvIDVMACRtz+Dtt4x0Bl4te9uIWF6ZplWb2RDV+Qdtv/zgfzfS1V+zTrEtIz17HWp5rbRqQ/cNMc9wDVX2sOsyIh4C/go4tvK1nb8k3eo6kPSIYCAPH077a24Lqva3iX1j9q2OIqJxQrwDsAfpNjikK6KGP2mabWqOtxrbfbm870XEQaT9+TbSnRxIbeTtTe1nu4j4aZu41kfEKRGxJ+kOzN9KOqDFpM311IinVRtuZQ7pJOHXklaSnuGOJm1baNpOuex2Oq3jMtIt7FY6tsOIeJx0gnJM/nwnItZXlnl60zK3j4gL8rxfi4j9SXUUpPzT1ognP0lvlTQpn1mvy4OfIL2M8ASDK+UC4D2S9pC0I6lhXpTPfi8GjpD0MknbkDL6UDvaWNJl/UP5LOyvRmq9hoh12PJZ5BHA65rOKImIFaSXRj4paSdJW0naS9Ir8yRjSbegH5Q0Ffj7TVifT5HO8L+SlyFJY0lnWg3nk7bBa/OZ/7b5KwbTWpY42MXA4ZL2z9vvowxub2cBp0vaHUDSJEmzN2F9hrIAOEzSAZJGk55HPEp6NtasU2xfBQ6UdJSkUZKeIalRZ6tov9NDakMfzOVNJN0aPr/D9BtiRMuOiAdIt+g/nAeNJdXXWlKy+NgGFHcZMEPSX+Q3Tf+awUlmRPatViR9SNJLJG0jaVvS89x1wO0RsYaUON6a2/fbePqJzS7AXyt9LelNpGdcl+e7MLPzif2jpP3yiTzPWcD7JM3IMeyc520Y1E4kHS7pWTnJPki6y/AET3c58GxJf5nb3ptJz9a/M4x6mAocQHphZJ/8eQEpQTTe+lyQ13WapPGkl77a6bSO3wGmSHq3pDGSxuqpr1WsAgaGOOn+Gumq+i25u+ELwDvyVaEk7SDpsFz+cyS9WtIY0jPhR2hdh0/qxlcdDgYWS3oI+DTpOcwj+bbl6cBP8iXrfqSHm18hPSe8Owf9/wAiYnHuvpB0RvIQ6Z7uox2W/Xeks5j1pIq6aATXq22sGyoiFuf1a+U40m3HW0gPsy/mqduMHyF9HeFB0gHlmxuz/BzD/aSXg35PugJdT3puNpZ80hARy0hn++8nnbwsIyXcIdtNXr+TSY13RV6XeyuTfJp0a+37ktaTXtbYoO8dbYiIuJ10pfzvpLfZjgCOiIg/tJi8bWwR8WvS86xTeOpZ4wvyfF8C9s7t+1styj2N9Cb0TcCvgOvzsJHQjbLPAA6V9HzSLfp7SMniFlKdDEtua28C5pOS53TSs5yGEdu3Wi0e+E/SNr+P9MLTYfnqFuD/ktr0WtJLcM0nQz/P8d5POn69MSLWkvaBv81lPkB67tTYby4hJZULlW4R38zgrzidCpyb28lRufwfkI5xPwM+GxFXPm1F0nIPJ7W9taSXtg7P9TuUY0kvKH0/IlY2PsBngOdLeh7pmPk90lvd19Ph+NJpHfOV2kGkfWwl6e36V+VZv57/rpV0fZuyf066Kt+V9GyxMXwRaXudSTqeLCE9l4X00st80nZaSTppeV+nClHTxUfPymeE60i3NO+uOx7bskj6NelloavrjsWsF0m6mvRSU/O7Cn2pp7/kLukISdvnWwv/SjqjXVpvVLalUXrgPwm3LbOW8jP7PUlX5luEnk5+pFtujS/tTifdQu2PS1XrC5JeQrot8+/5lqaZVUjahXQr8Yc8/SW9vtU3tz3NzMxGSq9f+ZmZmY24LfEHfweZOHFiDAwM1B2GmVlfue666+6PiElDT9mftvjkNzAwwKJFi+oOw8ysr0jq9Asvfc+3Pc3MrDhOfmZmVhwnPzMzK46Tn5mZFcfJz8zMiuPkZ2ZmxXHyMzOz4jj5mZlZcZz8zMysOFv8L7yY9bKBeZfVstyl8w+rZblmvcJXfmZmVhwnPzMzK45ve5oVqK7breBbrtYbfOVnZmbFcfIzM7PiOPmZmVlxnPzMzKw4Tn5mZlYcJz8zMyuOk5+ZmRXHyc/MzIrj5GdmZsVx8jMzs+I4+ZmZWXFqTX6SdpN0paRbJC2W9Dd5+ARJV0i6I/8dn4dL0mckLZF0k6QX1Rm/mZn1p7qv/B4DTomIvYH9gJMl7Q3MAxZGxHRgYe4HOASYnj9zgc9t/pDNzKzf1Zr8ImJFRFyfu9cDtwJTgdnAuXmyc4Ejc/ds4LxIrgHGSZqymcM2M7M+V/eV35MkDQAvBH4OTI6IFXnUSmBy7p4KLKvMdm8e1lzWXEmLJC1as2ZN12I2M7P+1BPJT9KOwDeAd0fEb6vjIiKA2JDyIuLsiJgZETMnTZo0gpGamdmWoPbkJ2k0KfF9NSK+mQevatzOzH9X5+HLgd0qs0/Lw8zMzIat7rc9BXwJuDUiPlUZdSkwJ3fPAb5dGX5cfutzP+DByu1RMzOzYRlV8/L/HDgW+JWkG/Kw9wPzgQWSTgTuAY7K4y4HDgWWAA8DJ2zecM3MbEtQa/KLiB8DajP6gBbTB3ByV4MyM7MtXu3P/MzMzDY3Jz8zMyuOk5+ZmRXHyc/MzIrj5GdmZsVx8jMzs+I4+ZmZWXHq/pK7We0G5l1Wdwhmtpn5ys/MzIrj5GdmZsVx8jMzs+I4+ZmZWXGc/MzMrDhOfmZmVhwnPzMzK46Tn5mZFcfJz8zMiuPkZ2ZmxXHyMzOz4jj5mZlZcZz8zMysOE5+ZmZWHCc/MzMrjpOfmZkVx8nPzMyK4+RnZmbFcfIzM7PiOPmZmVlxnPzMzKw4Tn5mZlYcJz8zMyuOk5+ZmRXHyc/MzIrj5GdmZsVx8jMzs+I4+ZmZWXGc/MzMrDhOfmZmVpzak5+kL0taLenmyrBTJS2XdEP+HFoZ9z5JSyTdLum19URtZmb9rPbkB5wDHNxi+L9FxD75czmApL2Bo4EZeZ7PStp6s0VqZmZbhNqTX0RcDTwwzMlnAxdGxKMRcTewBHhp14IzM7MtUu3Jr4N3Sbop3xYdn4dNBZZVprk3DxtE0lxJiyQtWrNmzeaI1czM+kivJr/PAXsB+wArgE9uyMwRcXZEzIyImZMmTepGfGZm1sd6MvlFxKqIeDwingC+wFO3NpcDu1UmnZaHmZmZDVtPJj9JUyq9rwcab4JeChwtaYykPYDpwLWbOz4zM+tvo+oOQNIFwCxgoqR7gX8EZknaBwhgKfB2gIhYLGkBcAvwGHByRDxeR9xmZta/ak9+EXFMi8Ff6jD96cDp3YvIzMy2dD1529PMzKybnPzMzKw4Tn5mZlYcJz8zMyuOk5+ZmRXHyc/MzIrj5GdmZsVx8jMzs+I4+ZmZWXGc/MzMrDhOfmZmVhwnPzMzK46Tn5mZFcfJz8zMiuPkZ2ZmxXHyMzOz4jj5mZlZcZz8zMysOE5+ZmZWnFF1B2BmZRmYd1kty106/7Balmu9yVd+ZmZWHCc/MzMrjpOfmZkVx8nPzMyK4+RnZmbFcfIzM7PiOPmZmVlxnPzMzKw4Tn5mZlYcJz8zMyuOk5+ZmRXHv+1pPaOu33w0s/L4ys/MzIrj5GdmZsVx8jMzs+I4+ZmZWXGc/MzMrDhOfmZmVpzak5+kL0taLenmyrAJkq6QdEf+Oz4Pl6TPSFoi6SZJL6ovcjMz61e1Jz/gHODgpmHzgIURMR1YmPsBDgGm589c4HObKUYzM9uC1J78IuJq4IGmwbOBc3P3ucCRleHnRXINME7SlM0TqZmZbSlqT35tTI6IFbl7JTA5d08FllWmuzcPG0TSXEmLJC1as2ZNdyM1M7O+06vJ70kREUBs4DxnR8TMiJg5adKkLkVmZmb9qleT36rG7cz8d3UevhzYrTLdtDzMzMxs2Ho1+V0KzMndc4BvV4Yfl9/63A94sHJ71MzMbFhq/68Oki4AZgETJd0L/CMwH1gg6UTgHuCoPPnlwKHAEuBh4ITNHrCZmfW92pNfRBzTZtQBLaYN4OTuRmRmZlu6Xr3taWZm1jVOfmZmVhwnPzMzK46Tn5mZFcfJz8zMiuPkZ2ZmxXHyMzOz4jj5mZlZcZz8zMysOE5+ZmZWHCc/MzMrjpOfmZkVx8nPzMyK4+RnZmbFcfIzM7PiOPmZmVlxnPzMzKw4Tn5mZlYcJz8zMyuOk5+ZmRXHyc/MzIrj5GdmZsVx8jMzs+I4+ZmZWXGc/MzMrDhOfmZmVhwnPzMzK46Tn5mZFcfJz8zMiuPkZ2ZmxXHyMzOz4jj5mZlZcZz8zMysOE5+ZmZWHCc/MzMrzqi6A7DeMzDvsrpDMDPrKl/5mZlZcZz8zMysOD1921PSUmA98DjwWETMlDQBuAgYAJYCR0XEb+qK0czM+k8/XPm9KiL2iYiZuX8esDAipgMLc7+Zmdmw9UPyazYbODd3nwscWWMsZmbWh3o9+QXwfUnXSZqbh02OiBW5eyUwuZ7QzMysX/X0Mz9g/4hYLmkX4ApJt1VHRkRIiuaZcqKcC/DMZz5z80RqZmZ9o6eTX0Qsz39XS7oEeCmwStKUiFghaQqwusV8ZwNnA8ycOfNpydHMylPn91eXzj+stmVbaz1721PSDpLGNrqB1wA3A5cCc/Jkc4Bv1xOhmZn1q16+8psMXCIJUpxfi4j/lvQLYIGkE4F7gKNqjNHMzPpQzya/iLgLeEGL4WuBAzZ/RGZmtqXo2dueZmZm3eLkZ2ZmxXHyMzOz4jj5mZlZcZz8zMysOE5+ZmZWHCc/MzMrjpOfmZkVx8nPzMyK4+RnZmbFcfIzM7PiOPmZmVlxnPzMzKw4Tn5mZlYcJz8zMyuOk5+ZmRXHyc/MzIrj5GdmZsVx8jMzs+I4+ZmZWXGc/MzMrDhOfmZmVhwnPzMzK46Tn5mZFWdU3QFYawPzLqs7BDOzLZav/MzMrDhOfmZmVhwnPzMzK46Tn5mZFcfJz8zMiuO3Pc3Muqyut7eXzj+sluX2A1/5mZlZcZz8zMysOE5+ZmZWHD/zG4J/acXMbMvjKz8zMyuOk5+ZmRXHyc/MzIrj5GdmZsXpy+Qn6WBJt0taImle3fGYmVl/6bvkJ2lr4D+AQ4C9gWMk7V1vVGZm1k/6LvkBLwWWRMRdEfEH4EJgds0xmZlZH+nH7/lNBZZV+u8F9q1OIGkuMDf3PiTp9i7GMxG4v4vljyTH2j39FK9j7Z6eilcf7zh6qFh3H9Fgekw/Jr8hRcTZwNmbY1mSFkXEzM2xrE3lWLunn+J1rN3TT/H2U6zd0I+3PZcDu1X6p+VhZmZmw9KPye8XwHRJe0jaBjgauLTmmMzMrI/03W3PiHhM0ruA7wFbA1+OiMU1hrRZbq+OEMfaPf0Ur2Ptnn6Kt59iHXGKiLpjMDMz26z68banmZnZJnHyMzOz4jj5tSBpgqQrJN2R/45vM92cPM0dkubkYWMl3VD53C/pjDzueElrKuNOqjvePPyq/HNxjbh2ycPHSLoo/4zczyUN1BmrpO0lXSbpNkmLJc2vTD9idTvUz+d1qhdJ78vDb5f02uGWuSk2Nl5JB0m6TtKv8t9XV+Zp2SZqjHVA0iOVeM6qzPPivA5LJH1GkmqO9S1Nx4AnJO2Tx3WlXocZ7yskXS/pMUlvbBrX7tjQlbrtCRHhT9MH+AQwL3fPAz7eYpoJwF357/jcPb7FdNcBr8jdxwNn9lq8wFXAzBbzvBM4K3cfDVxUZ6zA9sCr8jTbAD8CDhnJuiW9RHUnsGdexo3A3sOpF9LP7d0IjAH2yOVsPZwya4r3hcCuuft5wPLKPC3bRI2xDgA3tyn3WmA/QMB3G22irlibpvkz4M5u1usGxDsAPB84D3jjUPtbt+q2Vz6+8mttNnBu7j4XOLLFNK8FroiIByLiN8AVwMHVCSQ9G9iFdJDuphGJd4hyLwYOGIEzv42ONSIejogrASL9tN31pO95jqTh/Hxeu3qZDVwYEY9GxN3AklxeN3+Sb6PjjYhfRsR9efhiYDtJY0YorhGNtV2BkqYAO0XENZGO1ufRuk3VFesxed5uGzLeiFgaETcBTzTN23J/62Ld9gQnv9YmR8SK3L0SmNximlY/sza1aZrG2WD1ldo3SLpJ0sWSdmNkjES8/5lvw3yosgM/OU9EPAY8CDyjB2JF0jjgCGBhZfBI1O1wtmu7emk373DK3FibEm/VG4DrI+LRyrBWbaLOWPeQ9EtJP5T08sr09w5RZh2xNrwZuKBp2EjX63Dj3dB5u1W3PaHvvuc3UiT9APiTFqM+UO2JiJC0sd8HORo4ttL/X8AFEfGopLeTzhpf3XLOzRvvWyJiuaSxwDdyzOdtYBlP6nbdShpFOqB8JiLuyoM3um5LJ2kG8HHgNZXBI9omRsAK4JkRsVbSi4Fv5bh7lqR9gYcj4ubK4F6r12IVm/wi4sB24yStkjQlIlbkS//VLSZbDsyq9E8j3c9vlPECYFREXFdZ5trK9F8kPf+qPd6IWJ7/rpf0NdItlPN46qfk7s0JZ2egug6bPdbsbOCOiDijssyNrtsWyx7q5/Pa1Uunebv1k3ybEi+SpgGXAMdFxJ2NGTq0iVpizXdPHs0xXSfpTuDZefrqre+RqttNqtfsaJqu+rpUr8ONt9O8s5rmvYru1W1vqPuhYy9+gH9h8EsZn2gxzQTgbtID4vG5e0Jl/HzgI03zTKl0vx64pu54SSdAE/M0o0nPLt6R+09m8AP9BXXXLXAa6Yx5q27Uba6Pu0gvrDReHJjRNE3LegFmMPiFl7tILyIMWeYm1OemxDsuT/8XLcps2SZqjHUSsHXu3pN0EG60ieaXMg6tM9bcv1WOcc9u1+tw461Mew5Pf+Gl3f424nXbK5/aA+jFD+m+/ULgDuAHlYYwE/hiZbq3kV5qWAKc0FTGXcCfNg37Z9KLBTcCVzaPryNeYAfSG6k35dg+XTnIbAt8PU9/bXVHrinWaUAAtwI35M9JI123wKHA/5LenvtAHvZR4HVD1Qvp1u6dwO1U3oxrVeYItteNihf4IPC7Sl3eQHpBq22bqDHWN+RYbiC96HREpcyZwM25zDPJv1xVV6x53CyaTsC6Wa/DjPclpOd2vyNdoS7utL91s2574eOfNzMzs+L4bU8zMyuOk5+ZmRXHyc/MzIrj5GdmZsVx8jMzs+I4+ZmZWXGc/MzMrDj/HwWXdfIUdpDFAAAAAElFTkSuQmCC\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.hist(means)\n",
    "plt.title(\"Histogram of Mean Gender Projection for Random Subsets of Adjectives\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 404,
   "metadata": {},
   "outputs": [],
   "source": [
    "Xbooks = np.array([getwv(mbooks, x) for x in adjs_all if x in mbooks.wv.vocab])\n",
    "Xbooks = StandardScaler().fit_transform(Xbooks)\n",
    "bookspointsy = [swn_score(x) for x in adjs_all if x in mbooks.wv.vocab]\n",
    "Xbooks_s = np.hstack((Xbooks, 10*np.array([bookspointsy]).T))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 405,
   "metadata": {},
   "outputs": [],
   "source": [
    "kmeansbookslist = []\n",
    "inertias = []\n",
    "for i in range(1, 40):\n",
    "    temp = KMeans(n_clusters=5*i, random_state=0).fit(Xbooks_s)\n",
    "    kmeansbookslist.append(temp)\n",
    "    inertias.append(temp.inertia_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 408,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(677, 777)"
      ]
     },
     "execution_count": 408,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(wordsp), len(wordsb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 406,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Text(0.5, 1.0, 'Within-Cluster Sum-of-Squares (Inertia) vs \\n Number of Clusters for KMeans')"
      ]
     },
     "execution_count": 406,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.plot([5*i for i in range(1,40)], inertias)\n",
    "plt.title(\"Within-Cluster Sum-of-Squares (Inertia) vs \\n Number of Clusters for KMeans\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 454,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(0, 8), (1, 77), (2, 17), (3, 55), (5, 15), (6, 5), (7, 83), (9, 25), (10, 23), (11, 11), (17, 110), (26, 3), (27, 8), (28, 17), (30, 14), (35, 18), (46, 113), (47, 20), (51, 13), (59, 17), (60, 3), (70, 7), (71, 43)]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(18,\n",
       " {0: ['ridiculous',\n",
       "   'dirty',\n",
       "   'awful',\n",
       "   'nasty',\n",
       "   'ugly',\n",
       "   'stupid',\n",
       "   'silly',\n",
       "   'crazy'],\n",
       "  2: ['mechanical',\n",
       "   'artificial',\n",
       "   'rational',\n",
       "   'capable',\n",
       "   'vague',\n",
       "   'pure',\n",
       "   'positive',\n",
       "   'arbitrary',\n",
       "   'objective',\n",
       "   'coherent'],\n",
       "  5: ['precise',\n",
       "   'composed',\n",
       "   'colorful',\n",
       "   'solid',\n",
       "   'stylish',\n",
       "   'delightful',\n",
       "   'polished',\n",
       "   'captivating',\n",
       "   'meticulous',\n",
       "   'crisp'],\n",
       "  6: ['surprising', 'careful', 'obvious', 'quick', 'helpful'],\n",
       "  9: ['normal',\n",
       "   'suspicious',\n",
       "   'false',\n",
       "   'disturbing',\n",
       "   'criminal',\n",
       "   'cynical',\n",
       "   'cruel',\n",
       "   'grim',\n",
       "   'unpleasant',\n",
       "   'hostile'],\n",
       "  10: ['patient',\n",
       "   'insecure',\n",
       "   'maternal',\n",
       "   'generous',\n",
       "   'sensitive',\n",
       "   'brave',\n",
       "   'selfish',\n",
       "   'anxious',\n",
       "   'daring',\n",
       "   'mature'],\n",
       "  11: ['elegant',\n",
       "   'clever',\n",
       "   'wonderful',\n",
       "   'sly',\n",
       "   'lovely',\n",
       "   'witty',\n",
       "   'amazing',\n",
       "   'lyrical',\n",
       "   'playful',\n",
       "   'lively'],\n",
       "  26: ['beautiful', 'smart', 'charming'],\n",
       "  27: ['familial',\n",
       "   'romantic',\n",
       "   'artistic',\n",
       "   'intense',\n",
       "   'youthful',\n",
       "   'profound',\n",
       "   'constant',\n",
       "   'dynamic'],\n",
       "  28: ['competitive',\n",
       "   'motivated',\n",
       "   'secure',\n",
       "   'preoccupied',\n",
       "   'stable',\n",
       "   'direct',\n",
       "   'efficient',\n",
       "   'modest',\n",
       "   'active',\n",
       "   'content'],\n",
       "  30: ['shrewd',\n",
       "   'spirited',\n",
       "   'talented',\n",
       "   'attractive',\n",
       "   'energetic',\n",
       "   'affectionate',\n",
       "   'glamorous',\n",
       "   'wise',\n",
       "   'frank',\n",
       "   'sexy'],\n",
       "  35: ['lazy',\n",
       "   'delicate',\n",
       "   'quiet',\n",
       "   'neat',\n",
       "   'lean',\n",
       "   'gloomy',\n",
       "   'slight',\n",
       "   'narrow',\n",
       "   'shallow',\n",
       "   'dull'],\n",
       "  47: ['unsentimental',\n",
       "   'scrupulous',\n",
       "   'skeptical',\n",
       "   'passionate',\n",
       "   'casual',\n",
       "   'blunt',\n",
       "   'idiosyncratic',\n",
       "   'articulate',\n",
       "   'quirky',\n",
       "   'contradictory'],\n",
       "  51: ['sophisticated',\n",
       "   'wary',\n",
       "   'decent',\n",
       "   'progressive',\n",
       "   'sensible',\n",
       "   'respected',\n",
       "   'healthy',\n",
       "   'reliable',\n",
       "   'innovative',\n",
       "   'tolerant'],\n",
       "  59: ['admirable',\n",
       "   'ambitious',\n",
       "   'thorough',\n",
       "   'eloquent',\n",
       "   'honest',\n",
       "   'provocative',\n",
       "   'earnest',\n",
       "   'insightful',\n",
       "   'imaginative',\n",
       "   'intelligent'],\n",
       "  60: ['sweet', 'clean', 'warm'],\n",
       "  70: ['predictable',\n",
       "   'subtle',\n",
       "   'striking',\n",
       "   'dramatic',\n",
       "   'persuasive',\n",
       "   'realistic',\n",
       "   'conventional'],\n",
       "  71: ['attentive',\n",
       "   'skillful',\n",
       "   'experienced',\n",
       "   'cheerful',\n",
       "   'devious',\n",
       "   'fastidious',\n",
       "   'logical',\n",
       "   'adventurous',\n",
       "   'flexible',\n",
       "   'cute']})"
      ]
     },
     "execution_count": 454,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "kmeansbooks_s = KMeans(n_clusters=K+6, random_state=0).fit(Xbooks_s)\n",
    "wordsb = list(filter(lambda x: x in mbooks.wv.vocab, adjs_all))\n",
    "dbookss2 = {k:[] for k in range(K+6)}\n",
    "for i in range(len(Xbooks)):\n",
    "    dbookss2[kmeansbooks_s.labels_[i]].append(wordsb[i])\n",
    "print([(x, len(dbookss2[x])) for x in range(K) if len(dbookss2[x]) > 2])\n",
    "bstereos2 = {k:v[:10] for k,v in dbookss2.items() if shuffle(v) is None and len(v) > 2 and len(v) < 50}\n",
    "len(bstereos2), bstereos2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 484,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "-0.055\t p: \t0.05084\n",
      "2\n",
      "0.025\t p: \t0.01894\n",
      "5\n",
      "-0.040\t p: \t0.19352\n",
      "10\n",
      "-0.120\t p: \t0.00000\n",
      "11\n",
      "-0.113\t p: \t0.00000\n",
      "26\n",
      "-0.216\t p: \t0.00000\n",
      "27\n",
      "-0.045\t p: \t0.13207\n",
      "28\n",
      "0.028\t p: \t0.01232\n",
      "30\n",
      "-0.072\t p: \t0.00414\n",
      "35\n",
      "-0.068\t p: \t0.00877\n",
      "51\n",
      "0.023\t p: \t0.02572\n",
      "59\n",
      "0.036\t p: \t0.00365\n",
      "60\n",
      "-0.163\t p: \t0.00000\n",
      "71\n",
      "-0.037\t p: \t0.22969\n"
     ]
    }
   ],
   "source": [
    "for k,v in bstereos2.items():\n",
    "    mval = np.mean([gproj(mbooks, x) for x in v])\n",
    "    zval = (mval - np.mean(means2))/(np.std(means2)/np.sqrt(3))\n",
    "    pval = norm.pdf(zval)\n",
    "    if pval > 0.25:\n",
    "        continue\n",
    "    print(k)\n",
    "    print(f\"{mval:.3f}\\t\", f\"p: \\t{pval:.5f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 435,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(-0.018669227, 0.030667638, 0.04266604967415333, -0.08000450395047665)"
      ]
     },
     "execution_count": 435,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "means2 = [np.mean([gproj(mbooks, x) for x in sample(wordsb, randint(6, 11))]) for i in range(1000)]\n",
    "np.mean(means2), np.std(means2), np.mean(means2) + 2*np.std(means2), np.mean(means2) - 2*np.std(means2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getg(model):\n",
    "    matrix = []\n",
    "    for a, b in pairs:\n",
    "        if a not in model.wv.vocab or b not in model.wv.vocab:\n",
    "            continue\n",
    "        center = (model.wv.get_vector(a) + model.wv.get_vector(b))/2\n",
    "        matrix.append(model.wv.get_vector(a) - center)\n",
    "        matrix.append(model.wv.get_vector(b) - center)\n",
    "    matrix = np.array(matrix)\n",
    "    p = PCA(n_components = 10)\n",
    "    p.fit(matrix)\n",
    "#     bar(range(10), p.explained_variance_ratio_)\n",
    "#     print(matrix.shape)\n",
    "    return p.components_[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gproj(m, w1):\n",
    "    wvec = m.wv.get_vector(w1)\n",
    "    g = getg(m)\n",
    "    p = np.dot(g, wvec)/(np.linalg.norm(g)*np.linalg.norm(wvec))\n",
    "    return p"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(0, 404),\n",
       " (1, 5),\n",
       " (3, 24),\n",
       " (4, 63),\n",
       " (7, 14),\n",
       " (9, 14),\n",
       " (17, 84),\n",
       " (19, 4),\n",
       " (24, 15),\n",
       " (26, 6),\n",
       " (35, 11)]"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[(x, len(dpol[x])) for x in range(40) if len(dpol[x]) > 2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "temp = KMeans(n_clusters=5, random_state=0).fit(np.array([getwv(mpol, x) for x in dpol[24] if x in mpol.wv.vocab]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['dedicated',\n",
       " 'directed',\n",
       " 'educated',\n",
       " 'focused',\n",
       " 'friendly',\n",
       " 'loyal',\n",
       " 'organized',\n",
       " 'sympathetic',\n",
       " 'hostile',\n",
       " 'ardent',\n",
       " 'devoted',\n",
       " 'experienced',\n",
       " 'respected',\n",
       " 'revered']"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dpol[9]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.corpus import wordnet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Lemma('amicable.a.01.amicable')]"
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# \"friendly\" and \"hostile\" grouped together...\n",
    "wordnet.synset('hostile.a.01').lemmas()[0].antonyms()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0.74429077, 0.47319326, 0.29986393, 0.58892566)"
      ]
     },
     "execution_count": 88,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cos_sim(mpol, \"good\", \"bad\"), cos_sim(mpol, \"good\", \"nice\"), \\\n",
    "cos_sim(mpol, \"good\", \"kind\"), cos_sim(mpol, \"nice\", \"bad\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0.42275077,\n",
       " 0.32262966,\n",
       " 0.17268197,\n",
       " 0.10527108,\n",
       " 0.1828795,\n",
       " -0.22784947,\n",
       " -0.17796314,\n",
       " -0.18152553,\n",
       " -0.21603599,\n",
       " -0.07453782)"
      ]
     },
     "execution_count": 111,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gproj(mpol, \"she\"), gproj(mpol, \"woman\"), gproj(mpol, \"beautiful\"), gproj(mpol, \"teacher\"), gproj(mpol, \"sassy\"), \\\n",
    "gproj(mpol, \"president\"), gproj(mpol, \"professor\"), gproj(mpol, \"he\"), gproj(mpol, \"man\"), gproj(mpol, \"strong\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "677"
      ]
     },
     "execution_count": 140,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "polpoints = [(gproj(mpol, x), swn_score(x)) for x in adjs_all if x in mpol.wv.vocab]\n",
    "len(polpoints)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "metadata": {},
   "outputs": [],
   "source": [
    "polpointsx = [x[0] for x in polpoints]\n",
    "polpointsy = [x[1] for x in polpoints]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<matplotlib.lines.Line2D at 0x1287656d8>]"
      ]
     },
     "execution_count": 142,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.plot(polpointsx, polpointsy, \"o\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Gender-neutral adjectives describing people"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "url = 'http://ideonomy.mit.edu/essays/traits.html'\n",
    "file = urllib2.urlopen(url)\n",
    "html = file.read()\n",
    "file.close()\n",
    "soup = BeautifulSoup(html)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 196,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Accessible'"
      ]
     },
     "execution_count": 196,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "soup.find_all('li')[0].contents[0].strip()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "adjs = [x.contents[0].strip().lower() for x in soup.find_all('li')]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 201,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(393, 638, 'accessible', 'able')"
      ]
     },
     "execution_count": 201,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(adjectives), len(adjs), adjs[0], adjectives[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "882"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "adjs_all = adjs + [x.strip() for x in adjectives if x not in adjs]\n",
    "len(adjs_all)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 203,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[3]"
      ]
     },
     "execution_count": 203,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list(filter(lambda x: x > 2, [1,2,3]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "adjs_all = list(filter(lambda x: x , adjs_all))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len([x for x in adjs_all if x in vocab])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "[x for x in adjectives if x not in adjs][:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "adjs[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('adjectives.csv') as csv_file:\n",
    "    adjectives = csv_file.read().split(\",\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 309,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('grammar.csv') as csv_file:\n",
    "    grammar = csv_file.read().split(\",\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "adjectives[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dill.dump_session('thesis_env.db')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Parsing and Processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Doc:\n",
    "    def __init__(self, id: int, gdescriptors: List[str], descriptors: List[str], text: str) -> None:\n",
    "        self.id = id\n",
    "        self.gdescriptors = gdescriptors\n",
    "        self.descriptors = descriptors\n",
    "        self.text = text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def parse_doc(file: str) -> Doc:\n",
    "    tree = ET.parse(file)\n",
    "    root = tree.getroot()\n",
    "    try:\n",
    "        fulltext = root.find('body').find('body.content').find(\"*[@class='full_text']\")\n",
    "        text = reduce(lambda x, y: x + y, [x.text for x in fulltext.findall('p')])\n",
    "        id = int(root.find('head').find('docdata').find('doc-id').attrib['id-string'])\n",
    "        classifiers = root.find('head').find('docdata').find('identified-content')\n",
    "        gclassifiers = classifiers.findall(\"*[@class='online_producer'][@type='general_descriptor']\")\n",
    "        dclassifiers = classifiers.findall(\"*[@class='online_producer'][@type='descriptor']\")\n",
    "        gdescriptors = [c.text for c in gclassifiers]\n",
    "        descriptors = [c.text for c in dclassifiers]\n",
    "        return Doc(id, gdescriptors, descriptors, text)\n",
    "    except:\n",
    "        return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_docs_by_month(year: int, month: int) -> List[Doc]:\n",
    "    days = monthrange(year, month)[1]\n",
    "    docs = []\n",
    "    for day in range(1, days+1):\n",
    "        for file in os.listdir(f'data/{year}/{month:02}/{day:02}/'):\n",
    "            doc = parse_doc(f'data/{year}/{month:02}/{day:02}/{file}')\n",
    "            if doc:\n",
    "                docs.append(doc)\n",
    "    return docs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "t = time.process_time()\n",
    "all_ = []\n",
    "for y in range(1988, 2007):\n",
    "    print(y)\n",
    "    for i in range(1,13):\n",
    "        all_ += get_docs_by_month(y, i)\n",
    "time.process_time() - t"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(all_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dill.dump_session('thesis_env.db')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Process by descriptor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "15.562200000000004"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "t = time.process_time()\n",
    "alld = {}\n",
    "for doc in all_:\n",
    "    doc = Doc(doc.id, doc.gdescriptors, doc.descriptors, doc.text)\n",
    "    for d in doc.descriptors:\n",
    "        count, docs = alld.get(d, (0, set()))\n",
    "        docs.add(doc)\n",
    "        alld[d] = (count + 1, docs)\n",
    "time.process_time() - t"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('Terrorism', 12393)\n",
      "('Books and Literature', 11222)\n",
      "('Politics and Government', 10589)\n",
      "('Baseball', 9039)\n",
      "('United States Politics and Government', 8295)\n",
      "('Motion Pictures', 7748)\n",
      "('Music', 7317)\n",
      "('Weddings and Engagements', 7280)\n",
      "('Medicine and Health', 6829)\n",
      "('Travel and Vacations', 6537)\n",
      "('Deaths (Obituaries)', 6438)\n",
      "('Television', 6171)\n",
      "('Football', 6070)\n",
      "('Computers and the Internet', 5952)\n",
      "('Basketball', 5746)\n"
     ]
    }
   ],
   "source": [
    "alldlist = sorted([(d, alld[d][0], alld[d][1]) for d in alld], key=lambda x: x[1], reverse=True)\n",
    "for x in range(15):\n",
    "    print(alldlist[x][0:2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "def exclude(e, x):\n",
    "    for d in x.descriptors:\n",
    "        if d == e:\n",
    "            continue\n",
    "        if d in [t[0] for t in ['Politics and Government', 'Motion Pictures', 'Books and Literature']]:\n",
    "            return False\n",
    "    return True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getlist(topic):\n",
    "    return list(filter(lambda x: exclude(topic, x), alld[topic][1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "politics = getlist('Politics and Government')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "movies = getlist('Motion Pictures')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "books = getlist('Books and Literature')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sports = getlist('Baseball') + getlist('Football')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(10394, 7728, 11191, 15045)"
      ]
     },
     "execution_count": 131,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(politics), len(movies), len(books), len(sports)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "long = politics + movies + books\n",
    "len(long)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "politics = sample(politics, 7370)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "movies = sample(movies, 7370)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "books = sample(books, 7370)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {},
   "outputs": [],
   "source": [
    "sports = sample(sports, 7370)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# SentiWordNet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package sentiwordnet to\n",
      "[nltk_data]     /Users/mayjiang/nltk_data...\n",
      "[nltk_data]   Package sentiwordnet is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "nltk.download('sentiwordnet')\n",
    "from nltk.corpus import sentiwordnet as swn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "metadata": {},
   "outputs": [],
   "source": [
    "sman = swn.senti_synset('man.n.01')\n",
    "swoman = swn.senti_synset('woman.n.01')\n",
    "tall = swn.senti_synset('tall.a.01')\n",
    "maid = swn.senti_synset('maid.n.01')\n",
    "smart = swn.senti_synset('smart.a.01')\n",
    "awesome = swn.senti_synset('awesome.a.01')\n",
    "shy = swn.senti_synset('shy.a.01')\n",
    "liberal = swn.senti_synset('liberal.a.01')\n",
    "conservative = swn.senti_synset('conservative.a.01')\n",
    "moderate = swn.senti_synset('moderate.a.01')\n",
    "demanding = swn.senti_synset('demanding.a.01')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<man.n.01: PosScore=0.0 NegScore=0.0>\n",
      "<woman.n.01: PosScore=0.0 NegScore=0.0>\n",
      "<tall.a.01: PosScore=0.5 NegScore=0.0>\n",
      "<maid.n.01: PosScore=0.0 NegScore=0.0>\n",
      "<smart.a.01: PosScore=0.5 NegScore=0.0>\n",
      "<amazing.s.02: PosScore=0.875 NegScore=0.125>\n",
      "<diffident.a.02: PosScore=0.25 NegScore=0.25>\n",
      "<broad.s.08: PosScore=0.625 NegScore=0.0>\n",
      "<conservative.a.01: PosScore=0.0 NegScore=0.0>\n",
      "<moderate.a.01: PosScore=0.25 NegScore=0.625>\n",
      "<demanding.a.01: PosScore=0.0 NegScore=0.375>\n"
     ]
    }
   ],
   "source": [
    "for s in [sman, swoman, tall, maid, smart, awesome, shy, liberal, conservative, moderate, demanding]:\n",
    "    print(s)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sentiment Bias - weighted projection bias, each word multiplied by (pos_score - neg_score)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 204,
   "metadata": {},
   "outputs": [],
   "source": [
    "adjs_bias = []\n",
    "for x in adjs_all:\n",
    "    try:\n",
    "        s = swn.senti_synset(x + \".a.01\")\n",
    "        if s.pos_score() - s.neg_score() != 0:\n",
    "            adjs_bias.append(x)\n",
    "    except:\n",
    "        continue"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 205,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "672"
      ]
     },
     "execution_count": 205,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(adjs_bias)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 491,
   "metadata": {},
   "outputs": [],
   "source": [
    "temp = [gproj(getg(mpol), mpol, x) for x in adjs_bias if x in mpol.wv.vocab]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 280,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "209"
      ]
     },
     "execution_count": 280,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(list(filter(lambda x: x > 0, temp)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 308,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[-0.047809258, -0.05460444, 0.026453687, 0.062143218]"
      ]
     },
     "execution_count": 308,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[gproj(getg(mpol), mpol, x) for x in ['above', 'about', 'ago', 'although'] if x in mpol.wv.vocab]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 311,
   "metadata": {},
   "outputs": [],
   "source": [
    "grams = [gproj(getg(mpol), mpol, x) for x in grammar if x in mpol.wv.vocab]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 326,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(-0.0039149676, 0.056799952, -0.114319265, 0.11247943)"
      ]
     },
     "execution_count": 326,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.mean(grams), np.std(grams), np.min(grams), np.max(grams)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 291,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(-0.017466776, 0.07178123)"
      ]
     },
     "execution_count": 291,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tmean, tstd = np.mean(temp), np.std(temp)\n",
    "tmean, tstd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 281,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(-0.3098677, 0.21781564)"
      ]
     },
     "execution_count": 281,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tmin, tmax = np.min(temp), np.max(temp)\n",
    "tmin, tmax"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 493,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.hist(temp)\n",
    "plt.title(\"Projections of Adjectives on Gender Vector - Politics\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 499,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "74"
      ]
     },
     "execution_count": 499,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(list(filter(lambda x: x < tmean - tstd, temp)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 528,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "209"
      ]
     },
     "execution_count": 528,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(list(filter(lambda x: x > tmean + tstd, temp)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 521,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "629"
      ]
     },
     "execution_count": 521,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(list(filter(lambda x: x in mpol.wv.vocab, adjs_all)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 540,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_adjs = 209"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 329,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_adj_lists(m):\n",
    "    temp = sorted([(x, gproj(getg(m), m, x)) for x in adjs_bias \\\n",
    "                   if x in m.wv.vocab], key=lambda x: x[1])\n",
    "    return [x[0] for x in temp[:num_adjs]], \\\n",
    "           [x[0] for x in temp[len(temp)-num_adjs:]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 235,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(300, 300)"
      ]
     },
     "execution_count": 235,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "apol1, apol2 = get_adj_lists(mpol)\n",
    "len(apol1), len(apol2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 482,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('loyal', 0.21781564, 0.5),\n",
       " ('calm', 0.15803167, -0.375),\n",
       " ('admirable', 0.15252876, 0.75),\n",
       " ('charismatic', 0.14838041, 0.5),\n",
       " ('clumsy', 0.14027488, -0.125)]"
      ]
     },
     "execution_count": 482,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sorted([(x, gproj(getg(mpol), mpol, x), swn_score(x)) for x in apol2], \\\n",
    "       key=lambda x: x[1], reverse=True)[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 483,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('direct', 0.24215876, -0.25),\n",
       " ('civil', 0.22271426, 0.125),\n",
       " ('questioning', 0.22204797, -0.125),\n",
       " ('independent', 0.20418105, 0.125),\n",
       " ('frank', 0.17268638, 0.125)]"
      ]
     },
     "execution_count": 483,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sorted([(x, gproj(getg(mmov), mmov, x), swn_score(x)) for x in amov2], \\\n",
    "       key=lambda x: x[1], reverse=True)[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 486,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('patriotic', 0.29985127, 0.125),\n",
       " ('liberal', 0.28159603, 0.625),\n",
       " ('grand', 0.22807221, 0.25),\n",
       " ('progressive', 0.22761683, 0.625),\n",
       " ('moderate', 0.22250967, -0.375)]"
      ]
     },
     "execution_count": 486,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sorted([(x, gproj(getg(mbooks), mbooks, x), swn_score(x)) for x in abooks2], \\\n",
    "       key=lambda x: x[1], reverse=True)[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 484,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('demure', -0.40574434, 0.5),\n",
       " ('fabulous', -0.40027168, 0.75),\n",
       " ('sexy', -0.38357872, 0.625),\n",
       " ('lovely', -0.34983596, 0.625),\n",
       " ('beautiful', -0.33718085, 0.75)]"
      ]
     },
     "execution_count": 484,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sorted([(x, gproj(getg(mmov), mmov, x), swn_score(x)) for x in amov1], \\\n",
    "       key=lambda x: x[1])[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 485,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('patient', -0.3098677, 0.25),\n",
       " ('healthy', -0.2604588, 0.75),\n",
       " ('diligent', -0.22501096, 0.625),\n",
       " ('decent', -0.19484079, 0.875),\n",
       " ('lovely', -0.18960457, 0.625)]"
      ]
     },
     "execution_count": 485,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sorted([(x, gproj(getg(mpol), mpol, x), swn_score(x)) for x in apol1], \\\n",
    "       key=lambda x: x[1])[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 489,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('beautiful', -0.36147895, 0.75),\n",
       " ('sexy', -0.31237495, 0.625),\n",
       " ('glamorous', -0.28069398, 0.375),\n",
       " ('sensual', -0.26773366, 0.125),\n",
       " ('caring', -0.23501632, 0.25)]"
      ]
     },
     "execution_count": 489,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sorted([(x, gproj(getg(mbooks), mbooks, x), swn_score(x)) for x in abooks1], \\\n",
    "       key=lambda x: x[1])[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sorted([(x, swn_score(x)) for x in apol2], key=lambda x: x[1])[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 262,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(300, 300)"
      ]
     },
     "execution_count": 262,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "abooks1, abooks2 = get_adj_lists(mbooks)\n",
    "len(abooks1), len(abooks2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 263,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(300, 300)"
      ]
     },
     "execution_count": 263,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "amov1, amov2 = get_adj_lists(mmov)\n",
    "len(amov1), len(amov2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 268,
   "metadata": {},
   "outputs": [],
   "source": [
    "def weighted_sb(m):\n",
    "    a1, a2 = get_adj_lists(m)\n",
    "    return sum([abs(gproj(getg(m), m, x))*swn_score(x) for x in a1]), \\\n",
    "           sum([abs(gproj(getg(m), m, x))*swn_score(x) for x in a2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 345,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_gdir(m):\n",
    "    h, s = gproj(getg(m), m, 'he'), gproj(getg(m), m, 'she')\n",
    "    print(h, s)\n",
    "    return h, s, (h+s)/2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 270,
   "metadata": {},
   "outputs": [],
   "source": [
    "def norm_sb(m):\n",
    "    a1, a2 = get_adj_lists(m)\n",
    "    h, s, hs = get_gdir(m)\n",
    "    return sum([abs(gproj(getg(m), m, x)/s)*swn_score(x) for x in a1]), \\\n",
    "           sum([abs(gproj(getg(m), m, x)/h)*swn_score(x) for x in a2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 300,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.22568104 -0.4520073\n",
      "0.18597512 -0.49779502\n",
      "0.18463565 -0.42947298\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "((3.162538355216384, 1.4912605471909046),\n",
       " (3.3112002988010647, 4.077601205557585),\n",
       " (5.51348378136754, 5.36511062271893))"
      ]
     },
     "execution_count": 300,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "norm_sb(mpol), norm_sb(mbooks), norm_sb(mmov)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 272,
   "metadata": {},
   "outputs": [],
   "source": [
    "def scale_sb(m):\n",
    "    a1, a2 = get_adj_lists(m)\n",
    "    h, s, hs = get_gdir(m)\n",
    "    return sum([abs(gproj(getg(m), m, x) - hs)*swn_score(x) for x in a1]), \\\n",
    "           sum([abs(gproj(getg(m), m, x) - hs)*swn_score(x) for x in a2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 261,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.22568104 -0.4520073\n",
      "0.18597512 -0.49779502\n",
      "0.18463565 -0.42947298\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "((1.7247874721918488, 0.6310292075386883),\n",
       " (0.14364903768710796, 1.4876473221679907),\n",
       " (1.4787104708887635, 2.0745052116617444))"
      ]
     },
     "execution_count": 261,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "scale_sb(mpol), scale_sb(mbooks), scale_sb(mmov)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 223,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.22568104 -0.4520073\n",
      "0.18597512 -0.49779502\n",
      "0.18463565 -0.42947298\n"
     ]
    }
   ],
   "source": [
    "hpol, spol, avgpol = get_gdir(mpol)\n",
    "hbooks, sbooks, avgbooks = get_gdir(mbooks)\n",
    "hmov, smov, avgmov = get_gdir(mmov)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [],
   "source": [
    "def swn_score(x):\n",
    "    try:\n",
    "        s = swn.senti_synset(x + \".a.01\")\n",
    "        return s.pos_score() - s.neg_score()\n",
    "    except:\n",
    "        return 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 227,
   "metadata": {},
   "outputs": [],
   "source": [
    "def swn_bias(m):\n",
    "    a1, a2 = get_adj_lists(m)\n",
    "    return sum([swn_score(x) for x in a1]), sum([swn_score(x) for x in a2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 536,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((1.7336519272215665, 0.2859736363653269),\n",
       " (2.061730328530073, 0.593745810049586),\n",
       " (1.6817176950350405, 1.0290070636183373))"
      ]
     },
     "execution_count": 536,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "weighted_sb(mpol), weighted_sb(mbooks), weighted_sb(mmov)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 537,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.22568104 -0.4520073\n",
      "0.18597512 -0.49779502\n",
      "0.18463565 -0.42947298\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "((3.8354512557163836, 1.267158538839567),\n",
       " (4.141725453048945, 3.1926088630706073),\n",
       " (3.915770623087883, 5.573176347956178))"
      ]
     },
     "execution_count": 537,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "norm_sb(mpol), norm_sb(mbooks), norm_sb(mmov)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 538,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.22568104 -0.4520073\n",
      "0.18597512 -0.49779502\n",
      "0.18463565 -0.42947298\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "((0.886271051492542, -0.2091150508012447),\n",
       " (0.789029377579689, 0.9314467736659573),\n",
       " (1.2545430466532705, 2.6077588960688445))"
      ]
     },
     "execution_count": 538,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "scale_sb(mpol), scale_sb(mbooks), scale_sb(mmov)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 539,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((13.506, -4.375), (11.237, 2.1660000000000004), (2.4030000000000005, 13.5))"
      ]
     },
     "execution_count": 539,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "swn_bias(mpol), swn_bias(mbooks), swn_bias(mmov)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 376,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((1.3533039325848222, 0.29404603131115437),\n",
       " (1.7124517742097378, 0.6149269239977002),\n",
       " (2.0845603682100773, 0.7970253718085587))"
      ]
     },
     "execution_count": 376,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "weighted_sb(mpol), weighted_sb(mbooks), weighted_sb(mmov)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 353,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.22568104 -0.4520073\n",
      "0.18597512 -0.49779502\n",
      "0.18463565 -0.42947298\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "((3.2606441378593445, 1.7218275777995586),\n",
       " (3.284544779419899, 3.9604589119553566),\n",
       " (5.282709550112486, 5.394994001835585))"
      ]
     },
     "execution_count": 353,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "norm_sb(mpol), norm_sb(mbooks), norm_sb(mmov)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 355,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((8.375, 3.25), (6.862, 6.25), (7.75, 13.125))"
      ]
     },
     "execution_count": 355,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "swn_bias(mpol), swn_bias(mbooks), swn_bias(mmov)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 240,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((21.881, 3.875), (5.84, 5.666), (6.256, 7.25))"
      ]
     },
     "execution_count": 240,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "swn_bias(mpol), swn_bias(mbooks), swn_bias(mmov)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# WEAT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [],
   "source": [
    "stereotypes_m = ['president', 'dean', \\\n",
    "                 'director', 'officer', 'manager', 'captain', 'coach', 'commissioner', 'sergeant', \"principal\", \"programmer\"]\n",
    "stereotypes_f = ['teacher', 'author', 'secretary', 'writer', \\\n",
    "                 'editor', \"nurse\", \"housekeeper\", \"maid\", \"dancer\", \"artist\", \"homemaker\"]\n",
    "male = [\"male\", \"man\", \"brother\", \"he\", \"him\", \"son\", \"father\", \"boy\", \"masculine\", \"mr\", \"john\"]\n",
    "female = [\"female\", \"woman\", \"sister\", \"she\", \"her\", \\\n",
    "          \"daughter\", \"mother\", \"girl\", \"feminine\", \"mrs\", \"mary\"]\n",
    "def cos_sim(m, x, y):\n",
    "    wv1 = m.wv.get_vector(x)\n",
    "    wv2 = m.wv.get_vector(y)\n",
    "    sim = np.dot(wv1, wv2)/(np.linalg.norm(wv1)*np.linalg.norm(wv2))\n",
    "    return sim\n",
    "def cos_sim2(wv1, wv2):\n",
    "    return np.dot(wv1, wv2)/(np.linalg.norm(wv1)*np.linalg.norm(wv2))\n",
    "def s_word(m, w, A, B):\n",
    "    return np.mean([cos_sim(m, w, a) for a in A]) - np.mean([cos_sim(m, w, b) for b in B])\n",
    "def effect_size(m, X, Y, A, B):\n",
    "    num = np.mean([s_word(m, x, A, B) for x in X]) - np.mean([s_word(m, y, A, B) for y in Y])\n",
    "    return num/np.std([s_word(m, x, A, B) for x in X + Y])\n",
    "def wbias(m, l, shuf):\n",
    "    if shuf:\n",
    "#         shuffle(stereotypes_m)\n",
    "#         shuffle(stereotypes_f)\n",
    "        shuffle(male)\n",
    "        shuffle(female)\n",
    "    return abs(effect_size(m, male[:l], female[:l], stereotypes_m, stereotypes_f))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training Word Embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getw2v(category):\n",
    "    sentences = []\n",
    "    for t in category:\n",
    "        sentences += nltk.sent_tokenize(t.text)\n",
    "    docs = [simple_preprocess(s) for s in sentences]\n",
    "    return Word2Vec(docs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "84.06553399999939"
      ]
     },
     "execution_count": 138,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "t = time.process_time()\n",
    "msports = getw2v(sports)\n",
    "time.process_time() - t"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "139.561173"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "t = time.process_time()\n",
    "mpol = getw2v(politics)\n",
    "time.process_time() - t"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "115.320582"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "t = time.process_time()\n",
    "mmov = getw2v(movies)\n",
    "time.process_time() - t"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "245.515803"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "t = time.process_time()\n",
    "mbooks = getw2v(books)\n",
    "time.process_time() - t"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def bias(g, m, vocab, wordlist):\n",
    "    b = 0\n",
    "    l = list(filter(lambda x: x in vocab, wordlist))\n",
    "    for x in l:\n",
    "        b += abs(gproj(g, m, x))\n",
    "    return b/len(l)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
